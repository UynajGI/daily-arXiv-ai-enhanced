<div id=toc></div>

# Table of Contents

- [math.ST](#math.ST) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [hep-lat](#hep-lat) [Total: 2]
- [stat.ME](#stat.ME) [Total: 13]
- [cs.SI](#cs.SI) [Total: 2]
- [math.NA](#math.NA) [Total: 11]
- [math.OC](#math.OC) [Total: 8]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 2]
- [eess.SY](#eess.SY) [Total: 14]
- [physics.ao-ph](#physics.ao-ph) [Total: 5]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 27]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 6]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1] [Adaptive thresholding for wavelet-based nonparametric heteroskedastic variance estimation on the sphere](https://arxiv.org/abs/2601.03920)
*Claudio Durastanti,Radomyra Shevchenko*

Main category: math.ST

TL;DR: 提出了一种基于needlet的小波估计方法，用于球面上异方差方差函数的非参数估计，该方法在Besov空间下达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 为了在球面回归模型中对未知平滑度的异方差方差函数进行有效估计。

Method: 采用基于needlet的多分辨率分析结合硬阈值方法进行非参数估计。

Result: 所提方法在Besov正则类下实现了最小最大最优收敛速率。

Conclusion: 该估计器能自适应地处理未知光滑性，并在理论上具有优良的收敛性能。

Abstract: This paper investigates the nonparametric estimation of a heteroskedastic variance function on the sphere in a regression framework, assuming the variance belongs to a Besov regularity class. A needlet-based estimator is proposed, combining multiresolution analysis with hard thresholding. The method exploits the spatial and spectral localization of needlets to adapt to unknown smoothness and is shown to attain minimax-optimal convergence rates over Besov spaces.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [2] [Quantum computing for multidimensional option pricing: End-to-end pipeline](https://arxiv.org/abs/2601.04049)
*Julien Hok,Álvaro Leitao*

Main category: q-fin.CP

TL;DR: 提出了一种结合市场一致风险中性密度恢复与量子加速数值积分的多资产期权定价端到端框架，使用NIG模型和高斯Copula建模边际与联合分布，并采用量子加速蒙特卡洛方法显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统多资产期权定价在高维积分上存在计算瓶颈，且需保证无套利特性，本文旨在结合量子计算优势与金融建模，提升定价效率与实践可行性。

Method: 首先利用NIG模型从欧式期权价格中校准无套利边际分布，再通过高斯Copula构建联合分布；针对高维积分问题，采用基于量子振幅估计的量子加速蒙特卡洛（QAMC）方法进行数值求解。

Result: 理论分析给出了密度估计与多维定价的精度与查询复杂度界；实证结果表明，在法兴、安盛、米其林等股票上QAMC相比经典蒙特卡洛方法在相同精度下仅需1/10至1/100的查询次数。

Conclusion: 该框架有效融合了无套利金融建模与量子计算优势，为复杂衍生品的高效定价提供了可扩展的实践路径。

Abstract: This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [3] [Phases of the $q$-deformed $\mathrm{SU}(N)$ Yang-Mills theory at large $N$](https://arxiv.org/abs/2601.03843)
*Tomoya Hayata,Yoshimasa Hidaka,Hiromasa Watanabe*

Main category: hep-lat

TL;DR: 本文研究了(2+1)维q形变SU(N)_k杨-米尔斯理论在晶格哈密顿形式下的大N相结构，发现拓扑有序相在适当参数缩放下对大N保持鲁棒性，暗示大N规范理论的连续极限比预期更复杂。


<details>
  <summary>Details</summary>
Motivation: 探索在不同参数 regime 下，如耦合常数和色数N的变化，如何影响规范理论中的禁闭和拓扑序等关键性质。

Method: 采用变分平均场分析方法，在强耦合和弱耦合之间插值，研究以't Hooft耦合λ_tH = g²N和k/N为变量的大N相结构。

Result: 发现拓扑有序相在大N极限下仍保持稳定，只要适当地缩放λ_tH和k/N；表明连续极限行为比传统预期更复杂。

Conclusion: 大N规范理论的连续极限可能具有非平凡结构，需超越平均场理论的研究来深入理解禁闭机制，并为大N规范理论的量子模拟提供指导。

Abstract: We investigate the $(2+1)$-dimensional $q$-deformed $\mathrm{SU}(N)_k$ Yang-Mills theory in the lattice Hamiltonian formalism, which is characterized by three parameters: the number of colors $N$, the coupling constant $g$, and the level $k$. By treating these as tunable parameters, we explore how key properties of the theory, such as confinement and topological order, emerge in different regimes. Employing a variational mean-field analysis that interpolates between the strong- and weak-coupling regimes, we determine the large-$N$ phase structure in terms of the 't Hooft coupling $λ_\mathrm{tH}=g^2N$ and the ratio $k/N$. We find that the topologically ordered phase remains robust at large $N$ under appropriate scalings of these parameters. This result indicates that the continuum limit of large-$N$ gauge theory may be more intricate than naively expected, and motivates studies beyond the mean-field theory, both to achieve a further understanding of confinement in gauge theories and to guide quantum simulations of large-$N$ gauge theories.

</details>


### [4] [Three-particle scattering amplitudes from lattice QCD](https://arxiv.org/abs/2601.04147)
*Stephen R. Sharpe*

Main category: hep-lat

TL;DR: 综述了利用格点QCD计算涉及三个粒子的散射振幅和共振性质的最新进展，通过积分方程求解有限体积谱得到可解析延拓到复平面以寻找共振极点的散射振幅，并展望了未来的发展与应用。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地研究多粒子相互作用系统中的散射过程和共振现象，需要从第一性原理出发的方法，格点QCD提供了非微扰计算的可能性。

Method: 基于格点QCD获得的有限体积能谱，利用积分方程方法反推出无限体积下的散射振幅，并将其解析延拓至复平面以确定共振态的位置。

Result: 成功建立了从有限体积谱提取三体散射振幅的理论框架，并可用于寻找共振极点，为理解强相互作用系统中的多体动力学提供了工具。

Conclusion: 该方法为从格点QCD数据中提取多粒子散射信息提供了可靠途径，具有良好的扩展性和广泛的应用前景。

Abstract: I review recent progress in calculating scattering amplitudes and resonance properties involving three particles using results from lattice QCD. The necessary input is the finite-volume spectrum, and the outputs -- via solutions of integral equations -- are scattering amplitudes that can be continued into the complex plane to search for resonance poles. I describe the outlook for future extensions and applications of this work.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [5] [Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation](https://arxiv.org/abs/2601.03299)
*Richik Chakraborty*

Main category: stat.ME

TL;DR: 提出一种渐进式贝叶斯置信架构，用于在个人健康数据分析中实现早期、可靠且具有不确定性量化的洞察，解决冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 用户期望在数据收集初期就获得有意义的洞察，但传统统计方法需要较长时间的数据积累，导致用户流失；现有方法要么延迟反馈，要么缺乏不确定性量化，存在误导风险。

Method: 基于贝叶斯更新和金融风险建模中的认知策略，构建一个分阶段解释后验不确定性的框架，将后验收缩映射为可解释的洞察层级，实现早期推断与认知谦逊的平衡。

Result: 在合成的N-of-1健康数据上验证，新方法可在5–7天内提供校准后的早期洞察，相比需30天以上的传统方法显著提前（平均5.3 vs 31.7天，p<0.001），同时控制假发现率低于6%，并在第90天保持76%的可信区间覆盖率。

Conclusion: 该框架在满足用户参与需求的同时维持了统计严谨性，为个性化健康分析中的早期推断提供了兼顾不确定性量化和实用性的方法路径。

Abstract: Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p<0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor.

</details>


### [6] [On estimands in target trial emulation](https://arxiv.org/abs/2601.03377)
*Edoardo Efrem Gervasoni,Liesbet De Bus,Stijn Vansteelandt,Oliver Dukes*

Main category: stat.ME

TL;DR: 本文提出了一种无需建模假设的“目标试验”分析新方法，通过明确定义估计量来提高因果推断的可解释性和稳健性，适用于纵向观察数据中多时点干预效果的评估。


<details>
  <summary>Details</summary>
Motivation: 传统目标试验框架常依赖时间恒定处理效应等强模型假设，当真实效应随时间变化时会导致解释困难，且因使用不可坍缩估计量而加剧问题，因此需要一种更清晰、稳健的分析策略。

Method: 本文围绕估计量的选择而非模型设定，提出一种无模型假设的分析策略，设计了适用于不同研究设计的估计量，并开发了相应的G-computation和逆概率加权估计方法。

Result: 在模拟数据和真实ICU抗生素降阶梯治疗数据上的应用表明，所提方法相比传统技术具有更高的清晰度和可靠性。

Conclusion: 通过聚焦估计量定义，该方法能在模型误设情况下仍保持处理效应的明确解释性，提升了纵向观察数据中因果推断的透明度与实用性。

Abstract: The target trial framework enables causal inference from longitudinal observational data by emulating randomized trials initiated at multiple time points. Precision is often improved by pooling information across trials, with standard models typically assuming - among other things - a time-constant treatment effect. However, this obscures interpretation when the true treatment effect varies, which we argue to be likely as a result of relying on noncollapsible estimands. To address these challenges, this paper introduces a model-free strategy for target trial analysis, centered around the choice of the estimand, rather than model specification. This ensures that treatment effects remain clearly interpretable for well-defined populations even under model misspecification. We propose estimands suitable for different study designs, and develop accompanying G-computation and inverse probability weighted estimators. Applications on simulations and real data on antimicrobial de-escalation in an intensive care unit setting demonstrate the greater clarity and reliability of the proposed methodology over traditional techniques.

</details>


### [7] [Measures of classification bias derived from sample size analysis](https://arxiv.org/abs/2601.03453)
*Ioannis Ivrissimtzis,Shauna Concannon,Matthew Houliston,Graham Roberts*

Main category: stat.ME

TL;DR: 提出一种基于显著性差异样本量的算法分类偏差度量方法，通过卡方检验近似公式验证其合理性，并比较其与误差率差值和比值的优势。


<details>
  <summary>Details</summary>
Motivation: 现有分类算法偏差度量方法（如误差率差值或比值）可能无法准确反映统计显著性与样本量之间的关系，需提出更合理的度量准则。

Method: 基于卡方检验的样本量近似公式，提出一种新的偏差度量方法：偏差程度与检测到差异所需的样本量成反比；在两类人群和非参数误差估计假设下进行分析，并对比差值e2-e1和比值e2/e1两种常用指标。

Result: 新度量方法具有期望的基本性质，且与传统指标存在本质不同，可能导致不同的算法偏见排序；部分优良性质源于方法本身的根本特征，可推广至多于两类人群的复杂场景。

Conclusion: 所提出的基于检测所需样本量的偏差度量更具直观性和统计合理性，优于传统的误差率差或比值方法，适用于评估分类算法的公平性。

Abstract: We propose the use of a simple intuitive principle for measuring algorithmic classification bias: the significance of the differences in a classifier's error rates across the various demographics is inversely commensurate with the sample size required to statistically detect them. That is, if large sample sizes are required to statistically establish biased behavior, the algorithm is less biased, and vice versa. In a simple setting, we assume two distinct demographics, and non-parametric estimates of the error rates on them, e1 and e2, respectively. We use a well-known approximate formula for the sample size of the chi-squared test, and verify some basic desirable properties of the proposed measure. Next, we compare the proposed measure with two other commonly used statistics, the difference e2-e1 and the ratio e2/e1 of the error rates. We establish that the proposed measure is essentially different in that it can rank algorithms for bias differently, and we discuss some of its advantages over the other two measures. Finally, we briefly discuss how some of the desirable properties of the proposed measure emanate from fundamental characteristics of the method, rather than the approximate sample size formula we used, and thus, are expected to hold in more complex settings with more than two demographics.

</details>


### [8] [Improving operating characteristics of clinical trials by augmenting control arm using propensity score-weighted borrowing-by-parts power prior](https://arxiv.org/abs/2601.03480)
*Apu Chandra Das,Sakib Salam,Aninda Roy,Rakhi Chowdhury,Antar Chandra Das,Ashim Chandra Das*

Main category: stat.ME

TL;DR: 提出了一种结合倾向评分加权与分部借力先验的贝叶斯方法（PSW-BPP），用于在存在协变量不平衡和结果异质性时，稳健地整合外部数据以提高估计效率。


<details>
  <summary>Details</summary>
Motivation: 在利用外部数据提升估计效率的同时，避免因人群间协变量分布或结果变异性差异导致的偏倚，需在数据间实现合理平衡。

Method: 采用倾向评分加权对齐外部数据与当前研究的协变量分布，减少混杂；随后通过分部借力先验（borrowing-by-parts power prior）将加权后的外部似然纳入贝叶斯模型，分别设定均值和方差成分的幂参数，并结合最小合理性指数（mPI）确定参数，实现差异化信息借用。

Result: 模拟研究表明，相较于不借用或固定借用方法，PSW-BPP在中等协变量不平衡和结果异质性下具有更高的估计效率和稳定性；实例分析验证了其实际应用价值。

Conclusion: PSW-BPP为观察性及混合研究设计中的贝叶斯外部数据整合提供了原理性强、可扩展且稳健的方法框架。

Abstract: Borrowing external data can improve estimation efficiency but may introduce bias when populations differ in covariate distributions or outcome variability. A proper balance needs to be maintained between the two datasets to justify the borrowing. We propose a propensity score weighting borrowing-by-parts power prior (PSW-BPP) that integrates causal covariate adjustment through propensity score weighting with a flexible Bayesian borrowing approach to address these challenges in a unified framework. The proposed approach first applies propensity score weighting to align the covariate distribution of the external data with that of the current study, thereby targeting a common estimand and reducing confounding due to population heterogeneity. The weighted external likelihood is then incorporated into a Bayesian model through a borrowing-by-parts power prior, which allows distinct power parameters for the mean and variance components of the likelihood, enabling differential and calibrated information borrowing. Additionally, we adopt the idea of the minimal plausibility index (mPI) to calculate the power parameters. This separate borrowing provides greater robustness to prior-data conflict compared with traditional power prior methods that impose a single borrowing parameter. We study the operating characteristics of PSW-BPP through extensive simulation and a real data example. Simulation studies demonstrate that PSW-BPP yields more efficient and stable estimation than no borrowing and fixed borrowing, particularly under moderate covariate imbalance and outcome heterogeneity. The proposed framework offers a principled and extensible methodological contribution for Bayesian inference with external data in observational and hybrid study designs.

</details>


### [9] [Differentially Private Bayesian Inference for Gaussian Copula Correlations](https://arxiv.org/abs/2601.03497)
*Shuo Wang,Joseph Feldman,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 提出了一种满足差分隐私的高斯藤关联估计方法，通过将数据转换为基于中位数的二值计数表并加入噪声，利用后验分布或最大似然法估计相关性。


<details>
  <summary>Details</summary>
Motivation: 在保护数据隐私的前提下准确估计多变量间的依赖关系，解决现有方法在差分隐私下估计高斯藤关联的不足。

Method: 将原始数据转化为基于边际中位数的二维计数表，添加噪声以满足差分隐私，利用复合似然函数对真实计数进行边缘化，进而推断藤关联参数的后验分布；同时提供最大似然估计方法用于点估计。

Result: 通过模拟实验表明，所提方法在隐私保护条件下相比已有方法能更准确地估计高斯藤相关性。

Conclusion: 该方法在保证差分隐私的同时，能够有效估计高斯藤模型中的相关性结构，具有良好的统计性能和应用潜力。

Abstract: Gaussian copulas are widely used to estimate multivariate distributions and relationships. We present algorithms for estimating Gaussian copula correlations that ensure differential privacy. We first convert data values into sets of two-way tables of counts above and below marginal medians. We then add noise to these counts to satisfy differential privacy. We utilize the one-to-one correspondence between the true counts and the copula correlation to estimate a posterior distribution of the copula correlation given the noisy counts, marginalizing over the distribution of the underlying true counts using a composite likelihood. We also present an alternative, maximum likelihood approach for point estimation. Using simulation studies, we compare these methods to extant methods in the literature for computing differentially private copula correlations.

</details>


### [10] [Small area estimation of dependent extreme value indices](https://arxiv.org/abs/2601.03647)
*Koki Momoki,Takuma Yoshida*

Main category: stat.ME

TL;DR: 提出了一种基于混合效应模型的极值指数（EVI）预测方法，用于分析来自多个子群体或区域的重尾数据，通过引入相关随机效应捕捉区域间关系，实现了所有区域EVI的同时预测，并应用于日本强降雨风险评估。


<details>
  <summary>Details</summary>
Motivation: 针对多区域重尾数据，传统方法难以有效利用区域间信息进行极值指数（EVI）预测，因此需要一种能整合区域关联性的建模方法以提高预测效率和准确性。

Method: 提出一个混合效应模型，将不同区域的EVI差异表示为称为随机效应的潜在变量，并通过设定跨区域相关的随机效应来融入区域间的关系，从而实现所有区域EVI的同时预测；同时研究了模型参数估计与随机效应预测的方法及其理论性质。

Result: 数值实验表明所提方法在EVI预测上优于对比方法，能够更准确地捕捉尾部特征；应用于日本各地的强降雨数据分析时，成功评估了各地区的极端降水风险。

Conclusion: 所提出的混合效应模型能有效整合多区域间的相关信息，提升重尾数据下极值指数的预测性能，具有良好的理论支持与实际应用价值，尤其适用于小区域估计问题。

Abstract: In extreme value analysis, tail behavior of a heavy-tailed data distribution is modeled by a Pareto-type distribution in which the so-called extreme value index (EVI) controls the tail behavior. For heavy-tailed data obtained from multiple population subgroups, or areas, this study efficiently predicts the EVIs of all areas using information among areas. For this purpose, we propose a mixed effects model, which is a useful approach in small area estimation. In this model, we represent differences among areas in the EVIs by latent variables called random effects. Using correlated random effects across areas, we incorporate the relations among areas into the model. The obtained model achieves simultaneous prediction of EVIs of all areas. Herein, we describe parameter estimation and random effect prediction in the model, and clarify theoretical properties of the estimator. Additionally, numerical experiments are presented to demonstrate the effectiveness of the proposed method. As an application of our model, we provide a risk assessment of heavy rainfall in Japan.

</details>


### [11] [Multi-transport Distributional Regression](https://arxiv.org/abs/2601.03674)
*Yuanying Chen,Tongyu Li,Yang Bai,Zhenhua Lin*

Main category: stat.ME

TL;DR: 提出了一种基于Wasserstein空间中加权Fréchet均值的内在回归框架，用于处理分布对多个分布预测变量的回归问题，具有良好的可解释性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 在Wasserstein空间的非线性几何结构下，现有方法难以有效处理多个分布型预测变量对响应分布的影响，需要一种可识别、稳健且具解释性的回归模型。

Method: 通过在Wasserstein空间中对各预测变量对应的传输分布进行加权Fréchet均值聚合，构建具有不变性、可解释权重和灵活回归算子的内在回归模型。

Result: 理论方面建立了回归算子的可识别性，并在预测Wasserstein半范数下获得了估计的渐近保证；实验表明该方法在预测性能和可解释性上优于现有Wasserstein回归方法。

Conclusion: 所提出的回归框架能有效处理多分布预测变量下的分布-分布回归问题，兼具理论保证与实际优越性。

Abstract: We study distribution-on-distribution regression problems in which a response distribution depends on multiple distributional predictors. Such settings arise naturally in applications where the outcome distribution is driven by several heterogeneous distributional sources, yet remain challenging due to the nonlinear geometry of the Wasserstein space. We propose an intrinsic regression framework that aggregates predictor-specific transported distributions through a weighted Fréchet mean in the Wasserstein space. The resulting model admits multiple distributional predictors, assigns interpretable weights quantifying their relative contributions, and defines a flexible regression operator that is invariant to auxiliary construction choices, such as the selection of a reference distribution. From a theoretical perspective, we establish identifiability of the induced regression operator and derive asymptotic guarantees for its estimation under a predictive Wasserstein semi-norm, which directly characterizes convergence of the composite prediction map. Extensive simulation studies and a real data application demonstrate the improved predictive performance and interpretability of the proposed approach compared with existing Wasserstein regression methods.

</details>


### [12] [Maximum smoothed likelihood method for the combination of multiple diagnostic tests, with application to the ROC estimation](https://arxiv.org/abs/2601.03675)
*Fangyong Zheng,Pengfei Li,Tao Yu*

Main category: stat.ME

TL;DR: 本文提出了一种灵活的半参数模型，通过未知单调变换的生物标志物线性组合来建模疾病与健康人群的密度比，并引入平滑似然框架以提高估计效率，能够更准确地估计ROC曲线和AUC，在模拟和实际数据中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于指数倾斜或密度比模型的方法在实际应用中可能因假设过强而受限，需要更灵活且稳健的方法来提升多生物标志物联合诊断的分类准确性。

Method: 采用一种半参数模型，将患病与健康人群的密度比表示为生物标志物线性组合的未知单调变换形式；提出平滑似然框架，利用密度函数和平滑变换的光滑性，构建最大平滑似然估计方法，并设计有效算法进行求解，同时推导估计量的渐近性质。

Result: 所提方法在估计密度函数、模型参数以及ROC曲线和AUC方面表现出更高的准确性和效率；模拟研究和真实数据分析均显示其优于现有方法。

Conclusion: 该方法提供了一种更为灵活和高效的多生物标志物整合分析工具，具有良好的理论性质和实际应用前景。

Abstract: In medical diagnostics, leveraging multiple biomarkers can significantly improve classification accuracy compared to using a single biomarker. While existing methods based on exponential tilting or density ratio models have shown promise, their assumptions may be overly restrictive in practice. In this paper, we adopt a flexible semiparametric model that relates the density ratio of diseased to healthy subjects through an unknown monotone transformation of a linear combination of biomarkers. To enhance estimation efficiency, we propose a smoothed likelihood framework that exploits the smoothness in the underlying densities and transformation function. Building on the maximum smoothed likelihood methodology, we construct estimators for the model parameters and the associated probability density functions. We develop an effective computational algorithm for implementation, derive asymptotic properties of the proposed estimators, and establish procedures for estimating the receiver operating characteristic (ROC) curve and the area under the curve (AUC). Through simulation studies and a real-data application, we demonstrate that the proposed method yields more accurate and efficient estimates than existing approaches.

</details>


### [13] [High-Dimensional Precision Matrix Quadratic Forms: Estimation Framework for $p > n$](https://arxiv.org/abs/2601.03815)
*Shizhe Hong,Weiming Li,Guangming Pan*

Main category: stat.ME

TL;DR: 提出了一种在高维情况下对精度矩阵的二次泛函进行估计的新框架，尤其适用于特征维度p超过样本量n的情形。


<details>
  <summary>Details</summary>
Motivation: 传统基于矩的估计方法在p>n时失效，由于秩不足和高维复杂性导致无法一致估计，因此需要一种能在p>n情况下仍保持一致性的新方法。

Method: 结合谱矩表示与约束优化，构建了一种新的估计框架，在较弱的矩条件下实现一致估计。

Result: 该方法在p>n的情况下仍能有效工作，通过模拟研究验证了其能够克服传统方法在高维情形下的失效问题，并在投资组合优化和回归分析中展示了良好的应用效果。

Conclusion: 所提出的估计框架为高维统计推断提供了一个统一且稳健的解决方案，能够在p>n的挑战性场景下实现对精度矩阵二次泛函的一致估计。

Abstract: We propose a novel estimation framework for quadratic functionals of precision matrices in high-dimensional settings, particularly in regimes where the feature dimension $p$ exceeds the sample size $n$. Traditional moment-based estimators with bias correction remain consistent when $p<n$ (i.e., $p/n \to c <1$). However, they break down entirely once $p>n$, highlighting a fundamental distinction between the two regimes due to rank deficiency and high-dimensional complexity. Our approach resolves these issues by combining a spectral-moment representation with constrained optimization, resulting in consistent estimation under mild moment conditions.
  The proposed framework provides a unified approach for inference on a broad class of high-dimensional statistical measures. We illustrate its utility through two representative examples: the optimal Sharpe ratio in portfolio optimization and the multiple correlation coefficient in regression analysis. Simulation studies demonstrate that the proposed estimator effectively overcomes the fundamental $p>n$ barrier where conventional methods fail.

</details>


### [14] [Asymptotic distribution of the likelihood ratio test statistic with inequality-constrained nuisance parameters](https://arxiv.org/abs/2601.03909)
*Clara Bertinelli Salucci*

Main category: stat.ME

TL;DR: 本文研究了在参数边界和干扰参数同时位于边界时，似然比检验统计量的渐近分布问题，提出了基于锥几何变化的一般性刻画方法，并通过内在体积的秩聚合给出了混合权重的精确近似。


<details>
  <summary>Details</summary>
Motivation: 当感兴趣参数和干扰参数均位于边界时，现有理论对似然比统计量渐近分布的描述仍不完整，本文旨在填补这一理论空白。

Method: 分析从K个边界上的感兴趣参数到K-m个感兴趣参数和m个干扰参数时锥几何的变化，引入基于内在体积的秩聚合方法，在正交情形下推导出闭式差分模式，并推广至一般协方差结构。

Result: 得到了任意数量边界参数与干扰参数情形下似然比统计量渐近分布的一般性刻画；发现正交情况下权重变化具有闭式差分模式；提出的新秩聚合方法能准确逼近混合权重，模拟验证了其精度。

Conclusion: 本文统一并扩展了边界参数情形下的似然比检验理论，为含有多个边界约束的实际问题提供了坚实的理论基础和实用的近似方法。

Abstract: The asymptotic distribution of the likelihood-ratio statistic for testing parameters on the boundary is well known to be a chi-squared mixture. The mixture weights have been shown to correspond to the intrinsic volumes of an associated tangent cone, unifying a wide range of previously isolated special cases. While the weights are fully understood for an arbitrary number of parameters of interest on the boundary, much less is known when nuisance parameters are also constrained to the boundary, a situation that frequently arises in applications. We provide the first general characterization of the asymptotic distribution of the likelihood-ratio test statistic when both the number of parameters of interest and the number of nuisance parameters on the boundary are arbitrary. We analyze how the cone geometry changes when moving from a problem with K parameters of interest on the boundary to one with K-m parameters of interest and m nuisances. In the orthogonal case we show that the resulting change in the chi-bar weights admits a closed-form difference pattern that redistributes probability mass across adjacent degrees of freedom, and that this pattern remains the dominant component of the weight shift under arbitrary covariance structures when the nuisance vector is one-dimensional. For a generic number of nuisance parameters, we introduce a new rank-based aggregation of intrinsic volumes that yields an accurate approximation of the mixture weights. Comprehensive simulations support the theory and demonstrate the accuracy of the proposed approximation.

</details>


### [15] [Online robust covariance matrix estimation and outlier detection](https://arxiv.org/abs/2601.03957)
*Paul Guillot,Antoine Godichon-Baggioni,Stéphane Robin,Laure Sansonnet*

Main category: stat.ME

TL;DR: 本文提出了一种新的在线方法，用于同时估计几何中位数和方差，以实现实时的协方差矩阵估计和异常值检测。


<details>
  <summary>Details</summary>
Motivation: 随着数据集中污染观测值比例的增加，协方差矩阵的鲁棒估计和异常值检测仍然是统计数据分析中的主要挑战。现有方法缺乏在线设置下的实现。

Method: 使用几何中位数进行位置的鲁棒估计，使用中位数协方差矩阵进行离散参数的鲁棒估计，并在线计算每个新到达数据点的马氏距离以判断是否为异常值。

Result: 所提方法能够在模拟数据集上有效识别异常值，并缓解掩蔽效应。

Conclusion: 该方法支持实时异常值检测和鲁棒参数估计，适用于顺序观测数据的场景。

Abstract: Robust estimation of the covariance matrix and detection of outliers remain major challenges in statistical data analysis, particularly when the proportion of contaminated observations increases with the size of the dataset. Outliers can severely bias parameter estimates and induce a masking effect, whereby some outliers conceal the presence of other outliers, further complicating their detection. Although many approaches have been proposed for covariance estimation and outlier detection, to our knowledge, none of these methods have been implemented in an online setting. In this paper, we focus on online covariance matrix estimation and outlier detection. Specifically, we propose a new method for simultaneously and online estimating the geometric median and variance, which allows us to calculate the Mahalanobis distance for each incoming data point before deciding whether it should be considered an outlier. To mitigate the masking effect, robust estimation techniques for the mean and variance are required. Our approach uses the geometric median for robust estimation of the location and the median covariance matrix for robust estimation of the dispersion parameters. The new online methods proposed for parameter estimation and outlier detection allow real-time identification of outliers as data are observed sequentially. The performance of our methods is demonstrated on simulated datasets.

</details>


### [16] [On the estimation of inclusion probabilities for weighted analyses of nested case control studies](https://arxiv.org/abs/2601.04066)
*Tomeu López-Nieto-Veitch,Rossella De Sabbata,Ryung Kim,Sven Ove Samuelsen,Nathalie C. Støer,Vivian Viallon*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Nested case-control (NCC) studies are a widely adopted design in epidemiology to investigate exposure-disease relationships. This paper examines weighted analyses in NCC studies, focusing on two prominent weighting methods: Kaplan-Meier (KM) weights and Generalized Additive Model (GAM) weights. We consider three target estimands: log-hazard ratios, conditional survival, and associations between exposures. While KM- and GAM-weights are generally robust, we identify specific scenarios where they can lead to biased estimates. We demonstrate that KM-weights can lead to biased estimates when a proportion of the originating cohort is effectively ineligible for NCC selection, particularly with small case proportions or numerous matching factors. Instead, GAM-weights can yield biased results if interactions between matching factors influence disease risk and are not adequately incorporated into weight calculation. Using Directed Acyclic Graphs (DAGs), we develop a framework to systematically determine which variables should be included in weight calculations. We show that the optimal set of variables depends on the target estimand and the causal relationships between matching factors, exposures, and disease risk. We illustrate our findings with both synthetic and real data from the European Prospective Investigation into Cancer and nutrition (EPIC) study. Additionally, we extend the application of GAM-weights to "untypical" NCC studies, where only a subset of cases are included. Our work provides crucial insights for conducting accurate and robust weighted analyses in NCC studies.

</details>


### [17] [Prediction Intervals for Interim Events in Randomized Clinical Trials with Time-to-Event Endpoints](https://arxiv.org/abs/2601.04192)
*Edoardo Ratti,Federico L. Perlino,Stefania Galimberti,Maria G. Valsecchi*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time-to-event endpoints are central to evaluate treatment efficacy across many disease areas. Many trial protocols include interim analyses within group-sequential designs that control type I error via spending functions or boundary methods. The corresponding operating characteristics depend on the number of looks and the information accrued. Planning interim analyses with time-to-event endpoints is challenging because statistical information depends on the number of observed events. Ensuring adequate follow-up to accrue the required events is therefore critical, making interim prediction of information at scheduled looks and at the final analysis essential. While several methods have been developed to predict the calendar time required to reach a target number of events, to the best of our knowledge there is no established framework that addresses the prediction of the number of events at a future date with corresponding prediction intervals. Starting from an prediction interval approach originally developed in reliability engineering for the number of future component failures, we reformulated and extended it to the context of interim monitoring in clinical trials. This adaptation yields a general framework for event-count prediction intervals in the clinical setting, taking the patient as the unit of analysis and accommodating a range of parametric survival models, patient-level covariates, stagged entry and possible dependence between entry dates and lost to follow-up. Prediction intervals are obtained in a frequentist framework from a bootstrap estimator of the conditional distribution of future events. The performance of the proposed approach is investigated via simulation studies and illustrated by analyzing a real-world phase III trial in childhood acute lymphoblastic leukaemia.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [18] [From Risk Perception to Behavior Large Language Models-Based Simulation of Pandemic Prevention Behaviors](https://arxiv.org/abs/2601.03552)
*Lujia Bo,Mingxuan Chen,Youduo Chen,Xiaofan Gui,Jiang Bian,Chunyan Wang,Yi Liu*

Main category: cs.SI

TL;DR: 本文提出了一种基于大语言模型（LLM）的预防行为模拟框架，用于预测和模拟传染病暴发期间个体预防行为的演变，该框架在不同数据可用性条件下均表现出良好的预测性能，并揭示了政策放松背景下特定低成本行为的持续增强趋势。


<details>
  <summary>Details</summary>
Motivation: 在新型传染病暴发初期，个体预防行为至关重要，但其采纳程度具有异质性且难以预测，尤其是在实证数据稀缺和政策环境快速变化的情况下。因此，需要一种能够动态模拟行为变化的新型方法。

Method: 开发了一个基于大语言模型的两模块模拟框架：静态模块用于在给定外部情境下预测行为强度，动态模块则随时间更新居民感知风险并驱动行为演化；通过第一人称结构化提示工程实现，并在北京市居民两轮调查数据上进行零样本、少样本和跨情境迁移评估。

Result: 该框架在Kolmogorov-Smirnov检验下表现稳健，预测准确率从零样本的72.7%提升至少样本的81.8%，跨情境迁移仍保持77.8%的高准确率；模拟结果显示2022年12月中国防疫政策放松后整体行为松懈，但与排水相关的消毒行为反而上升。

Conclusion: 基于LLM的行为模拟框架能够在数据有限的情况下有效预测和解释公众预防行为的动态变化，尤其适用于快速演变的公共卫生情境，并揭示低摩擦成本行为可能在政策放松后持续存在的潜在环境影响。

Abstract: Individual prevention behaviors are a primary line of defense during the early stages of novel infectious disease outbreaks, yet their adoption is heterogeneous and difficult to forecast-especially when empirical data are scarce and epidemic-policy contexts evolve rapidly. To address this gap, we develop an LLM-based prevention-behavior simulation framework that couples (i) a static module for behavior-intensity prediction under a specified external context and (ii) a dynamic module that updates residents' perceived risk over time and propagates these updates into behavior evolution. The model is implemented via structured prompt engineering in a first-person perspective and is evaluated against two rounds of survey data from Beijing residents (R1: December 2020; R2: August 2021) under progressively realistic data-availability settings: zero-shot, few-shot, and cross-context transfer. Using Kolmogorov-Smirnov tests to compare simulated and observed behavior distributions (p > 0.001 as the validity criterion), the framework demonstrates robust performance and improves with limited reference examples; reported predictive accuracy increases from 72.7% (zero-shot) to 81.8% (few-shot), and remains high at 77.8% under transfer to novel contexts. We further apply the framework to simulate behavior changes during China's December 2022 policy relaxation and to stress-test behavioral responses across 120 systematically varied epidemic conditions (R0, CFR, and control-measure tiers). Results indicate broad behavioral loosening under relaxation but a distinctive counter-trend increase in drain-related disinfection, highlighting how low-cost, low-friction behaviors may persist or intensify even when external constraints recede-raising a potential environmental tradeoff.

</details>


### [19] [Celebrity messages reduce online hate and limit its spread](https://arxiv.org/abs/2601.04134)
*Eaman Jahani,Blas Kolic,Manuel Tonneau,Hause Lin,Daniel Barkoczi,Edwin Ikhuoria,Victor Orozco,Samuel Fraiberger*

Main category: cs.SI

TL;DR: 一项在X平台尼日利亚进行的大规模随机对照试验表明，通过尼日利亚名人发送亲社会视频信息，可有效减少2.5%至5.5%的种族仇恨内容，且效果在四个月内持续，同时显著降低仇恨内容的传播。


<details>
  <summary>Details</summary>
Motivation: 在线仇恨言论传播迅速，但目前尚缺乏可预防且可扩展的有效策略。本研究旨在探索是否可通过非内容删除的方式，大规模干预并减少仇恨言论。

Method: 研究在X平台尼日利亚开展为期20周的信息干预实验，针对曾参与仇恨言论的73,136名用户，随机分配接收尼日利亚名人发布的亲社会视频信息，并评估其对仇恨内容发布和传播的影响。

Result: 干预期间仇恨内容减少2.5%至5.5%，约75%的效果在后续四个月内持续；用户受众接触比例越高，其仇恨帖子的转发量下降越明显，最高减少超过50%。

Conclusion: 可扩展的信息干预手段可在不删除内容的前提下有效抑制在线仇恨言论，具有实际应用和推广价值。

Abstract: Online hate spreads rapidly, yet little is known about whether preventive and scalable strategies can curb it. We conducted the largest randomized controlled trial of hate speech prevention to date: a 20-week messaging campaign on X in Nigeria targeting ethnic hate. 73,136 users who had previously engaged with hate speech were randomly assigned to receive prosocial video messages from Nigerian celebrities. The campaign reduced hate content by 2.5% to 5.5% during treatment, with about 75% of the reduction persisting over the following four months. Reaching a larger share of a user's audience reduced amplification of that user's hate posts among both treated and untreated users, cutting hate reposts by over 50% for the most exposed accounts. Scalable messaging can limit online hate without removing content.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [20] [Local Interpolation via Low-Rank Tensor Trains](https://arxiv.org/abs/2601.03885)
*Siddhartha E. Guzman,Egor Tiunov,Leandro Aolita*

Main category: math.NA

TL;DR: 提出了一种低秩张量列车（TT）插值框架，能够在保持误差界的同时高效地将粗网格上的函数表示扩展到细网格，并实现指数级压缩和对数复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的TT分解方法在处理高维数据时难以有效恢复低秩结构，且计算成本高昂，缺乏适用于复杂几何和多尺度问题的可扩展求解器。

Method: 提出一种低秩TT插值方法，利用已有粗网格上的TT表示，构造细网格上的高分辨率TT表示，其中新增的m个核心保持恒定秩，保证ℓ²误差界，并实现与总核心数无关的精度控制。

Result: 方法实现了固定精度下的指数压缩和关于网格点数量的对数复杂度，成功应用于1D、2D、3D场景如图像超分辨率、气动翼型掩模嵌入和3D合成湍流，能以对数复杂度直接生成TT格式的分形噪声场。

Conclusion: 该工作为面向复杂几何和多尺度生成模型的可扩展TT-native求解器提供了新路径，在科学计算、成像和实时图形学中具有广泛应用前景。

Abstract: Tensor Train (TT) decompositions provide a powerful framework to compress grid-structured data, such as sampled function values, on regular Cartesian grids. Such high compression, in turn, enables efficient high-dimensional computations. Exact TT representations are only available for simple analytic functions. Furthermore, global polynomial or Fourier expansions typically yield TT-ranks that grow proportionally with the number of basis terms. State-of-the-art methods are often prohibitively expensive or fail to recover the underlying low-rank structure. We propose a low-rank TT interpolation framework that, given a TT describing a discrete (scalar-, vector-, or tensor-valued) function on a coarse regular grid with $n$ cores, constructs a finer-scale version of the same function represented by a TT with $n+m$ cores, where the last $m$ cores maintain constant rank. Our method guarantees a $\ell^{2}$-norm error bound independent of the total number of cores, achieves exponential compression at fixed accuracy, and admits logarithmic complexity with respect of the number of grid points. We validate its performance through numerical experiments, including 1D, 2D, and 3D applications such as: 2D and 3D airfoil mask embeddings, image super-resolution, and synthetic noise fields such as 3D synthetic turbulence. In particular, we generate fractal noise fields directly in TT format with logarithmic complexity and memory. This work opens a path to scalable TT-native solvers with complex geometries and multiscale generative models, with implications from scientific simulation to imaging and real-time graphics.

</details>


### [21] [Constrained dynamics for searching saddle points on general Riemannian manifolds](https://arxiv.org/abs/2601.03931)
*Yukuan Hu,Laura Grazioli*

Main category: math.NA

TL;DR: 本文提出了一种适用于一般黎曼流形上光滑函数的约束鞍点动力学方法，基于Grassmann丛几何并通过引入第二基本形式实现了普适性，首次为流形上的离散鞍点搜索算法提供了收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于具有全局正则水平集表示的特殊流形，无法应用于如电子激发态计算等问题，因此需要发展适用于一般黎曼流形的约束鞍点求解方法。

Method: 在切丛的Grassmann丛上紧凑地构建动力学系统，利用Grassmann丛几何结构并结合第二基本形式来刻画沿轨迹变化的切空间，尊重内在商结构以去除对黎曼Hessian特征值非退化性的不必要假设。

Result: 建立了动力学系统的局部线性稳定性与算法的局部线性收敛性，首次为流形情形下的离散鞍点搜索算法提供了收敛保证，并通过线性特征值问题和电子激发态计算验证了算法有效性。

Conclusion: 所提方法具有普遍适用性，能处理更广泛的物理化学能量景观分析问题，且理论分析揭示了鞍点定位比极小值寻找更具病态性，需采用非冗余参数化策略。

Abstract: Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory.

</details>


### [22] [On the importance of smoothness, interface resolution and numerical sensitivities in shape and topological sensitivity analysis](https://arxiv.org/abs/2601.03967)
*M. H. Gfrerer,P. Gangl*

Main category: math.NA

TL;DR: 本文研究了偏微分方程约束的离散化对形状和拓扑导数的影响，采用标准方法与增强方法对比，发现只有增强方法在拓扑导数上收敛，且形状导数的正则性依赖于基函数的正则性。


<details>
  <summary>Details</summary>
Motivation: 探讨不同离散化方法对形状和拓扑导数计算的影响，特别是界面位置处理的重要性。

Method: 使用跟踪型泛函和一维双材料泊松问题，比较标准样条离散方法与引入界面位置信息的增强离散方法。

Result: 形状导数的正则性取决于基函数的正则性；点态收敛要求在试探空间中考虑界面；拓扑导数仅在增强方法下收敛。

Conclusion: 为准确计算形状和拓扑导数，特别是在存在材料界面的情况下，应采用能捕捉界面位置的离散化方法。

Abstract: In this paper we investigate the influence of the discretization of PDE constraints on shape and topological derivatives. To this end, we study a tracking-type functional and a two-material Poisson problem in one spatial dimension. We consider the discretization by a standard method and an enriched method. In the standard method we use splines of degree $p$ such that we can control the smoothness of the basis functions easily, but do not take any interface location into consideration. This includes for p=1 the usual hat basis functions. In the enriched method we additionally capture the interface locations in the ansatz space by enrichment functions. For both discretization methods shape and topological sensitivity analysis is performed. It turns out that the regularity of the shape derivative depends on the regularity of the basis functions. Furthermore, for point-wise convergence of the shape derivative the interface has to be considered in the ansatz space. For the topological derivative we show that only the enriched method converges.

</details>


### [23] [Posterior error bounds for prior-driven balancing in linear Gaussian inverse problems](https://arxiv.org/abs/2601.03971)
*Josie König,Han Cheng Lie*

Main category: math.NA

TL;DR: 本文研究了在贝叶斯反问题中使用近似前向模型时的误差控制，特别针对线性动力系统初始条件推断问题，利用平衡截断方法建立了系统理论与反问题之间的新联系，并提出了基于Hankel奇异值的先验误差界。


<details>
  <summary>Details</summary>
Motivation: 在大规模贝叶斯反问题中，为降低计算成本常需使用近似前向模型，但需控制其逼近质量。本文旨在建立线性近似模型对后验均值和协方差影响的误差界。

Method: 采用逆算子的扰动理论来界定近似后验均值和协方差的误差；针对线性时不变系统的平滑问题，结合基于平衡截断的降阶方法，揭示了先验驱动系统的脉冲响应与先验预条件化Hessian矩阵之间的关系。

Result: 建立了首个将系统理论降阶方法应用于平滑问题的先验误差界，误差界由底层系统的截断Hankel奇异值控制。

Conclusion: 通过系统理论与反问题的交叉分析，实现了对降阶模型在贝叶斯平滑问题中近似误差的有效控制，为高效求解提供了理论支持。

Abstract: In large-scale Bayesian inverse problems, it is often necessary to apply approximate forward models to reduce the cost of forward model evaluations, while controlling approximation quality. In the context of Bayesian inverse problems with linear forward models, Gaussian priors, and Gaussian noise, we use perturbation theory for inverses to bound the error in the approximate posterior mean and posterior covariance resulting from a linear approximate forward model. We then focus on the smoothing problem of inferring the initial condition of linear time-invariant dynamical systems, using finitely many partial state observations. For such problems, and for a specific model order reduction method based on balanced truncation, we show that the impulse response of a certain prior-driven system is closely related to the prior-preconditioned Hessian of the inverse problem. This reveals a novel connection between systems theory and inverse problems. We exploit this connection to prove the first a priori error bounds for system-theoretic model order reduction methods applied to smoothing problems. The bounds control the approximation error of the posterior mean and covariance in terms of the truncated Hankel singular values of the underlying system.

</details>


### [24] [A Bivariate Spline Construction of Orthonormal Polynomials over Polygonal Domains and Its Applications to Quadrature](https://arxiv.org/abs/2601.04022)
*Ming-Jun Lai*

Main category: math.NA

TL;DR: 本文提出了在任意多边形域上构造正交/标准正交多项式的方法，利用双变量样条函数和成熟的MATLAB实现，开发了两种算法，并通过数值例子展示了多项式的结构与零曲线，还提出了基于奇偶次多项式约简的新积分方案。


<details>
  <summary>Details</summary>
Motivation: 为了在任意多边形域上高效、精确地构造正交多项式并发展相应的高精度积分方法，克服传统方法在复杂几何区域上的局限性。

Method: 使用双变量样条函数，在任意三角剖分下构建多项式空间；提出两种算法：一种构造给定次数的正交多项式，另一种在其正交补空间中构造更高次的正交多项式；结合多项式约简与插值技术设计新的求积规则。

Result: 实现了对1到5次多项式的数值构造，展示了其零曲线分布；发现中心对称区域上可能不存在高斯求积；提出了有效的多项式约简策略和高精度求积方案。

Conclusion: 所提方法能有效构造任意多边形域上的正交多项式，为复杂区域上的函数逼近与数值积分提供了新工具，具有良好的应用前景。

Abstract: We present computational methods for constructing orthogonal/orthonormal polynomials over arbitrary polygonal domains in $\mathbb{R}^2$
  using bivariate spline functions. Leveraging a mature MATLAB implementation which generates spline spaces of any degree, any smoothness over any triangulation, we have exact polynomial representation over the polygonal domain of interest. Two algorithms are developed: one constructs orthonormal polynomials of degree $d>0$
  over a polygonal domain, and the other constructs orthonormal polynomials of degree $d+1$ in the orthogonal complement of $\mathbb{P}_d$. Numerical examples for degrees $d=1--5$ illustrate the structure and zero curves of these polynomials, providing evidence against the existence of Gauss quadrature on centrally symmetric domains. In addition, we introduce polynomial reduction strategies based on odd- and even-degree orthogonal polynomials, reducing the integration to the integration of its residual quadratic or linear polynomials. These reductions motivate new quadrature schemes, which we further extend through polynomial interpolation to obtain efficient, high-precision quadrature rules for various polygonal domains.

</details>


### [25] [A higher order sparse grid combination technique](https://arxiv.org/abs/2601.04075)
*Julia Muñoz-Echániz,Christoph Reisinger*

Main category: math.NA

TL;DR: 通过将多变量外推有限差分法与标准组合公式结合，该研究实现了从二阶到四阶精度的稀疏网格解，并在高维泊松问题中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 提升稀疏网格组合技术的精度，克服传统方法在高维问题中收敛阶数低的问题。

Method: 采用多变量外推有限差分法与广义稀疏网格组合技术相结合的方法，形式化分析误差展开并验证其在泊松问题中的适用性。

Result: 理论分析表明该方法可将二阶精度格式提升至四阶，数值实验在高达七维的情况下展示了良好的收敛性。

Conclusion: 所提出的方法能有效提高稀疏网格解的精度，适用于高维光滑问题，具有良好的扩展性和应用前景。

Abstract: We show that a generalised sparse grid combination technique which combines multi-variate extrapolation of finite difference solutions with the standard combination formula lifts a second order accurate scheme on regular meshes to a fourth order combined sparse grid solution. In the analysis, working in a general dimension, we characterise all terms in a multivariate error expansion of the scheme as solutions of a sequence of semi-discrete problems. This is first carried out formally under suitable assumptions on the truncation error of the scheme, stability and regularity of solutions. We then verify the assumptions on the example of the Poisson problem with smooth data, and illustrate the practical convergence in up to seven dimensions.

</details>


### [26] [Algebraic Multigrid with Overlapping Schwarz Smoothers and Local Spectral Coarse Grids for Least Squares Problems](https://arxiv.org/abs/2601.04112)
*Ben S. Southworth,Hussam Al Daas,Golo A> Wimmer,Ed Threlfall*

Main category: math.NA

TL;DR: 本文提出了一种新的代数多重网格（AMG）方法，用于求解形式为 $A=G^TG$ 的稀疏最小二乘系统，结合了聚合型粗化、重叠Schwarz平滑器和局部构造的谱粗空间，在无需几何信息的情况下实现了对强各向异性问题的鲁棒且高效的收敛。


<details>
  <summary>Details</summary>
Motivation: 针对科学计算中经典AMG方法失效的挑战性应用，特别是强各向异性问题（如磁约束聚变中的热传导算子），发展一种适用于稀疏最小二乘系统的新型AMG方法。

Method: 融合基于聚合的粗化策略、重叠Schwarz平滑器与局部构造的谱粗空间，利用矩阵 $A=G^TG$ 的因子结构构造廉价的对称半正定分裂，导出局部广义特征问题，其解生成稀疏非重叠的粗基函数，构建完全代数化且可自然递归的多层层次结构。

Result: 数值实验表明，该方法在旋转扩散问题上具有与各向异性无关的收敛速度，并在问题规模增大时保持可扩展性；对于轻度各向异性问题，其收敛性和算子复杂度与经典AMG相当；最关键的是，在经典AMG和光滑聚合方法几乎无法收敛的极端各向异性热传导问题上，该方法仍能实现跨多个数量级强度的稳健高效收敛。

Conclusion: 所提出的最小二乘AMG-DD方法通过局部谱信息构造粗空间，无需几何信息或全局特征值求解，具备良好的并行性和代数特性，显著拓展了AMG方法在极端各向异性问题上的适用范围。

Abstract: This paper develops a new algebraic multigrid (AMG) method for sparse least-squares systems of the form $A=G^TG$ motivated by challenging applications in scientific computing where classical AMG methods fail. First we review and relate the use of local spectral problems in distinct fields of literature on AMG, domain decomposition (DD), and multiscale finite elements. We then propose a new approach blending aggregation-based coarsening, overlapping Schwarz smoothers, and locally constructed spectral coarse spaces. By exploiting the factorized structure of $A$, we construct an inexpensive symmetric positive semidefinite splitting that yields local generalized eigenproblems whose solutions define sparse, nonoverlapping coarse basis functions. This enables a fully algebraic and naturally recursive multilevel hierarchy that can either coarsen slowly to achieve AMG-like operator complexities, or coarsen aggressively-with correspondingly larger local spectral problems-to ensure robustness on problems that cannot be solved by existing AMG methods. The method requires no geometric information, avoids global eigenvalue solves, and maintains efficient parallelizable setup through localized operations. Numerical experiments demonstrate that the proposed least-squares AMG-DD method achieves convergence rates independent of anisotropy on rotated diffusion problems and remains scalable with problem size, while for small amounts of anisotropy we obtain convergence and operator complexities comparable with classical AMG methods. Most notably, for extremely anisotropic heat conduction operators arising in magnetic confinement fusion, where AMG and smoothed aggregation fail to reduce the residual even marginally, our method provides robust and efficient convergence across many orders of magnitude in anisotropy strength.

</details>


### [27] [Quantitative Constraints for Stable Sampling on the Sphere](https://arxiv.org/abs/2601.04119)
*Martin Ehler,Karlheinz Gröchenig*

Main category: math.NA

TL;DR: 本文推导了满足Marcinkiewicz-Zygmund不等式的球面上采样测度的体积约束，利用Jacobi多项式的局部估计给出了测度在自然尺度下的显式上下界，并得到了Hausdorff维数和曲线长度的最优下界。


<details>
  <summary>Details</summary>
Motivation: 为了获得Marcinkiewicz-Zygmund采样测度的定量体积约束，特别是显式常数下的精确估计。

Method: 使用Jacobi多项式的精确局部化估计方法，分析测度在单位球面上的分布特性，并推导出测度在t^{-1}尺度下对测地球的质量上下界。

Result: 得到了采样测度μ_t在测地球上的显式上下界，进而给出了s维Hausdorff体积的定量约束以及Marcinkiewicz-Zygmund曲线长度的最优下界。

Conclusion: 该研究提供了具有显式常数的定量结果，增强了对高维球面采样结构的理解，适用于需要精确控制采样分布的应用场景。

Abstract: We derive quantitative volume constraints for sampling measures $μ_t$ on the unit sphere $\mathbb{S}^d$ that satisfy Marcinkiewicz-Zygmund inequalities of order $t$. Using precise localization estimates for Jacobi polynomials, we obtain explicit upper and lower bounds on the $μ_t$-mass of geodesic balls at the natural scale $t^{-1}$. Whereas constants are typically left implicit in the literature, we place special emphasis on fully explicit constants, and the results are genuinely quantitative. Moreover, these bounds yield quantitative constraints for the $s$-dimensional Hausdorff volume of Marcinkiewicz-Zygmund sampling sets and, in particular, optimal lower bounds for the length of Marcinkiewicz-Zygmund curves.

</details>


### [28] [Active subspace methods and derivative-based Shapley effects for functions with non-independent variables](https://arxiv.org/abs/2601.04132)
*Matieyendou Lamboni,Sergei Kucherenko*

Main category: math.NA

TL;DR: 本文扩展了基于导数的主动子空间方法和基于导数的Shapley效应，提出了针对非独立变量函数的敏感性基主动子空间方法，旨在通过减少函数方差来识别关键变量，并结合理论与计算优化实现了偏差控制和收敛速率提升。


<details>
  <summary>Details</summary>
Motivation: 现有主动子空间方法主要适用于独立变量，难以处理变量间相关性；为更有效地估计非独立变量下函数的不确定性并识别关键变量组合，需发展新的敏感性分析框架。

Method: 结合非独立变量函数梯度的最新理论进展，提出敏感性基主动子空间方法，利用最优梯度计算策略，构建具有维度无关偏差上界和参数收敛速率的算法，并与基于导数的方法进行对比分析。

Result: 理论结果表明新方法具有良好的偏差控制和收敛性质；模拟实验显示，基于导数和基于敏感性的主动子空间方法在不同函数上的相对性能存在差异，验证了所提方法的有效性和适用性。

Conclusion: 敏感性基主动子空间方法能有效处理非独立变量情形下的变量降维与方差缩减问题，为复杂系统不确定性分析提供了新的工具，且其性能依赖于具体函数特性。

Abstract: Lower-dimensional subspaces that impact estimates of uncertainty are often described by Linear combinations of input variables, leading to active variables. This paper extends the derivative-based active subspace methods and derivative-based Shapley effects to cope with functions with non-independent variables, and it introduces sensitivity-based active subspaces. While derivative-based subspace methods focus on directions along which the function exhibits significant variation, sensitivity-based subspace methods seek a reduced set of active variables that enables a reduction in the function's variance. We propose both theoretical results using the recent development of gradients of functions with non-independent variables and practical settings by making use of optimal computations of gradients, which admit dimension-free upper-bounds of the biases and the parametric rate of convergence. Simulations show that the relative performance of derivative-based and sensitivity-based active subspaces methods varies across different functions.

</details>


### [29] [Efficient third-order iterative algorithms for computing zeros of special functions](https://arxiv.org/abs/2601.04148)
*Dhivya Prabhu K,Sanjeev Singh,Antony Vijesh*

Main category: math.NA

TL;DR: 提出了一种用于计算二阶常微分方程解的零点的新型三阶迭代方法，基于梯形法则逼近Riccati微分方程的解，并建立了非局部收敛性的充分条件。


<details>
  <summary>Details</summary>
Motivation: 为了更高效、可靠地计算二阶常微分方程解的零点，特别是针对满足特定条件的特殊函数和正交多项式。

Method: 通过使用梯形法则近似相关Riccati微分方程的解，推导出三阶迭代方法，并提供合适的初始猜测以在给定区间内计算所有零点。

Result: 该方法在包括勒让德、埃尔米特多项式以及贝塞尔、库仑波等特殊函数上均满足收敛条件，数值模拟验证了其有效性。

Conclusion: 所提出的三阶迭代方法具有良好的理论收敛性与数值表现，适用于广泛的一类二阶常微分方程解的零点求解问题。

Abstract: This manuscript presents a novel and reliable third-order iterative procedure for computing the zeros of solutions to second-order ordinary differential equations. By approximating the solution of the related Riccati differential equation using the trapezoidal rule, this study has derived the proposed third-order method. This work establishes sufficient conditions to ensure the theoretical non-local convergence of the proposed method. This study provides suitable initial guesses for the proposed third-order iterative procedure to compute all zeros in a given interval of the solutions to second-order ordinary differential equations. The orthogonal polynomials like Legendre and Hermite, as well as the special functions like Bessel, Coulomb wave, confluent hypergeometric, and cylinder functions, satisfy the proposed conditions for convergence. Numerical simulations demonstrate the effectiveness of the proposed theory. This work also presents a comparative analysis with recent studies.

</details>


### [30] [From Penrose to Melrose: Computing Scattering Amplitudes at Infinity for Unbounded Media](https://arxiv.org/abs/2601.04167)
*Anıl Zenginoğlu*

Main category: math.NA

TL;DR: 提出了一种基于Penrose共形紧化和Melrose几何散射理论的Helmholtz方程散射振幅计算方法，适用于具有可能长程渐近特性的无界变介质问题，无需依赖显式解或格林函数表示，数值实验显示了在多种介质和入射条件下散射振幅的谱收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理无界变介质中的Helmholtz散射问题时依赖格林函数或显式解，难以应对长程渐近行为；本文旨在建立一个统一且无需这些假设的高效计算框架。

Method: 结合Penrose的共形紧化与Melrose的几何散射理论，将时间调和散射问题转化为紧化流形上的边值问题，并构建两步求解器，通过域分解与内部求解器耦合，实现对无穷远处散射振幅的渐近计算。

Result: 实现了对常数、短程和长程介质中单模及高斯束入射情况下的散射振幅的高效计算，数值实验表明所有情况下均具有谱收敛性，且无需使用格林函数或显式解。

Conclusion: 该方法为变介质中Helmholtz散射问题提供了统一、高效的远场计算框架，特别适用于具有复杂渐近结构的无界问题，具备良好的扩展性和数值精度。

Abstract: We develop a method to compute scattering amplitudes for the Helmholtz equation in variable, unbounded media with possibly long-range asymptotics. Combining Penrose's conformal compactification and Melrose's geometric scattering theory, we formulate the time-harmonic scattering problem on a compactified manifold with boundary and construct a two-step solver for scattering amplitudes at infinity. The construction is asymptotic: it treats a neighborhood of infinity, and is meant to couple to interior solvers via domain decomposition. The method provides far-field data without relying on explicit solutions or Green's function representation. Scattering in variable media is treated in a unified framework where both the incident and scattered fields solve the same background Helmholtz operator. Numerical experiments for constant, short-range, and long-range media with single-mode and Gaussian beam incidence demonstrate spectral convergence of the computed scattering amplitudes in all cases.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [31] [A Quantum Model for Constrained Markowitz Modern Portfolio Using Slack Variables to Process Mixed-Binary Optimization under QAOA](https://arxiv.org/abs/2601.03278)
*Pablo Thomassin,Guillaume Guerard,Sonia Djebali,Vincent Marc Lambert*

Main category: math.OC

TL;DR: 提出了一种通过引入松弛变量和辅助量子比特将约束优化问题转化为无约束QUBO形式的量子模型，成功应用于Markowitz投资组合优化，并在QAOA框架下表现出优于传统惩罚方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决量子算法在金融优化中难以有效处理不等式约束的问题。

Method: 将松弛变量映射到专用的辅助量子比特上，直接嵌入问题哈密顿量中，从而将带约束的优化问题转化为适合QAOA求解的QUBO形式。

Result: 模拟实验表明，该方法能持续找到最优投资组合，而传统的基于惩罚项的QAOA则失败；同时改变了问题的能量景观以利于优化。

Conclusion: 通过松弛-辅助量子比特方案修改哈密顿量结构，为在量子计算机上求解约束优化问题提供了稳健且有效的新途径；并提出了投资组合风险与收益同时精度的量子极限。

Abstract: Effectively encoding inequality constraints is a primary obstacle in applying quantum algorithms to financial optimization. A quantum model for Markowitz portfolio optimization is presented that resolves this by embedding slack variables directly into the problem Hamiltonian. The method maps each slack variable to a dedicated ancilla qubit, transforming the problem into a Quadratic Unconstrained Binary Optimization (QUBO) formulation suitable for the Quantum Approximate Optimization Algorithm (QAOA). This process internalizes the constraints within the quantum state, altering the problem's energy landscape to facilitate optimization. The model is empirically validated through simulation, showing it consistently finds the optimal portfolio where a standard penalty-based QAOA fails. This work demonstrates that modifying the Hamiltonian architecture via a slack-ancilla scheme provides a robust and effective pathway for solving constrained optimization problems on quantum computers. A fundamental quantum limit on the simultaneous precision of portfolio risk and return is also posited.

</details>


### [32] [An overview of the fractional-order gradient descent method and its applications](https://arxiv.org/abs/2601.03318)
*Higor V. M. Ferreira,Camila A. Tavares,Nelson H. T. Lemes,José Claudinei Ferreira,José P. C. dos Santos*

Main category: math.OC

TL;DR: 本文探讨了使用分数阶导数推广梯度下降法的优缺点，提出采用分数阶连续时间算法以改善收敛性，并验证了其在化学优化问题中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于传统梯度下降法在分数阶微积分中存在收敛难题，本文旨在通过引入分数阶连续时间算法来解决收敛至极值点的问题。

Method: 将分数阶引入时间导数而非梯度中，采用分数阶连续时间算法推广梯度方法，并通过数值模拟验证其在不同分数阶范围内的收敛行为。

Result: 当分数阶α在0到1之间时，该方法能保证收敛至极值点；模拟结果表明当1≤α≤2时也可能获得类似效果，且在具有11和24个参数的化学优化问题中表现出良好性能。

Conclusion: 分数阶连续时间方法有效解决了收敛至极值点的问题，但在平衡点的稳定性方面仍需进一步研究。

Abstract: Recent studies have shown that fractional calculus is an effective alternative mathematical tool in various scientific fields. However, some investigations indicate that results established in differential and integral calculus do not necessarily hold true in fractional calculus. In this work we will compare various methods presented in the literature to improve the Gradient Descent Method, in terms of convergence of the method, convergence to the extreme point, and convergence rate. In general, these methods that generalize the gradient descent algorithm by replacing the gradient with a fractional-order operator are inefficient in achieving convergence to the extremum point of the objective function. To avoid these difficulties, we proposed to choose the Fractional Continuous Time algorithm to generalize the gradient method. In this approach, the convergence of the method to the extreme point of the function is guaranteed by introducing the fractional order in the time derivative, rather than in of the gradient. In this case, the issue of finding the extreme point is resolved, while the issue of stability at the equilibrium point remains.
  Fractional Continuous Time method converges to extreme point of cost function when fractional-order is between 0 and 1. The simulations shown in this work suggests that a similar result can be found when $1 \leq α\leq 2$. { This paper highlights the main advantages and disadvantages of generalizations of the gradient method using fractional derivatives, aiming to optimize convergence in complex problems. Some chemical problems, with n=11 and 24 optimization parameters, are employed as means of evaluating the efficacy of the propose algorithms. In general, previous studies are restricted to mathematical questions and simple illustrative examples.

</details>


### [33] [Optimal Quantization of Finite Uniform Data on the Sphere](https://arxiv.org/abs/2601.03333)
*Mrinal Kanti Roychowdhury*

Main category: math.OC

TL;DR: 本文建立了单位球面S²上均匀概率分布的最优量化的系统性几何理论，提出了三个基本结构定理，并给出了球面上Lloyd算法的类比形式。


<details>
  <summary>Details</summary>
Motivation: 研究球面上有限均匀概率分布在完整二维曲面上的最优量化问题，而非低维测地子集，以建立更系统的几何理论。

Method: 利用球面Voronoi剖分和Karcher均值，结合分离组件、纬向环带分配和稳定性分析，推导最优n-均值的存在性与结构特性。

Result: 证明了聚类纯度定理、环带分配定理和Lipschitz型稳定性定理，并给出显式失真公式和球面Lloyd算法。

Conclusion: 为S²上的最优量化提供了统一且透明的几何与算法框架。

Abstract: This paper develops a systematic and geometric theory of optimal quantization on the unit sphere $\mathbb S^2$, focusing on finite uniform probability distributions supported on the spherical surface - rather than on lower-dimensional geodesic subsets such as circles or arcs. We first establish the existence of optimal sets of $n$-means and characterize them through centroidal spherical Voronoi tessellations. Three fundamental structural results are obtained. First, a cluster - purity theorem shows that when the support consists of well-separated components, each optimal Voronoi region remains confined to a single component. Second, a ring - allocation (discrete water - filling) theorem provides an explicit rule describing how optimal representatives are distributed across multiple latitudinal rings, together with closed-form distortion formulas. Third, a Lipschitz - type stability theorem quantifies the robustness of optimal configurations under small geodesic perturbations of the support. In addition, a spherical analogue of Lloyd's algorithm is presented, in which intrinsic (Karcher) means replace Euclidean centroids for iterative refinement. These results collectively provide a unified and transparent framework for understanding the geometric and algorithmic structure of optimal quantization on $\mathbb S^2$.

</details>


### [34] [Provably Convergent Decentralized Optimization over Directed Graphs under Generalized Smoothness](https://arxiv.org/abs/2601.03566)
*Yanan Bo,Yongqiang Wang*

Main category: math.OC

TL;DR: 本文研究了在广义(L_0, L_1)平滑性框架下的去中心化优化，该框架允许Hessian范数随梯度范数线性增长，适用于梯度快速变化的场景。通过结合梯度跟踪与梯度裁剪技术，并针对有向通信图设计裁剪阈值，实现了在无界梯度差异下的稳定收敛。实验表明，该方法在标准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统去中心化优化方法依赖Lipschitz平滑假设，难以处理梯度快速变化的问题。本文旨在突破这一限制，扩展至更广泛的非光滑场景。

Method: 采用梯度跟踪技术结合梯度裁剪，设计适应有向图的裁剪阈值，在广义(L_0, L_1)平滑性条件下实现优化。

Result: 算法在无界梯度差异下仍能保证收敛，适用于异构数据环境；在LIBSVM和CIFAR-10等数据集上验证了其优越的稳定性和更快的收敛速度。

Conclusion: 所提方法克服了经典平滑性假设的局限，在更现实的条件下实现了高效去中心化优化，具有更强的适用性和鲁棒性。

Abstract: Decentralized optimization has become a fundamental tool for large-scale learning systems; however, most existing methods rely on the classical Lipschitz smoothness assumption, which is often violated in problems with rapidly varying gradients. Motivated by this limitation, we study decentralized optimization under the generalized $(L_0, L_1)$-smoothness framework, in which the Hessian norm is allowed to grow linearly with the gradient norm, thereby accommodating rapidly varying gradients beyond classical Lipschitz smoothness. We integrate gradient-tracking techniques with gradient clipping and carefully design the clipping threshold to ensure accurate convergence over directed communication graphs under generalized smoothness. In contrast to existing distributed optimization results under generalized smoothness that require a bounded gradient dissimilarity assumption, our results remain valid even when the gradient dissimilarity is unbounded, making the proposed framework more applicable to realistic heterogeneous data environments. We validate our approach via numerical experiments on standard benchmark datasets, including LIBSVM and CIFAR-10, using regularized logistic regression and convolutional neural networks, demonstrating superior stability and faster convergence over existing methods.

</details>


### [35] [Matrix Riccati BSDEs with singular terminal condition and stochastic LQ control with linear terminal constraint](https://arxiv.org/abs/2601.03747)
*Julia Ackermann,Thomas Kruse,Petr Petrov,Alexandre Popier*

Main category: math.OC

TL;DR: 本文研究了一类具有随机系数的多维线性二次随机控制问题，源于多资产最优交易执行。通过惩罚方法建立了Riccati BSDE的最小上解，并用于刻画值函数和最优控制，分析了其渐近行为并讨论了可显式求解的特例。


<details>
  <summary>Details</summary>
Motivation: 受多资产最优交易执行问题驱动，研究具有终端约束和随机系数的多维线性二次控制问题。

Method: 采用惩罚方法处理Riccati倒向随机微分方程（BSDE）的奇异终端条件，构造其最小上解，并结合终端约束进行分析。

Result: 证明了Riccati BSDE存在最小上解，利用该解完整刻画了原问题的值函数与最优控制策略，并分析了解在终端附近的渐近行为。

Conclusion: 该框架有效解决了带有随机系数和终端线性子空间约束的控制问题，部分情形下可获得闭式解，为最优交易执行等应用提供了理论支持。

Abstract: We analyze a class of multidimensional linear-quadratic stochastic control problems with random coefficients, motivated by multi-asset optimal trade execution. The problems feature non-diffusive controlled state dynamics and a terminal constraint that restricts the terminal state to a prescribed random linear subspace. We derive the associated Riccati backward stochastic differential equation (BSDE) and identify a suitable formalization of its singular terminal condition. Via a penalization approach, we establish existence of a minimal supersolution of the Riccati BSDE and use it to characterize both the value function and the optimal control. We analyze the asymptotic behavior of the supersolution near terminal time and discuss special cases where closed-form solutions can be obtained.

</details>


### [36] [GPU-Accelerated Cholesky Factorization of Block Tridiagonal Matrices](https://arxiv.org/abs/2601.03754)
*Roland Schwan,Daniel Kuhn,Colin N. Jones*

Main category: math.OC

TL;DR: 本文提出了一种基于GPU加速的框架，用于求解在工程和科学计算中广泛存在的块三对角线性系统，通过多级重排策略将计算复杂度从O(Nn^3)降低到O(log₂(N)n³)，并在多种硬件上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决实时应用中大规模块三对角线性系统的高效求解问题，尤其是在机器人、自主系统等领域对重复求解结构化线性系统的需求日益增长。

Method: 采用基于嵌套剖分的多阶段置换策略，结合NVIDIA Warp库和CUDA技术，在分解算法的多个层次上实现并行化，从而优化计算效率。

Result: 相比QDLDL稀疏求解器速度提升超过100倍，比基于BLASFEO的高度优化CPU实现快25倍，且比NVIDIA的CUDSS库快2倍以上，展现出良好的对数尺度扩展性。

Conclusion: 该框架为机器人和自主系统等领域的实时优化求解器提供了高效的GPU加速基础，并已开源发布。

Abstract: This paper presents a GPU-accelerated framework for solving block tridiagonal linear systems that arise naturally in numerous real-time applications across engineering and scientific computing. Through a multi-stage permutation strategy based on nested dissection, we reduce the computational complexity from $\mathcal{O}(Nn^3)$ for sequential Cholesky factorization to $\mathcal{O}(\log_2(N)n^3)$ when sufficient parallel resources are available, where $n$ is the block size and $N$ is the number of blocks. The algorithm is implemented using NVIDIA's Warp library and CUDA to exploit parallelism at multiple levels within the factorization algorithm. Our implementation achieves speedups exceeding 100x compared to the sparse solver QDLDL, 25x compared to a highly optimized CPU implementation using BLASFEO, and more than 2x compared to NVIDIA's CUDSS library. The logarithmic scaling with horizon length makes this approach particularly attractive for long-horizon problems in real-time applications. Comprehensive numerical experiments on NVIDIA GPUs demonstrate the practical effectiveness across different problem sizes and precisions. The framework provides a foundation for GPU-accelerated optimization solvers in robotics, autonomous systems, and other domains requiring repeated solution of structured linear systems. The implementation is open-source and available at https://github.com/PREDICT-EPFL/socu.

</details>


### [37] [Connecting Max-entropy With Computational Geometry, LP And SDP](https://arxiv.org/abs/2601.03759)
*Jean B Lasserre*

Main category: math.OC

TL;DR: 本文研究了带有矩约束的最大相对熵问题，并揭示其与计算几何、线性规划和半定规划之间的深刻联系，特别是通过Cramér变换和对数壁垒方法建立了优化问题之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 旨在理解最大熵问题与经典优化问题（如线性规划和半定规划）之间的内在联系，并利用信息论工具为优化问题提供新的视角和求解路径。

Method: 通过引入Kullback-Leibler散度和矩约束，将最大熵问题与Cramér变换相结合，并构造适当的参考测度和线性映射，建立其与对数壁垒法及对偶优化问题的联系。

Result: 证明了当约束维数m ≤ d时，最大熵函数Θ是某个几何问题解的Cramér变换；并展示了线性规划和半定规划的对偶问题可通过最大熵框架下的视角函数精确表示，且随参数趋于零收敛到原问题最优值。

Conclusion: 最大熵原理不仅在信息论中具有基础地位，还可作为统一框架连接计算几何与凸优化，为线性与半定规划提供新的理论解释和分析工具。

Abstract: We consider the well-known max-(relative) entropy problem $Θ$(y) = infQ$\ll$P DKL(Q P ) with Kullback-Leibler divergence on a domain $Ω$ $\subset$ R d , and with ''moment'' constraints h dQ = y, y $\in$ R m . We show that when m $\le$ d, $Θ$ is the Cram{é}r transform of a function v that solves a simply related computational geometry problem. Also, and remarkably, to the canonical LP: min x$\ge$0 {c T x\,: A x = y}, with A $\in$ R mxd , one may associate a max-entropy problem with a suitably chosen reference measure P on R d + and linear mapping h(x) = Ax, such that its associated perspective function $ε$ $Θ$(y/$ε$) is the optimal value of the log-barrier formulation (with parameter $ε$) of the dual LP (and so it converges to the LP optimal value as $ε$ $\rightarrow$ 0). An analogous result also holds for the canonical SDP: min X 0 { C, X\,: A(X) = y }.

</details>


### [38] [Continuation methods for higher-order topology optimization](https://arxiv.org/abs/2601.04003)
*P. Gangl,M. Winkler*

Main category: math.OC

TL;DR: 本文提出了一种结合同伦方法和障碍策略的密度基拓扑优化方法，用于求解非凸问题中的局部最优设计，能够在无需先验知识的情况下保持密度函数的可行性并实现收敛。


<details>
  <summary>Details</summary>
Motivation: 在拓扑优化中，由于问题通常为非凸且需要满足密度变量的上下界约束，传统牛顿法对初值敏感且易失败，因此需要一种更鲁棒的方法来寻找局部最优解。

Method: 采用同伦（连续）方法，通过求解一系列参数化问题逼近原问题的解，并结合障碍策略以强制满足密度变量的0到1之间的约束，同时使用步长规则提升收敛性。

Result: 数值实验结果显示该方法能够保持密度函数的可行性，在PDE约束的柔顺性最小化问题中成功收敛到局部最优设计候选解。

Conclusion: 所提出的同伦与障碍策略相结合的方法能有效处理密度基拓扑优化中的非凸性和约束条件，无需先验知识即可实现鲁棒收敛。

Abstract: We aim to solve a topology optimization problem where the distribution of material in the design domain is represented by a density function. To obtain candidates for local minima, we want to solve the first order optimality system via Newton's method. This requires the initial guess to be sufficiently close to the a priori unknown solution. Introducing a stepsize rule often allows for less restrictions on the initial guess while still preserving convergence. In topology optimization one typically encounters nonconvex problems where this approach might fail. We therefore opt for a homotopy (continuation) approach which is based on solving a sequence of parametrized problems to approach the solution of the original problem. In the density based framework the values of the design variable are constrained by 0 from below and 1 from above. Coupling the homotopy method with a barrier strategy enforces these constraints to be satisified. The numerical results for a PDE-constrained compliance minimization problem demonstrate that this combined approach maintains feasibility of the density function and converges to a (candidate for a) locally optimal design without a priori knowledge of the solution.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [39] [Electronic Structure of UGe$_{2\pm x}$ Thin Films from Photoelectron Spectroscopy](https://arxiv.org/abs/2601.03902)
*Sonu George Alex,Oleksandr Romanyuk,Ivan Zorilo,Alexander Andreev,Frank Huber,Thomas Gouder,Petr Malinsky,Alexander B. Shick,Evgenia A. Tereshina-Chitrova*

Main category: cond-mat.str-el

TL;DR: UGe2薄膜在化学计量比偏差下仍保持稳定的电子结构，其费米能级附近的主导U-5f态具有鲁棒性，为铀基异质结构研究提供了可靠的电子基础。


<details>
  <summary>Details</summary>
Motivation: 研究可控化学计量比偏差对UGe2电子结构的影响，以理解U-5f态在非理想组分下的稳定性。

Method: 通过三电极溅射制备UGe2±x薄膜，并利用X射线光电子能谱（XPS）和紫外光电子能谱（UPS）对原始表面进行表征，结合DFT+U(ED)方法进行理论计算。

Result: 实验显示在整个成分范围内（0 ≤ x ≤ 1），价带保持金属性，费米能级处始终有显著的U-5f贡献，且谱形无明显变化；DFT+U(ED)计算很好地再现了实验结果。

Conclusion: UGe2薄膜的电子结构对适度的化学计量偏差具有强韧性，表明其可作为未来铀基界面和异质结构研究的稳定平台。

Abstract: Uranium digermanide UGe$_2$, the first ferromagnetic superconductor, represents a key composition in the U-Ge system dominated by U-5$f$ states. To examine the impact of controlled stoichiometric deviations on the electronic structure, UGe$_{2\pm x}$ thin films ($0 \le x \le 1$) were prepared by triode sputtering and studied on pristine surfaces by X-ray (XPS) and Ultraviolet (UPS) photoelectron spectroscopy. XPS and UPS reveal a robust metallic valence band with a dominant U-5$f$ contribution at the Fermi level and a broad incoherent feature at higher binding energies, without qualitative changes in spectral line shape across the composition range. The experimental spectrum of UGe$_2$ thin films is well reproduced by DFT+U(ED) valence-band calculations combining density functional theory with exact diagonalization of the multiconfigurational U-5$f$ shell. These results demonstrate that the overall U-Ge electronic framework of UGe$_2$ thin films remains resilient to moderate stoichiometric deviations, providing a reliable electronic baseline for future studies of interface- and heterostructure-driven phenomena in uranium-based systems.

</details>


### [40] [Superconductivity, Kondo physics and magnetic order: Tuning the groundstate in the La$_{1-x}$Ce$_x$FeSiH solid solution through the interplay between $3d$ and $4f$ correlated electrons](https://arxiv.org/abs/2601.04097)
*J. Sourd,B. Vignolle,E. Gaudin,S. Burdin,S. Tencé*

Main category: cond-mat.str-el

TL;DR: 研究了La$_{1-x}$Ce$_x$FeSiH固溶体的超导性、单离子Kondo效应和磁有序，揭示了随铈浓度变化的丰富相图。


<details>
  <summary>Details</summary>
Motivation: 探索铈浓度对La$_{1-x}$Ce$_x$FeSiH体系中3d与4f电子Kondo纠缠的影响。

Method: 通过调节铈含量，研究不同浓度下的电输运和磁性性质。

Result: 发现低铈浓度下存在超导性，中等浓度下出现单离子Kondo效应，高浓度下呈现Kondo相干和重费米子行为，且部分区域共存。

Conclusion: 铈浓度可调控3d与4f电子间的相互作用，实现从超导到Kondo相干态的转变。

Abstract: We report a study of the La$_{1-x}$Ce$_x$FeSiH solid solution ($0 \leq x \leq 1$), a family of intermetallic hydrides of ZrCuSiAs-type structure, with space group $P4/nmm$. For low cerium concentrations $x \leq 0.20$, we observe the presence of superconductivity, which originates from the correlated $3d$ electrons of iron. The superconducting regime is progressively suppressed by the cerium substitution. For moderate cerium concentration $0.07 \leq x \leq 0.50$, we observe evidence of the single-ion Kondo effect and no magnetic phase transition down to 2 K. For $0.07 \leq x \leq 0.20$, the single-ion Kondo effect coexists with a superconducting ground state at low temperatures. From $x > 0.50$, we observe signatures of Kondo coherence and a heavy Fermi liquid regime at low temperature. Finally, at high cerium concentration $x \geq 0.85$, we observe signatures of magnetic ordering at low temperatures. We discuss our results by introducing temperature scales related to superconductivity, the Kondo effect, and magnetic order, which permits building a rich phase diagram temperature versus cerium content $x$. This shows that using the cerium concentration $x$ as a unique control parameter, we can explore the Kondo entanglement between correlated $3d$ and $4f$ electrons, which suggests an unusual change between the superconducting state related to the $3d$ electrons and the Kondo coherent state involving both $3d$ and $4f$ electrons.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [41] [Modeling and Control for UAV with Off-center Slung Load](https://arxiv.org/abs/2601.03386)
*Zongyang Lv,Yanmei Jia,Yongqing Liu,Alan F. Lynch,Qing Zhao,Yuhu Wu*

Main category: eess.SY

TL;DR: 本文提出了一种基于悬挂点建模的无人机吊挂负载系统的级联控制策略，通过新的参考系避免了传统建模中对耦合效应的简化，实现了对吊挂摆角和无人机姿态的有效控制，并证明了闭环系统的局部指数稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于任务需求或机械干涉，吊挂点常偏离无人机质心，导致系统非线性动力学耦合，传统以质心建模的方法难以精确控制，因此需要一种更有效的建模与控制方法。

Method: 采用悬挂点作为参考点建立系统模型，设计基于加速度输出的中环控制器来调节吊挂摆角，内环控制器利用偏心参考系跟踪无人机姿态，不需简化耦合效应，并使用李雅普诺夫方法分析稳定性。

Result: 所提控制策略在仿真和实验中均有效抑制了吊挂负载的摆动并准确跟踪了期望轨迹，验证了控制系统的有效性。

Conclusion: 基于悬挂点的建模与级联控制框架能够有效处理吊挂系统中的复杂耦合问题，无需对动力学进行简化，具有良好的稳定性和应用前景。

Abstract: Unmanned aerial vehicle (UAV) with slung load system is a classic air transportation system. In practical applications, the suspension point of the slung load does not always align with the center of mass (CoM) of the UAV due to mission requirements or mechanical interference. This offset creates coupling in the system's nonlinear dynamics which leads to a complicated motion control problem. In existing research, modeling of the system are performed about the UAV's CoM. In this work we use the point of suspension instead. Based on the new model, a cascade control strategy is developed. In the middle-loop controller, the acceleration of the suspension point is used to regulate the swing angle of the slung load without the need for considering the coupling between the slung load and the UAV. Using the off-center reference frame, an inner-loop controller is designed to track the UAV's attitude without the need of simplification on the coupling effects. We prove local exponential stability of the closed-loop using Lyapunov approach. Finally, simulations and experiments are conducted to validate the proposed control system.

</details>


### [42] [Provable Acceleration of Distributed Optimization with Local Updates](https://arxiv.org/abs/2601.03442)
*Zuang Wang,Yongqiang Wang*

Main category: eess.SY

TL;DR: 本文研究了在分布式优化中引入多次局部更新的影响，利用PEP方法证明了在适当步长下，仅两次局部更新即可达到最大加速效果，且更多更新不会带来额外收益。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中多次局部更新已被证明有效，但在分布式优化中其作用尚不明确，尤其是当梯度精确时是否仍能加速收敛仍不清楚。此外，现有理论常要求减小步长，可能抵消局部更新的潜在优势。

Method: 基于经典的DIGing算法，采用性能估计问题（PEP）提供的紧致性能界进行理论分析，并通过合成与真实数据集上的实验验证结论。

Result: 理论上证明了在适当步长下，引入局部更新可以加速分布式优化；且仅需两次局部更新即可获得最大改进，更多更新无额外收益但增加计算成本。实验结果支持该理论发现。

Conclusion: 这是首次对广泛目标函数类严格证明局部更新可加速分布式优化的工作，结果为高效实现提供了实用指导：建议使用两次局部更新并选择合适步长。

Abstract: In conventional distributed optimization, each agent performs a single local update between two communication rounds with its neighbors to synchronize solutions. Inspired by the success of using multiple local updates in federated learning, incorporating local updates into distributed optimization has recently attracted increasing attention. However, unlike federated learning, where multiple local updates can accelerate learning by improving gradient estimation under mini-batch settings, it remains unclear whether similar benefits hold in distributed optimization when gradients are exact. Moreover, existing theoretical results typically require reducing the step size when multiple local updates are employed, which can entirely offset any potential benefit of these additional local updates and obscure their true impact on convergence. In this paper, we focus on the classic DIGing algorithm and leverage the tight performance bounds provided by Performance Estimation Problems (PEP) to show that incorporating local updates can indeed accelerate distributed optimization. To the best of our knowledge, this is the first rigorous demonstration of such acceleration for a broad class of objective functions. Our analysis further reveals that, under an appropriate step size, performing only two local updates is sufficient to achieve the maximal possible improvement, and that additional local updates provide no further gains. Because more updates increase computational cost, these findings offer practical guidance for efficient implementation. Extensive experiments on both synthetic and real-world datasets corroborate the theoretical findings.

</details>


### [43] [Policy Synthesis for Interval MDPs via Polyhedral Lyapunov Functions](https://arxiv.org/abs/2601.03445)
*Negar Monir,Sadegh Soudjani*

Main category: eess.SY

TL;DR: 提出了一种基于多面体Lyapunov函数的多目标区间马尔可夫决策过程策略合成新方法，通过将值迭代算法重构为具有区间不确定性的切换仿射系统，实现了在不确定性下的高效策略生成，并通过不变吸引集确保收敛性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，需要在不确定性下进行决策，而传统Lyapunov方法多依赖二次函数，精度有限，且现有方法常需昂贵的Pareto曲线计算，因此需要更高效、准确的策略合成方法。

Method: 利用多面体Lyapunov函数，将值迭代重新表述为具有区间不确定性的切换仿射系统，结合控制理论中的稳定性原理，构造不变吸引集以综合满足多目标约束的策略。

Result: 成功在回收机器人和电动汽车电池案例中验证了该方法的有效性，避免了Pareto优化的计算开销，能够直接生成使目标值进入指定范围的策略，并提供收敛保证。

Conclusion: 所提方法提高了处理不确定性的准确性，降低了计算复杂度，为多目标不确定环境下的决策提供了可靠且高效的策略合成框架。

Abstract: Decision-making under uncertainty is central to many safety-critical applications, where decisions must be guided by probabilistic modeling formalisms. This paper introduces a novel approach to policy synthesis in multi-objective interval Markov decision processes using polyhedral Lyapunov functions. Unlike previous Lyapunov-based methods that mainly rely on quadratic functions, our method utilizes polyhedral functions to enhance accuracy in managing uncertainties within value iteration of dynamic programming. We reformulate the value iteration algorithm as a switched affine system with interval uncertainties and apply control-theoretic stability principles to synthesize policies that guide the system toward a desired target set. By constructing an invariant set of attraction, we ensure that the synthesized policies provide convergence guarantees while minimizing the impact of transition uncertainty in the underlying model. Our methodology removes the need for computationally intensive Pareto curve computations by directly determining a policy that brings objectives within a specified range of their target values. We validate our approach through numerical case studies, including a recycling robot and an electric vehicle battery, demonstrating its effectiveness in achieving policy synthesis under uncertainty.

</details>


### [44] [Developing a Quantitative Resiliency Approach](https://arxiv.org/abs/2601.03452)
*Vincent P. Paglioni,Graeme Troxell,Aaron Brown,Steve Conrad,Mazdak Arabi*

Main category: eess.SY

TL;DR: 本文探讨了在关键基础设施管理中，韧性（resiliency）作为系统性能指标的重要性，并提出了一种基于量化理解的韧性决策方法，以应对复杂互联系统中传统可靠性指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于现有风险与可靠性评估方法在高度复杂和相互依赖的关键系统中存在局限，难以充分反映系统健康状况，因此需要建立对韧性的定量理解，以支持更有效的决策。

Method: 通过分析可靠性和韧性工程的基础理论，比较其在实际应用中的差异，提出一种将量化韧性纳入决策框架的方法。

Result: 建立了支持韧性中心化决策的理论基础，并提出了一个可量化的韧性评估途径，有助于推动韧性从定性概念向可操作的定量指标转化。

Conclusion: 韧性应被视为与可靠性同等重要的系统性能指标，通过引入量化方法，可以实现韧性在现实决策框架中的有效实施。

Abstract: Resiliency has garnered attention in the management of critical infrastructure as a metric of system performance, but there are significant roadblocks to its implementation in a realistic decision-making framework. Contrasted to risk and reliability, which have robust quantification approaches and undergird many regulatory approaches to system safety (e.g., "risk-informed decision-making"), resiliency is a diffuse, qualitatively-understood characteristic, often treated differently or distinctly. However, in the emerging context of highly-complex, highly-interdependent critical systems, the idea of reliability (as the probability of non-failure) may not be an appropriate metric of system health. As a result, focus is shifting towards resiliency-centered approaches that value the response to failure as much as the avoidance of failure. Supporting this approach requires a robustly-defined, quantitative understanding of resiliency. In this paper, we explore the foundations of reliability and resiliency engineering, and propose an approach to resiliency-informed decision-making bolstered by a quantitative understanding of resiliency.

</details>


### [45] [Online Decision-Making Under Uncertainty for Vehicle-to-Building Systems](https://arxiv.org/abs/2601.03476)
*Rishav Sen,Yunuo Zhang,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: eess.SY

TL;DR: 本文提出了一种基于马尔可夫决策过程（MDP）的车辆到建筑（V2B）优化框架，通过在线搜索和领域启发式方法应对大规模状态和动作空间的挑战，并在实际数据上验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有V2B优化方法多采用单次组合优化建模，难以应对电价波动、长规划周期、充电设备异构性和用户需求多样性等挑战，因此需要更灵活、动态的建模方式。

Method: 将V2B优化问题建模为马尔可夫决策过程（MDP），利用在线搜索处理大状态空间，并结合领域特定的启发式策略剪枝以减少动作空间。

Result: 在日产先进技术中心的电动汽车测试平台数据上进行验证，结果表明所提方法显著优于当前最先进的V2B优化方法。

Conclusion: 将V2B问题建模为MDP并结合搜索与启发式策略是有效且可行的，能够更好地适应复杂现实条件下的能源管理需求。

Abstract: Vehicle-to-building (V2B) systems integrate physical infrastructures, such as smart buildings and electric vehicles (EVs) connected to chargers at the building, with digital control mechanisms to manage energy use. By utilizing EVs as flexible energy reservoirs, buildings can dynamically charge and discharge them to optimize energy use and cut costs under time-variable pricing and demand charge policies. This setup leads to the V2B optimization problem, where buildings coordinate EV charging and discharging to minimize total electricity costs while meeting users' charging requirements. However, the V2B optimization problem is challenging because of: (1) fluctuating electricity pricing, which includes both energy charges ($/kWh) and demand charges ($/kW); (2) long planning horizons (typically over 30 days); (3) heterogeneous chargers with varying charging rates, controllability, and directionality (i.e., unidirectional or bidirectional); and (4) user-specific battery levels at departure to ensure user requirements are met. In contrast to existing approaches that often model this setting as a single-shot combinatorial optimization problem, we highlight critical limitations in prior work and instead model the V2B optimization problem as a Markov decision process (MDP), i.e., a stochastic control process. Solving the resulting MDP is challenging due to the large state and action spaces. To address the challenges of the large state space, we leverage online search, and we counter the action space by using domain-specific heuristics to prune unpromising actions. We validate our approach in collaboration with Nissan Advanced Technology Center - Silicon Valley. Using data from their EV testbed, we show that the proposed framework significantly outperforms state-of-the-art methods.

</details>


### [46] [Adaptive Model-Based Reinforcement Learning for Orbit Feedback Control in NSLS-II Storage Ring](https://arxiv.org/abs/2601.03486)
*Zeyu Dong,Yuke Tian,Yu Sun*

Main category: eess.SY

TL;DR: 提出了一种基于模型的强化学习自适应训练框架，用于稳定NSLS-II电子束流，通过轨迹优化和在线模型优化，有效降低对齐误差至约1μm。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法和监督学习在处理机器状态漂移、环境噪声和非线性加速器动力学方面存在不足，需更鲁棒的控制方法。

Method: 采用基于模型的强化学习框架，结合轨迹优化（最小化预期总奖励）和在线模型优化（通过智能体-环境交互学习非线性动力学）。

Result: 在NSLS-II上进行仿真和实地实验，实现了束流位置的稳定控制，对齐误差（RMS）降至约1μm。

Conclusion: 所提出的自适应强化学习框架能有效应对加速器中的动态变化和非线性问题，显著提升束流稳定性，优于传统方法。

Abstract: The National Synchrotron Light Source II (NSLS-II) uses highly stable electron beam to produce high-quality X-ray beams with high brightness and low-emittance synchrotron radiation. The traditional algorithm to stabilize the beam applies singular value decomposition (SVD) on the orbit response matrix to remove noise and extract actions. Supervised learning has been studied on NSLS-II storage ring stabilization and other accelerator facilities recently. Several problems, for example, machine status drifting, environment noise, and non-linear accelerator dynamics, remain unresolved in the SVD-based and supervised learning algorithms. To address these problems, we propose an adaptive training framework based on model-based reinforcement learning. This framework consists of two types of optimizations: trajectory optimization attempts to minimize the expected total reward in a differentiable environment, and online model optimization learns non-linear machine dynamics through the agent-environment interaction. Through online training, this framework tracks the internal status drifting in the electron beam ring. Simulation and real in-facility experiments on NSLS-II reveal that our method stabilizes the beam position and minimizes the alignment error, defined as the root mean square (RMS) error between adjusted beam positions and the reference position, down to ~1$μ$m.

</details>


### [47] [DSP-Based Sub-Switching-Period Current-Limiting Control for Grid-Tied Inverter under Grid Faults](https://arxiv.org/abs/2601.03638)
*Jaeyeon Park,Jiyu Lee,Junyeol Maeng,Shenghui Cui*

Main category: eess.SY

TL;DR: 提出了一种子开关周期电流限制控制方法，用于并网逆变器在电网故障期间防止瞬态过电流，并实现无缝故障穿越。


<details>
  <summary>Details</summary>
Motivation: 为了解决电网电压突变（如电压暂降或相位跳变）引起的瞬态大电流问题，尤其是在低开关频率下，可能导致并网逆变器出现过流和运行中断。

Method: 在检测到电网扰动后，立即修改脉宽调制载波信号，实现在远短于一个开关周期的时间内对逆变器输出电流的连续调节，且不中断电流流动。该方法可在通用数字信号处理器上实现，无需专用硬件或高速计算设备。

Result: 实验结果基于一个3.6 kHz开关频率的两电平三相逆变器，验证了该方法在对称和不对称电压暂降及相位跳变下的有效性。

Conclusion: 所提方法能有效抑制电网故障初期的瞬态过电流，提升逆变器的故障穿越能力，具有良好的实用性与工程应用前景。

Abstract: This paper presents a sub-switching period current-limiting control for a grid-tied inverter to prevent transient overcurrents during grid faults and enable seamless fault ride-through (FRT). Sudden grid-voltage disturbances, such as voltage sags or phase jumps, can induce large transient currents within a switching period, particularly at low switching frequencies. Upon disturbance detection, the proposed method immediately modifies the pulse-width modulation carrier, enabling continuous regulation of the inverter output current within a time much shorter than a switching period without interrupting current flow. The proposed method can be implemented on commonly used digital signal processors without requiring specialized analog or digital circuits or high-speed computing devices. Experimental results from a 2-level, 3-phase inverter switching at 3.6 kHz validate the effectiveness of the proposed method under symmetric and asymmetric voltage sags and phase jumps.

</details>


### [48] [Exact Continuous Reformulations of Logic Constraints in Nonlinear Optimization and Optimal Control Problems](https://arxiv.org/abs/2601.03906)
*Jad Wehbeh,Eric C. Kerrigan*

Main category: eess.SY

TL;DR: 本文提出了一种将逻辑约束精确重构为无二元变量表达式的方法，适用于非线性优化问题，通过合取范式和最大-最小约束转换并保持可行性，提升了求解效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统混合整数规划在处理连续动态与离散逻辑结合的非线性优化问题时存在的可扩展性和求解器依赖问题。

Method: 将逻辑命题重写为合取范式，转化为等价的max-min约束，并应用保持可行域的平滑处理。

Result: 在四旋翼轨迹优化和双罐系统两个基准问题上验证了该方法比现有二元变量消除技术更高效且能更一致地获得最优解。

Conclusion: 所提方法能够有效集成逻辑约束到非线性规划中，无需引入二元变量，具有良好的可微性且保持精确可行集。

Abstract: Many nonlinear optimal control and optimization problems involve constraints that combine continuous dynamics with discrete logic conditions. Standard approaches typically rely on mixed-integer programming, which introduces scalability challenges and requires specialized solvers. This paper presents an exact reformulation of broad classes of logical constraints as binary-variable-free expressions whose differentiability properties coincide with those of the underlying predicates, enabling their direct integration into nonlinear programming models. Our approach rewrites arbitrary logical propositions into conjunctive normal form, converts them into equivalent max--min constraints, and applies a smoothing procedure that preserves the exact feasible set. The method is evaluated on two benchmark problems, a quadrotor trajectory optimization with obstacle avoidance and a hybrid two-tank system with temporal logic constraints, and is shown to obtain optimal solutions more consistently and efficiently than existing binary variable elimination techniques.

</details>


### [49] [Derivation of the Thermal Conductivity in a Latent Thermal Energy Storage Unit for Use in Simplified System Models](https://arxiv.org/abs/2601.03716)
*Lauritz Zendel,Chiara Springer,Frank Dammel,Peter Stephan*

Main category: eess.SY

TL;DR: 本研究旨在为基于朗肯循环的卡诺电池中的潜热储能单元（LTES）开发简化的系统模型，通过COMSOL建立单根带翅片管的详细模型，并利用等效热导率方法将复杂模型的结果简化，以用于系统级瞬态模拟。


<details>
  <summary>Details</summary>
Motivation: 由于LTES中自然对流等物理现象难以在系统级瞬态模型中直接模拟，需要建立考虑几何结构影响的简化模型，以准确描述其充放电过程中的温度分布和动态行为。

Method: 采用COMSOL软件建立包含六角翅片和相变材料（PCM）的单根换热管的详细数值模型，通过调整简化模型中的等效热导率并对比复杂模型的参考结果，确定有效热导率值。

Result: 获得了可用于简化系统模型的有效热导率数据，能够反映翅片与PCM之间的复杂相互作用，支持在不显式建模翅片的情况下准确模拟LTES单元的热性能。

Conclusion: 所提出的方法可有效将高精度局部模型的结果转化为适用于系统级仿真的简化模型输入，提升了卡诺电池等系统中LTES建模的准确性与可行性。

Abstract: Latent Thermal Energy Storages (LTES) can store thermal energy in a narrow temperature range. Therefore, they are favorable for integration into Rankine-based Carnot Batteries. For the design of such systems, simulations based on accurate models are desirable. However, physical phenomena such as natural convection in LTES units cannot be modeled directly in transient system models. Simplified models are required. Therefore, the objective of this work is to derive simplified LTES unit models for use in system models. In transient simulations the state of charge of the LTES influences its temperature profile. The temperature profile depends on the geometry of the LTES unit. Therefore, the geometry must be considered to model the transient behavior of an LTES unit. The LTES unit under investigation has a shell and tube heat exchanger structure. The phase change material (PCM) is located between the hexagonal fins and in the space between the finned tubes. Aluminum fins are used. They have a high thermal conductivity and thus compensate for the low thermal conductivity of the sodium nitrate used as PCM. The interaction between fins and PCM is complex. Therefore, a numerical approach can be used to gain insight into the behavior of the LTES unit. To transfer the results of a complex model to a simplified model where fins and PCM are not considered individually, the effective thermal conductivity of a single finned tube can be used to approximate the performance of the LTES unit. In this study, a model of a section with a single finned tube is developed using the COMSOL software. The effective thermal conductivity of the system is determined by varying the effective thermal conductivity in a simplified model and comparing the results with reference cases based on a complex modeling approach. The results can serve as model input for simplified system models of Carnot Batteries, among others.

</details>


### [50] [Unified and Efficient Analysis of Machining Chatter and Surface Location Error](https://arxiv.org/abs/2601.03819)
*Woraphrut Kornmaneesang,Tsu-Chin Tsao,Niloufar Esfandi,Shyh-Leh Chen*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Although machining chatter can be suppressed by the choice of stable cutting parameters through means of stability lobe diagram (SLD), surface roughness still remains due to the forced vibration, which limits surface quality, especially in the surface finish. Better cutting parameters can be achieved considering surface location error (SLE) together with SLD. This paper proposes an innovative modeling framework of the machining dynamic system that enables efficient computation of the chatter stability and SLE. The framework mainly embodies two techniques, namely semi-discretization method (SDM) and lifting method. The machining dynamics system is mathematically expressed as an angle-varying delay differential equation (DDE). The SDM approximates the angle-varying and delayed terms to ordinary terms using zero-phase interpolations and governs the discrete angle-varying dynamics system. Then, the system is merged over the tooth passing angle using the lifted approach to establish an explicit dynamic system in the compact state-space form. Based on the compact state-space model, the chatter stability and SLE prediction are easily and efficiently conducted. Simulation results show the improved efficiency of the proposed method over other well-known methods.

</details>


### [51] [A Systems-Engineered ESP32 DAQ Architecture and FAIR Data Workflow for Small-Scale Wind Turbine Performance Measurement in Tropical Environments](https://arxiv.org/abs/2601.03867)
*Asitha Lakruwan Kulasekera*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Small-scale wind turbine research in resource-constrained academic settings frequently produces unreliable or unpublishable datasets due to ad-hoc instrumentation, inadequate time synchronization, storage failures, and weak data governance. This paper presents a systematic data acquisition (DAQ) methodology and ESP32-based reference implementation design for field characterization of small wind turbines (100~W--5~kW), emphasizing tropical/coastal deployment constraints typical of Low- and Middle-Income Countries (LMIC). We integrate (i)~a student-adapted V-model with requirements traceability, (ii)~hardware selection strategies for high-humidity and salt-spray environments, (iii)~an embedded firmware architecture featuring interrupt-driven rotor speed measurement, state-machine fault handling, and NTP-based time synchronization, (iv)~a local-first hybrid storage design combining SD-card persistence with optional MQTT cloud telemetry, and (v)~a data-management workflow adapting CRISP-DM and FAIR principles with explicit quality dimensions and publication templates. A detailed helical vertical-axis wind turbine (VAWT) design scenario for coastal Sri Lanka illustrates the complete methodology, targeting $>90\%$ data completeness over six-month campaigns. The methodology is accompanied by open-source firmware, hardware templates, and data-publication workflow artifacts released via GitHub and Zenodo.

</details>


### [52] [Smooth Sampling-Based Model Predictive Control Using Deterministic Samples](https://arxiv.org/abs/2601.03893)
*Markus Walker,Marcel Reith-Braun,Tai Hoang,Gerhard Neumann,Uwe D. Hanebeck*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sampling-based model predictive control (MPC) is effective for nonlinear systems but often produces non-smooth control inputs due to random sampling. To address this issue, we extend the model predictive path integral (MPPI) framework with deterministic sampling and improvements from cross-entropy method (CEM)--MPC, such as iterative optimization, proposing deterministic sampling MPPI (dsMPPI). This combination leverages the exponential weighting of MPPI alongside the efficiency of deterministic samples. Experiments demonstrate that dsMPPI achieves smoother trajectories compared to state-of-the-art methods.

</details>


### [53] [A Load Impedance Emulation Active Interface for Piezoelectric Vibration Energy Harvesters](https://arxiv.org/abs/2601.04136)
*Alessandro Lo Schiavo,Luigi Costanzo,Massimo Vitelli*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A single stage active AC/DC interface able to emulate the optimal load impedance of a Resonant Piezoelectric Vibration Energy Harvester (RPVEH) is proposed. As theoretically shown, unlike an electronic interface that emulates an optimal load generator, an interface that emulates an optimal load impedance does not require adaptation to the acceleration of input vibrations. This allows the use of a very simple control, avoiding the implementation of Maximum Power Point Tracking (MPPT) algorithms that require lossy microcontrollers. Thus, the proposed interface is equipped with a simple analog controller allowing the RPVEH to work in its Maximum Power Point (MPP) in both steady-state and variable conditions of vibrations, without recurring to multivariable perturbative approaches, as it happens for the most of single stage AC/DC interfaces proposed in the literature. The absence of perturbative techniques allows a significant improvement of both stationary and dynamic performances. Experimental tests of a prototype of the proposed interface confirm the theoretical findings and the predicted behavior.

</details>


### [54] [Solar Panel-based Visible Light Communication for Batteryless Systems](https://arxiv.org/abs/2601.04190)
*Juan F. Gutierrez,Nhung Nguyen,Jesus M. Quintero,Andres Gomez*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a batteryless wireless communication node for the Internet of Things, powered entirely by ambient light and capable of receiving data through visible light communication. A solar panel serves dual functions as an energy harvester and an optical antenna, capturing modulated signals from LED light sources. A lightweight analog front-end filters and digitizes the signals for an 8-bit low-power processor, which manages the system's operational states based on stored energy levels. The main processor is selectively activated to minimize energy consumption. Data reception is synchronized with the harvester's open-circuit phase, reducing interference and improving signal quality. The prototype reliably decodes 32-bit VLC frames at 800\,Herz, consuming less than 2.8\,mJ, and maintains sleep-mode power below 30\,uW.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [55] [How different are deterministic physics suites when coupled to fixed model dynamics and why?](https://arxiv.org/abs/2601.03393)
*Edward Groot,Hannah Christensen,Xia Sun,Kathryn Newman,Wahiba Lfarh,Romain Roehrig,Lisa Bengtsson,Julia Simonson*

Main category: physics.ao-ph

TL;DR: 本研究通过使用相同的动力框架和不同的次网格物理方案，分析了大气模型中降水和物理趋势的不确定性。结果显示，不同物理方案在粗网格模型中表现出高度相似的降水和水汽沉降特征，但与对流允许的基准模拟相比存在显著差异，表明粗网格模型可能过于自信。


<details>
  <summary>Details</summary>
Motivation: 难以将大气模型中的不确定性和误差归因于特定的模型组件，因为次网格参数化过程与大尺度传输强烈相互作用。

Method: 采用固定的大尺度动力框架，并结合四种不同的次网格物理方案进行实验，构建了MUMIP数据集；使用来自ICON DYAMOND实验（2.5km网格）的高分辨率模拟作为基准，比较各方案在印度洋区域一个月内的降水及其相关物理趋势的联合概率分布。

Result: 四种物理方案产生的降水量非常相似，模型间相关性超过0.95，但在与对流允许基准比较时相关性降至约0.80；自由对流层中平均物理趋势在各方案间也高度一致，但与重建的基准结果不同；水汽沉降与降水紧密关联，表明粗网格模型可能低估了不确定性。

Conclusion: 当前粗网格大气模型中的物理方案在模拟降水和水汽沉降方面表现出过高的一致性，未能反映真实条件下由未解析对流结构引起的变异性，因而可能过于自信，需改进以更好地表征模型不确定性。

Abstract: It is often difficult to attribute uncertainty and errors in atmospheric models to designated model components. This is because sub-grid parameterised processes interact strongly with the large-scale transport represented by the explicit model dynamics. We carry out experiments with prescribed large-scale dynamics and different sub-grid physics suites. This dataset has been constructed for the Model Uncertainty Model Intercomparison Project (MUMIP), in which each suite forecasts sub-grid tendencies at a 22km grid. The common dynamics is derived from a convection-permitting benchmark: an ICON DYAMOND experiment (2.5km grid).
  We compare four different physics suites for atmospheric models in an Indian Ocean experiment. We analyse their joint PDFs of precipitation and associated physics tendencies for a full month. Precipitation is selected because it is a dominant uncertainty in the models that redistributes large amounts of heat. We find that all physics suites produce very similar precipitation amounts, with very high correlations between models, which exceed 0.95 at the native grid. However, the convection-permitting benchmark is more dissimilar from each of the physics suites, with correlations of $\approx$0.80.
  Similarly, we show that the vertically averaged physics tendencies in the free-troposphere are highly similar between the four physics suites, yet different if reconstructed for the benchmark. The water vapour sink is very closely linked with precipitation in the four physics suites. This suggests that the coarse-grid models are overconfident.
  We hypothese is that variation in unresolved convective structures can lead to variation in the dynamics, following a given amount of latent heating at fine grids, but not in our physics suites.
  The abstract length limit of ArXiv requires you to proceed in the PDF.

</details>


### [56] [Tidal motions in the deep Mediterranean](https://arxiv.org/abs/2601.03771)
*Hans van Haren*

Main category: physics.ao-ph

TL;DR: 地中海的潮汐运动虽小，但在特定条件下仍能在深海温度记录中观测到，并可通过校正方法分离出内波信号。


<details>
  <summary>Details</summary>
Motivation: 研究地中海微弱潮汐在深海温度记录中的表现及其与内波和惯性重力波的关系。

Method: 利用局部绝热递减率将压力变化转换为温度单位，分析2500米深海底的温度数据，识别并校正半日周期的正压潮汐信号。

Result: 发现当浮力频率小于惯性频率时，微小潮汐可在深海温度记录中显现；通过校正可去除1.5×10⁻⁵°C振幅的半日正压潮汐信号，剩余的斜压潮汐嵌入在宽频的惯性重力波带中。

Conclusion: 尽管地中海潮汐幅度极小，但在近均匀层结条件下仍能影响深海温度观测，且可通过适当方法分离出潮汐相关的内波成分。

Abstract: The Mediterranean Sea is known for its limited tidal motions. For example, surface barotropic tidal elevations have an amplitude of 0.1 m in the Northwestern Mediterranean. Nevertheless, these small tides are noticeable in temperature records at the 2500-m deep seafloor, but only under near-homogeneous conditions when buoyancy frequency N < f, the inertial frequency. After transfer of pressure to temperature units via the local adiabatic lapse rate, the observed internal-wave temperature signals may thus be corrected for 1.5x10-5-degrC amplitude semidiurnal barotropic tides. The remaining baroclinic tides are embedded in the broad and featureless inertio-gravity wave band, with some energy enhancement near its boundaries, also under tenfold-larger energetic stratified water conditions.

</details>


### [57] [Turbulence demonstrates height variations in closely spaced deep-sea mooring lines](https://arxiv.org/abs/2601.03870)
*Hans van Haren*

Main category: physics.ao-ph

TL;DR: 提出了一种利用高分辨率温度传感器在深海多线密集布设中进行相对高度测定的替代方法，通过带通滤波最强温度梯度下的湍流运动数据，在强分层和强湍流期间实现了±0.2米内的精确高度测量。


<details>
  <summary>Details</summary>
Motivation: 需要精确知道海洋仪器的布放高度，特别是在密集布设或地形复杂的情况下，微小的高度差异可能影响观测数据的准确性。

Method: 使用高分辨率温度传感器在多个密集分布的锚系线上进行观测，通过对强温度梯度区域中最高分辨率湍流运动数据进行带通滤波，反演相对高度差异。

Result: 在强密度分层和强湍流活动期间，成功检测到显著的高度变化，测量精度达到±0.2米；而均匀条件下因电子漂移难以校正，无法利用绝热递减率实现高度测定。

Conclusion: 该方法为深海锚系仪器的相对高度测定提供了一种有效的替代方案，尤其适用于存在较强温度梯度和湍流活动的环境。

Abstract: It may be important to precisely know heights of moored oceanographic instrumentation. For example, moorings can be closely spaced or accidentally be located on small rocks or in small gullies. Height variations O(1 m) will yield registration of different values when conditions such as small-scale density stratification vary strongly. Such little height variations may prove difficult to measure in the deep sea, requiring high-accuracy pressure sensors preferably on all instruments in a mooring-array. In this paper, an alternative method for relative height determination is presented using high-resolution temperature sensors moored on multiple densely-spaced lines in the deep Western Mediterranean. While it was anticipated that height variations between lines could be detected under near-homogeneous conditions via adiabatic lapse rate O(0.0001degrC m-1) by the 0.00003degrC-noise-level sensors, such was prevented by the impossibility of properly correcting for short-term bias due to electronic drift. Instead, a satisfactory height determination was found during a period of relatively strong stratification and large turbulence activity. By band-pass filtering data of the highest-resolved turbulent motions across the strongest temperature gradient, significant height variations were detectable to within +/-0.2 m.

</details>


### [58] [Stratified-turbulence observations in the deep Mediterranean](https://arxiv.org/abs/2601.03913)
*Hans van Haren*

Main category: physics.ao-ph

TL;DR: 通过在地中海深海部署约3000个高分辨率温度传感器，研究发现内波和亚中尺度涡旋引起的湍流对深海生态系统的影响远超地热加热和开阔海洋过程，且其强度与大气扰动密切相关。


<details>
  <summary>Details</summary>
Motivation: 了解深海湍流的来源及其对深海生命的重要性，特别是在弱分层和近均质条件下的影响机制。

Method: 利用约3000个高分辨率温度传感器对地中海深海进行全年观测，并采用带通滤波与垂直位移重排序方法比较不同湍流计算方式。

Result: 发现约一半时间来自数百米高处的较暖层化水体被移动到海底附近，内波和亚中尺度涡旋引起的湍流比地热加热强一个数量级，比开阔海洋过程强两个数量级；且湍流强度冬季比夏季高35%，滞后大气扰动约一周。

Conclusion: 亚中尺度涡旋引发的层化湍流可能比罕见的深水形成事件对深海生命更为重要，研究还提出了适用于弱分层和近均质条件的滤波截止频率推广方案。

Abstract: A nearly half-cubic hectometer of deep Mediterranean-Sea waters is yearlong sampled with about 3000 high-resolution temperature sensors to study different sources of turbulent waterflows, which are vital for life. Although temperature differences are never larger than 0.01degrC, daily, weekly, and seasonal variations are observed. About half the time, relatively warm stratified waters are moved from 100's of meters higher levels to near the seafloor. These internal-wave and sub-mesoscale eddy-induced motions are half an order of magnitude more turbulent than those induced via general geothermal heating from below, and about one order of magnitude more turbulent than those from open-ocean processes. A rough estimate shows that eddy-induced stratified turbulence is likely more important for deep-sea life than rare, not observed, deep dense-water formation at the abyssal-plain mooring site. With a delay of about a week, the stratified turbulence tracks atmospheric disturbances, which are found 35% more energetic in winter than in summer. From comparison of turbulence-calculation methods, of band-pass filtering with vertical-displacement reordering, for data over one-four days, a generalization is proposed for the filter cut-offs under weakly stratified and near-homogeneous conditions in the deep Mediterranean.

</details>


### [59] [Effects of Horizontal Discretization on Triangular and Hexagonal Grids on Linear Baroclinic and Symmetric Instabilities](https://arxiv.org/abs/2601.04013)
*Steffen Maaß,Sergey Danilov*

Main category: physics.ao-ph

TL;DR: 本文分析了在三角形和六边形网格上用于海洋环流模型的不同变量交错离散化方法对斜压不稳定性增长速率的影响，揭示了几种数值上的细微问题，包括非物理模态的不稳定性和对伽利略不变性的违背，并指出适度的双谐粘性和扩散可抑制这些非物理解，但需针对不同离散方案仔细校准参数。


<details>
  <summary>Details</summary>
Motivation: 随着全球海洋环流模型运行在允许涡旋的分辨率下，选择运动方程离散化方法时，如何准确再现斜压不稳定的增长速率成为一个主要关注点。为此，需要系统评估复杂网格（如三角形和六边形）上不同变量交错方式的数值表现。

Method: 通过将Eady构型下的线性斜压不稳定性分析扩展到更复杂的网格离散化方案，研究不同网格类型和变量交错方式对不稳定性增长速率的影响，特别关注非物理模态的出现及其与背景流和网格取向的关系。

Result: 发现与四边形网格相比，三角形和六边形网格上的离散化更容易激发由网格几何引起的非稳定虚假模态；这些虚假模态不满足伽利略不变性，导致其增长率依赖于背景流与网格的对齐方式及背景流强度；在对称不稳定性轴上，物理与虚假不稳定性分支更难区分；适度的双谐粘性和扩散可在大多数情况下抑制虚假分支，但需针对每种离散化方案精细调整参数。

Conclusion: 在复杂网格上进行海洋环流模拟时，必须谨慎选择离散化方案并合理配置数值耗散参数，以避免虚假模态干扰物理过程的准确模拟，特别是在高分辨率下对斜压不稳定的建模中。

Abstract: As global ocean general circulation models are run at eddy-permitting resolutions, reproducing accurate growth rates of baroclinic instabilities is a major concern when choosing a discretization of the equations of motion. From this viewpoint, we analyze discretizations on triangular and hexagonal grids with different types of variable staggering used in several ocean circulation models. By extending the linear baroclinic instability analysis in the Eady configuration to discretizations on more complex grids, several numerical subtleties are revealed. In comparison to discretizations on quadrilateral grids, the analyzed discretizations are less robust against unstable spurious modes, partly created by the mesh geometry. Some of the subtleties arise because spurious modes on staggered triangular and hexagonal grids do not adhere to Galilean invariance. As a consequence, their growth rates demonstrate a dependence on the alignment between the background flow and the grid, as well as the strength of a uniform background flow. The interactions with spurious modes become more significant on the axis of symmetric instabilities where the physical and spurious branches of instability are more difficult to separate in wavenumber space. Our analysis shows that in most cases moderate biharmonic viscosity and diffusion suppress spurious branches. However, one needs to carefully calibrate the viscosity and diffusivity parameters for each of the considered discretizations in order to achieve this.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [60] [Assessing Meteo-HySEA Performance for Adriatic Meteotsunami Events](https://arxiv.org/abs/2601.03856)
*Alejandro González,Cléa Denamiel,Jorge Macías*

Main category: physics.geo-ph

TL;DR: 本研究评估了基于GPU的Meteo-HySEA模型在亚得里亚海气象海啸模拟中的性能，并与基于CPU的AdriSC-ADCIRC系统进行对比，结果显示前者在计算效率上显著提升，具有用于业务化早期预警的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了提高气象海啸模拟的计算效率和预测精度，特别是在共振海湾中可能引发危险性沿海洪水的情况下，需要评估新型GPU加速模型的性能。

Method: 使用WRF对ERA再分析数据进行降尺度生成大气强迫，驱动Meteo-HySEA（GPU）和AdriSC-ADCIRC（CPU）模型模拟三个已记录的气象海啸事件（2014、2017、2020年），并利用验潮仪和微气压计观测数据进行验证。

Result: 两个模型均受限于对中尺度气压扰动的低估；Meteo-HySEA能较好再现海平面振荡的时序和空间变化，常产生比ADCIRC更大的振幅，但倾向于高估主波周期，尤其在封闭盆地中；振荡持续时间的差异表明需更多高分辨率数据验证其对港湾共振或能量耗散的模拟能力。

Conclusion: Meteo-HySEA在保持合理模拟精度的同时，得益于GPU加速，计算效率提升一个数量级以上，支持快速的高分辨率、多网格及淹没模拟，具备发展为业务化早期预警系统的潜力。

Abstract: Meteotsunamis are atmospherically driven sea-level oscillations that can trigger hazardous coastal flooding, particularly in resonant bays. This study assesses the GPU-based Meteo-HySEA model for meteotsunami simulation in the Adriatic Sea, benchmarking its performance against the CPU-based AdriSC-ADCIRC system. Three documented events (2014, 2017, 2020) were simulated using WRF downscaling of ERA reanalyses and validated with tide-gauge and microbarograph observations. Both models are limited by the underestimation of mesoscale pressure disturbances in the atmospheric forcing. Meteo-HySEA generally reproduces the timing and spatial variability of sea-level oscillations and often yields larger amplitudes than ADCIRC, but it tends to overestimate dominant wave periods, particularly in enclosed basins. Differences in oscillation persistence underscore the need for further validation against high-resolution tide-gauge data to assess whether Meteo-HySEA captures harbor seiches more realistically or ADCIRC better represents physical energy dissipation. Crucially, GPU acceleration provides order-of-magnitude gains in computational efficiency, enabling rapid high-resolution, multi-grid simulations including inundation, and thus offering strong potential for operational early warning.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [61] [Multifractality, percolation threshold and critical point of a nuclear reactor](https://arxiv.org/abs/2601.03399)
*V. V. Ryazanov*

Main category: cond-mat.dis-nn

TL;DR: 本文利用多重分形模型分析核反应堆中的中子演化，研究了链式反应中中子行为的多重分形特性，并探讨了其与渗流理论及临界性的关系。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多重分形理论更深入理解核反应堆中中子链式反应的随机性与层级结构，揭示临界状态的形成机制。

Method: 采用多重分形模型分析中子演化，结合Cayley树描述层级统计系综，利用Tsallis幂律刻画稳态分布，并引入渗流理论分析中子集群的形成与临界性。

Result: 确定了多重分形载体维度、信息维、关联维、分形熵、最大最小维度及多重分形谱函数；发现中子乘法因子在自持链式反应初期的演化规律；建立了渗流概率与反应堆临界性的联系。

Conclusion: 多重分形与渗流理论为描述核反应堆中中子行为和判断临界区域提供了有效框架，具有在反应堆理论中应用的潜力。

Abstract: A multifractal model is used to analyze neutron evolution within a reactor. For chain reactions, various characteristics of multifractal neutron behavior have been determined. These include the dimension of the multifractal carrier, information and correlation dimensions, the entropy of the fractal set, maximum and minimum dimension values, and the multifractal spectrum function. The geometric features of a multifractal allow for the description of a stochastic system consisting of hierarchically subordinate statistical ensembles, which are characterized by Cayley trees. A stationary distribution over hierarchical levels is established, which follows the Tsallis power law. The text also points out some potential applications of fractal patterns in nuclear reactor theory. The chance of percolation, which is when we see a state in the Bethe lattice where there's at least one continuous path through neighboring conducting nodes all the way across, is similar to the likelihood of a self-sustaining fission chain reaction happening. When this probability hits a critical point, we get a (conditionally) infinite cluster of neutrons forming. The percolation probability, influenced by how long the reactor has been running and its size, is linked to the reactor's criticality. We take a look at how the neutron multiplication factor behaves over time. We especially focus on the early stages of a self-sustaining nuclear fission chain reaction. We also highlight the ways to identify the boundaries of the critical region.

</details>


### [62] [Anderson Localization on Husimi Trees and its implications for Many-Body localization](https://arxiv.org/abs/2601.04155)
*Dafne Prado Bandeira,Marco Tarzia*

Main category: cond-mat.dis-nn

TL;DR: 本文研究了在具有有限局部环密度的Husimi树上局域化现象，揭示了局部环对多体局域化与单粒子安德森局域化之间类比的重要影响。


<details>
  <summary>Details</summary>
Motivation: 受多体局域化（MBL）与分层图上的单粒子安德森局域化之间类比的启发，研究局部环对局域化的影响。

Method: 通过Husimi树这一模型的精确解，系统分析局部环对共振过程和局域本征态空间扩展的影响。

Result: 局部环增强了共振过程，降低了临界无序强度，并增加了局域本征态的空间范围。

Conclusion: 局部环是构建更真实单粒子类比于多体希尔伯特空间的关键结构要素。

Abstract: Motivated by the analogy between many-body localization (MBL) and single-particle Anderson localization on hierarchical graphs, we study localization on the Husimi tree, a generalization of the Bethe lattice with a finite density of local loops of arbitrary but finite length. The exact solution of the model provides a transparent and quantitative framework to systematically inspect the effect of loops on localization. Our analysis indicates that local loops enhance resonant processes, thereby reducing the critical disorder with increasing their number and size. At the same time, loops promote local hybridization, leading to an increase in the spatial extent of localized eigenstates. These effects reconcile key discrepancies between MBL phenomenology and its single-particle Anderson analog. These results show that local loops are a crucial structural ingredient for realistic single-particle analogies to many-body Hilbert spaces.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [63] [Accelerated simulation of multiscale gas-radiation coupling flows via a general synthetic iterative scheme](https://arxiv.org/abs/2601.03935)
*Jianan Zeng,Qi Li,Yanbing Zhang,Wei Su,Lei Wu*

Main category: physics.comp-ph

TL;DR: 本研究提出了一种基于广义合成迭代法（GSIS）的玻尔兹曼型动力学模型，用于高效模拟高超音速再入流中的气体-辐射耦合问题，能够在粗网格上准确捕捉非平衡效应和辐射传热，并显著加速收敛。


<details>
  <summary>Details</summary>
Motivation: 高超音速再入飞行中极端温度导致气体和辐射传输处于非平衡状态，传统模拟方法难以兼顾精度与效率，亟需一种能跨多尺度流动高效求解的辐射气动动力学模型。

Method: 采用广义合成迭代法（GSIS），结合非结构化有限体积离散速度法与宏观合成方程，通过动力学模型为宏观方程提供高阶闭合关系，同时利用宏观方程引导介观系统演化，实现快速收敛。

Result: 该方法被证明具有渐近保持性，能在粗网格下恢复连续介质和光学厚极限；数值实验显示其在三维阿波罗返回舱高超音速流动模拟中比传统方法快数个量级，且能准确捕捉非平衡效应和辐射热传递。

Conclusion: GSIS为多尺度辐射气体动力学问题提供了高效、稳健且精确的求解框架，适用于航天器热防护系统的可靠设计。

Abstract: Gas-radiation coupling critically influences hypersonic reentry flows, where extreme temperatures induce pronounced non-equilibrium gas and radiative heat transport. Accurate and efficient simulation of radiative gas dynamics is therefore indispensable for reliable design of thermal protection systems for atmospheric entry vehicles. In this study, a Boltzmann-type kinetic model for radiative gas flows is solved across a broad spectrum of flow and radiation transport regimes using the general synthetic iterative scheme (GSIS). The approach integrates an unstructured finite-volume discrete velocity method with a set of macroscopic synthetic equations. Within this framework, the kinetic model provides high-order closures for the constitutive relations in the synthetic equations. Simultaneously, the macroscopic synthetic equations drive the evolution of the mesoscopic kinetic system, significantly accelerating steady-state convergence in near-continuum regimes, as substantiated by linear Fourier stability analysis. Crucially, the algorithm is proven to be asymptotic-preserving, correctly recovering the continuum and optically thick limits, represented by the radiative Navier-Stokes-Fourier equations governing distinct translational, rotational, vibrational, and radiative temperatures, on coarse meshes independent of the mean free path. Numerical simulations of challenging benchmarks, including three-dimensional hypersonic flow over an Apollo reentry capsule, demonstrate that GSIS achieves orders-of-magnitude speedup over conventional iterative schemes in multiscale simulations of radiative gas flows while accurately capturing non-equilibrium effects and radiative heat transfer in hypersonic environments.

</details>


### [64] [A constrained-transport embedded boundary method for compressible resistive magnetohydrodynamics](https://arxiv.org/abs/2601.04099)
*Samuel W. Jones,Colin P. McNally,Meritt Reynolds*

Main category: physics.comp-ph

TL;DR: 提出了一种在笛卡尔网格上实现任意形状嵌入边界的方法，用于求解可压缩电阻性磁流体动力学方程，并通过有限体积法和黎曼求解器处理界面通量，避免了切割单元的时间步长问题。


<details>
  <summary>Details</summary>
Motivation: 近年来对脉冲功率磁惯性聚变装置的兴趣增加，促使需要更精确的数值方法来模拟复杂几何边界下的磁流体动力学过程。

Method: 基于有限体积法，在笛卡尔网格上使用黎曼求解器计算网格面间的通量，并采用面中心约束传输法求解感应方程；通过幽灵流体法扩展以处理不同材料之间的运动界面。

Result: 该方法避免了切割单元引起的小时间步长问题，能够模拟激波驱动和磁场驱动的动力学压缩过程；验证结果显示在无间断时二阶收敛，存在材料属性间断时一阶收敛。

Conclusion: 所提出的方法能有效处理复杂几何形状的嵌入边界，适用于可压缩电阻性磁流体动力学模拟，具有良好的收敛性和稳定性。

Abstract: Motivated by the increased interest in pulsed-power magneto-inertial fusion devices in recent years, we present a method for implementing an arbitrarily shaped embedded boundary on a Cartesian mesh while solving the equations of compressible resistive magnetohydrodynamics. The method is built around a finite volume formulation of the equations in which a Riemann solver is used to compute fluxes on the faces between grid cells, and a face-centered constrained transport formulation of the induction equation. The small time step problem associated with the cut cells is avoided by always computing fluxes on the faces and edges of the Cartesian mesh. We extend the method to model a moving interface between two materials with different properties using a ghost-fluid approach, and show some preliminary results including shock-wave-driven and magnetically-driven dynamical compressions of magnetohydrostatic equilibria. We present a thorough verification of the method and show that it converges at second order in the absence of discontinuities, and at first order with a discontinuity in material properties.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [65] [Discrete symmetries in classical and quantum oscillators](https://arxiv.org/abs/2601.01960)
*Alexander D. Popov*

Main category: quant-ph

TL;DR: 本文通过Bargmann-Fock-Segal表示下的谐振子模型探讨波函数的本质，指出量子本征函数对应于具有特定能量的经典谐振子坐标，并定义在带有锥角的复空间上；叠加态的出现源于对初值条件的不完全知晓。


<details>
  <summary>Details</summary>
Motivation: 探讨波函数的物理本质及其与经典系统的关系，特别是在复表示下量子态如何反映经典动力学特性。

Method: 利用复Bargmann-Fock-Segal表示分析谐振子的量子本征函数，将其解释为定义在带离散对称性破缺的锥形子空间上的经典坐标，并研究叠加态的成因。

Result: 发现量子本征函数ψ_n对应于能量为E_n=ℏωn的经典谐振子坐标，定义在商空间ℂ/ℤ_n上；叠加态ψ=∑c_nψ_n的出现是由于未施加ℤ_n离散群不变性条件，导致所有可能初值被纳入通解。

Conclusion: 波函数的结构可由经典相空间中的几何约束和对称性破缺来理解，量子叠加反映了对初始信息的不完全掌握。

Abstract: We consider the nature of the wave function using the example of a harmonic oscillator. We show that the eigenfunctions $ψ_n{=}z^n$ of the quantum Hamiltonian in the complex Bargmann-Fock-Segal representation with $z\in\mathbb C$ are the coordinates of a classical oscillator with energy $E_n=\hbarωn$, $n=0,1,2,...\,$. They are defined on conical spaces ${\mathbb C}/{\mathbb Z}_n$ with cone angles $2π/n$, which are embedded as subspaces in the phase space $\mathbb C$ of the classical oscillator. Here ${\mathbb Z}_n$ is the finite cyclic group of rotations of the space $\mathbb C$ by an angle $2π/n$. The superposition $ψ=\sum_n c_nψ_n$ of the eigenfunctions $ψ_n$ arises only with incomplete knowledge of the initial data for solving the Schrödinger equation, when the conditions of invariance with respect to the discrete groups ${\mathbb Z}_n$ are not imposed and the general solution takes into account all possible initial data parametrized by the numbers $n\in\mathbb N$.

</details>


### [66] [Duality and measurement: the Copenhagen reconciliation](https://arxiv.org/abs/2601.03310)
*Vincenzo Chilla*

Main category: quant-ph

TL;DR: 本文通过重新强调量子理论的双重视角解读，利用对应原理和互补性原则，提出一个多视角框架来解决测量问题及相关二分法难题。


<details>
  <summary>Details</summary>
Motivation: 动机在于恢复哥本哈根解释中的二元论视角，以解决在后来的一元论或普遍主义哲学背景下出现的问题。

Method: 采用多视角框架，涵盖本体论、分析、认识论、因果关系和信息维度，重新阐述量子理论。

Result: 该框架成功消解了所谓的测量问题以及知识-信息和宏观-微观二分法。

Conclusion: 双重视角的解读更符合传统哥本哈根精神，并能有效解决量子力学中的一些长期争议问题。

Abstract: Duality, not monism, constitutes the hermeneutic lens that characterizes the original Copenhagen interpretation of Quantum Mechanics. Therefore, evoking the principles of correspondence and complementarity, in this work we re assert a dual-aspect reading of quantum theory, structured through a multi-perspective schema encompassing its ontological, analytical, epistemological, causal, and information dimensions. We then show how this schema dissolves the so-called measurement problem, along with the associated knowledge-information and macro-micro dichotomies, issues historically raised within later monistic or universalist philosophical settings that ultimately depart from the traditional Copenhagen spirit.

</details>


### [67] [Local Scale Invariance in Quantum Theory: A Non-Hermitian Pilot-Wave Formulation](https://arxiv.org/abs/2601.03567)
*Indrajit Sen,Matthew Leifer*

Main category: quant-ph

TL;DR: 本文提出了一种在导波理论中自然实现量子层面局部尺度不变性的方法，通过复数量子化电磁规范耦合参数得到魏尔协变导数，并将守恒流密度从|ψ|^2修改为依赖于导波轨迹的局部尺度不变形式。该方法适用于薛定谔、泡利和狄拉克方程以及量子场论中的轴子-电磁场相互作用系统。


<details>
  <summary>Details</summary>
Motivation: 重新审视魏尔放弃的局部尺度不变性思想，并探索其在量子力学特别是导波理论中的自然实现方式。

Method: 通过将电磁规范耦合参数复化，获得魏尔协变导数；利用导波理论中的轨迹依赖性引入尺度因子1[C]，构建局部尺度不变的守恒流密度表达式。

Result: 成功实现了局部尺度不变的导波理论框架，得到了修正的守恒电流密度|ψ|^2/1^2[C]，并在多个物理体系（如Schroedinger、Pauli、Dirac方程及量子场论中的轴子-光子系统）中实现了该理论。发现相应的平衡概率密度对应唯一轨迹。

Conclusion: 局部尺度不变性可在导波理论中自然实现，且非厄米性具有明确物理解释；该框架具有一般性，适用于多种量子系统，为理解量子态的几何与对称性质提供了新视角。

Abstract: We show that Weyl's abandoned idea of local scale invariance has a natural realization at the quantum level in pilot-wave (deBroglie-Bohm) theory. We obtain the Weyl covariant derivative by complexifying the electromagnetic gauge coupling parameter. The resultant non-hermiticity has a natural interpretation in terms of local scale invariance of the quantum state in pilot-wave theory. The conserved current density is modified from $|ψ|^2$ to the local scale invariant, trajectory-dependent ratio $|ψ|^2/ \mathbf{1}^2[\mathcal{C}]$, where $\mathbf 1[\mathcal C]$ is a scale factor that depends on the pilot-wave trajectory $\mathcal C$ in configuration space. Our approach is general, and we implement it for the Schrödinger, Pauli, and Dirac equations coupled to an external electromagnetic field. We also implement it in quantum field theory for the case of a quantized axion field interacting with a quantized electromagnetic field. We discuss the equilibrium probability density and show that the corresponding trajectories are unique.

</details>


### [68] [Extracting scattering phase shift in quantum mechanics on quantum computers](https://arxiv.org/abs/2601.04092)
*Peng Guo,Paul LeVan,Frank X. Lee,Yong Zhao*

Main category: quant-ph

TL;DR: 本研究探讨了在量子计算机上使用一维量子力学模型提取无限体积散射相移的可行性，通过离散化系统并设计量子电路，在IBM硬件上验证了方法的有效性，但三量子比特时因误差导致失败。


<details>
  <summary>Details</summary>
Motivation: 探索在当前量子计算架构上实现无限体积散射相移提取的可行性，推动量子计算在粒子物理问题中的应用。

Method: 采用文献中提出的积分关联函数（ICF）方法，将受限系统的ICF与无限体积散射相移联系起来；系统在有限盒中离散化并施加周期性边界条件，构建量子电路并在真实时间中进行模拟，结合多种后数据处理方法抑制快速振荡行为。

Result: 在两量子比特系统中，实验结果与理论吻合良好；但在三量子比特系统中，由于双量子比特门操作错误和热弛豫错误，模拟完全失败。

Conclusion: 该方法在小规模系统中可行，但扩展到更多量子比特时受限于当前硬件的噪声和误差，需进一步优化纠错和电路设计以实现更大规模应用。

Abstract: We investigate the feasibility of extracting infinite volume scattering phase shift on quantum computers in a simple one-dimensional quantum mechanical model, using the formalism established in Ref.~\cite{Guo:2023ecc} that relates the integrated correlation functions (ICF) for a trapped system to the infinite volume scattering phase shifts through a weighted integral. The system is first discretized in a finite box with periodic boundary conditions, and the formalism in real time is verified by employing a contact interaction potential with exact solutions. Quantum circuits are then designed and constructed to implement the formalism on current quantum computing architectures. To overcome the fast oscillatory behavior of the integrated correlation functions in real-time simulation, different methods of post-data analysis are proposed and discussed. Test results on IBM hardware show that good agreement can be achieved with two qubits, but complete failure ensues with three qubits due to two-qubit gate operation errors and thermal relaxation errors.

</details>


### [69] [Time-Dependent Dunkl-Pauli Oscillator in the Presence of the Aharonov-Bohm Effect](https://arxiv.org/abs/2601.03365)
*Boubakeur Khantoul,Ahmed Tedjani*

Main category: quant-ph

TL;DR: 提出了一种在Aharonov-Bohm通量存在下由Dunkl算符变形的二维Pauli振子的精确含时解，利用Lewis-Riesenfeld不变量方法得到了系统的本征值和自旋本征函数。


<details>
  <summary>Details</summary>
Motivation: 研究Dunkl对称性与拓扑规范相之间的相互作用对量子系统的影响。

Method: 通过将传统动量替换为Dunkl动量，并引入质量与频率的时间依赖性，构建变形的Pauli哈密顿量；采用Lewis-Riesenfeld不变量方法求解。

Result: 发现AB通量对Dunkl参数施加了对称性约束（ν₁ = ∓ ν₂），从而影响角动量算符和不变量算符的能谱与波函数。

Conclusion: 揭示了拓扑与Dunkl对称性共同作用下的新谱特性，为冷原子和量子点等人工系统的量子模拟提供了理论基础。

Abstract: We present an exact, time-dependent solution for a two-dimensional Pauli oscillator deformed by Dunkl operators in the presence of an Aharonov--Bohm (AB) flux. By replacing conventional momenta with Dunkl momenta and allowing arbitrary time dependence in both, mass and frequency, we derive a deformed Pauli Hamiltonian that encodes reflection symmetries and topological gauge phases. Employing the Lewis-Riesenfeld invariant method, we derive exact expressions for the eigenvalues and spinor eigenfunctions of the system. Crucially, the AB flux imposes symmetry constraints on the Dunkl parameters of the form $ν_1 = \mp ν_2 $, linking the reflection symmetry ($ε= \pm 1 $) to the quantization of angular momentum. These constraints modify the energy spectrum and wavefunctions of the angular operator and the invariant operator. Our framework reveals novel spectral characteristics arising from the interplay between topology and Dunkl symmetry, with potential implications for quantum simulation in engineered systems such as cold atoms and quantum dots.

</details>


### [70] [Many-body Quantum Score: a scalable benchmark for digital and analog quantum processors and first test on a commercial neutral atom device](https://arxiv.org/abs/2601.03461)
*Harold Erbin,Pierre-Louis Burdeau,Corentin Bertrand,Thomas Ayral,Grégoire Misguich*

Main category: quant-ph

TL;DR: 提出了一种名为Many-body Quantum Score (MBQS)的基准测试协议，用于评估量子处理器在模拟多体量子动力学方面的性能，特别适用于门式和模拟式量子处理器，并通过理论、模拟和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了量化当前量子处理器在多体量子系统模拟任务中的实际能力，需要一种可扩展且面向应用的基准测试方法。

Method: 设计MBQS协议，通过测量量子处理器在特定量子淬火后对横向场伊辛模型关联函数的再现能力，确定其能可靠处理的最大量子比特数。结合解析分析、经典模拟和实验数据进行验证。

Result: 在基于里德堡原子的模拟量子处理器Ruby上实现了MBQS，并成功展示了其评估能力，证明该协议能够有效反映设备在多体物理任务中的表现。

Conclusion: MBQS是一种实用、可扩展且信息丰富的基准工具，适用于评估近期量子设备在多体量子动力学模拟中的性能。

Abstract: We propose the Many-body Quantum Score (MBQS), a practical and scalable application-level benchmark protocol designed to evaluate the capabilities of quantum processing units (QPUs)--both gate-based and analog--for simulating many-body quantum dynamics. MBQS quantifies performance by identifying the maximum number of qubits with which a QPU can reliably reproduce correlation functions of the transverse-field Ising model following a specific quantum quench. This paper presents the MBQS protocol and highlights its design principles, supported by analytical insights, classical simulations, and experimental data. It also displays results obtained with Ruby, an analog QPU based on Rydberg atoms developed by the Pasqal company. These findings demonstrate MBQS's potential as a robust and informative tool for benchmarking near-term quantum devices for many-body physics.

</details>


### [71] [Non-Markovian dynamics of the giant atom beyond the rotating-wave approximation](https://arxiv.org/abs/2601.03383)
*Mei Yu,Walter T. Strunz,Stefan Nimmrichter*

Main category: quant-ph

TL;DR: 该研究利用层级运动方程（HEOM）突破了以往在零温、弱耦合近似下的限制，揭示了巨型人工原子在有限温度和强耦合下仍具有显著的非马尔可夫动力学特性，并观察到仅需两个耦合点即可形成束缚态。


<details>
  <summary>Details</summary>
Motivation: 以往对超导量子比特与声波系统耦合的研究局限于零温、弱耦合和旋转波近似条件下，无法捕捉长时浴记忆效应和多声子过程，因此需要更精确的方法来研究非马尔可夫动力学。

Method: 采用层级运动方程（HEOM）方法，精确模拟巨型人工原子在不同温度和耦合强度下的开放量子系统动力学，并与微扰性的Redfield理论进行对比。

Result: HEOM在零温弱耦合下能准确捕捉精确动力学，而Redfield理论因长时浴记忆失效；非马尔可夫效应在有限温度下依然存在；在强耦合下效应增强，并在零温下仅用两个耦合点即观察到束缚态形成。

Conclusion: 巨型人工原子是研究非马尔可夫开放量子动力学及其在量子信息与热力学中应用的强大平台。

Abstract: Superconducting qubits coupled to meandering transmission lines or surface acoustic waves may realize giant artificial atoms, whose spatially separated coupling points give rise to long-lived non-Markovian dynamics. Previous studies were limited to the zero-temperature, weak-coupling regime, where the rotating-wave approximation applies and only single-phonon processes contribute. Here we go beyond these limits using the hierarchical equations of motion (HEOM). We show that HEOM accurately captures the exact dynamics at zero temperature and weak coupling, whereas perturbative Redfield theory fails due to long bath memory times. The non-Markovian effects persist at finite temperatures. In the strong-coupling regime, they are further enhanced, and we observe bound-state formation at zero temperature with only two coupling points. These results establish giant atoms as a powerful platform for exploring non-Markovian open quantum dynamics and their applications in quantum information and thermodynamics.

</details>


### [72] [Multiphoton Interference with a symmetric SU(N) beam splitter and the generalization of the extended Hong-Ou-Mandel effect](https://arxiv.org/abs/2601.03395)
*Paul M. Alsing,Richard J. Birrittella,Peter L. Kaulfuss*

Main category: quant-ph

TL;DR: 本文研究了对称SU(N)分束器中的多光子干涉现象，推广了Hong-Ou-Mandel效应，并揭示了零振幅输出态中破坏性干涉的结构机制。


<details>
  <summary>Details</summary>
Motivation: 旨在理解多端口系统中多光子量子干涉的本质，特别是扩展已知的HOM效应至更一般的SU(N)情形，并揭示干涉相消的内在对称性机制。

Method: 通过构建与SU(N)分束器相关的矩阵Λ(S_N)，分析其积和式Perm(Λ)为零的条件，推导出任意N下的解析约束方程，并研究奇偶性与输出概率分布节点线的关系。

Result: 发现了广义eHOM效应中零振幅由多个子振幅组间的破坏性干涉构成；提出了Perm(Λ)=0的解析判据；将SU(2)中的中心节点线特性推广至N=2×N'（N'为奇数）的情形。

Conclusion: 多光子在SU(N)分束器中的干涉行为可通过矩阵积和式的对称性质系统描述，揭示了高维量子干涉中的普适相消机制。

Abstract: We examine multiphoton interference with a symmetric $SU(N)$ beam splitter $S_N$, an extension of features of the $SU(2)$ 50/50 beam splitter extended Hong-Ou-Mandel (eHOM) effect, whereby one obtains a zero amplitude (probability) for the output coincidence state (defined by equal number of photons $n/N$ in each output port), when a total number $n$ of photons impinges on the $N$-port device. These are transitions of the form $|n_1,n_2,\ldots,n_N\rangle\overset{S_N}{\to}|n/N\rangle^{\otimes N}$, where $n=\sum_{i=1}^N n_i$, which generalize the Hong-Ou-Mandel (HOM) effect $|1,1\rangle \overset{S_2}{\to}|1,1\rangle $, the eHOM effect $|n_1,n_2\rangle \overset{S_2}{\to}|\tfrac{n_1+n_2}{2},\tfrac{n_1+n_2}{2}\rangle $, and the generalized HOM effect (gHOM) $|1\rangle^{\otimes N}\overset{S_N}{\to}|1\rangle^{\otimes N}$, which have previously been studied in the literature. The emphasis of this work is on illuminating how the overall destructive interference occurs in separate groups of destructive interferences of sub-amplitudes of the total zero amplitude. We develop symmetry properties for the generalized eHOM effect (geHOM) $|n_1,n_2,\ldots,n_N\rangle\overset{S_N}{\to}|n/N\rangle^{\otimes N}$ involving a zero amplitude governed by Perm($Λ$)=0, for an appropriately constructed matrix $Λ(S_N)$ built from the matrix elements of $S_N$. We develop an analytical constraint equation for Perm$(Λ)$ for arbitrary $N$ that allows us to determine when it is zero. We generalize the SU(2) beam splitter feature of central nodal line (CNL), which has a zero diagonal along the output probability distribution when one of the input states is of odd parity (containing only odd number of photons), to the general case of $N = 2 * N'$ where $N'\in odd$.

</details>


### [73] [Hybrid non-degenerate parametric amplifier for a microwave cavity mode and an NV ensemble](https://arxiv.org/abs/2601.03407)
*Roman Ovsiannikov,Kurt Jacobs,Andrii G. Sotnikov,Matthew E. Trusheim,Denys I. Bondar*

Main category: quant-ph

TL;DR: 提出了一种非简并参量放大器的实现方案，通过调制自旋系综频率，在微波模式和自旋系综之间实现信号放大和双模/单模压缩，具有在室温和低温下工作的能力。


<details>
  <summary>Details</summary>
Motivation: 探索在无传统振荡器调制效应的情况下，如何利用自旋系综与微波腔耦合系统实现有效的参量放大和量子态压缩。

Method: 通过在腔模和自旋频率之和处调制自旋系综的频率（作为经典泵浦），并在两个系统间保持足够失谐，实现线性区域内的参量放大。

Result: 在室温下使用典型参数可实现约18 dB的放大（耗时1.7 μs，带宽0.5 MHz）；在10 mK下可产生约5 dB的压缩。分析了实验实现所需条件。

Conclusion: 该方案能有效实现微波信号的快速放大和量子压缩，且无需直接调制腔体，为量子信息处理提供了新途径。

Abstract: We introduce an implementation of a non-degenerate parametric amplifier in which the signal and idler modes, respectively, a microwave mode and an ensemble of spins (e.g., nitrogen-vacancy centers in diamond), are operated in their linear regime. This paramp, which amplifies signals in both parts at room and cryogenic temperatures, can be used to generate both the two-mode and single-mode squeezing of either system. It requires merely modulating the frequency of the spin ensemble at the sum of the cavity and spin frequencies (providing the classical pump) with the two systems sufficiently detuned. This effect is remarkable given that modulating a spin ensemble by itself produces neither amplification nor squeezing, unlike modulating an oscillator, and that an off-resonant perturbative analysis would suggest that modulating the spin ensemble merely parametrically drives the cavity mode. With typical cavity parameters including a cavity quality factor~$Q=10^4$, and a 1 GHz modulation amplitude, the microwave signal can be amplified by approximately $18~\mbox{dB}$ in $1.7~\mbox{$μ$s}$, with a resonant bandwidth of about $0.5~\mbox{MHz}$. At $10~\mbox{mK}$ with the same modulation amplitude and a cavity and spin $Q=5\times 10^4$ it generates approximately $5~\mbox{dB}$ of squeezing. We also examine the experimental requirements for implementation.

</details>


### [74] [Testing measurement-based computational phases of quantum matter on a quantum processor](https://arxiv.org/abs/2601.03426)
*Ryohei Weil,Dmytro Bondarenko,Arnab Adhikary,Robert Raussendorf*

Main category: quant-ph

TL;DR: 本文实验验证了量子物质中对称保护或对称丰富相的四种理论预测，证实了在存在对称性缺陷的情况下，基于测量的量子计算仍具有操作稳定性，并探究了逻辑退相干的缓解机制及算法效率的保持。


<details>
  <summary>Details</summary>
Motivation: 研究量子物质中计算相的普适计算能力及其在实际设备中的稳定性，特别是在存在对称性破坏和相关测量情况下的表现。

Method: 在IBM超导量子设备上实验研究具有对称性的资源态中的缺陷如何导致逻辑退相干，并测试其缓解方法；通过中心实验探测计算能力均匀性的标度律，同时分析局部测量产生集体逻辑操作的相关区域，验证算法效率的保持。

Result: 实验结果支持在具有对称性的量子物相中，基于测量的量子计算具备操作稳定性；对称性缺陷会导致逻辑退相干，但可通过方法缓解；即便存在相关测量，最密集的算法打包方式仍保持最高效率；标度律实验验证了计算能力的统一性。

Conclusion: 计算相中的对称性能够稳定支持基于测量的量子计算，即使在存在缺陷和相关效应的情况下，其计算能力和效率依然得以维持，展示了该框架在现实量子设备中的可行性与鲁棒性。

Abstract: Many symmetry protected or symmetry enriched phases of quantum matter have the property that every ground state in a given such phase endows measurement based quantum computation with the same computational power. Such phases are called computational phases of quantum matter. Here, we experimentally verify four theoretical predictions for them on an IBM superconducting quantum device. We comprehensively investigate how symmetric imperfections of the resource states translate into logical decoherence, and how this decoherence is mitigated. In particular, the central experiment probes the scaling law from which the uniformity of computational power follows. We also analyze the correlated regime, where local measurements give rise to logical operations collectively. We test the prediction that densest packing of a measurement-based algorithms remains the most efficient, in spite of the correlations. Our experiments corroborate the operational stability of measurement based quantum computation in quantum phases of matter with symmetry.

</details>


### [75] [Double interval entanglement in quasiparticle excited states](https://arxiv.org/abs/2601.03651)
*Zhouhao Guo,Jiaju Zhang*

Main category: quant-ph

TL;DR: 研究了准粒子激发态下双区间纠缠度量（如反射熵、互信息和对数负性）在经典、玻色和费米系统中的表现，提出了一种高效算法，并发现了大动量差下的普适可加性规律。


<details>
  <summary>Details</summary>
Motivation: 探索不同系统中准粒子激发态的纠缠特性，理解经典极限如何从量子系统中涌现。

Method: 发展了一种可在非正交基下计算密度矩阵的算法，用于高效计算各类纠缠度量。

Result: 发现大动量差下纠缠度量具有普适可加性，且玻色与费米系统的结果在此条件下均收敛至经典行为。

Conclusion: 准粒子激发态的纠缠具有通用加法性质，经典极限是该性质的一个特例。

Abstract: We investigate double-interval entanglement measures, specifically reflected entropy, mutual information, and logarithmic negativity, in quasiparticle excited states for classical, bosonic, and fermionic systems. We develop an algorithm that efficiently calculates these measures from density matrices expressed in a non-orthonormal basis, enabling straightforward numerical implementation. We find a universal additivity property that emerges at large momentum differences, where the entanglement measures for states with distinct quasiparticle sets equal the sum of their individual contributions. The classical limit arises as a special case of this additivity, with both bosonic and fermionic results converging to classical behavior when all momentum differences are large.

</details>


### [76] [Tailoring Dynamical Quantum Phase Transitions via Double-Mode Squeezing Manipulation](https://arxiv.org/abs/2601.03494)
*Kaiyuan Cao,Haodong Wang,Xiang-Ping Jiang,Shu chen,Jian Wang*

Main category: quant-ph

TL;DR: 提出通过双模压缩初态来调控XY链中动力学量子相变（DQPTs）的协议，发现当粒子-空穴对称性保持且压缩强度达到特定值时，会出现与淬火路径无关的普适DQPT，并揭示纠缠饱和与非解析动力学之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过初态调控来实现对动力学量子相变的精确控制，并理解对称性和纠缠在其中的作用。

Method: 利用双模压缩算符作用于XY链的初态，结合费舍零点分析和几何相位计算，研究不同对称性条件下DQPT的行为。

Result: 发现当粒子-空穴对称性被破坏时，DQPT可被诱导或抑制；当对称性保持且压缩强度r=π/4时，出现普适DQPT，表现为费舍零点坍缩到实时间轴、模式间纠缠达到最大，且临界动量与最大纠缠模式一致，同时动力学相消失并出现π跳跃的几何相。

Conclusion: 初态压缩是调控远离平衡态临界性的有效工具，纠缠饱和与普适非解析动力学之间存在直接关联。

Abstract: We propose a protocol to tailor dynamical quantum phase transitions (DQPTs) by double-mode squeezing onto the initial state in the XY chain. The effect of squeezing depends critically on the system's symmetry and parameters. When the squeezing operator breaks particle-hole symmetry (PHS), DQPTs become highly tunable, allowing one to either induce transitions within a single phase or suppress them. Remarkably, when PHS is preserved and the squeezing strength reaches $r=π/4$, a universal class of DQPTs emerges, independent of the quench path. This universality is characterized by two key features: (i) the collapse of all Fisher zeros onto the real-time axis, and (ii) the saturation of intermode entanglement to its maximum in each $(k,-k)$ modes. Moreover, the critical momenta governing the DQPTs coincide exactly with the modes attaining the maximal entanglement. At this universal point, the dynamical phase vanishes, leading to a purely geometric evolution marked by $π$-jumps in the Pancharatnam geometric phase. Our work establishes initial-state squeezing as a versatile tool for tailoring far-from-equilibrium criticality and reveals a direct link between entanglement saturation and universal nonanalytic dynamics.

</details>


### [77] [Transmutation based Quantum Simulation for Non-unitary Dynamics](https://arxiv.org/abs/2601.03616)
*Shi Jin,Chuwen Ma,Enrique Zuazua*

Main category: quant-ph

TL;DR: 提出了一种基于Kannai变换的量子算法，用于模拟由正半定算子生成的耗散扩散动力学，相较于传统方法在复杂度上有所改进，并可应用于热方程、双调和扩散及粘性Hamilton-Jacobi方程的熵惩罚方案中。


<details>
  <summary>Details</summary>
Motivation: 为了高效模拟由正半定算子A=L†L描述的耗散扩散过程，克服传统量子模拟方法在时间T、算子范数‖A‖和精度ε上的高复杂度问题。

Method: 利用Kannai变换将扩散半群e^(-TA)表示为酉波传播子的高斯加权叠加，实现线性酉组合（LCU）形式的量子算法，并将其应用于具体偏微分方程和线性求解问题。

Result: 实现了查询复杂度为Õ(√(‖A‖T log(1/ε)))的量子模拟算法，在长时间极限下进一步得到求解Ax=b的量子算法，复杂度为Õ(κ^(3/2) log²(1/ε))，优于现有方法的条件数依赖。

Conclusion: 该算法在处理具有L†L结构的正半定算子时，显著提升了量子模拟和线性求解的效率，尤其适用于椭圆型和抛物型偏微分方程的量子求解。

Abstract: We present a quantum algorithm for simulating dissipative diffusion dynamics generated by positive semidefinite operators of the form $A=L^\dagger L$, a structure that arises naturally in standard discretizations of elliptic operators. Our main tool is the Kannai transform, which represents the diffusion semigroup $e^{-TA}$ as a Gaussian-weighted superposition of unitary wave propagators. This representation leads to a linear-combination-of-unitaries implementation with a Gaussian tail and yields query complexity $\tilde{\mathcal{O}}(\sqrt{\|A\| T \log(1/\varepsilon)})$, up to standard dependence on state-preparation and output norms, improving the scaling in $\|A\|, T$ and $\varepsilon$ compared with generic Hamiltonian-simulation-based methods. We instantiate the method for the heat equation and biharmonic diffusion under non-periodic physical boundary conditions, and we further use it as a subroutine for constant-coefficient linear parabolic surrogates arising in entropy-penalization schemes for viscous Hamilton--Jacobi equations. In the long-time regime, the same framework yields a structured quantum linear solver for $A\mathbf{x}=\mathbf{b}$ with $A=L^\dagger L$, achieving $\tilde{\mathcal{O}}(κ^{3/2}\log^2(1/\varepsilon))$ queries and improving the condition-number dependence over standard quantum linear-system algorithms in this factorized setting.

</details>


### [78] [Strip-Symmetric Quantum Codes for Biased Noise: Z-Decoupling in Stabilizer and Floquet Codes](https://arxiv.org/abs/2601.03623)
*Mohammad Rowshan*

Main category: quant-ph

TL;DR: 本文提出了一类称为条带对称偏置码的新型量子纠错码框架，统一描述了XZZX表面码、域壁颜色码和X³Z³弗洛凯特码在去相位偏置下的共同机制，并通过条带分解实现高效的最大似然Z解码。


<details>
  <summary>Details</summary>
Motivation: 为了理解高偏置阈值量子纠错码（如XZZX码和弗洛凯特码）在无限偏置极限下性能优异的共性机制，并建立统一的理论框架以指导新码的设计。

Method: 定义条带对称偏置码的概念，分析其在纯去相位噪声和理想测量下Z故障局限于条带且探测器-故障关联矩阵块对角化的结构特性，利用每条带上的稳定子乘积作为Z₂ 1-形式对称性进行刻画，并引入合成条带模型和分域Clifford构造作为设计工具。

Result: 成功将XZZX码、域壁颜色码和X³Z³弗洛凯特码纳入该框架，揭示其Z探测器超图可分解为独立条带成分，使最大似然Z解码可在条带间分解，降低匹配解码器复杂度。

Conclusion: 条带对称性是实现高去相位偏置容错阈值的关键结构特征，所提出的框架为设计新型偏置定制弗洛凯特码提供了有效方法和理论基础。

Abstract: Bias-tailored codes such as the XZZX surface code and the domain wall color code achieve high dephasing-biased thresholds because, in the infinite-bias limit, their $Z$ syndromes decouple into one-dimensional repetition-like chains; the $X^3Z^3$ Floquet code shows an analogous strip-wise structure for detector events in spacetime. We capture this common mechanism by defining strip-symmetric biased codes, a class of static stabilizer and dynamical (Floquet) codes for which, under pure dephasing and perfect measurements, each elementary $Z$ fault is confined to a strip and the Z-detector--fault incidence matrix is block diagonal. For such codes the Z-detector hypergraph decomposes into independent strip components and maximum-likelihood $Z$ decoding factorizes across strips, yielding complexity savings for matching-based decoders. We characterize strip symmetry via per-strip stabilizer products, viewed as a $\mathbb{Z}_2$ 1-form symmetry, place XZZX, the domain wall color code, and $X^3Z^3$ in this framework, and introduce synthetic strip-symmetric detector models and domain-wise Clifford constructions that serve as design tools for new bias-tailored Floquet codes.

</details>


### [79] [Computational hardness of estimating quantum entropies via binary entropy bounds](https://arxiv.org/abs/2601.03734)
*Yupan Liu*

Main category: quant-ph

TL;DR: 本文研究了量子α-Rényi熵和q-Tsallis熵在不同阶数下的计算复杂性，证明了对于所有正实数阶，其低秩版本的近似问题具有BQP-完全性，扩展了此前仅针对冯诺依曼熵的结果。


<details>
  <summary>Details</summary>
Motivation: 扩展对量子熵不同形式（特别是α-Rényi和q-Tsallis熵）的计算复杂性理解，填补先前仅覆盖von Neumann熵和部分Tsallis熵的空白。

Method: 通过建立不同阶数下二元熵之间的新不等式，构造从已知BQP-hard问题到目标熵近似问题的归约，从而证明计算难度。

Result: 证明了对所有α>0和0<q≤1，低秩情形下的RényiQEA_α和TsallisQEA_q是BQP-完全的；对所有q>1，TsallisQEA_q也是BQP-完全的。

Conclusion: 量子α-Rényi熵和q-Tsallis熵的估计问题在广泛参数范围内均为BQP-完全，揭示了这些熵量在量子计算中的固有计算难度。

Abstract: We investigate the computational hardness of estimating the quantum $α$-Rényi entropy ${\rm S}^{\tt R}_α(ρ) = \frac{\ln {\rm Tr}(ρ^α)}{1-α}$ and the quantum $q$-Tsallis entropy ${\rm S}^{\tt T}_q(ρ) = \frac{1-{\rm Tr}(ρ^q)}{q-1}$, both converging to the von Neumann entropy as the order approaches $1$. The promise problems Quantum $α$-Rényi Entropy Approximation (RényiQEA$_α$) and Quantum $q$-Tsallis Entropy Approximation (TsallisQEA$_q$) ask whether $ {\rm S}^ {\tt R}_α(ρ)$ or ${\rm S}^{\tt T}_q(ρ)$, respectively, is at least $τ_{\tt Y}$ or at most $τ_{\tt N}$, where $τ_{\tt Y} - τ_{\tt N}$ is typically a positive constant. Previous hardness results cover only the von Neumann entropy (order $1$) and some cases of the quantum $q$-Tsallis entropy, while existing approaches do not readily extend to other orders.
  We establish that for all positive real orders, the rank-$2$ variants Rank2RényiQEA$_α$ and Rank2TsallisQEA$_q$ are ${\sf BQP}$-hard. Combined with prior (rank-dependent) quantum query algorithms in Wang, Guan, Liu, Zhang, and Ying (TIT 2024), Wang, Zhang, and Li (TIT 2024), and Liu and Wang (SODA 2025), our results imply:
  - For all real orders $α> 0$ and $0 < q \leq 1$, LowRankRényiQEA$_α$ and LowRankTsallisQEA$_q$ are ${\sf BQP}$-complete, where both are restricted versions of RényiQEA$_α$ and TsallisQEA$_q$ with $ρ$ of polynomial rank.
  - For all real order $q>1$, TsallisQEA$_q$ is ${\sf BQP}$-complete.
  Our hardness results stem from reductions based on new inequalities relating the $α$-Rényi or $q$-Tsallis binary entropies of different orders, where the reductions differ substantially from previous approaches, and the inequalities are also of independent interest.

</details>


### [80] [Detection-loophole-free nonlocality in the simplest scenario](https://arxiv.org/abs/2601.03817)
*Nandana T Raveendranath,Travis J. Baker,Emanuele Polino,Marwan Haddara,Lynden K. Shalm,Varun B. Verma,Geoff J. Pryde,Sergei Slussarenko,Howard M. Wiseman,Nora Tischler*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Loophole-free quantum nonlocality often demands experiments with high complexity (defined by all parties' settings and outcomes) and multiple efficient detectors. Here, we identify the fundamental efficiency and complexity thresholds for quantum steering using two-qubit entangled states. Remarkably, it requires only one photon detector on the untrusted side, with efficiency $ε> 1/X$, where $X \geq 2$ is the number of settings on that side. This threshold applies to all pure entangled states, in contrast to analogous Bell-nonlocality tests, which require almost unentangled states to be loss-tolerant. We confirm these predictions in a minimal-complexity ($X = 2$ for the untrusted party and a single three-outcome measurement for the trusted party), detection-loophole-free photonic experiment with $ε= (51.6 \pm 0.4)\% $.

</details>


### [81] [Topological Sensing in the Dynamics of Quantum Walks with Defects](https://arxiv.org/abs/2601.03821)
*Xiaowei Tong,Xingze Qiu,Xiang Zhan,Quan Lin,Kunkun Wang,Franco Nori,Peng Xue*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Topological quantum sensing leverages unique topological features to suppress noise and improve the precision of parameter estimation, emerging as a promising tool in both fundamental research and practical application. In this Letter, we propose a sensing protocol that exploits the dynamics of topological quantum walks incorporating localized defects. Unlike conventional schemes that rely on topological protection to suppress disorder and defects, our protocol harnesses the evolution time as a resource to enable precise estimation of the defect parameter. By utilizing topologically nontrivial properties of the quantum walks, the sensing precision can approach the Heisenberg limit. We further demonstrate the performance and robustness of the protocol through Bayesian estimation. Our results show that this approach maintains high precision over a broad range of parameters and exhibits strong robustness against disorder, offering a practical pathway for topologically enhanced quantum metrology.

</details>


### [82] [Finite-size security of QKD: comparison of three proof techniques](https://arxiv.org/abs/2601.03829)
*Gabriele Staffieri,Giovanni Scala,Cosmo Lupo*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We compare three proof techniques for composable finite-size security of quantum key distribution under collective attacks, with emphasis on how the resulting secret-key rates behave at practically relevant block lengths. As a benchmark, we consider the BB84 protocol and evaluate finite-size key-rate estimates obtained from entropic uncertainty relations (EUR), from the asymptotic equipartition property (AEP), and from a direct finite-block analysis based on the conditional min-entropy, which we refer to as the finite-size min-entropy (FME) approach. For BB84 we show that the EUR-based bound provides the most favorable performance across the considered parameter range, while the AEP bound is asymptotically tight but can become overly pessimistic at moderate and small block sizes, where it may fail to certify a positive key. The FME approach remains effective in this small-block regime, yielding nonzero rates in situations where the AEP estimate vanishes, although it is not asymptotically optimal for BB84. These results motivate the use of FME-type analyses for continuous-variable protocols in settings where tight EUR-based bounds are unavailable, notably for coherent-state schemes where current finite-size analyses typically rely on AEP-style corrections.

</details>


### [83] [Iterative Matrix Product State Simulation for Scalable Grover's Algorithm](https://arxiv.org/abs/2601.03832)
*Mei Ian Sam,Tzu-Ling Kuo,Tai-Yue Li*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Grover's algorithm is a cornerstone of quantum search algorithm, offering quadratic speedup for unstructured problems. However, limited qubit counts and noise in today's noisy intermediate-scale quantum (NISQ) devices hinder large-scale hardware validation, making efficient classical simulation essential for algorithm development and hardware assessment. We present an iterative Grover simulation framework based on matrix product states (MPS) to efficiently simulate large-scale Grover's algorithm. Within the NVIDIA CUDA-Q environment, we compare iterative and common (non-iterative) Grover's circuits across statevector and MPS backends. On the MPS backend at 29 qubits, the iterative Grover's circuit runs about 15x faster than the common (non-iterative) Grover's circuit, and about 3-4x faster than the statevector backend. In sampling experiments, Grover's circuits demonstrate strong low-shot stability: as the qubit number increases beyond 13, a single-shot measurement still closely mirrors the results from 4,096 shots, indicating reliable estimates with minimal sampling and significant potential to cut measurement costs. Overall, an iterative MPS design delivers speed and scalability for Grover's circuit simulation, enabling practical large-scale implementations.

</details>


### [84] [MPM-QIR: Measurement-Probability Matching for Quantum Image Representation and Compression via Variational Quantum Circuit](https://arxiv.org/abs/2601.03855)
*Chong-Wei Wang,Mei Ian Sam,Tzu-Ling Kuo,Nan-Yow Chen,Tai-Yue Li*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present MPM-QIR, a variational-quantum-circuit (VQC) framework for classical image compression and representation whose core objective is to achieve equal or better reconstruction quality at a lower Parameter Compression Ratio (PCR). The method aligns a generative VQC's measurement-probability distribution with normalized pixel intensities and learns positional information implicitly via an ordered mapping to the flattened pixel array, thus eliminating explicit coordinate qubits and tying compression efficiency directly to circuit (ansatz) complexity. A bidirectional convolutional architecture induces long-range entanglement at shallow depth, capturing global image correlations with fewer parameters. Under a unified protocol, the approach attains PSNR $\geq$ 30 dB with lower PCR across benchmarks: MNIST 31.80 dB / SSIM 0.81 at PCR 0.69, Fashion-MNIST 31.30 dB / 0.91 at PCR 0.83, and CIFAR-10 31.56 dB / 0.97 at PCR 0.84. Overall, this compression-first design improves parameter efficiency, validates VQCs as direct and effective generative models for classical image compression, and is amenable to two-stage pipelines with classical codecs and to extensions beyond 2D imagery.

</details>


### [85] [Integration and Resource Estimation of Cryoelectronics for Superconducting Fault-Tolerant Quantum Computers](https://arxiv.org/abs/2601.03922)
*Shiro Kawabata*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scaling superconducting quantum computers to the fault-tolerant regime calls for a commensurate scaling of the classical control and readout stack. Today's systems largely rely on room-temperature, rack-based instrumentation connected to dilution-refrigerator cryostats through many coaxial cables. Looking ahead, superconducting fault-tolerant quantum computers (FTQCs) will likely adopt a heterogeneous quantum-classical architecture that places selected electronics at cryogenic stages -- for example, cryo-CMOS at 4~K and superconducting digital logic at 4~K and/or mK stages -- to curb wiring and thermal-load overheads. This review distills key requirements, surveys representative room-temperature and cryogenic approaches, and provides a transparent first-order accounting framework for cryoelectronics. Using an RSA-2048-scale benchmark as a concrete reference point, we illustrate how scaling targets motivate constraints on multiplexing and stage-wise cryogenic power, and discuss implications for functional partitioning across room-temperature electronics, cryo-CMOS, and superconducting logic.

</details>


### [86] [Cavity-Driven Multispectral Gain for High-Sensitivity NV Center Magnetometers](https://arxiv.org/abs/2601.03975)
*Himanshu Kumar,Rahul Gupta,Saikat Ghosh,Himadri Shekhar Dhar,Kasturi Saha*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We report a cavity-enabled solid-state magnetometer based on an NV ensemble coupled with a dielectric cavity, achieving 12 pT/$\sqrt{\rm{Hz}}$ sensitivity and a nearly threefold gain from multispectral features. The features originate from cavity-induced splitting of the NV hyperfine levels and leverages robust quantum coherence in the doubly dressed states of the system to achieve high sensitivity. We project simulated near-term sensitivities approaching 100 fT/$\sqrt{\rm{Hz}}$, close to the Johnson-Nyquist limit. Our results establish frequency multiplexing as a new operational paradigm, offering a robust and scalable quantum resource for metrology under ambient conditions.

</details>


### [87] [Limitations for adaptive quantum state tomography in the presence of detector noise](https://arxiv.org/abs/2601.04020)
*Adrian Skasberg Aasen,Martin Gärttner*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Assumption-free reconstruction of quantum states from measurements is essential for benchmarking and certifying quantum devices, but it remains difficult due to the extensive measurement statistics and experimental resources it demands. An approach to alleviating these demands is provided by adaptive measurement strategies, which can yield up to a quadratic improvement in reconstruction accuracy for pure states by dynamically optimizing measurement settings during data acquisition. A key open question is whether these asymptotic advantages remain in realistic experiments, where readout is inevitably noisy. In this work, we analyze the impact of readout noise on adaptive quantum state tomography with readout-error mitigation, focusing on the challenging regime of reconstructing pure states using mixed-state estimators. Using analytical arguments based on Fisher information optimization and extensive numerical simulations using Bayesian inference, we show that any nonzero readout noise eliminates the asymptotic quadratic scaling advantage of adaptive strategies. We numerically investigate the behavior for finite measurement statistics for single- and two-qubit systems with exact readout-error mitigation and find a gradual transition from ideal to sub-optimal scaling. We furthermore investigate realistic scenarios where detector tomography is performed with a limited number of state copies for calibration, showing that insufficient detector characterization leads to estimator bias and limited reconstruction accuracy. Although our result imposes an upper bound on the reconstruction accuracy that can be achieved with adaptive strategies, we nevertheless observe numerically a constant-factor gain in reconstruction accuracy, which becomes larger as the readout noise decreases. This indicates potential practical benefits in using adaptive measurement strategies in well-calibrated experiments.

</details>


### [88] [Phase-Randomized Laser Pulse Generation at 10 GHz for Quantum Photonic Applications](https://arxiv.org/abs/2601.04031)
*Yuen San Lo,Adam H. Brzosko,Peter R. Smith,Robert I. Woodward,Davide G. Marangon,James F. Dynes,Sergio Juárez,Taofiq K. Paraïso,R. Mark Stevenson,Andrew J. Shields*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gain-switching laser diodes is a well-established technique for generating optical pulses with random phases, where the quantum randomness arises naturally from spontaneous emission. However, the maximum switching rate is limited by phase diffusion: at high repetition rates, residual photons in the cavity seed subsequent pulses, leading to phase correlations, which degrade randomness. We present a method to overcome this limitation by employing an external source of spontaneous emission in conjunction with the laser. Our results show that this approach effectively removes interpulse phase correlations and restores phase randomization at repetition rates as high as 10 GHz. This technique opens new opportunities for high-rate quantum key distribution and quantum random number generation.

</details>


### [89] [Surface Optimization of Aluminum Resonators for Robust Quantum Device Fabrication](https://arxiv.org/abs/2601.04082)
*Simon J. K. Lang,Ignaz Eisele,Alwin Maiwald,Emir Music,Luis Schwarzenbach,Carla Morán-Guizán,Johannes Weber,Daniela Zahn,Thomas Mayer,Rui N. Pereira,Christoph Kutter*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Aluminum remains the central material for superconducting qubits, and considerable effort has been devoted to optimizing its deposition and patterning for quantum devices. However, while post-processing of Nb- and Ta-based resonators has been widely explored, primarily focusing on oxide removal using buffered oxide etch (BOE), post-treatment strategies for Al resonators remain underdeveloped. This challenge becomes particularly relevant for industry-scale fabrication with multichip bonding, where delays between sample preparation and cooldown require surface treatments that preserve low dielectric loss during extended exposure to ambient conditions. In this work, we investigate surface modification approaches for Al resonators subjected to a 24-hour delay prior to cryogenic measurement. Passivation using self-limiting oxygen and fluorine chemistries was evaluated utilizing different plasma processes. Remote oxygen plasma treatment reduced dielectric losses, in contrast to direct plasma, likely due to additional ashing of residual resist despite the formation of a thicker oxide layer on both Si and Al surfaces. A fluorine-based plasma process was developed that passivated the Al surface with fluorine for subsequent BOE treatment. However, increasing fluorine incorporation in the aluminum oxide correlated with higher loss, identifying fluorine as an unsuitable passivation material for Al resonators. Finally, selective oxide removal using HF vapor and phosphoric acid was assessed for surface preparation. HF vapor selectively etched SiO2 while preserving Al2O3, whereas phosphoric acid exhibited the opposite selectivity. Sequential application of both etches yielded dielectric losses as low as $δ_\mathrm{LP} = 5.2 \times 10^{-7}$ ($Q\mathrm{i} \approx 1.9\,\mathrm{M}$) in the single photon regime, demonstrating a promising pathway for robust Al-based resonator fabrication.

</details>


### [90] [Below-shot-noise capacity in phase estimation using nonlinear interferometers](https://arxiv.org/abs/2601.04139)
*Cristofero Oglialoro,Gerard J. Machado,Felix Farsch,Daniel F. Urrego,Alejandra A. Padilla,Raj B. Patel,Ian A. Walmsley,Markus Gräfe,Juan P. Torres,Enno Giese*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Over the past decade, several schemes for imaging and sensing based on nonlinear interferometers have been proposed and demonstrated experimentally. These interferometers exhibit two main advantages. First, they enable probing a sample at a chosen wavelength while detecting light at a different wavelength with high efficiency (bicolor quantum imaging and sensing with undetected light). Second, they can show quantum-enhanced sensitivities below the shot-noise limit, potentially reaching Heisenberg-limited precision in parameter estimation. Here, we compare three quantum-imaging configurations using only easily accessible intensity-based measurements for phase estimation: a Yurke-type SU(1,1) interferometer, a Mandel-type induced-coherence interferometer, and a hybrid scheme that continuously interpolates between them. While an ideal Yurke interferometer can exhibit Heisenberg scaling, this advantage is known to be fragile under realistic detection constraints and in the presence of loss. We demonstrate that differential intensity detection in the Mandel interferometer provides the highest and most robust phase sensitivity among the considered schemes, reaching but not surpassing the shot-noise limit, even in the presence of loss. Intensity measurements in a Yurke-type configuration can achieve genuine sub-shot-noise sensitivity under balanced losses and moderate gain; however, their performance degrades in realistic high-gain regimes. Consequently, in this regime, the Mandel configuration with differential detection outperforms the Yurke-type setup and constitutes the most robust approach for phase estimation.

</details>


### [91] [Improved Lower Bounds for Learning Quantum Channels in Diamond Distance](https://arxiv.org/abs/2601.04180)
*Aadil Oufkir,Filippo Girardi*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We prove that learning an unknown quantum channel with input dimension $d_A$, output dimension $d_B$, and Choi rank $r$ to diamond distance $\varepsilon$ requires $ Ω\!\left( \frac{d_A d_B r}{\varepsilon \log(d_B r / \varepsilon)} \right)$ queries. This improves the best previous $Ω(d_A d_B r)$ bound by introducing explicit $\varepsilon$-dependence, with a scaling in $\varepsilon$ that is near-optimal when $d_A=rd_B$ but not tight in general. The proof constructs an ensemble of channels that are well-separated in diamond norm yet admit Stinespring isometries that are close in operator norm.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [92] [Critical aging and relaxation dynamics in long-range systems](https://arxiv.org/abs/2601.03355)
*Valerio Pagni,Friederike Ihssen,Nicolò Defenu*

Main category: cond-mat.stat-mech

TL;DR: 研究了长程O(N)模型在突然淬火到临界温度后的动力学标度，利用泛函重整化群方法分析了短时老化和长时弛豫行为，并展示了长程相互作用对临界热机性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 理解长程相互作用下O(N)模型在临界淬火后的非平衡动力学行为，并改进微扰理论预测的不足。

Method: 采用泛函重整化群方法，结合与蒙特卡洛模拟及大N极限的对比验证结果。

Result: 获得了N、σ和d依赖的动力学标度行为，结果优于微扰理论预测，并发现长程系统可提高临界热机性能。

Conclusion: 长程O(N)模型在临界淬火后表现出丰富的动力学行为，且长程相互作用有助于增强临界热机的性能。

Abstract: We study the dynamical scaling of long-range $\mathrm{O}(N)$ models after a sudden quench to the critical temperature, using the functional renormalization group approach. We characterize both short-time aging and long-time relaxation as a function of the symmetry index $N$, the interaction range decay exponent $σ$ and the dimension $d$. Our results substantially improve on perturbative predictions, as demonstrated by benchmarks against Monte Carlo simulations and the large-$N$ limit. Finally, we demonstrate that long-range systems increase the performance of critical heat engines with respect to a local active medium.

</details>


### [93] [The Zubarev Double Time Greens function-A Vintage Many Body Technique](https://arxiv.org/abs/2601.03439)
*Vijay Singh,Shraddha Singh*

Main category: cond-mat.stat-mech

TL;DR: 本文介绍了Zubarev于1960年提出的双时间格林函数方法，这是一种强大的多体技术，曾被Hubbard和Roth等物理学家广泛使用。文章以教学方式讲解该方法，并应用于非相互作用的电子和玻色气体，进一步推广到Hubbard模型，推导出铁磁性的Stoner判据，并指出其易于扩展至超导等问题。


<details>
  <summary>Details</summary>
Motivation: 旨在以易于理解的方式介绍Zubarev双时间格林函数方法，使仅具备二次量子化基础知识的读者也能掌握这一在凝聚态物理中广泛应用的重要多体技术。

Method: 采用Zubarev的双时间格林函数方法，结合二次量子化框架，对非相互作用粒子系统及Hubbard模型进行分析，并推导相关物性判据。

Result: 成功应用该方法于非相互作用电子和玻色气体系统；在Hubbard模型下得出Stoner铁磁性判据；展示了该方法在处理超导等多体问题中的可扩展性。

Conclusion: Zubarev双时间格林函数方法是一种系统且灵活的多体理论工具，适用于多种凝聚态物理问题，具有良好的教学价值和实际应用前景。

Abstract: These lecture notes present a comprehensive and powerful many-body technique pioneered in 1960 by D. N. Zubarev. The technique, known as the Zubarev Double Time Greens Function method, was used extensively by leading solid state physicists such as John Hubbard and Laura Roth in the 1960s. We present the technique and apply it to the non-interacting electron and boson gas. We next consider the (many-body) Hubbard model and show how it yields the Stoner criterion for ferromagnetism. It is easily extendable to superconductivity and related problems. Our treatment is pedagogical and understandable to those with just an elementary understanding of second quantization.

</details>


### [94] [Interplay of activity and non-reciprocity in tracer dynamics: From non-equilibrium fluctuation-dissipation to giant diffusion](https://arxiv.org/abs/2601.03591)
*Subhajit Paul,Debasish Chaudhuri*

Main category: cond-mat.stat-mech

TL;DR: 研究了非互易相互作用下示踪粒子在活性或惰性环境中的动力学，推导出包含记忆效应的广义朗之万方程，发现有效扩散系数随非互易性非单调变化，并在中间值处发散，导致巨扩散现象，为活性软物质中的增强输运提供了普适机制。


<details>
  <summary>Details</summary>
Motivation: 理解非互易相互作用如何影响活性与被动系统中的输运行为，揭示非平衡态下的动力学特性。

Method: 从全系统的微观随机动力学出发，推导出示踪粒子的过阻尼广义朗之万方程，包含描述浴场介导关联的记忆核，并据此计算速度与位移响应、广义非平衡涨落-耗散关系及均方位移（MSD）。

Result: 均方位移最终呈扩散行为，但有效扩散系数随非互易程度非单调变化，并在某一中间值处发散，出现巨扩散现象；数值模拟验证了理论预测。

Conclusion: 非互易相互作用可通过巨扩散机制显著增强活性软物质中的输运，该结果对具有追逃或捕食关系的生物系统具有直接意义。

Abstract: Non-reciprocal interactions play a key role in shaping transport in active and passive systems, giving rise to striking nonequilibrium behavior. Here, we study the dynamics of a tracer -- active or passive -- embedded in a bath of active or passive particles, coupled through non-reciprocal interactions. Starting from the microscopic stochastic dynamics of the full system, we derive an overdamped generalized Langevin equation for the tracer, incorporating a non-Markovian memory kernel that captures bath-mediated correlations. This framework enables us to compute the tracer's velocity and displacement response, derive a generalized nonequilibrium fluctuation-dissipation relation that quantifies deviations from equilibrium behavior, and determine the mean-squared displacement (MSD). We find that while the MSD becomes asymptotically diffusive, the effective diffusivity depends non-monotonically on the degree of non-reciprocity and diverges at an intermediate value. This regime of giant diffusivity provides a generic mechanism for enhanced transport in active soft matter and has direct implications for biological systems exhibiting chase-and-run or predator-prey interactions. Our analytical predictions are supported by numerical simulations of active Brownian particles, highlighting experimentally accessible signatures of non-reciprocal interactions in soft matter.

</details>


### [95] [Counterexamples to the conjectured ordering between the waiting-time bound and the thermodynamic uncertainty bound on entropy production](https://arxiv.org/abs/2601.04039)
*Jie Gu*

Main category: cond-mat.stat-mech

TL;DR: 本文研究了连续时间马尔可夫跳跃过程中两种广泛使用的稳态熵产生率下界——热力学不确定性关系（TUR）和等待时间分布（WTD）下界之间的关系。作者通过构造四个状态的反例，证明在非平衡稳态下，当基于相同的部分观测链路时，WTD下界可能小于TUR下界，从而反驳了此前认为WTD始终更紧的猜想，表明这两种推断界限之间不存在普适的序关系。


<details>
  <summary>Details</summary>
Motivation: 探讨在部分观测条件下，TUR与WTD两种熵产生率下界之间是否存在普遍的紧致性排序关系。

Method: 通过构建非平衡稳态下的四态马尔可夫模型反例，并比较由相同部分观测链路导出的σ_ℒ与σ_TUR值。

Result: 发现存在σ_ℒ < σ_TUR的情况，表明WTD下界并不总是比TUR下界更紧。

Conclusion: 在部分观测条件下，TUR与WTD下界之间不存在普适的紧致性顺序，两种界限的相对大小依赖于具体系统参数和动力学结构。

Abstract: Two widely used model-free lower bounds on the steady-state entropy production rate of a continuous-time Markov jump process are the thermodynamic uncertainty relation (TUR) bound $σ_\text{TUR}$, derived from the mean and variance of a current, and the waiting-time distribution (WTD) bound $σ_\mathcal{L}$, derived from the irreversibility of partially observed transition sequences together with their waiting times. It has been conjectured that $σ_{\mathcal L}$ is always at least as tight as $σ_{\mathrm{TUR}}$ when both are constructed from the same partially observed link. Here we provide four-state counterexamples in a nonequilibrium steady state where $σ_{\mathcal L}<σ_{\mathrm{TUR}}$. This result shows that no universal ordering exists between these two inference bounds under partial observation.

</details>


### [96] [Random knotting in very long off-lattice self-avoiding polygons](https://arxiv.org/abs/2601.04102)
*Jason Cantarella,Tetsuo Deguchi,Henrik Schumacher,Clayton Shonkwiler,Erica Uehara*

Main category: cond-mat.stat-mech

TL;DR: 通过使用无格点自避多边形的珠链模型和Clisby的树数据结构，研究了随机多边形中素纽结加项的数量分布，发现其符合泊松分布，并精确估计了纽结特征长度。


<details>
  <summary>Details</summary>
Motivation: 探索随机多边形中纽结形成的统计规律及其与多边形大小的关系，验证纽结局域化和纽结熵猜想。

Method: 采用Clisby的树数据结构和无标度枢轴算法生成不同尺寸的多边形样本，结合新的纽结图简化和不变量无关分类方法确定每个多边形的精确纽结类型。

Result: 发现素纽结加项数量极好地符合泊松分布；估计纽结特征长度为656500±2500；大n下的加项计数提高了纽结速率和概率振幅比的测量精度。

Conclusion: 结果支持纽结局域化现象和纽结熵猜想，且与之前的格点计算结果吻合良好。

Abstract: We present experimental results on knotting in off-lattice self-avoiding polygons in the bead-chain model. Using Clisby's tree data structure and the scale-free pivot algorithm, for each $k$ between $10$ and $27$ we generated $2^{43-k}$ polygons of size $n=2^k$. Using a new knot diagram simplification and invariant-free knot classification code, we were able to determine the precise knot type of each polygon. The results show that the number of prime summands of knot type $K$ in a random $n$-gon is very well described by a Poisson distribution. We estimate the characteristic length of knotting as $656500 \pm 2500$. We use the count of summands for large $n$ to measure knotting rates and amplitude ratios of knot probabilities more accurately than previous experiments. Our calculations agree quite well with previous on-lattice computations, and support both knot localization and the knot entropy conjecture.

</details>


### [97] [Universality in driven systems with a multiply-degenerate umbilic point](https://arxiv.org/abs/2601.04116)
*Johannes Schmidt,Žiga Krajnik,Vladislav Popkov*

Main category: cond-mat.stat-mech

TL;DR: 研究了多车道非对称排斥过程中的驱动粒子系统，在保持每车道粒子数守恒且稳态完全无关联的情况下，发现从三车道起存在一个特征速度退化的脐形流形，并观察到具有z=3/2动力学指数的普适动力学行为，揭示了一类新的动力学普适类。


<details>
  <summary>Details</summary>
Motivation: 探索在高维退化条件下（多个模式特征速度重合）的弱双曲系统中，是否存在新的普适性标度行为，特别是在稳态时空涨落中的动力学特性。

Method: 构建多车道模型在脐形子空间内的有效模耦合理论（MCT），并通过蒙特卡洛模拟验证理论预测，分析稳态下的空间-时间涨落和标度函数。

Result: 发现稳健的z=3/2动力学指数；脐形模式的标度函数在不同相互作用参数下呈现普适形状；非退化模式的动力学行为可通过有效MCT解析预测，仅依赖非普适缩放因子。

Conclusion: 存在一类新的普适类，其动力学指数为3/2，出现在具有相等特征速度的长寿命流体动力学模式中，拓展了对退化系统中标度律的理解。

Abstract: We investigate a driven particle system, a multilane asymmetric exclusion process, where the particle number in every lane is conserved, and stationary state is fully uncorrelated. The phase space has, starting from three lanes and more, an umbilic manifold where characteristic velocities of all the modes but one coincide, thus allowing us to study a weakly hyperbolic system with arbitrarily large degeneracy. We then study space-time fluctuations in the steady state, at the umbilic manifold, which are expected to exhibit universal scaling features. We formulate an effective mode-coupling theory (MCT) for the multilane model within the umbilic subspace and test its predictions. Unlike in the bidirectional two-lane model with an umbilic point studied earlier, here we find a robust $z=3/2$ dynamical exponent for the umbilic mode. The umbilic scaling function, obtained from Monte-Carlo simulations, for the simplest 3-lane scenario, appears to have an universal shape for a range of interaction parameters. Remarkably, the shape and dynamic exponent of the non-degenerate mode can be analytically predicted on the base of effective MCT, up to non-universal scaling factor. Our findings suggest the existence of novel universality classes with dynamical exponent $3/2$, appearing in long-lived hydrodynamic modes with equal characteristic velocities.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [98] [A disease-spread model on hypergraphs with distinct droplet and aerosol transmission modes](https://arxiv.org/abs/2601.03563)
*Tung D. Nguyen,Mason A. Porter*

Main category: physics.soc-ph

TL;DR: 该论文研究了在超图上具有两种传播模式的传染病（如呼吸道病毒）的传播机制，分别通过成对交互和群体交互进行传播，并推导出疾病是否成为地方病的阈值条件。


<details>
  <summary>Details</summary>
Motivation: 为了更全面地理解传染病的传播机制，尤其是同时存在成对接触和群体环境暴露两种传播方式的情况，需要建立能够反映这两种交互模式的模型。

Method: 采用易感-感染-易感（SIS）模型在超图上建模，区分边（大小为2的超边）上传播的大飞沫和大小至少为3的超边上传播的气溶胶；推导了两类超图上的平均场近似模型，并进行了数值模拟。

Result: 得到了决定疾病消亡或持续存在的阈值条件，数值模拟揭示了超边大小、超边大小分布以及恢复率对疾病传播动态的影响。

Conclusion: 群体交互（如封闭环境中的气溶胶传播）在疾病传播中起重要作用，忽略这些因素可能导致对流行风险的低估。

Abstract: We examine the spread of an infectious disease, such as one that is caused by a respiratory virus, with two distinct modes of transmission. To do this, we consider a susceptible--infected--susceptible (SIS) disease on a hypergraph, which allows us to incorporate the effects of both dyadic (i.e., pairwise) and polyadic (i.e., group) interactions on disease propagation. This disease can spread either via large droplets through direct social contacts, which we associate with edges (i.e., hyperedges of size 2), or via infected aerosols in the environment through hyperedges of size at least 3 (i.e., polyadic interactions). We derive mean-field approximations of our model for two types of hypergraphs, and we obtain threshold conditions that characterize whether the disease dies out or becomes endemic. Additionally, we numerically simulate our model and a mean-field approximation of it to examine the impact of various factors, such as hyperedge size (when the size is uniform), hyperedge-size distribution (when the sizes are nonuniform), and hyperedge-recovery rates (when the sizes are nonuniform) on the disease dynamics.

</details>


### [99] [Multi-Dimensional Opinion Formation](https://arxiv.org/abs/2601.04074)
*Hanna Bartel,Martin Burger,Marie-Therese Wolfram*

Main category: physics.soc-ph

TL;DR: 提出并研究了一个多维舆论动力学模型，其中个体的舆论变化通过二元交互和基于完整舆论向量加权相似性的耦合机制实现，最终意见结构由个体对各议题的重要性权重决定。


<details>
  <summary>Details</summary>
Motivation: 为了更真实地模拟现实社会中人们在多个议题上的意见演化过程，考虑了不同议题对个体的不同重要性影响。

Method: 构建了一个包含意见和重要性权重的多维舆论动力学模型，通过二元交互中的加权相似性机制描述意见变化，并推导出相应的动力学方程（包括 kinetic 方程和均场偏微分方程）。

Result: 分析计算和数值模拟表明，该模型能够生成复杂的稳态结构，且最终的意见分布强烈依赖于个体对各议题的权重分配。

Conclusion: 个体在多议题舆论演化中的相对权重显著影响最终共识或分歧的形成，说明议题重要性结构在社会意见动态中起关键作用。

Abstract: In this paper we propose and investigate a multi-dimensional opinion dynamics model where people are characterised by both opinions and importance weights across these opinions. Opinion changes occur through binary interactions, with a novel coupling mechanism: the change in one topic depends on the weighted similarity across the full opinion vector. We state the kinetic equation for this process and derive its mean-field partial differential equation to describe the overall dynamics. Analytical computations and numerical simulations confirm that this model generates complex stationary states, and we demonstrate that the final opinion structures are critically determined by the peoples' opinion weights.

</details>
