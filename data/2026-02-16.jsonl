{"id": "2602.12808", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.12808", "abs": "https://arxiv.org/abs/2602.12808", "authors": ["Paride Crisafulli", "Angel del Río Mangada", "Juan José Segura Sampedro", "Claudio R. Mirasso", "Raúl Toral", "Tobias Galla"], "title": "Forecasting emergency department visits in the reference hospital of the Balearic Islands: the role of tourist and weather data", "comment": null, "summary": "Accurate forecasting of patient arrivals at emergency departments (EDs) is vital for efficient resource allocation and high-quality patient care. In this study we investigate the relevance of exogenous variables, namely tourism, weather, calendar and demographic variables, in forecasting ED visits in the reference hospital in Palma de Mallorca, a city with significant seasonal population fluctuations due to tourism. Using a machine learning approach, we develop a model that predicts ED visits based solely on these exogenous variables. We test different machine learning algorithms (random forests, support vector machines, and feedforward neural networks) with different combinations of input variables and compare their symmetric mean average percentage errors (SMAPEs). Our findings reveal that calendar information, resident, and tourist population data are statistically significant for the accuracy of the predictions, while the addition of weather data does not provide any further improvement. Comparison of non-time-series with time-series prediction models reveals that the latter provide better accuracy for short prediction horizons (e.g. shorter than a week). Furthermore, time-series models become less or equally accurate to models relying only on exogenous variables for long prediction horizons (e.g. fortnight or month). Our study highlights the importance of carefully selecting predictive variables to ensure short- and long-term, robust and reliable forecasts. This demonstrates that, despite their lower complexity, non-time-series models with well-chosen input variables can be as effective as time-series models when predicting for long time horizons."}
{"id": "2602.13038", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.13038", "abs": "https://arxiv.org/abs/2602.13038", "authors": ["Guo-Shiuan Lin", "Denise Hertwig", "Megan McGrory", "Tiancheng Ma", "Stefán Thor Smith", "Maider Llaguno-Munitxa", "Sue Grimmond", "Gabriele Manoli"], "title": "Modelling human activities in a system of cities", "comment": null, "summary": "Cities host most of the world population with diverse services and activities. One key challenge in urban modelling is the quantification of intra- and inter-city mobility patterns and the associated space-time dynamics of population density and anthropogenic activities. To address this, we apply the novel agent-based urban model DAVE (Dynamic Anthropogenic actiVities and feedback to Emissions) to simulate population behaviour and mobility in the Vaud and Geneva Cantons, a system of small- to medium-size cities in Switzerland. Simulation results provide detailed temporal (10 min) and spatial (500 m) population dynamics for different age groups and day types. DAVE further models the time-varying population distribution in 11 different microenvironments (e.g., home, work, leisure, outdoor) and the travel flows by different modes. Simulation results align with observations, confirming the possibility of driving urban system modelling with statistical information on residents' behaviour. Sustainability and health indicators like daily driving distance and walking time for each neighbourhood are also reflected by the model with urban-rural gradients displayed. This work serves as a foundation for future applications of DAVE to study bottom-up human-built environment interactions, from anthropogenic emissions and building energy to urban climate, exposure, and health in cities around the world."}
{"id": "2602.12881", "categories": ["cs.SI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12881", "abs": "https://arxiv.org/abs/2602.12881", "authors": ["Oktay Karakuş"], "title": "Semantic Communities and Boundary-Spanning Lyrics in K-pop: A Graph-Based Unsupervised Analysis", "comment": null, "summary": "Large-scale lyric corpora present unique challenges for data-driven analysis, including the absence of reliable annotations, multilingual content, and high levels of stylistic repetition. Most existing approaches rely on supervised classification, genre labels, or coarse document-level representations, limiting their ability to uncover latent semantic structure. We present a graph-based framework for unsupervised discovery and evaluation of semantic communities in K-pop lyrics using line-level semantic representations. By constructing a similarity graph over lyric texts and applying community detection, we uncover stable micro-theme communities without genre, artist, or language supervision. We further identify boundary-spanning songs via graph-theoretic bridge metrics and analyse their structural properties. Across multiple robustness settings, boundary-spanning lyrics exhibit higher lexical diversity and lower repetition compared to core community members, challenging the assumption that hook intensity or repetition drives cross-theme connectivity. Our framework is language-agnostic and applicable to unlabeled cultural text corpora."}
{"id": "2602.12337", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12337", "abs": "https://arxiv.org/abs/2602.12337", "authors": ["Shun Li", "Yan Jiang", "Mengping Zhang", "Tao Xiong"], "title": "Temporal-Stability-Enhanced and Energy-Stable Dynamical Low-Rank Approximation for Multiscale Linear Kinetic Transport Equations", "comment": "29 pages, 29 figures, 1 table, 40 references", "summary": "In this paper, we develop an asymptotic-preserving dynamical low-rank method for the multiscale linear kinetic transport equation. The proposed scheme is unconditionally stable in the diffusive regime while preserving the correct asymptotic behavior, and can achieve significant reductions in computational cost through a low-rank representation and large time step stability. A low-rank formulation consistent with the discrete energy is introduced under the discrete ordinates discretization, and energy stability of the resulting scheme is established. Numerical experiments confirm the energy stability and demonstrate that the method is efficient while maintaining accuracy across different regimes and capturing the correct asymptotic limits."}
{"id": "2602.12770", "categories": ["q-fin.CP", "q-fin.PR", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2602.12770", "abs": "https://arxiv.org/abs/2602.12770", "authors": ["Dohyun Ahn", "Agostino Capponi"], "title": "Efficient Monte Carlo Valuation of Corporate Bonds in Financial Networks", "comment": null, "summary": "Valuing corporate bonds in systemic economies is challenging due to intricate webs of inter-institutional exposures. When a bank defaults, cascading losses propagate through the network, with payments determined by a system of fixed-point equations lacking closed-form solutions. Standard Monte Carlo methods cannot capture rare yet critical default events, while existing rare-event simulation techniques fail to account for higher-order network effects and scale poorly with network size. To overcome these challenges, we propose a novel approach -- Bi-Level Importance Sampling with Splitting -- and characterize individual bank defaults by decoupling them from the network's complex fixed-point dynamics. This separation enables a two-stage estimation process that directly generates samples from the banks' default events. We demonstrate theoretically that the method is both scalable and asymptotically optimal, and validate its effectiveness through numerical studies on empirically observed networks."}
{"id": "2602.12283", "categories": ["eess.SY", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12283", "abs": "https://arxiv.org/abs/2602.12283", "authors": ["Shunsei Yamagishi", "Lei Jing"], "title": "A Lightweight Cubature Kalman Filter for Attitude and Heading Reference Systems Using Simplified Prediction Equations", "comment": null, "summary": "Attitude and Heading Reference Systems (AHRSs) are broadly applied wherever reliable orientation and motion sensing is required. In this paper, we present an improved Cubature Kalman Filter (CKF) with lower computational cost while maintaining estimation accuracy, which is named \"Kaisoku Cubature Kalman Filter (KCKF)\". The computationally efficient equations of the KCKF are derived by simplifying those of the CKF, while preserving equivalent mathematical relations. The lightweight prediction equations in the KCKF are derived by expanding the summation terms in the CKF and simplifying the result. This paper shows that the KCKF requires fewer floating-point operations (FLOPs) than the CKF. The controlled experimental results show that the KCKF reduces the computation time by approximately 19% compared to the CKF on a high-performance computer, whereas the KCKF reduces the computation time by approximately 15% compared to the CKF on a low-cost single-board computer. In addition, the KCKF maintains the attitude estimation accuracy of the CKF."}
{"id": "2602.12340", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.12340", "abs": "https://arxiv.org/abs/2602.12340", "authors": ["Pinaki Singha", "Nilanjan Roy", "Marcin Szyniszewski", "Auditya Sharma"], "title": "Controlled Zeno-Induced Localization of Free Fermions in a Quasiperiodic Chain", "comment": "19 pages, 4 figures, 1 table", "summary": "We investigate measurement-induced localization in a continuously monitored one-dimensional Aubry--André--Harper model, focusing on the quantum Zeno regime in which the measurements dominate coherent dynamics. The presence of a quasiperiodic potential renders the problem analytically tractable and enables a controlled study of the interplay between monitoring and disorder. We develop an analytical description based on an instantaneous Schrödinger equation with a measurement-induced effective potential constructed self-consistently from individual quantum trajectories, without relying on postselection. In the quantum Zeno regime, an emergent dominant energy scale reduces the problem to a transfer-matrix formulation of an effective non-Hermitian Hamiltonian, which allows direct computation of the Lyapunov exponent. Complementarily, we extract the localization length numerically from long-time steady-state quantum state diffusion trajectories by reconstructing the intrinsic localized single-particle wave functions and analyzing their spatial decay. These numerical results show quantitative agreement with the effective theory predictions, with controlled corrections of order $J^2/[λ^2+(γ/2)^2]$ (where $J$ is the hopping amplitude, $γ$ the measurement strength, and $λ$ the quasiperiodic potential). Our results underscore the connection between the effective non-Hermitian description and the stochastic monitored dynamics, showing the interplay between Zeno-like localization, coherent hopping, and quasiperiodic-disorder-induced localization, while also laying the groundwork for understanding and exploiting measurement-induced localization as a tool for quantum control and state preparation."}
{"id": "2602.12328", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2602.12328", "abs": "https://arxiv.org/abs/2602.12328", "authors": ["Joydeep Majhi", "Biplab Pal"], "title": "Compact localized states and magnetic flux-driven topological phase transition in a diamond-dodecagon lattice geometry", "comment": "12 pages, 8 (7+1) figures; Comments are welcome", "summary": "We propose and investigate a novel two-dimensional (2D) tight-binding model defined on a diamond-dodecagon lattice geometry that hosts multiple flat bands (FBs) and supports topological phase transitions driven by a magnetic flux. This lattice exhibits three completely flat, non-dispersive bands in the band structure in the absence of magnetic flux due to destructive interference in the electron hoppings, leading to the emergence of compact localized states (CLS). These CLS are analytically constructed and exhibit real-space confinement of the electrons, arising solely due to the lattice's geometrical frustration. It has been shown that these FBs are very robust against the introduction of weak random onsite disorder in the system. By tuning the uniform magnetic flux threaded through the diamond plaquettes, we demonstrate a tunable evolution of the band structure and show that certain bands develop nontrivial topological features with nonzero integer values of the Chern number. Additionally, we have computed the multi-terminal transport properties for this 2D lattice system, which display the flux-tunable resonances and transmission suppression linked to the FBs, establishing a clear interplay between the localization, topology, and transport. Our findings put forward the diamond-dodecagon lattice as a robust and tunable platform for studying the flat-band physics and magnetic flux-controlled topological phenomena, offering promising experimental feasibility in photonic lattices and ultracold atomic systems."}
{"id": "2602.12694", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.12694", "abs": "https://arxiv.org/abs/2602.12694", "authors": ["Jeremy A. McCulloch", "Scott L. Delp", "Ellen Kuhl"], "title": "Discovering the mechanics of ultra-low density elastomeric foams in elite-level racing shoes", "comment": "19 pages, 8 figures", "summary": "Ultra-low-density elastomeric foams enable lightweight systems that combine high compliance with efficient energy return. In high-performance racing shoes, these foams are critical for low weight, high cushioning, and efficient energy return; yet, their constitutive behavior remains difficult to model and poorly understood. Here we integrate mechanical testing and machine learning to discover the mechanics of two ultra-low density elastomeric polymeric foams used in elite-level racing shoes. Across uniaxial tension, confined and unconfined compression, and simple shear, both foams exhibit pronounced tension-compression asymmetry, negligible lateral strains consistent with an effective Poisson's ratio close to zero, and low hysteresis indicative of an efficient energy return. Both foams provide a similar compressive stiffness (268kPa vs. 299kPa), while one foam exhibits nearly double the shear stiffness (219kPa vs. 117kPa), implying a substantially greater lateral stability at a comparable vertical energy return (83% vs. 89%). By integrating these data into constitutive neural networks, paired with sparse regression, we discover compact, interpretable single-invariant models, supplemented by mixed-invariant or principal-stretch based terms, that capture the unique signature of the foams with R2 values close to one. From a human performance perspective, these models enable finite-element and gait-level simulations of high-performance racing shoes to quantify running economy, performance enhancements, and injury risks on an individual athlete level. More broadly, this work establishes a scalable and interpretable approach for constitutive modeling of highly compressible, ultra-light elastomeric foams with applications to wearable technologies, soft robotics, and energy-efficient mobility systems."}
{"id": "2602.12435", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.12435", "abs": "https://arxiv.org/abs/2602.12435", "authors": ["Samantha Shi-Jun", "Bo Li"], "title": "Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere", "comment": null, "summary": "We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event."}
{"id": "2602.12293", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12293", "abs": "https://arxiv.org/abs/2602.12293", "authors": ["Ayrton Almada", "Laurent Pagnier", "Igal Goldshtein", "Saif R. Kazi", "Michael", "Chertkov"], "title": "Real-Time Dynamic N-1 Screening: Identifying High-Risk Lines and Transformers After Common Faults", "comment": "9 pages, 7 figures, 3 Tables, 1 Algorithm. arXiv admin note: substantial text overlap with arXiv:2510.18007", "summary": "Power system operators routinely perform N-1 contingency analysis, yet conventional tools provide limited guidance on which lines or transformers deserve heightened attention during fast post-fault transients. In particular, static screening does not reveal whether (1) the same faulted line repeatedly triggers severe downstream overloads, or (2) a specific transformer emerges as vulnerable across many distinct fault scenarios. This paper introduces a real-time dynamic N-1 screening framework that addresses this gap by estimating, for each counterfactual single-phase transmission fault, the probability of transient overcurrent on critical grid elements. The output is an operator-facing dashboard that ranks (a) faulted lines whose outages most frequently lead to dangerous transformer overloads, and (b) transformers that consistently overload across top-risk scenarios, both of which are actionable indicators for real-time situational awareness. The approach models post-fault electromechanical dynamics using a linear stochastic formulation of the swing equations with short-lived, fault-localized uncertainty, and combines analytic transient evaluation with cross-entropy based importance sampling to efficiently estimate rare but high-impact events. All N-1 contingencies are evaluated in parallel with linear computational complexity. The framework is demonstrated on the IEEE 118-bus system, where it reveals latent high-risk lines and transformers that remain invisible under deterministic dynamic or static N-1 analysis. Results show orders-of-magnitude computational speedup relative to brute-force Monte Carlo, enabling practical deployment within real-time operational cycles."}
{"id": "2602.12568", "categories": ["math.ST", "cs.SI", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.12568", "abs": "https://arxiv.org/abs/2602.12568", "authors": ["Anirudh Sridhar", "Arnob Ghosh"], "title": "Finding Super-spreaders in SIS Epidemics", "comment": "6 pages, 3 figures", "summary": "In network epidemic models, controlling the spread of a disease often requires targeted interventions such as vaccinating high-risk individuals based on network structure. However, typical approaches assume complete knowledge of the underlying contact network, which is often unavailable. While network structure can be learned from observed epidemic dynamics, existing methods require long observation windows that may delay critical interventions.\n  In this work, we show that full network reconstruction may not be necessary: control-relevant features, such as high-degree vertices (super-spreaders), can be learned far more efficiently than the complete structure. Specifically, we develop an algorithm to identify such vertices from the dynamics of a Susceptible-Infected-Susceptible (SIS) process. We prove that in an $n$-vertex graph, vertices of degree at least $n^α$ can be identified over an observation window of size $Ω(1/α)$, for any $α\\in (0,1)$. In contrast, existing methods for exact network reconstruction requires an observation window that grows linearly with $n$. Simulations demonstrate that our approach accurately identifies super-spreaders and enables effective epidemic control."}
{"id": "2602.12855", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.12855", "abs": "https://arxiv.org/abs/2602.12855", "authors": ["Moritz Helias", "Javed Lindner", "Lars Schutzeichel", "Zohar Ringel"], "title": "Lecture notes: From Gaussian processes to feature learning", "comment": null, "summary": "These lecture notes develop the theory of learning in deep and recurrent neuronal networks from the point of view of Bayesian inference. The aim is to enable the reader to understand typical computations found in the literature in this field. Initial chapters develop the theoretical tools, such as probabilities, moment and cumulant-generating functions, and some notions of large deviation theory, as far as they are needed to understand collective network behavior with large numbers of parameters. The main part of the notes derives the theory of Bayesian inference for deep and recurrent networks, starting with the neural network Gaussian process (lazy-learning) limit, which is subsequently extended to study feature learning from the point of view of adaptive kernels. The notes also expose the link between the adaptive kernel approach and approaches of kernel rescaling."}
{"id": "2602.12290", "categories": ["physics.comp-ph", "math.AP", "math.NA", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.12290", "abs": "https://arxiv.org/abs/2602.12290", "authors": ["Sarswati Shah", "Gerardo Hernández-Dueñas"], "title": "A new model for two-layer liquid-gas stratified flows in pipes with general cross sections", "comment": "27 pages, 8 figures", "summary": "In this work, we derive a new model for immiscible two-layer gas-liquid stratified flows in pipes with general cross sections. The bottom layer is occupied by an incompressible fluid in liquid phase with hydrodynamics based on a hydrostatic pressure, following a shallow water approximation. The top layer is occupied by a compressible gas, following an ideal gas law leading to conservation of mass, momentum and energy. The two subsystems are linked through non-conservative products, representing momentum and energy exchanges between layers. The hyperbolic properties of the resulting model are analyzed, including the derivation of entropy inequalities, and the approximations of eigenvalues of the corresponding coefficient matrix. Numerical tests are included to demonstrate the merits of the model and the numerical approximations, including well-balancedness, Riemann problems, and perturbations and convergence toward steady states at rest. Besides simulations of water and air where the density difference between layers is significant, a case where such difference is not so pronounced (like gas and liquid hydrogen) is also shown."}
{"id": "2602.13031", "categories": ["nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.13031", "abs": "https://arxiv.org/abs/2602.13031", "authors": ["Yuki Araya", "Hiroaki Ito", "Hiroshi Kori", "Hiroyuki Kitahata"], "title": "Inferring Coupled Stuart-Landau Equations from Waveforms", "comment": "15 pages, 7 figures", "summary": "We present a data-driven framework to infer phase-amplitude equations of coupled limit-cycle oscillators directly from waveform measurements. Exploiting the universality of the Stuart-Landau normal form near a supercritical Hopf bifurcation, we reconstruct a near-identity transformation from two independent observables of an isolated oscillator and infer the intrinsic Stuart-Landau parameters. Using this reconstructed transformation, we then estimate linear coupling coefficients from paired measurements. The method accurately recovers parameters for coupled van der Pol oscillators, providing a quantitative benchmark. Applied to a high-dimensional hydrodynamic system of two coupled collapsible-channel oscillators, the inferred Stuart-Landau model captures bistability between in-phase and anti-phase synchronization and reveals that the anti-phase state is destabilized through a Neimark-Sacker bifurcation. Our approach enables quantitative prediction of synchronization transitions involving amplitude dynamics from experimentally accessible waveform data."}
{"id": "2602.12408", "categories": ["physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2602.12408", "abs": "https://arxiv.org/abs/2602.12408", "authors": ["Jonas Köhler", "Wei Li", "Johannes Faber", "Georg Rümpker", "Nishtha Srivastava"], "title": "Detecting Spatiotemporal b-Value Anomalies with a Progressive Deep Learning Architecture", "comment": null, "summary": "Identifying systematic patterns in seismicity that precede large earthquakes remains a central challenge in statistical seismology. In this work, we present a methodological framework for detecting spatiotemporal anomalies in seismicity using the evolution of gridded b-values. Focusing on the Japanese subduction zone, we construct daily b-value fields on a fine spatial grid by aggregating local seismicity over moving time windows, yielding a continuous 2+1D representation of seismic-state evolution.\n  We formulate the problem as a binary classification task in which spatiotemporal blocks extracted from these $b$-value fields are labeled according to the occurrence of a target earthquake with \\Mw $\\geq 5$ in the central region within the next day. To model this data, we introduce a hybrid deep-learning architecture that combines a spatial convolutional encoder with a temporal convolutional network, enabling joint learning of spatial structure and temporal dynamics. A progressive meta-epoch training scheme is employed, in which the model is iteratively updated using a time-forward strategy that mirrors operational deployment and mitigates issues related to nonstationarity.\n  This paper is strictly methodological in scope. It describes the construction of b-value fields, the spatiotemporal sampling strategy, the network architecture, and the progressive training and internal validation framework used for model development and parameter selection."}
{"id": "2602.12309", "categories": ["quant-ph", "cs.IT", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.12309", "abs": "https://arxiv.org/abs/2602.12309", "authors": ["Mahmoud Saad Abouamer", "Jaron Skovsted Gundersen", "Søren Pilegaard Rasmussen", "Petar Popovski"], "title": "Resource-Adaptive Teleportation Under Imperfect Entanglement: A Code-Puncturing Framework", "comment": "6 pages, 3 figures. Accepted for publication at INFOCOM 2026 Quantum Networked Applications and Protocols (QuNAP). This work was funded by the Danish National Research Foundation (DNRF) through the Center CLASSIQUE", "summary": "Quantum teleportation is a foundational protocol for sending quantum information through entanglement distribution and classical communication. Assuming ideal classical communication, the reliability of quantum teleportation is limited by the fidelity of the shared EPR pairs. This reliability can be improved through two mechanisms: entanglement purification and quantum error correction (QEC). Using both techniques in concert requires flexible QEC rates, since purification alters the structure of errors induced by imperfect-EPR teleportation, and fixed-rate codes cannot be uniformly effective across purification regimes or reliability targets. In this work, we supplement purification with punctured QEC codes, providing a family of code variants that can be adapted to error-channel characteristics and reliability targets. Punctured codes improve teleportation reliability across a broader range of purification regimes, enabling target reliability to be met without hardware-level code switching. This is corroborated by numerical results, showing that different punctured codes achieve the lowest logical error probability in different operating regimes, and that selecting among them reduces logical error relative to fixed-rate encoded teleportation. This reduction relaxes the requirement on the initial EPR fidelity or purification needed to achieve a target reliability. Overall, puncturing enables adaptation to varying entanglement conditions and reliability requirements while reusing a single stabilizer structure."}
{"id": "2602.12497", "categories": ["physics.ao-ph", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.12497", "abs": "https://arxiv.org/abs/2602.12497", "authors": ["Stjepan Marcelja"], "title": "Winter forecasting of September/October rainfall", "comment": null, "summary": "We formulate seasonal rainfall prediction as a reduced-order nonlinear forecasting problem, embedding coupled Indian-Pacific Ocean variability into a low-dimensional state space and projecting it forward using deep neural networks. Variables include Nino 3.4, the Indian Ocean Dipole (IOD), the Indian Ocean meridional SST gradient, and selected empirical orthogonal functions. Monthly time series of the variables then form the input into deep neural networks which project rainfall further into the future. Forecasts for the 2025 austral spring were generated and archived in the Mendeley database during the winter. Subsequent rainfall data demonstrated a high level of agreement with the forecasts, providing a validation of the method and supporting the hypothesis that chaotic yet conditionally predictable dynamics underpin spring rainfall variability in southeastern Australia."}
{"id": "2602.12283", "categories": ["eess.SY", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12283", "abs": "https://arxiv.org/abs/2602.12283", "authors": ["Shunsei Yamagishi", "Lei Jing"], "title": "A Lightweight Cubature Kalman Filter for Attitude and Heading Reference Systems Using Simplified Prediction Equations", "comment": null, "summary": "Attitude and Heading Reference Systems (AHRSs) are broadly applied wherever reliable orientation and motion sensing is required. In this paper, we present an improved Cubature Kalman Filter (CKF) with lower computational cost while maintaining estimation accuracy, which is named \"Kaisoku Cubature Kalman Filter (KCKF)\". The computationally efficient equations of the KCKF are derived by simplifying those of the CKF, while preserving equivalent mathematical relations. The lightweight prediction equations in the KCKF are derived by expanding the summation terms in the CKF and simplifying the result. This paper shows that the KCKF requires fewer floating-point operations (FLOPs) than the CKF. The controlled experimental results show that the KCKF reduces the computation time by approximately 19% compared to the CKF on a high-performance computer, whereas the KCKF reduces the computation time by approximately 15% compared to the CKF on a low-cost single-board computer. In addition, the KCKF maintains the attitude estimation accuracy of the CKF."}
{"id": "2602.12972", "categories": ["cs.SI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12972", "abs": "https://arxiv.org/abs/2602.12972", "authors": ["Siyun Yang", "Shixiao Yang", "Jian Wang", "Di Fan", "Kehe Cai", "Haoyan Fu", "Jiaming Zhang", "Wenjin Wu", "Peng Jiang"], "title": "Jointly Optimizing Debiased CTR and Uplift for Coupons Marketing: A Unified Causal Framework", "comment": null, "summary": "In online advertising, marketing interventions such as coupons introduce significant confounding bias into Click-Through Rate (CTR) prediction. Observed clicks reflect a mixture of users' intrinsic preferences and the uplift induced by these interventions. This causes conventional models to miscalibrate base CTRs, which distorts downstream ranking and billing decisions. Furthermore, marketing interventions often operate as multi-valued treatments with varying magnitudes, introducing additional complexity to CTR prediction.\n  To address these issues, we propose the \\textbf{Uni}fied \\textbf{M}ulti-\\textbf{V}alued \\textbf{T}reatment Network (UniMVT). Specifically, UniMVT disentangles confounding factors from treatment-sensitive representations, enabling a full-space counterfactual inference module to jointly reconstruct the debiased base CTR and intensity-response curves. To handle the complexity of multi-valued treatments, UniMVT employs an auxiliary intensity estimation task to capture treatment propensities and devise a unit uplift objective that normalizes the intervention effect. This ensures comparable estimation across the continuous coupon-value spectrum. UniMVT simultaneously achieves debiased CTR prediction for accurate system calibration and precise uplift estimation for incentive allocation. Extensive experiments on synthetic and industrial datasets demonstrate UniMVT's superiority in both predictive accuracy and calibration. Furthermore, real-world A/B tests confirm that UniMVT significantly improves business metrics through more effective coupon distribution."}
{"id": "2602.12365", "categories": ["math.NA", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.12365", "abs": "https://arxiv.org/abs/2602.12365", "authors": ["Mohit Pundir", "Flavio Lorez", "David S. Kammer"], "title": "A versatile FEM framework with native GPU scalability via globally-applied AD", "comment": null, "summary": "Energy-based finite-element formulations provide a unified framework for describing complex physical systems in computational mechanics. In these energy-based methods, the governing equations can be obtained directly by considering the derivatives of a single global energy functional. While Automatic Differentiation (AD) can be used to automate the generation of these derivatives, current frameworks face a clear trade-off based primarily on the scale upon which the AD method is applied. Globally applied AD offers high expressivity but cannot currently be scaled to large problems. Locally applied AD scales well through traditional assembly methods, but the variety of physics and couplings that the framework can easily represent is more limited than the global approach. Here, we introduce an energy-centric framework tatva (https://github.com/smec-ethz/tatva) that defines the physics of a problem as a single global functional and applies AD globally to generate residual and tangent operators. By leveraging Jacobian-vector products for matrix-free solvers and coloring-based sparse differentiation for materializing sparse tangent stiffness matrices when needed, our flexible design scales linearly with the problem size on GPUs. We demonstrate that our framework can handle large problems (with millions of degrees of freedom) without memory exhaustion. Additionally, it offers a unified, fully differentiable methodology that can address a wide range of problems, including multi-point constraints, mixed-dimensional coupling, and the incorporation of neural networks, while maintaining high performance and scalability on modern GPU architectures."}
{"id": "2602.12288", "categories": ["eess.SY", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12288", "abs": "https://arxiv.org/abs/2602.12288", "authors": ["Xiaowen Tao", "Yinuo Wang", "Haitao Ding", "Yuanyang Qi", "Ziyu Song"], "title": "Energy-Aware Reinforcement Learning for Robotic Manipulation of Articulated Components in Infrastructure Operation and Maintenance", "comment": null, "summary": "With the growth of intelligent civil infrastructure and smart cities, operation and maintenance (O&M) increasingly requires safe, efficient, and energy-conscious robotic manipulation of articulated components, including access doors, service drawers, and pipeline valves. However, existing robotic approaches either focus primarily on grasping or target object-specific articulated manipulation, and they rarely incorporate explicit actuation energy into multi-objective optimisation, which limits their scalability and suitability for long-term deployment in real O&M settings. Therefore, this paper proposes an articulation-agnostic and energy-aware reinforcement learning framework for robotic manipulation in intelligent infrastructure O&M. The method combines part-guided 3D perception, weighted point sampling, and PointNet-based encoding to obtain a compact geometric representation that generalises across heterogeneous articulated objects. Manipulation is formulated as a Constrained Markov Decision Process (CMDP), in which actuation energy is explicitly modelled and regulated via a Lagrangian-based constrained Soft Actor-Critic scheme. The policy is trained end-to-end under this CMDP formulation, enabling effective articulated-object operation while satisfying a long-horizon energy budget. Experiments on representative O&M tasks demonstrate 16%-30% reductions in energy consumption, 16%-32% fewer steps to success, and consistently high success rates, indicating a scalable and sustainable solution for infrastructure O&M manipulation."}
{"id": "2602.13070", "categories": ["cond-mat.stat-mech", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.13070", "abs": "https://arxiv.org/abs/2602.13070", "authors": ["Gunn Kim"], "title": "Topological Reorganization and Coordination-Controlled Crossover in Synchronization Onset on Regular Lattices", "comment": "9 pages, 7 figures", "summary": "The transition to global synchronization in coupled dynamical systems is governed by the interplay between coupling strength and structural topology. Although abrupt, first-order-like synchronization transitions have been extensively reported in heterogeneous networks, it is unclear whether comparable accelerated onset behavior can emerge purely from coordination geometry in spatially homogeneous, regular lattices. In this study, we investigate large-scale ($N=10^5$) stochastic Stuart-Landau oscillator networks defined on regular lattices with controlled coordination number. Using topological data analysis (TDA), simplicial-complex characterization, and optimal-transport-based geometric diagnostics, we identify a coordination-controlled crossover in synchronization onset dynamics at approximately $z_{c} \\approx 7$ within the class of regular lattices considered. Low-coordination lattices ($z < z_{c}$) exhibit persistent $H_2$ topological features in the dynamical amplitude field that correlate with delayed coherence and surface-limited propagation. In contrast, higher-coordination lattices ($z > z_{c}$) display rapid fragmentation of these features, reduced interface roughness, and predominantly positive Ricci curvature. This is consistent with enhanced path redundancy and improved transport efficiency. In this regime, the global order parameter exhibits accelerated exponential-like growth during the onset stage. Throughout this work, abrupt synchronization refers specifically to this exponential onset behavior rather than to thermodynamic first-order hysteresis. Our results demonstrate that increasing coordination density induces a qualitative reorganization of higher-order topological structure that strongly correlates with synchronization efficiency in regular lattice systems."}
{"id": "2602.12417", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12417", "abs": "https://arxiv.org/abs/2602.12417", "authors": ["William Skoglund", "Elton Giacomelli", "Yiqi Yang", "Jens H. Bardarson", "Erik van Loon"], "title": "Information lattice approach to the metal-insulator transition", "comment": null, "summary": "Correlation functions and correlation lengths are frequently used to describe phase transitions in quantum systems, but they require an explicit choice of observables. The recently introduced information lattice instead provides an observable-independent way to identify where and at which scale information is contained in quantum lattice models. Here, we use it to study the difference between the metallic and insulating regime of one-dimensional tight-binding chains. We find that the information per scale follows a power law in metals at low temperature and that Friedel-like oscillations are visible in the information lattice. At high temperature or in insulators at low temperature, the information per scale decays exponentially. Thus, the information lattice is a useful tool for analyzing the metal-insulator transition."}
{"id": "2602.12577", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.12577", "abs": "https://arxiv.org/abs/2602.12577", "authors": ["Weiben Zhang", "Ruben Loaiza-Maya", "Michael Stanley Smith", "Worapree Maneesoonthorn"], "title": "Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice", "comment": null, "summary": "Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called \"mixed\", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging."}
{"id": "2602.12455", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12455", "abs": "https://arxiv.org/abs/2602.12455", "authors": ["M. H. Alizadeh", "F. Lara"], "title": "Existence Results and KKT Optimality Conditions for Generalized Quasiconvex Functions", "comment": null, "summary": "We studied a new notion of generalized convex functions called $e$-quasi\\-con\\-ve\\-xi\\-ty, which encompasses both quasiconvex and $e$-convex functions, including all Lipschitz functions. By extending the standard properties of quasiconvex functions to $e$-quasiconvex functions, we establish sufficient conditions for the nonemptiness and compactness of the solution set when minimizing an $e$-quasiconvex function, leveraging generalized asymptotic functions, a result which remains applicable even when the set of minimizers is nonconvex. Furthermore, in the differentiable case, we ensure the sufficiency of the KKT optimality conditions when the constraint functions in the mathematical programming problems are $e$-quasiconvex. Finally, we illustrate our new results with several nonconvex (non-quasiconvex) examples."}
{"id": "2602.12589", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.12589", "abs": "https://arxiv.org/abs/2602.12589", "authors": ["Zhijun Cai", "Xiang Li", "Lihu Xu"], "title": "Berry-Esseen Bounds and Moderate Deviations for Catoni-Type Robust Estimation", "comment": null, "summary": "A powerful robust mean estimator introduced by Catoni (2012) allows for mean estimation of heavy-tailed data while achieving the performance characteristics of classical mean estimator for sub-Gaussian data. While Catoni's framework has been widely extended across statistics, stochastic algorithms, and machine learning, fundamental asymptotic questions regarding the Central Limit Theorem and rare event deviations remain largely unaddressed. In this paper, we investigate Catoni-type robust estimators in two contexts: (i) mean estimation for heavy-tailed data, and (ii) linear regression with heavy-tailed innovations. For the first model, we establish the Berry--Esseen bound and moderate deviation principles, addressing both known and unknown variance settings. For the second model, we demonstrate that the associated estimator is consistent and satisfies a multi-dimensional Berry-Esseen bound."}
{"id": "2602.12294", "categories": ["physics.comp-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.12294", "abs": "https://arxiv.org/abs/2602.12294", "authors": ["Michael Kim", "Wei Cai"], "title": "Accelerated Markov Chain Monte Carlo Simulation via Neural Network-Driven Importance Sampling", "comment": null, "summary": "Atomistic simulations provide valuable insights into the physical processes governing material behavior. However, their applicability is fundamentally constrained by the limited time scales accessible to brute-force simulations. This bottleneck often stems from complex energy landscapes where the systems stay trapped in metastable states for long periods of time. Yet, the long-term evolution is controlled by the transitions between the metastable states, which are rare events and difficult to observe. We present an importance sampling method designed to accelerate the time scale of Markov chain Monte Carlo (MCMC) simulations. By employing a bias potential, our approach enhances the sampling of rare transition events while preserving the relative probabilities of distinct transition pathways. The bias potential is represented by a neural network which enables the flexibility needed for high-dimensional systems. We propose a rigorous formulation to obtain the original transition rates between metastable states using transition paths obtained from the biased simulation. We further use a branching random walk (BRW) technique to enhance efficiency and to reduce variance. The proposed methodology is validated on 2-dimensional and 14-dimensional systems, demonstrating its accuracy and scalability."}
{"id": "2602.13070", "categories": ["cond-mat.stat-mech", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.13070", "abs": "https://arxiv.org/abs/2602.13070", "authors": ["Gunn Kim"], "title": "Topological Reorganization and Coordination-Controlled Crossover in Synchronization Onset on Regular Lattices", "comment": "9 pages, 7 figures", "summary": "The transition to global synchronization in coupled dynamical systems is governed by the interplay between coupling strength and structural topology. Although abrupt, first-order-like synchronization transitions have been extensively reported in heterogeneous networks, it is unclear whether comparable accelerated onset behavior can emerge purely from coordination geometry in spatially homogeneous, regular lattices. In this study, we investigate large-scale ($N=10^5$) stochastic Stuart-Landau oscillator networks defined on regular lattices with controlled coordination number. Using topological data analysis (TDA), simplicial-complex characterization, and optimal-transport-based geometric diagnostics, we identify a coordination-controlled crossover in synchronization onset dynamics at approximately $z_{c} \\approx 7$ within the class of regular lattices considered. Low-coordination lattices ($z < z_{c}$) exhibit persistent $H_2$ topological features in the dynamical amplitude field that correlate with delayed coherence and surface-limited propagation. In contrast, higher-coordination lattices ($z > z_{c}$) display rapid fragmentation of these features, reduced interface roughness, and predominantly positive Ricci curvature. This is consistent with enhanced path redundancy and improved transport efficiency. In this regime, the global order parameter exhibits accelerated exponential-like growth during the onset stage. Throughout this work, abrupt synchronization refers specifically to this exponential onset behavior rather than to thermodynamic first-order hysteresis. Our results demonstrate that increasing coordination density induces a qualitative reorganization of higher-order topological structure that strongly correlates with synchronization efficiency in regular lattice systems."}
{"id": "2602.12334", "categories": ["quant-ph", "math.LO", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12334", "abs": "https://arxiv.org/abs/2602.12334", "authors": ["Jacopo Surace"], "title": "Reconstruction of finite Quasi-Probability and Probability from Principles: The Role of Syntactic Locality", "comment": "22 pages of main text, 3 pages of bibliography, 19 pages of appendix", "summary": "Quasi-probabilities appear across diverse areas of physics, but their conceptual foundations remain unclear: they are often treated merely as computational tools, and operations like conditioning and Bayes' theorem become ambiguous. We address both issues by developing a principled framework that derives quasi-probabilities and their conditional calculus from structural consistency requirements on how statements are valued across different universes of discourse, understood as finite Boolean algebras of statements.We begin with a universal valuation that assigns definite (possibly complex) values to all statements. The central concept is Syntactic Locality: every universe can be embedded within a larger ambient one, and the universal valuation must behave coherently under such embeddings and restrictions. From a set of structural principles, we prove a representation theorem showing that every admissible valuation can be re-expressed as a finitely additive measure on mutually exclusive statements, mirroring the usual probability sum rule. We call such additive representatives pre-probabilities. This representation is unique up to an additive regraduation freedom. When this freedom can be fixed canonically, pre-probabilities reduce to finite quasi-probabilities, thereby elevating quasi-probability theory from a computational device to a uniquely determined additive representation of universal valuations. Classical finite probabilities arise as the subclass of quasi-probabilities stable under relativisation, i.e., closed under restriction to sub-universes. Finally, the same framework enables us to define a coherent theory of conditionals, yielding a well-defined generalized Bayes' theorem applicable to both pre-probabilities and quasi-probabilities. We conclude by discussing additional regularity conditions, including the role of rational versus irrational probabilities in this setting."}
{"id": "2602.12914", "categories": ["quant-ph", "cond-mat.other", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.12914", "abs": "https://arxiv.org/abs/2602.12914", "authors": ["Harshita Sharma", "Sayan Choudhury", "Jayendra N. Bandyopadhyay"], "title": "Quantum metrology with partially accessible chaotic sensors", "comment": "9 pages, 8 figures", "summary": "Most quantum metrology protocols harness highly entangled probe states and globally accessible measurements to surpass the standard quantum limit. However, it is challenging to satisfy these requirements in realistic many-body sensors. We demonstrate that both of these constraints can be overcome in quantum chaotic sensors. Crucially, we establish that even in the presence of partial measurement accessibility, chaotic dynamics enables initial unentangled states to exhibit Heisenberg scaling of the quantum Fisher information, $I_α$ with time. In the weakly chaotic regime, we identify spin-coherent states placed at the edge of the regular islands in the mixed classical phase space as optimal initial states for enhanced sensitivity. On the other hand, in the strongly chaotic regime, $I_α$ is insensitive to the choice of the initial state. Notably, quantum-enhanced sensitivity is achieved even when a very low fraction ($\\sim 5\\%$) of the qubits are accessible. These results establish quantum chaos as a robust resource for quantum-enhanced sensing under realistic accessibility constraints on accessibility."}
{"id": "2602.12288", "categories": ["eess.SY", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12288", "abs": "https://arxiv.org/abs/2602.12288", "authors": ["Xiaowen Tao", "Yinuo Wang", "Haitao Ding", "Yuanyang Qi", "Ziyu Song"], "title": "Energy-Aware Reinforcement Learning for Robotic Manipulation of Articulated Components in Infrastructure Operation and Maintenance", "comment": null, "summary": "With the growth of intelligent civil infrastructure and smart cities, operation and maintenance (O&M) increasingly requires safe, efficient, and energy-conscious robotic manipulation of articulated components, including access doors, service drawers, and pipeline valves. However, existing robotic approaches either focus primarily on grasping or target object-specific articulated manipulation, and they rarely incorporate explicit actuation energy into multi-objective optimisation, which limits their scalability and suitability for long-term deployment in real O&M settings. Therefore, this paper proposes an articulation-agnostic and energy-aware reinforcement learning framework for robotic manipulation in intelligent infrastructure O&M. The method combines part-guided 3D perception, weighted point sampling, and PointNet-based encoding to obtain a compact geometric representation that generalises across heterogeneous articulated objects. Manipulation is formulated as a Constrained Markov Decision Process (CMDP), in which actuation energy is explicitly modelled and regulated via a Lagrangian-based constrained Soft Actor-Critic scheme. The policy is trained end-to-end under this CMDP formulation, enabling effective articulated-object operation while satisfying a long-horizon energy budget. Experiments on representative O&M tasks demonstrate 16%-30% reductions in energy consumption, 16%-32% fewer steps to success, and consistently high success rates, indicating a scalable and sustainable solution for infrastructure O&M manipulation."}
{"id": "2602.13082", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2602.13082", "abs": "https://arxiv.org/abs/2602.13082", "authors": ["Khristina Filonchik", "Jose Pedro Pinto", "Flávio L. Pinheiro", "Fernando Bacao"], "title": "Revealing Process Structure in Urban Mobility Networks", "comment": "This paper was presented at the Fourteenth International Conference on Complex Networks & Their Applications (Complex Networks 2025), Binghamton, USA, and appears in the conference proceedings", "summary": "Urban mobility is a multi-entity system that involves travelers, transport modes, and infrastructure. Beyond conventional origin/destination analysis, this paper investigates how process mining can structure and interpret mobility behavior from event data. Using Call Detail Records (CDRs) from Oeiras in the Lisbon metropolitan area (Portugal), we construct both case-centric and object-centric event logs and discover models that summarize flows and typical durations. Results show that most trips are intra-municipal, while inter-municipal flows connect strongly to neighboring areas, with typical inter-parish travel times of about 20 minutes. The object-centric perspective explicitly links trips and transport modes, revealing mode-specific duration differences (e.g., bus vs. car) that inform multimodal planning. Our contributions are: (i) a reproducible pipeline to transform CDRs into process mining artifacts, (ii) empirical evidence that mobility data exhibit a process-like structure, and (iii) the added value of object-centric models for multimodal analysis. Limitations include the low spatial precision of CDRs (tower-sector level) and heuristic transport-mode labels. Future work will integrate transport-network context (e.g., stations and routes) and model object-centric logs as heterogeneous graphs to enable richer and more reliable analysis."}
{"id": "2602.12409", "categories": ["math.NA", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.12409", "abs": "https://arxiv.org/abs/2602.12409", "authors": ["Bankim C. Mandal", "Deeksha Tomer"], "title": "Dirichlet-Neumann Waveform Relaxation Method with Multiple Subdomains for Reaction-Diffusion Equation with a Time Delay", "comment": null, "summary": "In this study, we present the numerical investigation of the Dirichlet-Neumann Waveform Relaxation (DNWR) algorithm applied to multiple subdomains for the reaction-diffusion equation with time delay. Various arrangements of transmission conditions between subdomains are explored and a series of numerical experiments are conducted to evaluate and compare the efficiency and effectiveness of these configurations."}
{"id": "2602.12289", "categories": ["eess.SY", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.12289", "abs": "https://arxiv.org/abs/2602.12289", "authors": ["Yuanliang Li", "Xun Gong", "Reza Iravani", "Bo Cao", "Heng Liu", "Ziming Chen"], "title": "String-Level Ground Fault Localization for TN-Earthed Three-Phase Photovoltaic Systems", "comment": null, "summary": "The DC-side ground fault (GF) poses significant risks to three-phase TN-earthed photovoltaic (PV) systems, as the resulting high fault current can directly damage both PV inverters and PV modules. Once a fault occurs, locating the faulty string through manual string-by-string inspection is highly time-consuming and inefficient. This work presents a comprehensive analysis of GF characteristics through fault-current analysis and a simulation-based case study covering multiple fault locations. Building on these insights, we propose an edge-AI-based GF localization approach tailored for three-phase TN-earthed PV systems. A PLECS-based simulation model that incorporates PV hysteresis effects is developed to generate diverse GF scenarios, from which correlation-based features are extracted throughout the inverter's four-stage shutdown sequence. Using the simulated dataset, a lightweight Variational Information Bottleneck (VIB)-based localization model is designed and trained, achieving over 93% localization accuracy at typical sampling rates with low computational cost, demonstrating strong potential for deployment on resource-constrained PV inverters."}
{"id": "2602.13080", "categories": ["cond-mat.stat-mech", "math.DS", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2602.13080", "abs": "https://arxiv.org/abs/2602.13080", "authors": ["Anna Gallo", "Wilfried Segnou", "Timoteo Carletti"], "title": "Turing patterns in Matrix-Weighted Networks", "comment": null, "summary": "Diffusion-driven instability is a fundamental mechanism underlying pattern formation in spatially extended systems. In almost all existing works, diffusion across the links of the underlying network is modeled through scalar weights, possibly complemented by cross-diffusion terms that are homogeneous across links. In this work, we investigate the emergence of Turing patterns on Matrix Weighted Networks (MWNs), a recently introduced framework in which each edge is associated with a matrix weight. Focusing on the class of coherent MWNs, we provide a novel characterization of coherence in terms of node-dependent orthonormal matrices, showing that link transformations can be written as relative rotations between nodes. This representation allows us to deal with coherent MWNs of any size and to introduce an orthonormal change of variables capable to reduce diffusion on a coherent MWN to diffusion on a standard weighted network with scalar weights. Building on this, we extend the classical Turing instability analysis to MWNs and derive the conditions under which a homogeneous equilibrium of the local dynamics loses stability due to matrix-weighted diffusion. Our results show how network topology, scalar weights, and inter-node transformations jointly shape pattern formation, and provide a constructive framework to analyze and design Turing patterns on matrix-weighted and higher-order networked systems."}
{"id": "2602.12737", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.12737", "abs": "https://arxiv.org/abs/2602.12737", "authors": ["Jiannan Hua", "Jing Ding", "W. Zhu", "Shui-gang Xu"], "title": "Quantum Anomalous Hall Effect in Rhombohedral Multilayer Graphene/hBN Moiré Superlattices", "comment": null, "summary": "The recent discovery of robust quantum anomalous Hall (QAH) effect in rhombohedral multilayer graphene (RMG) aligned with hexagonal boron nitride (hBN) has established a highly versatile platform for correlated topological matter. This review synthesizes the experimental and theoretical progress in understanding these interaction-driven topological phases. Experimentally, the landscape has rapidly expanded from initial Chern insulators in trilayer systems to fully quantized QAH states in thicker systems. Theoretically, it is believed that moiré potential and electron-electron interaction cooperate and produce the QAH effect in such systems. Theoretical calculations also bring interesting questions, such as the formation of an interaction-driven topological phase known as an anomalous Hall crystal (AHC). This review comprehensively covers the experimental hallmarks, the theoretical frameworks, including continuum models and many-body approaches, and the ensuing physical picture that reconciles the roles of interactions, displacement fields, and the moiré potentials. We conclude by outlining outstanding open questions and future directions, positioning RMG/hBN systems at the forefront of topological quantum matter."}
{"id": "2602.12682", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12682", "abs": "https://arxiv.org/abs/2602.12682", "authors": ["Taekwon Hong", "Woojung Bae", "Sang Kyu Lee", "Dongrak Choi", "Jong-Hyeon Jeong"], "title": "A Causal Framework for Quantile Residual Lifetime", "comment": null, "summary": "Estimating prognosis conditional on surviving an initial high-risk period is crucial in clinical research. Yet, standard metrics such as hazard ratios are often difficult to interpret, while mean-based summaries are sensitive to outliers and censoring. We propose a formal causal framework for estimating quantiles of residual lifetime among individuals surviving to a landmark time $t_0$. Our primary estimand, the \"Observed Survivor Quantile Contrast\" (OSQC), targets pragmatic prognostic differences within the observed survivor population. To estimate the OSQC, we develop a doubly robust estimator that combines propensity scores, outcome regression, and inverse probability of censoring weights, ensuring consistency under confounding and informative censoring provided that the censoring model is correctly specified and at least one additional nuisance model is correctly specified. Recognizing that the OSQC conflates causal efficacy and compositional selection, we also introduce a reweighting-based supplementary estimator for the \"Principal Survivor Quantile Contrast\" (PSQC) to disentangle these mechanisms under stronger assumptions. Extensive simulations demonstrate the robustness of the proposed estimators and clarify the role of post-treatment selection. We illustrate the framework using data from the SUPPORT study to assess the impact of right heart catheterization on residual lifetime among intensive care unit survivors, and from the NSABP B-14 trial to examine post-surgical prognosis under adjuvant tamoxifen therapy across multiple landmark times."}
{"id": "2602.12513", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12513", "abs": "https://arxiv.org/abs/2602.12513", "authors": ["Jiashuo Jiang", "Mengxiao Zhang"], "title": "An LP-Based Approach for Bilinear Saddle Point Problem with Instance-dependent Guarantee and Noisy Feedback", "comment": null, "summary": "In this work, we study the sample complexity of obtaining a Nash equilibrium (NE) estimate in two-player zero-sum matrix games with noisy feedback. Specifically, we propose a novel algorithm that repeatedly solves linear programs (LPs) to obtain an NE estimate with bias at most $\\varepsilon$ with a sample complexity of $O\\left(\\frac{m_1 m_2}{\\varepsilon\\min\\{δ^2,σ_0^2,σ^3\\}} \\log\\frac{m_1 m_2}{\\varepsilon}\\right)$ for general $m_1 \\times m_2$ game matrices, where $σ$, $σ_0$, $δ$ are some problem-dependent constants. To our knowledge, this is the first instance-dependent sample complexity bound for finding an NE estimate with $\\varepsilon$ bias in general-dimension matrix games with noisy feedback and potentially non-unique equilibria. Our algorithm builds on recent advances in online resource allocation and operates in two stages: (1) identifying the support set of an NE, and (2) computing the unique NE restricted to this support. Both stages rely on a careful analysis of LP solutions derived from noisy samples."}
{"id": "2602.12604", "categories": ["math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.12604", "abs": "https://arxiv.org/abs/2602.12604", "authors": ["Joowon Lee", "Guanhua Chen"], "title": "Differentially Private Two-Stage Empirical Risk Minimization and Applications to Individualized Treatment Rule", "comment": "27 pages, 2 figures. Technical proofs are omitted for the initial version. It will be included in future versions", "summary": "Differential Privacy (DP) provides a rigorous framework for deriving privacy-preserving estimators by injecting calibrated noise to mask individual contributions while preserving population-level insights. Its central challenge lies in the privacy-utility trade-off: calibrating noise levels to ensure robust protection without compromising statistical performance. Standard DP methods struggle with a particular class of two-stage problems prevalent in individualized treatment rules (ITRs) and causal inference. In these settings, data-dependent weights are first computed to satisfy distributional constraints, such as covariate balance, before the final parameter of interest is estimated. Current DP approaches often privatize stages independently, which either degrades weight efficacy-leading to biased and inconsistent estimates-or introduces excessive noise to account for worst-case scenarios.\n  To address these challenges, we propose the Differentially Private Two-Stage Empirical Risk Minimization (DP-2ERM), a framework that injects a carefully calibrated noise only into the second stage while maintaining privacy for the entire pipeline and preserving the integrity of the first stage weights. Our theoretical contributions include deterministic bounds on weight perturbations across various widely used weighting methods, and probabilistic bounds on sensitivity for the final estimator. Simulations and real-world applications in ITR demonstrate that DP-2ERM significantly enhances utility over existing methods while providing rigorous privacy guarantees."}
{"id": "2602.12558", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.12558", "abs": "https://arxiv.org/abs/2602.12558", "authors": ["Shangyong Wua", "Rui Chua", "Wenqian Konga", "Hongyu Zhanga", "Le Shia", "Kai Wua", "Yonghong Chenga", "Guodong Menga", "Bing Xiaoa"], "title": "Electrohydrodynamic instability of Cu, W and Ti metal nanomelts under radiofrequency E-fields from multiphysics molecular dynamics simulations with coarse-grained density field analysis", "comment": "55 pages, 11 figures, submitted to Physics of Fluids", "summary": "Employing both electrodynamics coupled with molecular dynamics (ED-MD) simulations for atomistic models and the dynamic instability theory of electrocapillary wave, we investigate the structure evolutions and thermal runaway process of Cu, Ti and W nanotips with radii of curvature of 1 nm and 5 nm under various radiofrequency electric field conditions. The associated critical parameters including the critical electric field, spatial and temporal scales of the electrohydrodynamic instability of molten apexes are obtained by proposing the workflows that utilize the atomistic models in ED-MD simulations to calculate kinematic viscosity tensor components and mass density spatial distributions for the nanomelts with electric fields. Our current ED-MD simulations for nanotips show a non-monotonical variation of the time delay versus the electric field frequency for metal nanotips, and the presence of a critical rf electric field amplitude triggering the thermal runaway regardless of the field frequency. The calculated mass densities and kinematic viscosities of nanomelts for metal nanotips are found to be drastically different to those of bulk liquid metals at the melting point. Specifically, the viscosity of nanomelt under the rf electric field is revealed to be several orders of magnitude higher than the bulk liquid metal, resulting in substantial increase of spatial and temporal scales in the instability theory of electrocapillary wave within the viscosity-dominated regime, compared to the results of ED-MD simulations for Cu and Ti metals, while good agreement between the two methods on the critical wavelength and time delay of thermal runway is found for W nanotips."}
{"id": "2602.12387", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12387", "abs": "https://arxiv.org/abs/2602.12387", "authors": ["Masih Mozakka", "Mohsen Heidari"], "title": "Accelerating Feedback-based Algorithms for Quantum Optimization Using Gradient Descent", "comment": "10 pages, 6 figures", "summary": "Feedback-based methods have gained significant attention as an alternative training paradigm for the Quantum Approximate Optimization Algorithm (QAOA) in solving combinatorial optimization problems such as MAX-CUT. In particular, Quantum Lyapunov Control (QLC) employs feedback-driven control laws that guarantee monotonic non-decreasing objective values, can substantially reduce the training overhead of QAOA, and mitigate barren plateaus. However, these methods might require long control sequences, leading to sub-optimal convergence rates. In this work, we propose a hybrid method that incorporates per-layer gradient estimation to accelerate the convergence of QLC while preserving its low training overhead and stability guarantees. By leveraging layer-wise gradient information, the proposed approach selects near-optimal control parameters, resulting in significantly faster convergence and improved robustness. We validate the effectiveness of the method through extensive numerical experiments across a range of problem instances and optimization settings."}
{"id": "2602.12289", "categories": ["eess.SY", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.12289", "abs": "https://arxiv.org/abs/2602.12289", "authors": ["Yuanliang Li", "Xun Gong", "Reza Iravani", "Bo Cao", "Heng Liu", "Ziming Chen"], "title": "String-Level Ground Fault Localization for TN-Earthed Three-Phase Photovoltaic Systems", "comment": null, "summary": "The DC-side ground fault (GF) poses significant risks to three-phase TN-earthed photovoltaic (PV) systems, as the resulting high fault current can directly damage both PV inverters and PV modules. Once a fault occurs, locating the faulty string through manual string-by-string inspection is highly time-consuming and inefficient. This work presents a comprehensive analysis of GF characteristics through fault-current analysis and a simulation-based case study covering multiple fault locations. Building on these insights, we propose an edge-AI-based GF localization approach tailored for three-phase TN-earthed PV systems. A PLECS-based simulation model that incorporates PV hysteresis effects is developed to generate diverse GF scenarios, from which correlation-based features are extracted throughout the inverter's four-stage shutdown sequence. Using the simulated dataset, a lightweight Variational Information Bottleneck (VIB)-based localization model is designed and trained, achieving over 93% localization accuracy at typical sampling rates with low computational cost, demonstrating strong potential for deployment on resource-constrained PV inverters."}
{"id": "2602.12568", "categories": ["math.ST", "cs.SI", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.12568", "abs": "https://arxiv.org/abs/2602.12568", "authors": ["Anirudh Sridhar", "Arnob Ghosh"], "title": "Finding Super-spreaders in SIS Epidemics", "comment": "6 pages, 3 figures", "summary": "In network epidemic models, controlling the spread of a disease often requires targeted interventions such as vaccinating high-risk individuals based on network structure. However, typical approaches assume complete knowledge of the underlying contact network, which is often unavailable. While network structure can be learned from observed epidemic dynamics, existing methods require long observation windows that may delay critical interventions.\n  In this work, we show that full network reconstruction may not be necessary: control-relevant features, such as high-degree vertices (super-spreaders), can be learned far more efficiently than the complete structure. Specifically, we develop an algorithm to identify such vertices from the dynamics of a Susceptible-Infected-Susceptible (SIS) process. We prove that in an $n$-vertex graph, vertices of degree at least $n^α$ can be identified over an observation window of size $Ω(1/α)$, for any $α\\in (0,1)$. In contrast, existing methods for exact network reconstruction requires an observation window that grows linearly with $n$. Simulations demonstrate that our approach accurately identifies super-spreaders and enables effective epidemic control."}
{"id": "2602.12483", "categories": ["math.NA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12483", "abs": "https://arxiv.org/abs/2602.12483", "authors": ["Sofiia Shvaiko", "Longxiu Huang", "Elizaveta Rebrova"], "title": "Quantile Randomized Kaczmarz Algorithm with Whitelist Trust Mechanism", "comment": "Accepted by ICASSP 2026", "summary": "Randomized Kaczmarz (RK) is a simple and fast solver for consistent overdetermined systems, but it is known to be fragile under noise. We study overdetermined $m\\times n$ linear systems with a sparse set of corrupted equations, $ {\\bf A}{\\bf x}^\\star = {\\bf b}, $where only $\\tilde{\\bf b} = {\\bf b} + \\boldsymbol{\\varepsilon}$ is observed with $\\|\\boldsymbol{\\varepsilon}\\|_0 \\le βm$. The recently introduced QuantileRK (QRK) algorithm addresses this issue by testing residuals against a quantile threshold, but computing a per-iteration quantile across many rows is costly. In this work we (i) reanalyze QRK and show that its convergence rate improves monotonically as the corruption fraction $β$ decreases; (ii) propose a simple online detector that flags and removes unreliable rows, which reduces the effective $β$ and speeds up convergence; and (iii) make the method practical by estimating quantiles from a small random subsample of rows, preserving robustness while lowering the per-iteration cost. Simulations on imaging and synthetic data demonstrate the efficiency of the proposed method."}
{"id": "2602.12296", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12296", "abs": "https://arxiv.org/abs/2602.12296", "authors": ["Maojiang Deng", "Shoufeng Lu", "Jiazhao Shi", "Wen Zhang"], "title": "Adaptive traffic signal control optimization using a novel road partition and multi-channel state representation method", "comment": null, "summary": "This study proposes a novel adaptive traffic signal control method leveraging a Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) to optimize signal timing by integrating variable cell length and multi-channel state representation. A road partition formula consisting of the sum of logarithmic and linear functions was proposed. The state variables are a vector composed of three channels: the number of vehicles, the average speed, and space occupancy. The set of available signal phases constitutes the action space, the selected phase is executed with a fixed green time. The reward function is formulated using the absolute values of key traffic state metrics - waiting time, speed, and fuel consumption. Each metric is normalized by a typical maximum value and assigned a weight that reflects its priority and optimization direction. The simulation results, using Sumo-TensorFlow-Python, demonstrate a cross-range transferability evaluation and show that the proposed variable cell length and multi-channel state representation method excels compared to fixed cell length in optimization performance."}
{"id": "2602.13173", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.13173", "abs": "https://arxiv.org/abs/2602.13173", "authors": ["Till Welker", "Patrick Pietzonka"], "title": "Accuracy Comes at a Cost: Optimal Localisation Against a Flow", "comment": "8 pages, 5 figures", "summary": "How much work does it cost for a propelled particle to stay localised near a stationary target, defying both thermal noise and a constant flow that would carry it away? We study the control of such a particle in finite time and find optimal protocols for time-dependent swim velocity and diffusivity, without feedback. Accuracy, quantified via the mean squared deviation from the target, and energetic cost turn out to be related by a trade-off, which complements the one between precision and cost known in stochastic thermodynamics. We show that accuracy better than a certain threshold requires active driving, which comes at a cost that increases with accuracy. The optimal protocols have discontinuous swim velocity and diffusivity, switching between a passive drift state with vanishing diffusivity and an active propulsion state. This study highlights how a time-dependent diffusivity enhances optimal control and sets benchmarks for cost and accuracy of artificial self-propelled particles navigating noisy environments."}
{"id": "2602.12990", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.12990", "abs": "https://arxiv.org/abs/2602.12990", "authors": ["Takayuki Yokoyama", "Yasuhiro Tada"], "title": "Diagnosing energy gap in quantum spin liquids via polarization amplitude", "comment": "9 pages, 7 figures", "summary": "Identifying whether a many-body ground state is gapped or gapless is a fundamental yet challenging problem, especially in quantum spin liquids. In this work, we develop a gap-diagnostic scheme based on the polarization amplitude defined via a twist operator, evaluated within the infinite density-matrix renormalization group (iDMRG) framework. As a benchmark, analysis of the spin-$1/2$ XXZ chain demonstrates that the polarization amplitude clearly distinguishes the gapless Tomonaga-Luttinger liquid from the gapped Néel phase. We then extend this framework to infinite cylinders of the spin-$1/2$ XY-$J_χ$ model on the square lattice. We find that the polarization amplitude sharply detects the transition between the gapless XY phase and the gapped chiral spin liquid phase. These results show that polarization amplitudes provide a strong energy-gap diagnostic in two-dimensional frustrated quantum magnets, including quantum spin liquids."}
{"id": "2602.12702", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12702", "abs": "https://arxiv.org/abs/2602.12702", "authors": ["Anna Nalpantidi", "Dimitris Karlis"], "title": "Modelling multivariate ordinal time series using pairwise likelihood", "comment": null, "summary": "We assume that we have multiple ordinal time series and we would like to specify their joint distribution. In general it is difficult to create multivariate distribution that can be easily used to jointly model ordinal variables and the problem becomes even more complex in the case of time series, since we have to take into consideration not only the autocorrelation of each time series and the dependence between time series, but also cross-correlation. Starting from the simplest case of two ordinal time series, we propose using copulas to specify their joint distribution. We extend our approach in higher dimensions, by approximating full likelihood with composite likelihood and especially conditional pairwise likelihood, where each bivariate model is specified by copulas. We suggest maximizing each bivariate model independently to avoid computational issues and synthesize individual estimates using weighted mean. Weights are related to the Hessian matrix of each bivariate model. Simulation studies showed that model fits well under different sample sizes. Forecasting approach is also discussed. A small real data application about unemployment state of different countries of European Union is presented to illustrate our approach."}
{"id": "2602.12698", "categories": ["math.OC", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.12698", "abs": "https://arxiv.org/abs/2602.12698", "authors": ["Hoai-Minh Nguyen"], "title": "Optimal bounds for the cost of fast controls of a KdV system", "comment": null, "summary": "We study the cost of fast controls for a linearized KdV system and a nonlinear KdV system locally, using right Neumann boundary control for non-critical lengths. Since the operator associated with the linearized system is neither self-adjoint nor skew-adjoint, its (known) spectral properties are not directly amenable to the moment method, leaving optimal cost bounds an open problem. We address this difficulty by shifting attention to a related KdV system and deriving the optimal bounds from the new one."}
{"id": "2602.12653", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.12653", "abs": "https://arxiv.org/abs/2602.12653", "authors": ["Tianxing Mei", "Chen Wang", "Jianfeng Yao"], "title": "Many-sample tests for the dimensionality hypothesis for large covariance matrices among groups", "comment": "41pages, 2 figures", "summary": "In this paper, we consider procedures for testing hypotheses on the dimension of the linear span generated by a growing number of $p\\times p$ covariance matrices from independent $q$ populations. Under a proper limiting scheme where all the parameters, $q$, $p$, and the sample sizes from the $q$ populations, are allowed to increase to infinity, we derive the asymptotic normality of the proposed test statistics. The proposed test procedures show satisfactory performance in finite samples under both the null and the alternative. We also apply the proposed many-sample dimensionality test to investigate a matrix-valued gene dataset from the Mouse Aging Project and gain some new knowledge about its covariance structures."}
{"id": "2602.12743", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.12743", "abs": "https://arxiv.org/abs/2602.12743", "authors": ["P. Elli Stamatopoulou", "Carsten Rockstuhl"], "title": "A T-matrix scattering formalism for electron-beam spectroscopy", "comment": "12 pages, 4 figures", "summary": "Advanced computational tools that describe the interaction of electrons with structured nanophotonic materials are crucial for theoretical predictions, specific design tasks, and the interpretation of experimental results. These tools open the door to systematic exploration of free-electron-driven nanophotonic light sources, among others. Here, we report on the implementation of electron-beam spectroscopy in a T-matrix-based scattering formulation. Such a framework is quite versatile in predicting the electromagnetic response of complex photonic materials composed of periodically or aperiodically arranged individual scatterers. By extending this formalism to describe interactions with fast electrons, we provide a fast and accurate numerical tool for simulating cathodoluminescence (CL) and electron energy-loss spectroscopy (EELS) measurements. The desired functionalities are implemented into the existing software suite treams for electromagnetic scattering computations, and the extended code treams_ebeam is available online at https://github.com/tfp-photonics/treams_ebeam. We demonstrate the implementation details on a carefully selected set of problems, including single scatterers of various shapes and materials, a periodic chain of elliptical nanodisks, and a finite cluster of nanospheres arranged in a two-dimensional (2D) lattice. By uniting fast-electron physics with advanced scattering theory, our framework unlocks new possibilities for designing, understanding, and engineering next-generation nanoscale light-matter interactions."}
{"id": "2602.12459", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12459", "abs": "https://arxiv.org/abs/2602.12459", "authors": ["Jakob Kaltoft Søndergaard", "René Bødker Christensen", "Petar Popovski"], "title": "Temporal Framework for Causality-Preserving Scheduling of Measurements in Quantum Networks", "comment": "To be published in 2026 International Conference on Quantum Communications, Networking, and Computing (QCNC)", "summary": "Distributed quantum protocols rely on classical feedforward information to process measurement outcomes, but heterogeneous hardware and uncertain local timing can make the causal order of measurements ambiguous when inferred solely from arrival times. Even in simple line networks with only Pauli measurements, end nodes cannot distinguish whether a missing outcome is caused by slow measurement or by delayed classical propagation. To resolve this ambiguity, we propose a time-division architecture for quantum networks in which nodes perform measurements in pre-assigned slots, ensuring a unique causal interpretation of outcomes. We formalize this temporal framework and derive the feedforward and adjacency constraints required to preserve measurement causality. For simple network topologies, we present an algorithm that yields optimal measurement schedules. Overall, the proposed time-division model provides a practical coordination layer that bridges the classical network timing with quantum measurement processing, enabling reliable and scalable measurement-based quantum networking."}
{"id": "2602.12296", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12296", "abs": "https://arxiv.org/abs/2602.12296", "authors": ["Maojiang Deng", "Shoufeng Lu", "Jiazhao Shi", "Wen Zhang"], "title": "Adaptive traffic signal control optimization using a novel road partition and multi-channel state representation method", "comment": null, "summary": "This study proposes a novel adaptive traffic signal control method leveraging a Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) to optimize signal timing by integrating variable cell length and multi-channel state representation. A road partition formula consisting of the sum of logarithmic and linear functions was proposed. The state variables are a vector composed of three channels: the number of vehicles, the average speed, and space occupancy. The set of available signal phases constitutes the action space, the selected phase is executed with a fixed green time. The reward function is formulated using the absolute values of key traffic state metrics - waiting time, speed, and fuel consumption. Each metric is normalized by a typical maximum value and assigned a weight that reflects its priority and optimization direction. The simulation results, using Sumo-TensorFlow-Python, demonstrate a cross-range transferability evaluation and show that the proposed variable cell length and multi-channel state representation method excels compared to fixed cell length in optimization performance."}
{"id": "2602.12491", "categories": ["math.NA", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.12491", "abs": "https://arxiv.org/abs/2602.12491", "authors": ["Dominic Blanco"], "title": "Proving periodic solutions and branches in the 2D Swift Hohenberg PDE with hexagonal and triangular symmetry", "comment": null, "summary": "In this article, we enforce space group symmetries in Fourier series to rigorously prove the existence of smooth, periodic solutions in partial differential equations (PDEs) with hexagonal and triangular symmetries. In particular, we provide the necessary analytical and numerical tools to construct Fourier series of functions on the hexagonal lattice. This allows one to build approximate solutions that are periodic. Moreover, to generate the periodic tiling, we can use one symmetric hexagon for $D_6$ symmetry and two symmetric triangles for $D_3$ symmetry. We derive a Newton-Kantorovich approach based on the construction of an approximate inverse around an approximate solution, $\\overline{u}$. More specifically, we verify a condition based on the computation of explicit bounds. The strategy for constructing $\\overline{u}$, the approximate inverse, and the computation of these bounds will be presented. We demonstrate our approach on the 2D Swift-Hohenberg PDE by proving the existence of $D_3$ and $D_6$ periodic solutions. We then perform proofs of branches of solutions by using Chebyshev series. The algorithmic details to perform the proof can be found on Github."}
{"id": "2602.12436", "categories": ["eess.SY", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.12436", "abs": "https://arxiv.org/abs/2602.12436", "authors": ["Mohammed Adib Oumer", "Vishnu Murali", "Majid Zamani"], "title": "Interpolation-Inspired Closure Certificates", "comment": null, "summary": "Barrier certificates, a form of state invariants, provide an automated approach to the verification of the safety of dynamical systems. Similarly to barrier certificates, recent works explore the notion of closure certificates, a form of transition invariants, to verify dynamical systems against $ω$-regular properties including safety. A closure certificate, defined over state pairs of a dynamical system, is a real-valued function whose zero superlevel set characterizes an inductive transition invariant of the system. The search for such a certificate can be effectively automated by assuming it to be within a specific template class, e.g. a polynomial of a fixed degree, and then using optimization techniques such as sum-of-squares (SOS) programming to find it. Unfortunately, one may not be able to find such a certificate for a fixed template. In such a case, one must change the template, e.g. increase the degree of the polynomial. In this paper, we consider a notion of multiple closure certificates dubbed interpolation-inspired closure certificates. An interpolation-inspired closure certificate consists of a set of functions which jointly over-approximate a transition invariant by first considering one-step transitions, then two, and so on until a transition invariant is obtained. The advantage of interpolation-inspired closure certificates is that they allow us to prove properties even when a single function for a fixed template cannot be found using standard approaches. We present SOS programming and a scenario program to find these sets of functions and demonstrate the effectiveness of our proposed method to verify persistence and general $ω$-regular specifications in some case studies."}
{"id": "2602.12998", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.12998", "abs": "https://arxiv.org/abs/2602.12998", "authors": ["Andreas Raikos", "Sylvain Capponi", "Fabien Alet"], "title": "Variational study of the magnetization plateaus in the spin-1/2 kagome Heisenberg antiferromagnet: an approach from vision transformer neural quantum states", "comment": "16 pages, 9 figures", "summary": "We analyze the magnetization curve of the spin-1/2 kagome Heisenberg model in a magnetic field. Using state-of-the-art variational wavefunctions based on neural networks, we confirm the presence of robust magnetization plateaus at $m=1/3$, $5/9$ and $7/9$ of the saturation value, stabilized by a spontaneous symmetry breaking of lattice translations with a $\\sqrt{3}\\times \\sqrt{3}$ unit cell. Regarding the more challenging $m=1/9$ plateau, we find two competing valence bond crystals depending on the system size, both breaking translation as well as point group symmetries and with a larger $3\\times 3$ unit cell. Such quantum states with local modulations of the magnetization average values could be observed experimentally in the near future."}
{"id": "2602.12842", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12842", "abs": "https://arxiv.org/abs/2602.12842", "authors": ["Brajesh Kumar Dhakad", "Jayant Jha", "Debepsita Mukherjee"], "title": "Some bivariate distributions on a discrete torus with application to wind direction datasets", "comment": null, "summary": "Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed."}
{"id": "2602.12807", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12807", "abs": "https://arxiv.org/abs/2602.12807", "authors": ["Alessandra Cutrì", "Paola Mannucci", "Claudio Marchi", "Nicoletta Tchou"], "title": "Constrained Mean Field Games with Grushin type dynamics", "comment": "22 pages", "summary": "This paper is devoted to a class of finite horizon deterministic mean field games with Grushin type dynamics, state constraints and nonlocal coupling. First, we consider the optimal control problem that each agent aims to solve when the evolution of the population is given and we establish some properties as: the existence of an optimal trajectory for any starting point $(x,t)$, the closed graph property for the multivalued map which associates to each point $(x,t)$ the set of optimal trajectories starting from that point, endowed with a suitable notion of convergence, the continuity of the value function. The main issue to overcome is due to the local interplay at boundary points between the set of state constraints and the degenerate dynamics. To this end, we shall point out two different sets of assumptions which are both sufficient for these properties. Afterwards, we tackle the mean field games; taking advantage of the aforementioned properties, we prove the existence of a relaxed equilibrium (which describes the evolution of the game in terms of a probability on the set of admissible trajectories) and derive the existence of a mild solution (which is a couple formed by the value function for the generic player and a family of time dependent measures on the state)."}
{"id": "2602.12710", "categories": ["math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.12710", "abs": "https://arxiv.org/abs/2602.12710", "authors": ["Dietmar Bauer Kurtulus Kidik"], "title": "On the relation between Global VAR Models and Matrix Time Series Models with Multiple Terms", "comment": null, "summary": "Matrix valued time series (MaTS) and global vector autoregressive (GVAR) models both impose restrictions on the general VAR for multidimensional data sets, in order to bring down the number of parameters. Both models are motivated from a different viewpoint such that on first sight they do not have much in common. When investigating the models more closely, however, one notices many connections between the two model sets. This paper investigates the relations between the restrictions imposed by the two models. We show that under appropriate restrictions in both models we obtain a joint framework allowing to gain insight into the nature of GVARs from the viewpoint of MaTS."}
{"id": "2602.12835", "categories": ["physics.comp-ph", "physics.bio-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.12835", "abs": "https://arxiv.org/abs/2602.12835", "authors": ["Wouter Vervust", "Elias Wils", "Sina Safaei", "Daniel T. Zhang", "An Ghysels"], "title": "Estimating Full Path Lengths and Kinetics from Partial Path Transition Interface Sampling Simulations", "comment": "W.V. and E.W. contributed equally to this work. The authors declare no competing financial interest. This document is the Accepted Manuscript of a published article in the Journal of Chemical Theory and Computation, copyright \\c{opyright} American Chemical Society", "summary": "Assessing the time scale of biological processes using molecular dynamics (MD) simulations with sufficient statistical accuracy is a challenging task, as processes are often rare and/or slow events, which may extend largely beyond the time scale of what is accessible with modern day high performance computational infrastructure. Recently, the replica exchange partial path transition interface sampling (REPPTIS) algorithm was developed to study rare and slow events involving metastable states along their reactive pathways. REPPTIS is a path sampling method where paths are cut short to reduce the computational cost, while combining this with the efficiency offered by replica exchange between the partial path ensembles. However, REPPTIS still lacks a formalism to extract time-dependent properties, such as mean first passage times, fluxes, and rates, from the short partial paths. In this work, we introduce a Markov state model (MSM) framework to estimate full path lengths and kinetic properties from the overlapping partial paths generated by REPPTIS. The framework results in newly derived closed formulas for the REPPTIS crossing probability, mean first passage times (MFPTs), flux, and rate constant. Our approach is then validated using simulations of Brownian and Langevin particles on a series of one-dimensional potential energy profiles as well as the dissociation of KCl in solution, demonstrating that REPPTIS accurately reproduces the exact kinetics benchmark. The MSM framework is further applied to the trypsin-benzamidine complex to compute the dissociation rate as a test case of a biological system, albeit the computed rate underestimates the experimental value. In conclusion, our MSM framework equips REPPTIS simulations with a robust theoretical and practical foundation for extracting kinetic information from computationally efficient partial paths."}
{"id": "2602.12464", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12464", "abs": "https://arxiv.org/abs/2602.12464", "authors": ["Jawaher Kaldari", "Saif Al-Kuwari"], "title": "Challenge-Response Quantum Reinforcement Learning with Application to Quantum-Assisted Authentication", "comment": null, "summary": "Quantum reinforcement learning (QRL) has emerged as a promising research direction that integrates quantum information processing into reinforcement learning frameworks. While many existing QRL studies apply quantum agents to classical environments, it has been realized that the potential advantages of QRL are most naturally explored in environments that exhibit intrinsically quantum characteristics, where the agent's observations and interactions arise from quantum processes. In this work, we propose a quantum reinforcement learning environment formulated as a challenge-response task with hidden information. In the proposed environment, Alice encodes a classical bit into the parameters of a quantum circuit, while Bob, with a trained reinforcement learning agent, interacts with a limited number of quantum state copies to infer the hidden bit. The agent must select measurement strategies and decide when to terminate the interaction under explicit resource constraints. To study the solvability of the proposed environment, we consider three agents: a purely classical agent, a lightweight hybrid agent and a deep hybrid agent. Through experiments, we analyze the trade-off between inference accuracy and quantum resource consumption under varying interaction penalties. Our results show that the lightweight hybrid agent achieves reliable inference using as few as two quantum state copies, outperforming both the classical baseline and the deep hybrid agent in highly resource-constrained regimes. We further evaluate robustness under realistic quantum noise models and discuss the relevance of the proposed environment for security-oriented applications, including quantum-assisted authentication."}
{"id": "2602.12436", "categories": ["eess.SY", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.12436", "abs": "https://arxiv.org/abs/2602.12436", "authors": ["Mohammed Adib Oumer", "Vishnu Murali", "Majid Zamani"], "title": "Interpolation-Inspired Closure Certificates", "comment": null, "summary": "Barrier certificates, a form of state invariants, provide an automated approach to the verification of the safety of dynamical systems. Similarly to barrier certificates, recent works explore the notion of closure certificates, a form of transition invariants, to verify dynamical systems against $ω$-regular properties including safety. A closure certificate, defined over state pairs of a dynamical system, is a real-valued function whose zero superlevel set characterizes an inductive transition invariant of the system. The search for such a certificate can be effectively automated by assuming it to be within a specific template class, e.g. a polynomial of a fixed degree, and then using optimization techniques such as sum-of-squares (SOS) programming to find it. Unfortunately, one may not be able to find such a certificate for a fixed template. In such a case, one must change the template, e.g. increase the degree of the polynomial. In this paper, we consider a notion of multiple closure certificates dubbed interpolation-inspired closure certificates. An interpolation-inspired closure certificate consists of a set of functions which jointly over-approximate a transition invariant by first considering one-step transitions, then two, and so on until a transition invariant is obtained. The advantage of interpolation-inspired closure certificates is that they allow us to prove properties even when a single function for a fixed template cannot be found using standard approaches. We present SOS programming and a scenario program to find these sets of functions and demonstrate the effectiveness of our proposed method to verify persistence and general $ω$-regular specifications in some case studies."}
{"id": "2602.12559", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12559", "abs": "https://arxiv.org/abs/2602.12559", "authors": ["Zhiqiang Cai", "Anastassia Doktorova", "Robert D. Falgout", "César Herrera"], "title": "Convergence Analysis of Block Newton Methods for 1D Shallow Neural Network Approximation", "comment": null, "summary": "This paper analyzes local convergence of the block Newton (BN) method introduced in [5, 6] for one-dimensional shallow neural network approximation to functions and diffusion-reaction problems. The BN method consists of the 2x2 block nonlinear Gauss-Seidel, linear Gauss-Seidel, or Jacobi method for outer iteration and the Newton method for inner iteration. The blocks are corresponding to the linear and the nonlinear parameters. Under some reasonable assumptions, we establish local convergence of the BN methods as well as the reduced BN (rBN) method for one-dimensional diffusion-reaction problems and least-squares function approximation. Unlike common optimization methods, the rBN allows for the reduction of the number of parameters during the optimization process when some neurons contribute little to the approximation or are at nearly optimal locations."}
{"id": "2602.12452", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12452", "abs": "https://arxiv.org/abs/2602.12452", "authors": ["Jonathan E. Swindell", "David W. Cox", "Rebekah Edwards", "Emma Lever", "Adam C. Goad", "Austin Egbert", "Charles Baylis", "Robert J. Marks"], "title": "Implementation of a Directional Modulation Testbed for Reconfigurable Transmitters for Spatially Agile MIMO Systems", "comment": null, "summary": "This paper demonstrates the implementation and validation of a microwave testbed for directionally modulated transmission. Directional modulation enables multiple communication and/or radar signals to be transmitted in multiple directions simultaneously using a single phased array aperture, helping to relieve spectral congestion. A two-element transmitter array is driven by a Xilinx ZCU208 Radio Frequency System on a Chip (RFSoC). Our testbed provides a foundation for developing a fully reconfigurable array transmitter for multi-user multiple-input multiple-output (MU-MIMO) radar and communications, which will incorporate in-situ measurement, reconfigurable matching circuitry, and fast tuning algorithms for frequency and directional selectivity. This testbed enables development and validation of reconfigurable techniques for adaptive spectral and spatial coexistence."}
{"id": "2602.12294", "categories": ["physics.comp-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.12294", "abs": "https://arxiv.org/abs/2602.12294", "authors": ["Michael Kim", "Wei Cai"], "title": "Accelerated Markov Chain Monte Carlo Simulation via Neural Network-Driven Importance Sampling", "comment": null, "summary": "Atomistic simulations provide valuable insights into the physical processes governing material behavior. However, their applicability is fundamentally constrained by the limited time scales accessible to brute-force simulations. This bottleneck often stems from complex energy landscapes where the systems stay trapped in metastable states for long periods of time. Yet, the long-term evolution is controlled by the transitions between the metastable states, which are rare events and difficult to observe. We present an importance sampling method designed to accelerate the time scale of Markov chain Monte Carlo (MCMC) simulations. By employing a bias potential, our approach enhances the sampling of rare transition events while preserving the relative probabilities of distinct transition pathways. The bias potential is represented by a neural network which enables the flexibility needed for high-dimensional systems. We propose a rigorous formulation to obtain the original transition rates between metastable states using transition paths obtained from the biased simulation. We further use a branching random walk (BRW) technique to enhance efficiency and to reduce variance. The proposed methodology is validated on 2-dimensional and 14-dimensional systems, demonstrating its accuracy and scalability."}
{"id": "2602.13050", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13050", "abs": "https://arxiv.org/abs/2602.13050", "authors": ["Gennady Y. Chitov"], "title": "Topology of the Fermi surface and universality of the metal-metal and metal-insulator transitions: $d$-dimensional Hatsugai-Kohmoto model as an example", "comment": "34 pages (18 pp main text + 3 appendices), 13 figures", "summary": "The earlier theory [1] of the quantum phase transitions related to the change of the Fermi Surface Topology (FST) is advanced. For such transitions the Fermi surface as a quantum critical manifold determined by the Lee-Yang zeros, the order parameter $\\mathcal{P}$ as the $d$-volume of the Fermi sea, and the special FST universality class were introduced in [1]. The exactly solvable Hatsugai-Kohmoto (HK) $d$-dimensional ($d=1,2,3$) model of interacting fermions is analyzed. We explore the relation between the Lee-Yang zeros, the Luttinger and the plateau (Oshikawa) theorems. The validity of the Luttinger theorem in the HK model is confirmed. It is shown that the order parameter $\\mathcal{P}$ and the FST universality class describe the transitions between metal and band/Mott insulators, as well as the Lifshitz and van Hove gapless-to-gapless transitions. The gapless phases are established to be the Landau Fermi liquids (metals). In addition to the conventional paradigm with a continuous order parameter, we apply the homology theory to analyze the FST transitions. They are critical points of the Morse function. To quantify FST we use the Euler characteristic, which is calculated for each phase of the HK model. We claim that the FST universality class is robust with respect to interactions and other model details, under the condition that the critical points are non-degenerate."}
{"id": "2602.12845", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12845", "abs": "https://arxiv.org/abs/2602.12845", "authors": ["Donatas Šlevinskas", "Ieva Burakauskaitė", "Andrius Čiginas"], "title": "Small area estimation using incomplete auxiliary information", "comment": "21 pages, 3 figures", "summary": "Auxiliary information is increasingly available from administrative and other data sources, but it is often incomplete and of non-probability origin. We propose a two-step small area estimation approach in which the first step relies on design-based model calibration and exploits a large non-probability source providing a noisy proxy of the study variable for only part of the population. A unit-level measurement-error working model is fitted on the linked overlap between the probability survey and the external source, and its predictions are incorporated through domain-specific model-calibration constraints to obtain approximately design-unbiased domain totals. These totals and their variance estimates are then used in a Fay-Herriot area-level model with exactly known covariates to produce empirical best linear unbiased predictors. The approach is demonstrated in three enterprise survey settings from official statistics by integrating probability sample data with (i) administrative records, (ii) a cut-off data source, and (iii) web-scraped online information. Empirical comparisons show consistent improvements in domain-level precision over direct estimation and over a Fay-Herriot benchmark that directly incorporates the proxy information as an error-prone covariate. These gains are achieved without modeling the selection mechanism of the non-probability sample."}
{"id": "2602.12821", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12821", "abs": "https://arxiv.org/abs/2602.12821", "authors": ["Stephanie Caro", "Abderrahim Hantoute"], "title": "Explicit data-dependent characterizations of the subdifferential of convex pointwise suprema and optimality conditions", "comment": null, "summary": "We establish explicit data-dependent and symmetric characterizations of the subdifferential of the supremum of convex functions, formulated directly in terms of the underlying data functions. In our approach, both active and non-active functions contribute equally through their subdifferentials, thereby avoiding the need for additional geometric constructions, such as the domain of the supremum, that arise in previous developments. Applications to infinite convex optimization yield sharp Karush-Kuhn-Tucker and Fritz-John optimality conditions, expressed exclusively in terms of the objective and constraint functions and clearly distinguishing the roles of (almost) active and non-active constraints."}
{"id": "2602.12874", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.12874", "abs": "https://arxiv.org/abs/2602.12874", "authors": ["Markus Zobel", "Axel Munk"], "title": "Quantile characterization of univariate unimodality", "comment": "18 pages, 2 figures", "summary": "Unimodal univariate distributions can be characterized as piecewise convex-concave cumulative distribution functions. In this note we transfer this shape constraint characterization to the quantile function. We show that this characterization comes with the upside that the quantile function of a unimodal distribution is always absolutely continuous and consequently unimodality is equivalent to the quasi-convexity of its Radon-Nikodym derivative, i.e., the quantile density. Our analysis is based on the theory of generalized inverses of non-decreasing functions and relies on a version of the inverse function rule for non-decreasing functions."}
{"id": "2602.13092", "categories": ["physics.comp-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2602.13092", "abs": "https://arxiv.org/abs/2602.13092", "authors": ["Erik M. Åsgrim", "Luca Pennati", "Marco Pasquale", "Stefano Markidis"], "title": "Tensor Network Compression for Fully Spectral Vlasov-Poisson Simulation", "comment": "35 pages, 9 figures", "summary": "We propose a numerical method for kinetic plasma simulation in which the phase-space distribution function is represented by a low-rank tensor network with an adaptive level of compression. The Vlasov-Poisson system is advanced using Strang splitting, and each substep is treated spectrally in the corresponding variable. By expressing both the distribution function and the Fourier transform as tensor network objects (state and operator representations), spectral transforms are applied directly in compressed form, enabling time stepping without reconstructing the full phase-space grid. The self-consistent electric field is also computed within the tensor formalism. The charge density is obtained by contracting over velocity degrees of freedom and extracting the zero Fourier mode, which provides the source term for a spectral Poisson solver. We validate the approach on standard benchmarks, including Landau damping and the two-stream instability. Finally, we systematically study how compression parameters, including truncation tolerances and internal ranks (bond dimensions), affect momentum and energy conservation, positivity behavior, robustness to filamentation, and computational cost."}
{"id": "2602.12465", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12465", "abs": "https://arxiv.org/abs/2602.12465", "authors": ["Grier M. Jones", "Aviraj Newatia", "Alexander Lao", "Aditya K. Rao", "Viki Kumar Prasad", "Hans-Arno Jacobsen"], "title": "Probabilistic Design of Parametrized Quantum Circuits through Local Gate Modifications", "comment": null, "summary": "Within quantum machine learning, parametrized quantum circuits provide flexible quantum models, but their performance is often highly task-dependent, making manual circuit design challenging. Alternatively, quantum architecture search algorithms have been proposed to automate the discovery of task-specific parametrized quantum circuits using systematic frameworks. In this work, we propose an evolution-inspired heuristic quantum architecture search algorithm, which we refer to as the local quantum architecture search. The goal of the local quantum architecture search algorithm is to optimize parametrized quantum circuit architectures through a local, probabilistic search over a fixed set of gate-level actions applied to existing circuits. We evaluate the local quantum architecture search algorithm on two synthetic function-fitting regression tasks and two quantum chemistry regression datasets, including the BSE49 dataset of bond separation energies for first- and second-row elements and a dataset of water conformers generated using the data-driven coupled-cluster approach. Using state-vector simulation, our results highlight the applicability of local quantum architecture search algorithm for identifying competitive circuit architectures with desirable performance metrics. Lastly, we analyze the properties of the discovered circuits and demonstrate the deployment of the best-performing model on state-of-the-art quantum hardware."}
{"id": "2602.12452", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12452", "abs": "https://arxiv.org/abs/2602.12452", "authors": ["Jonathan E. Swindell", "David W. Cox", "Rebekah Edwards", "Emma Lever", "Adam C. Goad", "Austin Egbert", "Charles Baylis", "Robert J. Marks"], "title": "Implementation of a Directional Modulation Testbed for Reconfigurable Transmitters for Spatially Agile MIMO Systems", "comment": null, "summary": "This paper demonstrates the implementation and validation of a microwave testbed for directionally modulated transmission. Directional modulation enables multiple communication and/or radar signals to be transmitted in multiple directions simultaneously using a single phased array aperture, helping to relieve spectral congestion. A two-element transmitter array is driven by a Xilinx ZCU208 Radio Frequency System on a Chip (RFSoC). Our testbed provides a foundation for developing a fully reconfigurable array transmitter for multi-user multiple-input multiple-output (MU-MIMO) radar and communications, which will incorporate in-situ measurement, reconfigurable matching circuitry, and fast tuning algorithms for frequency and directional selectivity. This testbed enables development and validation of reconfigurable techniques for adaptive spectral and spatial coexistence."}
{"id": "2602.12580", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12580", "abs": "https://arxiv.org/abs/2602.12580", "authors": ["Yaguang Gu", "Guanghui Hu", "Tao Tang"], "title": "Adaptive mesh methods for hyperbolic conservation laws with bound-preserving flux limiters", "comment": "25 pages", "summary": "In this paper, we develop bound-preserving (BP) finite-volume schemes for hyperbolic conservation laws on adaptive moving meshes. For scalar conservative laws, we rewrite the conventional high-order discretization as a convex combination of first-order counterparts on each sub-cell, which is mathematically equivalent to introducing a bound-preserving flux limiter. Such a limiter is inexpensive to evaluate, with a feature that the corresponding BP CFL conditions depend solely on the first-order sub-cell schemes. A mild CFL restriction is derived under which high-order spatial accuracy is retained. The proposed BP schemes are extend to two nonlinear systems, namely, the Euler equations and the five-equation transport model of two-medium flows. Numerical results demonstrate that the present schemes possess high resolution and strong robustness properties."}
{"id": "2602.12473", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12473", "abs": "https://arxiv.org/abs/2602.12473", "authors": ["Bikram Panthee", "Haoming Yang", "Corey D. Harper", "Amritanshu Pandey"], "title": "Grid-ECO: Grid Aware Electric Vehicle Charging Stations Placement Optimizer", "comment": null, "summary": "The paper develops a methodology, Grid-ECO, to optimally allocate electric vehicle charging stations (EVCS) within a distribution feeder, while considering EV charging demand at census-level granularity. The underlying problem is NP-hard and requires satisfying nonlinear, nonconvex, three-phase unbalanced AC network constraints while including integer decision variables. Existing works cannot guarantee AC feasibility nor optimality of this problem without either i) relaxing the integer decision variable space or ii) convexifying AC constraints. Proposed Grid-ECO exactly solves the underlying mixed-integer nonlinear program (MINLP) to near-zero optimality gap while prioritizing candidate locations based on grid voltage and current sensitivities. To solve the MINLP exactly, Grid-ECO exactly reformulates it into mixed-integer bilinear program (MIBLP), enabling global optimization using the spatial branch-and-bound algorithm (sBnB). To ensure computational tractability for large-scale feeders, we develop and include a novel presolving strategy based on Sequential Bound Tightening (SBT) with variable filtering and decomposition. Case studies demonstrate that Grid-ECO outperforms the off-the-shelf Gurobi sBnB solver by solving cases where no feasible solution is found within 167 hours. When feasible solution is found by off-the-shelf solver, Grid-ECO reduces solution time by up to 73\\% and sBnB node exploration by up to 97\\%, while achieving a 0\\% optimality gap and guaranteed AC feasibility."}
{"id": "2602.12990", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.12990", "abs": "https://arxiv.org/abs/2602.12990", "authors": ["Takayuki Yokoyama", "Yasuhiro Tada"], "title": "Diagnosing energy gap in quantum spin liquids via polarization amplitude", "comment": "9 pages, 7 figures", "summary": "Identifying whether a many-body ground state is gapped or gapless is a fundamental yet challenging problem, especially in quantum spin liquids. In this work, we develop a gap-diagnostic scheme based on the polarization amplitude defined via a twist operator, evaluated within the infinite density-matrix renormalization group (iDMRG) framework. As a benchmark, analysis of the spin-$1/2$ XXZ chain demonstrates that the polarization amplitude clearly distinguishes the gapless Tomonaga-Luttinger liquid from the gapped Néel phase. We then extend this framework to infinite cylinders of the spin-$1/2$ XY-$J_χ$ model on the square lattice. We find that the polarization amplitude sharply detects the transition between the gapless XY phase and the gapped chiral spin liquid phase. These results show that polarization amplitudes provide a strong energy-gap diagnostic in two-dimensional frustrated quantum magnets, including quantum spin liquids."}
{"id": "2602.13113", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.13113", "abs": "https://arxiv.org/abs/2602.13113", "authors": ["Arnau Romaguera", "Elizabeth Skoropata", "Yun Yen", "Biaolong Liu", "Abhishek Nag", "Shih-Wen Huang", "Ludmila Leroy", "Katja Sophia Moos", "Gian Parusa", "Serhane Zerdane", "Ritwika Mandal", "Celine Mariette", "Matteo Levantino", "Eugenio Paris", "Luc Patthey", "Ekaterina Pomjakushina", "Urs Staub", "Monica Ciomaga Hatnean", "Michael Schueler", "Elia Razzoli", "Hiroki Ueda"], "title": "Hierarchical quasiparticle dynamics in antiferromagnets revealed by time- and momentum-resolved X-ray scattering", "comment": "57 pages, 21 figures", "summary": "Energy flows among coupled subsystems are essential for ultrafast dynamics and high-speed technologies. In magnetic materials, spin fluctuations -- magnons -- mediate these flows in ultrafast magnetism. Yet momentum-resolved access to low-energy magnons governing the microscopic dynamics has been lacking. Using time-resolved resonant diffuse scattering alongside complementary time-resolved X-ray techniques and quantum-kinetic simulations, we unveil the hierarchical energy pathways among correlated systems in the photoexcited antiferromagnet CuO. Above-bandgap excitation triggers near-instantaneous spin disorder, generating non-thermal magnons throughout reciprocal space within femtoseconds. Real-time momentum-resolved tracking reveals picosecond magnon quasi-thermalization, followed by nanosecond recovery via momentum-selective magnon-phonon scattering. The quasiparticle dispersion mismatch creates recovery bottlenecks that control non-equilibrium lifetimes. This microscopic framework transcends phenomenological models and generalizes across materials, establishing design principles for ultrafast control of material properties."}
{"id": "2602.12900", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12900", "abs": "https://arxiv.org/abs/2602.12900", "authors": ["Ganesh Vishnu Avhad", "Sudheesh K. Kattumannil"], "title": "A unified testing approach for log-symmetry using Fourier methods", "comment": null, "summary": "Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability."}
{"id": "2602.12867", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12867", "abs": "https://arxiv.org/abs/2602.12867", "authors": ["Kezang Yuden", "Levin Nemesch", "Stefan Ruzika"], "title": "Parametric Biobjective Linear Programming", "comment": null, "summary": "We consider parametric linear programming problems with multiple objective functions depending linearly on some parameter. Both parametric (single-objective) linear programming and (non-parametric) multi-objective linear programming are well-researched topics. However, literature on the combination of both, parametric linear programming with multiple objectives, is scarce. This research gap encourages our work in this field. Our main focus is on biobjective linear programs with a single parameter. We establish a connection of this problem to non-parametric multi-objective problems. Using the so-called weight set decomposition, we are able to explain the behavior of parametric biobjective linear programs when the parameter value is variated. We investigate two special cases of parametric biobjective linear programs: In the first, there is only one parametric objective and, in the second, the parametric dependency is the same for both objectives. We prove that there is a one-to-one correspondence between the solution of the parametric program and the solution of the triobjective program using the weighted sum scalarization. We provide structural insights to the solution of the parametric biobjective linear program with respect to extreme weights of the weight set of the triobjective linear program and develop solution strategies for the parametric program."}
{"id": "2602.12334", "categories": ["quant-ph", "math.LO", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12334", "abs": "https://arxiv.org/abs/2602.12334", "authors": ["Jacopo Surace"], "title": "Reconstruction of finite Quasi-Probability and Probability from Principles: The Role of Syntactic Locality", "comment": "22 pages of main text, 3 pages of bibliography, 19 pages of appendix", "summary": "Quasi-probabilities appear across diverse areas of physics, but their conceptual foundations remain unclear: they are often treated merely as computational tools, and operations like conditioning and Bayes' theorem become ambiguous. We address both issues by developing a principled framework that derives quasi-probabilities and their conditional calculus from structural consistency requirements on how statements are valued across different universes of discourse, understood as finite Boolean algebras of statements.We begin with a universal valuation that assigns definite (possibly complex) values to all statements. The central concept is Syntactic Locality: every universe can be embedded within a larger ambient one, and the universal valuation must behave coherently under such embeddings and restrictions. From a set of structural principles, we prove a representation theorem showing that every admissible valuation can be re-expressed as a finitely additive measure on mutually exclusive statements, mirroring the usual probability sum rule. We call such additive representatives pre-probabilities. This representation is unique up to an additive regraduation freedom. When this freedom can be fixed canonically, pre-probabilities reduce to finite quasi-probabilities, thereby elevating quasi-probability theory from a computational device to a uniquely determined additive representation of universal valuations. Classical finite probabilities arise as the subclass of quasi-probabilities stable under relativisation, i.e., closed under restriction to sub-universes. Finally, the same framework enables us to define a coherent theory of conditionals, yielding a well-defined generalized Bayes' theorem applicable to both pre-probabilities and quasi-probabilities. We conclude by discussing additional regularity conditions, including the role of rational versus irrational probabilities in this setting."}
{"id": "2602.13130", "categories": ["physics.comp-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.13130", "abs": "https://arxiv.org/abs/2602.13130", "authors": ["Magdalena Häupl", "Sebastian Falkner", "Peter G. Bolhuis", "Christoph Dellago", "Alessandro Coretti"], "title": "An Always-Accepting Algorithm for Transition Path Sampling", "comment": null, "summary": "We present a one-way shooting algorithm for transition path sampling that accepts every proposed trajectory yet samples the correct transition path ensemble for systems with overdamped stochastic dynamics. The method is based on two key elements: a procedure to propose trajectories that are always reactive, and a reweighting scheme that corrects for the bias introduced by always accepting the proposed paths. This approach significantly improves the efficiency of transition path sampling by eliminating the cost associated with generating trajectories that are then rejected. We demonstrate the algorithm by investigating the formation of CO$_2$ clathrate hydrates along different reaction mechanisms, showing that the increased efficiency allows proper sampling of the formation of crystalline hydrates at temperatures and pressures that are difficult to access with conventional algorithms."}
{"id": "2602.12472", "categories": ["quant-ph", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.12472", "abs": "https://arxiv.org/abs/2602.12472", "authors": ["Sofiane Chalal", "Nina H. Amini", "Hamed Amini", "Mathieu Laurière"], "title": "Dynamic Programming Principle and Stabilization for Mean-Field Quantum Filtering Systems", "comment": null, "summary": "Working within the quantum filtering framework, we establish a dynamic programming principle in an infinite-dimensional setting by embedding the state space into the Hilbert-Schmidt space. We then study a stabilization problem for continuously monitored Ising-coupled qubits and, in the mean-field limit, demonstrate quantum state reduction together with exponential convergence toward prescribed eigenstates under suitable feedback laws."}
{"id": "2602.12473", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12473", "abs": "https://arxiv.org/abs/2602.12473", "authors": ["Bikram Panthee", "Haoming Yang", "Corey D. Harper", "Amritanshu Pandey"], "title": "Grid-ECO: Grid Aware Electric Vehicle Charging Stations Placement Optimizer", "comment": null, "summary": "The paper develops a methodology, Grid-ECO, to optimally allocate electric vehicle charging stations (EVCS) within a distribution feeder, while considering EV charging demand at census-level granularity. The underlying problem is NP-hard and requires satisfying nonlinear, nonconvex, three-phase unbalanced AC network constraints while including integer decision variables. Existing works cannot guarantee AC feasibility nor optimality of this problem without either i) relaxing the integer decision variable space or ii) convexifying AC constraints. Proposed Grid-ECO exactly solves the underlying mixed-integer nonlinear program (MINLP) to near-zero optimality gap while prioritizing candidate locations based on grid voltage and current sensitivities. To solve the MINLP exactly, Grid-ECO exactly reformulates it into mixed-integer bilinear program (MIBLP), enabling global optimization using the spatial branch-and-bound algorithm (sBnB). To ensure computational tractability for large-scale feeders, we develop and include a novel presolving strategy based on Sequential Bound Tightening (SBT) with variable filtering and decomposition. Case studies demonstrate that Grid-ECO outperforms the off-the-shelf Gurobi sBnB solver by solving cases where no feasible solution is found within 167 hours. When feasible solution is found by off-the-shelf solver, Grid-ECO reduces solution time by up to 73\\% and sBnB node exploration by up to 97\\%, while achieving a 0\\% optimality gap and guaranteed AC feasibility."}
{"id": "2602.12676", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12676", "abs": "https://arxiv.org/abs/2602.12676", "authors": ["Changjian Xie"], "title": "Semi-implicit Structure Preserving Method for The Landau-Lifshitz Equation", "comment": null, "summary": "A critical challenge inherent to the projection method applied to the Landau-Lifshitz equation is the deficiency of rigorous theoretical justifications for the stability of its projection step. To mitigate this limitation, we introduce a semi-implicit numerical scheme, which is formulated on the basis of the first-order Backward Differentiation Formula (BDF) incorporated with one-sided extrapolation and a Crank-Nicolson-type norm-preserving procedure. This proposed scheme exhibits three fundamental characteristics: structure preservation, numerical stability, and first-order accuracy in time. In practical implementations, the scheme not only ensures stable computation and adheres to the norm constraint but also guarantees the uniqueness of the numerical solution, thereby providing substantial facilitation for the theoretical analysis of the normalizing step."}
{"id": "2602.12573", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12573", "abs": "https://arxiv.org/abs/2602.12573", "authors": ["Jiawei Zhang", "Gregor Verbic", "Frederik Geth", "Mohsen Aldaadi", "Rahmat Heidari", "Julio Braslavsky"], "title": "Dynamic Network Prices for Prosumer-aware Hosting Capacity Management", "comment": null, "summary": "The fast uptake of distributed energy resources (DERs) presents increasing challenges for managing hosting capacity in distribution networks. Existing solutions include direct load control, operating envelopes, and price-based control through dynamic energy prices. Despite their effectiveness, these methods often rely on assumed prosumer behavioural patterns and overlook prosumers' desire to retain control over their devices. Additionally, current fixed or Time-of-Use (ToU) prices are based on spatial and temporal averages, having limited impact on network conditions and DER operation. To address these limitations, this paper proposes a bilevel optimisation framework that explicitly models prosumer decision-making in the design of dynamic network prices. The upper level represents the distribution system operator (DSO), setting network prices under cost-recovery and network constraints, while the lower level models prosumers optimising DER operation in response. The proposed framework preserves customer prerogative, enhances DER flexibility, and offers actionable insights for network hosting capacity management and the evolution of network tariff structures under high DER penetration."}
{"id": "2602.13050", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13050", "abs": "https://arxiv.org/abs/2602.13050", "authors": ["Gennady Y. Chitov"], "title": "Topology of the Fermi surface and universality of the metal-metal and metal-insulator transitions: $d$-dimensional Hatsugai-Kohmoto model as an example", "comment": "34 pages (18 pp main text + 3 appendices), 13 figures", "summary": "The earlier theory [1] of the quantum phase transitions related to the change of the Fermi Surface Topology (FST) is advanced. For such transitions the Fermi surface as a quantum critical manifold determined by the Lee-Yang zeros, the order parameter $\\mathcal{P}$ as the $d$-volume of the Fermi sea, and the special FST universality class were introduced in [1]. The exactly solvable Hatsugai-Kohmoto (HK) $d$-dimensional ($d=1,2,3$) model of interacting fermions is analyzed. We explore the relation between the Lee-Yang zeros, the Luttinger and the plateau (Oshikawa) theorems. The validity of the Luttinger theorem in the HK model is confirmed. It is shown that the order parameter $\\mathcal{P}$ and the FST universality class describe the transitions between metal and band/Mott insulators, as well as the Lifshitz and van Hove gapless-to-gapless transitions. The gapless phases are established to be the Landau Fermi liquids (metals). In addition to the conventional paradigm with a continuous order parameter, we apply the homology theory to analyze the FST transitions. They are critical points of the Morse function. To quantify FST we use the Euler characteristic, which is calculated for each phase of the HK model. We claim that the FST universality class is robust with respect to interactions and other model details, under the condition that the critical points are non-degenerate."}
{"id": "2602.13132", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.13132", "abs": "https://arxiv.org/abs/2602.13132", "authors": ["Anastasia Enckell", "Stefan Kehrein"], "title": "Resonant level model coupled to a Sachdev-Ye-Kitaev bath", "comment": null, "summary": "We investigate the non-equilibrium dynamics of a resonant level model coupled to a strongly interacting electron bath modeled by a Sachdev-Ye-Kitaev (SYK) model. Different from the well-investigated case of a structureless non-interacting Fermi gas bath leading to a temperature-independent exponential decay of the impurity orbital occupation, we find a temperature-dependent oscillatory decay. We attribute this difference to the lack of quasiparticles in the SYK model, which is reflected in its singular density of states at the Fermi level. Our results are exact and can be obtained analytically by mapping to a suitably structured Fermi gas bath as an ancillary model for the SYK bath."}
{"id": "2602.12992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12992", "abs": "https://arxiv.org/abs/2602.12992", "authors": ["Reagan Mozer", "Nicole E. Pashley", "Luke Miratrix"], "title": "Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes", "comment": null, "summary": "In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group."}
{"id": "2602.12886", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12886", "abs": "https://arxiv.org/abs/2602.12886", "authors": ["Hui Huang", "Jethro Warnett"], "title": "Well-posedness and mean-field limit estimate of a consensus-based algorithm for min-max problems", "comment": "Main body is 28 pages long. arXiv admin note: text overlap with arXiv:2505.13632", "summary": "The recent work arXiv:2407.17373 proposes a derivative-free consensus-based particle method that computes global solutions to nonconvex-nonconcave min-max problems and establishes global exponential convergence in the sense of the mean-field law. This paper aims to address the theoretical gaps in arXiv:2407.17373, specifically by providing a quantitative estimate of the mean-field limit with respect to the number of particles, as well as establishing the well-posedness of both the finite particle model and the corresponding mean-field dynamics."}
{"id": "2602.12842", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12842", "abs": "https://arxiv.org/abs/2602.12842", "authors": ["Brajesh Kumar Dhakad", "Jayant Jha", "Debepsita Mukherjee"], "title": "Some bivariate distributions on a discrete torus with application to wind direction datasets", "comment": null, "summary": "Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed."}
{"id": "2602.12518", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12518", "abs": "https://arxiv.org/abs/2602.12518", "authors": ["Joseph Barreto", "Daniel Lidar"], "title": "Compressed Sensing Shadow Tomography", "comment": null, "summary": "Estimating many local expectation values over time is a central measurement bottleneck in quantum simulation and device characterization. We study the task of reconstructing the Pauli-signal matrix $S_{ij}=\\text{Tr}(O_i ρ(t_j))$ for a collection of $M$ low-weight Pauli observables $\\{O_i\\}_{i=1}^M$ over $N$ timesteps $\\{t_j\\}_{j=1}^N$, while minimizing the total number of device shots. We propose a Compressed Sensing Shadow Tomography (CSST) protocol that combines two complementary reductions. First, local classical shadows reduce the observable dimension by enabling many Pauli expectation values to be estimated from the same randomized snapshots at a fixed time. Second, compressed sensing reduces the time dimension by exploiting the fact that many expectation-value traces are spectrally sparse or compressible in a unitary (e.g., Fourier) transform basis. Operationally, CSST samples $m\\ll N$ timesteps uniformly at random, collects shadows only at those times, and then reconstructs each length-$N$ signal via standard $\\ell_1$-based recovery in the unitary transform domain. We provide end-to-end guarantees that explicitly combine shadow estimation error with compressed sensing recovery bounds. For exactly $s$-sparse signals in a unitary transform basis, we show that $m=O \\left(s\\log^2 s \\log N\\right)$ random timesteps suffice (with high probability), leading to total-shot savings scaling as $\\widetildeΘ(N/s)$ (i.e., up to polylogarithmic factors) relative to collecting shadows at all $N$ timesteps. For approximately sparse signals, the reconstruction error decomposes into a compressibility (tail) term plus a noise term. We present numerical experiments on noisy many-qubit dynamics that support strong Fourier compressibility of Pauli traces and demonstrate substantial shot reductions with accurate reconstruction."}
{"id": "2602.12573", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12573", "abs": "https://arxiv.org/abs/2602.12573", "authors": ["Jiawei Zhang", "Gregor Verbic", "Frederik Geth", "Mohsen Aldaadi", "Rahmat Heidari", "Julio Braslavsky"], "title": "Dynamic Network Prices for Prosumer-aware Hosting Capacity Management", "comment": null, "summary": "The fast uptake of distributed energy resources (DERs) presents increasing challenges for managing hosting capacity in distribution networks. Existing solutions include direct load control, operating envelopes, and price-based control through dynamic energy prices. Despite their effectiveness, these methods often rely on assumed prosumer behavioural patterns and overlook prosumers' desire to retain control over their devices. Additionally, current fixed or Time-of-Use (ToU) prices are based on spatial and temporal averages, having limited impact on network conditions and DER operation. To address these limitations, this paper proposes a bilevel optimisation framework that explicitly models prosumer decision-making in the design of dynamic network prices. The upper level represents the distribution system operator (DSO), setting network prices under cost-recovery and network constraints, while the lower level models prosumers optimising DER operation in response. The proposed framework preserves customer prerogative, enhances DER flexibility, and offers actionable insights for network hosting capacity management and the evolution of network tariff structures under high DER penetration."}
{"id": "2602.12788", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12788", "abs": "https://arxiv.org/abs/2602.12788", "authors": ["Daniele Corallo", "Pascal Lehner", "Christian Wieners"], "title": "A parallel space-time $p$-adaptive discontinuous Galerkin method for nonlinear acoustics", "comment": null, "summary": "In this paper, we introduce and analyze a space-time $p$-adaptive discontinuous Galerkin method for nonlinear acoustics. We first present the underlying mathematical model, which is based on a recently derived formulation involving, in particular, only first order in time derivatives. We then propose a spacetime discontinuous Galerkin discretization of this model, combining a symmetric Friedrichs systems discretization for symmetric hyperbolic systems with an interior penalty discretization for damping terms. The resulting nonlinear system is solved using Newton's method. Next, we present a well-posedness analysis of the discrete problem. The analysis begins with a linearized system, for which stability is shown. Using a fixed point argument, these results are extended to the fully discrete nonlinear system, yielding a priori error estimates in a natural discontinuous Galerkin norm. Finally, we present numerical experiments demonstrating the parallel solvability of the spacetime formulation and the effectiveness of p-adaptivity. The results confirm the theoretical convergence rates and show that adaptive refinement can reduce the number of degrees of freedom required to accurately approximate selected goal functionals. Moreover, the experiments demonstrate that the model reproduces characteristic phenomena of nonlinear acoustics, such as harmonic generation, thereby validating the proposed model."}
{"id": "2602.12603", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12603", "abs": "https://arxiv.org/abs/2602.12603", "authors": ["Ziyan Lin", "Liang Xu"], "title": "Curvature-Guided Safety Filters: State-Dependent Hessian-Weighted Projection with Provable Performance Bounds", "comment": null, "summary": "Safety filters provide a lightweight mechanism for enforcing state and input safety in learning-enabled control. However, common Euclidean projections onto the safe set disregard long-term performance, while directly optimizing the action-value function within the safe set can be nonconvex and computationally prohibitive. This paper proposes a state-dependent, Hessian-guided projection for safety filtering that preserves convexity while improving performance. The key idea is to select a weighted projection matrix from the curvature of the action-value function, thereby biasing the correction toward action directions with higher value sensitivity. We establish (i) a uniform bound on the performance gap between the weighted projection and the safe value-optimal action, and (ii) a condition under which the weighted projection outperforms the Euclidean projection in long-term value. To support black-box controllers, we further present a data-driven construction of the weighted projection matrix via an iterative Q-function learning algorithm with quadratic feature blocks and regularization that enforces curvature dominance and bounded higher-order terms. Simulations on a quadrotor tracking-and-avoidance task indicate that the proposed filter maintains safety while reducing value degradation relative to Euclidean projection, with computational overhead compatible with real-time operation."}
{"id": "2602.13095", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.13095", "abs": "https://arxiv.org/abs/2602.13095", "authors": ["Hironobu Yoshida", "Ryusuke Hamazaki"], "title": "Theory of Steady States for Lindblad Equations beyond Time-Independence: Classification, Uniqueness and Symmetry", "comment": "25 pages, 3 figures", "summary": "We present a rigorous and comprehensive classification of the asymptotic behavior of time-quasiperiodic Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equations under the assumption of Hermitian jump operators. Our main contributions are twofold: first, we establish a criterion for the uniqueness of steady states. The criterion is formulated in terms of the algebra generated by the GKSL generators and provides a necessary and sufficient condition when the generators are analytic functions of time. We demonstrate the utility of our criterion through prototypical examples, including quantum many-body spin chains. Second, we extend the concept of strong symmetry for time-dependent GKSL equations by introducing two distinct forms, strong symmetry in the Schrödinger picture and that in the interaction picture, and completely classify the asymptotic dynamics with them. More concretely, we rigorously uncover that the strong symmetry in the interaction picture is responsible for non-trivial time-dependent steady states, such as coherent oscillations, whereas that in the Schrödinger picture controls the existence of time-independent steady states. This classification not only encompasses established mechanisms underlying non-trivial oscillatory steady states, such as strong dynamical symmetry and Floquet dynamical symmetry, but also reveals symmetry-predicted, time-dependent asymptotic dynamics in a novel class of open quantum systems. Our framework thus provides a rigorous foundation for controlling dissipative quantum systems in a time-dependent manner."}
{"id": "2602.12685", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.12685", "abs": "https://arxiv.org/abs/2602.12685", "authors": ["Yoshito Watanabe", "Bianca Bannenberg", "Simon Trebst"], "title": "Floquet implementation of a 3d fermionic toric code with full logical code space", "comment": "12 pages, 9 figures", "summary": "Floquet quantum error-correcting codes provide an operationally economical route to fault tolerance by dynamically generating stabilizer structures using only two-body Pauli measurements. But while it is well established that stabilizer codes in higher spatial dimensions gain additional levels of intrinsic robustness, higher-dimensional Floquet codes have hitherto been explored only in limited scope. Here we introduce a 3d generalization of a Floquet code whose instantaneous stabilizer group realizes a 3d fermionic toric code, while crucially preserving all three logical qubits throughout the entire measurement sequence. One central ingredient is the identification of a 3d lattice geometry that generalizes the features of the Kekulé lattice underlying the 2d Hastings-Haah code - specifically, a structure where deleting any one edge color yields a two-color subgraph that decomposes into short, closed loops rather than homologically nontrivial chains. This loop property avoids the collapse of logical information that plagues naive sequential two-color measurement schedules on many 3d lattices. Although, for our lattice geometry, a simple 3-round cycle that sequentially measures the three types of parity checks does not expose the full error syndrome set, we show that one can append a measurement sequence to extract the missing syndromes without disturbing the logical subspace. Beyond code design, 3d tricoordinated lattice geometries define a family of 3d monitored Kitaev models, in which random measurements of the non-commuting parity checks give rise to dynamically created entangled phases with nontrivial topology. In discussing the general structure of their underlying phase diagrams and, in particular, the existence of certain quantum critical points, we again make a connection to the general preservation of logical information in time-ordered Floquet protocols."}
{"id": "2602.13098", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13098", "abs": "https://arxiv.org/abs/2602.13098", "authors": ["Rahul Manavalan", "Filip Tronarp"], "title": "Barron-Wiener-Laguerre models", "comment": null, "summary": "We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification."}
{"id": "2602.12935", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.12935", "abs": "https://arxiv.org/abs/2602.12935", "authors": ["Philippe Blondeel", "Filip Van Utterbeeck", "Ben Lauwens"], "title": "A Stochastic Optimal Control Formulation for Mine Counter Measure Simulations with Multiple Autonomous Survey Vehicles", "comment": null, "summary": "Modelling and simulating mine counter measure search missions performed by autonomous vehicles equipped with a sensor capable of detecting mines at sea is a challenging endeavour. To address this, we formulated and implemented the problem as a stochastic optimal control model. Our implementation computes an optimal path within a user chosen quadrilateral domain such that the mission duration is minimized for a given residual risk of undetected sea mines. First, we compare the stochastic optimal control implementation against the traditionally used boustrophedon implementation. We show that the mission duration in case of the stochastic optimal control implementation is shorter. Then, by building on our previous work, we introduce a novel mathematical approach that enables multiple autonomous survey vehicles to investigate the domain concurrently. We present results for up to six vehicles, including computed trajectories and an analysis of how mission duration varies with the number of vehicles. Our findings show that mission time decreases non-linearly, , i.e., we observe diminishing returns as more vehicles are added."}
{"id": "2602.12900", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12900", "abs": "https://arxiv.org/abs/2602.12900", "authors": ["Ganesh Vishnu Avhad", "Sudheesh K. Kattumannil"], "title": "A unified testing approach for log-symmetry using Fourier methods", "comment": null, "summary": "Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability."}
{"id": "2602.12539", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12539", "abs": "https://arxiv.org/abs/2602.12539", "authors": ["Jiaqing Jiang", "Jiaqi Leng", "Lin Lin"], "title": "Predicting properties of quantum thermal states from a single trajectory", "comment": "43 pages, 2 figues, 1 table", "summary": "Estimating thermal expectation values of observables is a fundamental task in quantum physics, quantum chemistry, and materials science. While recent quantum algorithms have enabled efficient quantum preparation of thermal states, observable estimation via sampling remains costly: a straightforward implementation separates successive measurements by a full mixing time in order to ensure samples are approximately independent. In this work, we show that the sampling cost can be substantially reduced by using a single Gibbs-sampling trajectory. After a single burn-in period, we interleave coherent measurements that satisfy detailed balance with respect to the target Gibbs state. The efficiency of this approach rests on the fact that, in many settings, the autocorrelation time can be significantly shorter than the mixing time. For energy estimation (and more generally for observables commuting with the Hamiltonian), we implement the required measurements using Gaussian-filtered quantum phase estimation with only logarithmic overhead. We also introduce a weighted operator Fourier transform technique to mitigate measurement-induced disturbance for general observables."}
{"id": "2602.12603", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12603", "abs": "https://arxiv.org/abs/2602.12603", "authors": ["Ziyan Lin", "Liang Xu"], "title": "Curvature-Guided Safety Filters: State-Dependent Hessian-Weighted Projection with Provable Performance Bounds", "comment": null, "summary": "Safety filters provide a lightweight mechanism for enforcing state and input safety in learning-enabled control. However, common Euclidean projections onto the safe set disregard long-term performance, while directly optimizing the action-value function within the safe set can be nonconvex and computationally prohibitive. This paper proposes a state-dependent, Hessian-guided projection for safety filtering that preserves convexity while improving performance. The key idea is to select a weighted projection matrix from the curvature of the action-value function, thereby biasing the correction toward action directions with higher value sensitivity. We establish (i) a uniform bound on the performance gap between the weighted projection and the safe value-optimal action, and (ii) a condition under which the weighted projection outperforms the Euclidean projection in long-term value. To support black-box controllers, we further present a data-driven construction of the weighted projection matrix via an iterative Q-function learning algorithm with quadratic feature blocks and regularization that enforces curvature dominance and bounded higher-order terms. Simulations on a quadrotor tracking-and-avoidance task indicate that the proposed filter maintains safety while reducing value degradation relative to Euclidean projection, with computational overhead compatible with real-time operation."}
{"id": "2602.12790", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12790", "abs": "https://arxiv.org/abs/2602.12790", "authors": ["Yu Feng", "Shuo Ling", "Wenjun Ying", "Zhennan Zhou"], "title": "A Stabilized Numerical Framework for Necrotic Tumor Growth via Coupled Boundary Integral and Obstacle Solvers", "comment": null, "summary": "We present a robust computational framework for Hele-Shaw tumor growth with necrotic cores, a problem identified as the incompressible limit of the Porous Media Equation. Simulating this system presents a fundamental challenge: while the outer boundary evolves via advection, the inner necrotic interface is defined by an obstacle problem and lacks an explicit advection structure, causing standard schemes to fail. To address this, we introduce a stabilized predictor-corrector strategy that iteratively resolves the bidirectional coupling between the nutrient-pressure fields and the domain geometry, ensuring robust time-stepping for both the advection-driven outer surface and the obstacle-defined necrotic core. We establish rigorous convergence theory for the single-interface case and demonstrate the method's robustness in capturing the topological transition of necrotic core nucleation and complex geometric evolution."}
{"id": "2602.12616", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12616", "abs": "https://arxiv.org/abs/2602.12616", "authors": ["Kaizer Rahaman", "Jyotirmoy V. Deshmukh", "Ashish R. Hota", "Lars Lindemann"], "title": "When Environments Shift: Safe Planning with Generative Priors and Robust Conformal Prediction", "comment": null, "summary": "Autonomous systems operate in environments that may change over time. An example is the control of a self-driving vehicle among pedestrians and human-controlled vehicles whose behavior may change based on factors such as traffic density, road visibility, and social norms. Therefore, the environment encountered during deployment rarely mirrors the environment and data encountered during training -- a phenomenon known as distribution shift -- which can undermine the safety of autonomous systems. Conformal prediction (CP) has recently been used along with data from the training environment to provide prediction regions that capture the behavior of the environment with a desired probability. When embedded within a model predictive controller (MPC), one can provide probabilistic safety guarantees, but only when the deployment and training environments coincide. Once a distribution shift occurs, these guarantees collapse. We propose a planning framework that is robust under distribution shifts by: (i) assuming that the underlying data distribution of the environment is parameterized by a nuisance parameter, i.e., an observable, interpretable quantity such as traffic density, (ii) training a conditional diffusion model that captures distribution shifts as a function of the nuisance parameter, (iii) observing the nuisance parameter online and generating cheap, synthetic data from the diffusion model for the observed nuisance parameter, and (iv) designing an MPC that embeds CP regions constructed from such synthetic data. Importantly, we account for discrepancies between the underlying data distribution and the diffusion model by using robust CP. Thus, the plans computed using robust CP enjoy probabilistic safety guarantees, in contrast with plans obtained from a single, static set of training data. We empirically demonstrate safety under diverse distribution shifts in the ORCA simulator."}
{"id": "2602.13130", "categories": ["physics.comp-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.13130", "abs": "https://arxiv.org/abs/2602.13130", "authors": ["Magdalena Häupl", "Sebastian Falkner", "Peter G. Bolhuis", "Christoph Dellago", "Alessandro Coretti"], "title": "An Always-Accepting Algorithm for Transition Path Sampling", "comment": null, "summary": "We present a one-way shooting algorithm for transition path sampling that accepts every proposed trajectory yet samples the correct transition path ensemble for systems with overdamped stochastic dynamics. The method is based on two key elements: a procedure to propose trajectories that are always reactive, and a reweighting scheme that corrects for the bias introduced by always accepting the proposed paths. This approach significantly improves the efficiency of transition path sampling by eliminating the cost associated with generating trajectories that are then rejected. We demonstrate the algorithm by investigating the formation of CO$_2$ clathrate hydrates along different reaction mechanisms, showing that the increased efficiency allows proper sampling of the formation of crystalline hydrates at temperatures and pressures that are difficult to access with conventional algorithms."}
{"id": "2602.13152", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.13152", "abs": "https://arxiv.org/abs/2602.13152", "authors": ["Rupsa Basu", "Sven Otto"], "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression", "comment": null, "summary": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series."}
{"id": "2602.13000", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.13000", "abs": "https://arxiv.org/abs/2602.13000", "authors": ["Hanfeng Zeng", "Wenqing Ouyang", "Andre Milzarek"], "title": "A linesearch-type normal map-based semismooth Newton method for nonsmooth nonconvex composite optimization", "comment": null, "summary": "We propose a novel linesearch variant of the trust region normal map-based semismooth Newton method developed in [Ouyang and Milzarek, Math. Program. 212(1-2), 389--435 (2025)] for solving a class of nonsmooth, nonconvex composite-type optimization problems. Our approach uses adaptive parameter estimation techniques, which allow us to avoid explicit and potentially expensive Lipschitz constant computations. We provide extensive convergence results including global convergence, convergence of the iterates under the Kurdyka-Łojasiewicz inequality, and transition to fast local q-superlinear convergence. Compared to the original trust region framework, the linesearch-based algorithm is simpler and the overall convergence analysis can be conducted under weaker assumptions -- in particular, without requiring explicit boundedness conditions on the Hessian approximations and iterates. Numerical experiments on sparse logistic regression, image compression, and nonlinear least squares with group penalty terms demonstrate the efficiency of the proposed approach."}
{"id": "2602.12992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12992", "abs": "https://arxiv.org/abs/2602.12992", "authors": ["Reagan Mozer", "Nicole E. Pashley", "Luke Miratrix"], "title": "Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes", "comment": null, "summary": "In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group."}
{"id": "2602.12664", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12664", "abs": "https://arxiv.org/abs/2602.12664", "authors": ["Xin-Xiang Ju", "Ya-Wen Sun", "Yang Zhao"], "title": "Sperner state and multipartite entanglement signals", "comment": "6 pages + appendices", "summary": "We establish a systematic classification scheme for multipartite entanglement structures. We define Sperner states -- a broad class of states where apparent multipartite entanglement decomposes into fewer-partite entanglement among subsystems of each party. Each class of Sperner states is associated with one antichain hypergraph and each hypergraph encodes the maximal entanglement structure permissible under its constraints. We introduce a Multi-entanglement Measure Space (MEMS) where each Sperner class corresponds to a linear subspace defined by the vanishing of specific linear combinations of bipartite and multipartite measures. The nonvanishing of such combinations signals multipartite entanglement beyond the associated hypergraph, thereby distinguishing entanglement structures. We build a two way connection between each hypergraph entanglement structure and a distinct set of combinations, thereby quantifying the entanglement pattern and providing a unified basis for classifying all multipartite entanglement."}
{"id": "2602.12616", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12616", "abs": "https://arxiv.org/abs/2602.12616", "authors": ["Kaizer Rahaman", "Jyotirmoy V. Deshmukh", "Ashish R. Hota", "Lars Lindemann"], "title": "When Environments Shift: Safe Planning with Generative Priors and Robust Conformal Prediction", "comment": null, "summary": "Autonomous systems operate in environments that may change over time. An example is the control of a self-driving vehicle among pedestrians and human-controlled vehicles whose behavior may change based on factors such as traffic density, road visibility, and social norms. Therefore, the environment encountered during deployment rarely mirrors the environment and data encountered during training -- a phenomenon known as distribution shift -- which can undermine the safety of autonomous systems. Conformal prediction (CP) has recently been used along with data from the training environment to provide prediction regions that capture the behavior of the environment with a desired probability. When embedded within a model predictive controller (MPC), one can provide probabilistic safety guarantees, but only when the deployment and training environments coincide. Once a distribution shift occurs, these guarantees collapse. We propose a planning framework that is robust under distribution shifts by: (i) assuming that the underlying data distribution of the environment is parameterized by a nuisance parameter, i.e., an observable, interpretable quantity such as traffic density, (ii) training a conditional diffusion model that captures distribution shifts as a function of the nuisance parameter, (iii) observing the nuisance parameter online and generating cheap, synthetic data from the diffusion model for the observed nuisance parameter, and (iv) designing an MPC that embeds CP regions constructed from such synthetic data. Importantly, we account for discrepancies between the underlying data distribution and the diffusion model by using robust CP. Thus, the plans computed using robust CP enjoy probabilistic safety guarantees, in contrast with plans obtained from a single, static set of training data. We empirically demonstrate safety under diverse distribution shifts in the ORCA simulator."}
{"id": "2602.12816", "categories": ["math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.12816", "abs": "https://arxiv.org/abs/2602.12816", "authors": ["Nan Deng", "Wanrong Cao"], "title": "Finite Difference Method for Stochastic Cahn-Hilliard Equation Driven by A Fractional Brownian Sheet", "comment": "38 pages", "summary": "The stochastic Cahn-Hilliard equation driven by a fractional Brownian sheet provides a more accurate model for correlated space-time random perturbations. This study delves into two key aspects: first, it rigorously examines the regularity of the mild solution to the stochastic Cahn-Hilliard equation, shedding light on the intricate behavior of solutions under such complex perturbations. Second, it introduces a fully discrete numerical scheme designed to solve the equation effectively. This scheme integrates the finite difference method for spatial discretization with the tamed exponential Euler method for temporal discretization. The analysis demonstrates that the proposed scheme achieves a strong convergence rate of $O\\big(h^{1-ε}+τ^{H_1-\\frac{1}{8}-\\fracε{2}}\\big)$, where $ε$ is an arbitrarily small positive constant, providing a solid foundation for the numerical treatment of such equations."}
{"id": "2602.12638", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12638", "abs": "https://arxiv.org/abs/2602.12638", "authors": ["Sunandan Adhikary", "Soumyajit Dey"], "title": "Safe Controller Synthesis Using Lyapunov-based Barriers for Linear Hybrid Systems with Simplex Architecture", "comment": null, "summary": "Modern cyber-physical systems often have a two-layered design, where the primary controller is AI-enabled or an analytical controller optimising some specific cost function. If the resulting control action is perceived as unsafe, a secondary safety-focused backup controller is activated. The existing backup controller design schemes do not consider a real-time deadline for the course correction of a potentially unsafe system trajectory or constrain maximisation of the safe operating region as a synthesis criterion. This essentially implies an eventual safety guarantee over a small operating region.\n  This paper proposes a novel design method for backup safe controllers (BSCs) that ensure invariance across the largest possible region in the safe state space, along with a guarantee for timely recovery when the system states deviate from their usual behaviour. This is the first work to synthesise safe controllers that ensure maximal safety and timely recovery while aiming at minimal resource usage by switching between BSCs with different execution rates. An online safe controller activation policy is also proposed to switch between BSCs (and the primary optimal controller) to optimise processing bandwidth for control computation. To establish the efficacy of the proposed method, we evaluate the safety and recovery time of the proposed safe controllers, as well as the activation policy, in closed loops with linear hybrid dynamical systems under budgeted bandwidth."}
{"id": "2602.13158", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13158", "abs": "https://arxiv.org/abs/2602.13158", "authors": ["Ryan Li", "Brian J. Reich", "Emily C. Hector", "Reetam Majumder"], "title": "A new mixture model for spatiotemporal exceedances with flexible tail dependence", "comment": null, "summary": "We propose a new model and estimation framework for spatiotemporal streamflow exceedances above a threshold that flexibly captures asymptotic dependence and independence in the tail of the distribution. We model streamflow using a mixture of processes with spatial, temporal and spatiotemporal asymptotic dependence regimes. A censoring mechanism allows us to use only observations above a threshold to estimate marginal and joint probabilities of extreme events. As the likelihood is intractable, we use simulation-based inference powered by random forests to estimate model parameters from summary statistics of the data. Simulations and modeling of streamflow data from the U.S. Geological Survey illustrate the feasibility and practicality of our approach."}
{"id": "2602.13005", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.13005", "abs": "https://arxiv.org/abs/2602.13005", "authors": ["Patrick Jung"], "title": "Optimizing Initial Feature-Mapping Variables from Given Designs via Tracking", "comment": "Master's thesis, Friedrich-Alexander-Universitaet Erlangen-Nuernberg (FAU), 2025. 116 pages, 76 figures", "summary": "A feature-mapping framework for inverse reconstruction of density-based topology optimization results is proposed. Unlike SIMP, whose voxelized outputs are hard to interpret or reuse, the method represents designs with high-level geometric primitives mapped to a fixed analysis grid. Capsule-shaped bars (endpoints plus radius) are used, with closed-form signed distances and smooth transition functions providing derivatives up to second order. Differentiable pseudo-densities are aggregated with smooth operators, enabling gradient-based optimization with exact Hessians. Robustness is improved through asymmetric transition functions that propagate sensitivities into void regions, a reward-only objective for initialization, and geometric safeguards against degenerate configurations. Reconstruction is performed in stages (exploration, bridging, convergence) with optional refinement that can add, remove, or merge features based on residuals and geometric criteria. Experiments on canonical SIMP benchmarks, including five-bar and cantilever layouts, show high-fidelity reconstructions using a moderate number of features. p-norm and softmax aggregation yield sharp results; pruning removes redundant features and additive refinement restores coverage. Exact Hessians accelerate convergence and improve robustness compared to quasi-Newton updates, providing a bridge from voxel-based outputs to explicit parametric models."}
{"id": "2602.13152", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.13152", "abs": "https://arxiv.org/abs/2602.13152", "authors": ["Rupsa Basu", "Sven Otto"], "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression", "comment": null, "summary": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series."}
{"id": "2602.12685", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.12685", "abs": "https://arxiv.org/abs/2602.12685", "authors": ["Yoshito Watanabe", "Bianca Bannenberg", "Simon Trebst"], "title": "Floquet implementation of a 3d fermionic toric code with full logical code space", "comment": "12 pages, 9 figures", "summary": "Floquet quantum error-correcting codes provide an operationally economical route to fault tolerance by dynamically generating stabilizer structures using only two-body Pauli measurements. But while it is well established that stabilizer codes in higher spatial dimensions gain additional levels of intrinsic robustness, higher-dimensional Floquet codes have hitherto been explored only in limited scope. Here we introduce a 3d generalization of a Floquet code whose instantaneous stabilizer group realizes a 3d fermionic toric code, while crucially preserving all three logical qubits throughout the entire measurement sequence. One central ingredient is the identification of a 3d lattice geometry that generalizes the features of the Kekulé lattice underlying the 2d Hastings-Haah code - specifically, a structure where deleting any one edge color yields a two-color subgraph that decomposes into short, closed loops rather than homologically nontrivial chains. This loop property avoids the collapse of logical information that plagues naive sequential two-color measurement schedules on many 3d lattices. Although, for our lattice geometry, a simple 3-round cycle that sequentially measures the three types of parity checks does not expose the full error syndrome set, we show that one can append a measurement sequence to extract the missing syndromes without disturbing the logical subspace. Beyond code design, 3d tricoordinated lattice geometries define a family of 3d monitored Kitaev models, in which random measurements of the non-commuting parity checks give rise to dynamically created entangled phases with nontrivial topology. In discussing the general structure of their underlying phase diagrams and, in particular, the existence of certain quantum critical points, we again make a connection to the general preservation of logical information in time-ordered Floquet protocols."}
{"id": "2602.12638", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12638", "abs": "https://arxiv.org/abs/2602.12638", "authors": ["Sunandan Adhikary", "Soumyajit Dey"], "title": "Safe Controller Synthesis Using Lyapunov-based Barriers for Linear Hybrid Systems with Simplex Architecture", "comment": null, "summary": "Modern cyber-physical systems often have a two-layered design, where the primary controller is AI-enabled or an analytical controller optimising some specific cost function. If the resulting control action is perceived as unsafe, a secondary safety-focused backup controller is activated. The existing backup controller design schemes do not consider a real-time deadline for the course correction of a potentially unsafe system trajectory or constrain maximisation of the safe operating region as a synthesis criterion. This essentially implies an eventual safety guarantee over a small operating region.\n  This paper proposes a novel design method for backup safe controllers (BSCs) that ensure invariance across the largest possible region in the safe state space, along with a guarantee for timely recovery when the system states deviate from their usual behaviour. This is the first work to synthesise safe controllers that ensure maximal safety and timely recovery while aiming at minimal resource usage by switching between BSCs with different execution rates. An online safe controller activation policy is also proposed to switch between BSCs (and the primary optimal controller) to optimise processing bandwidth for control computation. To establish the efficacy of the proposed method, we evaluate the safety and recovery time of the proposed safe controllers, as well as the activation policy, in closed loops with linear hybrid dynamical systems under budgeted bandwidth."}
{"id": "2602.12850", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12850", "abs": "https://arxiv.org/abs/2602.12850", "authors": ["Xin Liu", "Qinglin Tang", "Yong Zhang"], "title": "Fast convolution solvers using moment-matching", "comment": null, "summary": "We propose two easy-to-implement fast algorithms based on moment-matching to compute the nonlocal potential $\\varphi(\\textbf{x})=(U\\ast ρ)(\\textbf{x})$ on bounded domain, where the kernel $U$ is singular at the origin and the density $ρ$ is a fast-decaying smooth function. Each method requires merely minor modifications to commonly-used existing methods, i.e., the sine spectral/Fourier quadrature method, and achieves a much better convergence rate. The key lies in the introduction of a smooth auxiliary function $ρ_1$ whose moments match those of the density up to an integer order $m$. Specifically, $ρ_1$ is constructed using Gaussian function in an explicit way and the associated potential can be calculated analytically. The moments of residual density vanish up to order $m$, and the corresponding residual potential $U \\ast (ρ-ρ_1)$ decays much faster than the original potential $\\varphi$ at the far field. As for the residual potential evaluation, for classical kernels (e.g., the Coulomb kernel), we solve a differential/pseudo-differential equation on a rectangular domain with homogeneous Dirichlet boundary conditions via sine pseudospectral method, and achieve an arbitrary high convergence rate. While, for general kernels, the regularity of Fourier integrand increase by $m$ thanks to the moments-vanishing property, therefore, the standard trapezoidal rule/midpoint quadrature also converges much faster. To gain a better numerical performance, we utilize the domain expansion technique to obtain better accuracy, and improve the efficiency by simplifying the quadrature into one discrete convolution and applying Fast Fourier Transform (FFT) to a double-sized vector. Rigorous error estimates and extensive numerical investigations showcase the accuracy and efficiency for different nonlocal potentials."}
{"id": "2602.12663", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12663", "abs": "https://arxiv.org/abs/2602.12663", "authors": ["Gaoxin Zhang", "Ruixing Ren", "Junhui Zhao", "Xiaoke Sun"], "title": "Dual-Channel Feature Fusion for Joint Prediction in Dynamic Signed Weighted Networks", "comment": "10 pages, 9 figures", "summary": "Link prediction is central to unraveling social network evolution and node relationships, as well as understanding the characteristic mechanisms of complex networks. Currently, research on link prediction for complex dynamic networks integrating temporal evolution, relational polarity and edge weight information remains significantly underexplored, failing to meet practical demands. For dynamic signed-weighted networks, this paper proposes a tripartite joint prediction framework for unified forecasting of links, signs and weights. First, the dynamic network is decomposed into temporal snapshots, and node semantic embeddings are generated via sign-aware weighted random walks. We then design multi-hop structural balance and temporal difference features to capture the structural characteristics and dynamic evolution laws of the network, respectively. The model adopts a dual-channel feature decoupling mechanism: node semantic embeddings are used for link existence prediction, while relational sign features are fed into a Transformer encoder to model temporal dependencies. Finally, prediction results are output synergistically through a multi-task unit. Simulation experiments demonstrate that, compared with baseline methods, the proposed framework achieves an average 2%-4% improvement in the performance of link existence and relational sign prediction, and a significant 40%-50% reduction in edge weight prediction error."}
{"id": "2602.12483", "categories": ["math.NA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12483", "abs": "https://arxiv.org/abs/2602.12483", "authors": ["Sofiia Shvaiko", "Longxiu Huang", "Elizaveta Rebrova"], "title": "Quantile Randomized Kaczmarz Algorithm with Whitelist Trust Mechanism", "comment": "Accepted by ICASSP 2026", "summary": "Randomized Kaczmarz (RK) is a simple and fast solver for consistent overdetermined systems, but it is known to be fragile under noise. We study overdetermined $m\\times n$ linear systems with a sparse set of corrupted equations, $ {\\bf A}{\\bf x}^\\star = {\\bf b}, $where only $\\tilde{\\bf b} = {\\bf b} + \\boldsymbol{\\varepsilon}$ is observed with $\\|\\boldsymbol{\\varepsilon}\\|_0 \\le βm$. The recently introduced QuantileRK (QRK) algorithm addresses this issue by testing residuals against a quantile threshold, but computing a per-iteration quantile across many rows is costly. In this work we (i) reanalyze QRK and show that its convergence rate improves monotonically as the corruption fraction $β$ decreases; (ii) propose a simple online detector that flags and removes unreliable rows, which reduces the effective $β$ and speeds up convergence; and (iii) make the method practical by estimating quantiles from a small random subsample of rows, preserving robustness while lowering the per-iteration cost. Simulations on imaging and synthetic data demonstrate the efficiency of the proposed method."}
{"id": "2602.13032", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.13032", "abs": "https://arxiv.org/abs/2602.13032", "authors": ["Raghupati Vyas", "Kousik Das", "Veeraruna Kavitha", "Souvik Roy"], "title": "Multi-type random game dynamics: limits at discontinuities and cyclic limits", "comment": null, "summary": "We consider (random) strategic interactions in a large population consisting of a variety of players. A rational player chooses actions that maximize certain utility functions, while a behavioral player chooses actions based on preferences such as avoid-the-crowd or follow-the-majority. We specifically study a turn-by-turn dynamic process in which players choose their actions sequentially and once; the utilities are realized either immediately or at the end of the game.\n  In the literature, such dynamical systems are often analyzed using an appropriate approximating ordinary differential equation (ODE). However, the ODEs approximating the dynamics with pure actions are typically discontinuous. We adopt a differential inclusion (DI) based stochastic-approximation framework to derive the limiting analysis. The limits of the dynamics are characterized through the internally chain transitive (ICT) sets. We identify the presence of non-classical zeros as potential limits of the dynamics, a phenomenon not observed in classical settings involving continuous ODEs. These new limits arise precisely at the points of discontinuity of the dynamics. We further provide the conditions under which cyclic outcomes may occur at the limit.\n  Finally, we study a queuing game with differential priority-based services and examine the impact of the proportions of avoid-the-crowd and two types of rational populations on the long-run outcomes of the strategic interactions. We identify potential point limits and establish the possibility of cyclic outcomes for certain parameter configurations."}
{"id": "2602.12712", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12712", "abs": "https://arxiv.org/abs/2602.12712", "authors": ["Sergio A. Ortega", "Miguel A. Martin-Delgado"], "title": "Reverse Delegated Training and Private Inference via Perfectly-Secure Quantum Homomorphic Encryption", "comment": "RevTex 4.2, 19 pages, 11 color figures", "summary": "Quantum machine learning in cloud environments requires protecting sensitive data while enabling remote computation. Here we demonstrate the first realistic implementations of a perfectly-secure quantum homomorphic encryption (QHE) scheme applied to quantum neural networks (QNN). Using efficient Clifford+$T$ decomposition, we implement quantum convolutional neural networks for two complementary scenarios: (i) reverse delegated training, where encrypted data from multiple providers trains a user's network via federated aggregation; (ii) private inference, where users process encrypted data with remote quantum networks. Moreover, analysis of server circuit privacy reveals probabilistic model protection through Pauli gate concealment. These results establish perfectly-secure QHE as a practical framework for multi-party quantum machine learning."}
{"id": "2602.12663", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12663", "abs": "https://arxiv.org/abs/2602.12663", "authors": ["Gaoxin Zhang", "Ruixing Ren", "Junhui Zhao", "Xiaoke Sun"], "title": "Dual-Channel Feature Fusion for Joint Prediction in Dynamic Signed Weighted Networks", "comment": "10 pages, 9 figures", "summary": "Link prediction is central to unraveling social network evolution and node relationships, as well as understanding the characteristic mechanisms of complex networks. Currently, research on link prediction for complex dynamic networks integrating temporal evolution, relational polarity and edge weight information remains significantly underexplored, failing to meet practical demands. For dynamic signed-weighted networks, this paper proposes a tripartite joint prediction framework for unified forecasting of links, signs and weights. First, the dynamic network is decomposed into temporal snapshots, and node semantic embeddings are generated via sign-aware weighted random walks. We then design multi-hop structural balance and temporal difference features to capture the structural characteristics and dynamic evolution laws of the network, respectively. The model adopts a dual-channel feature decoupling mechanism: node semantic embeddings are used for link existence prediction, while relational sign features are fed into a Transformer encoder to model temporal dependencies. Finally, prediction results are output synergistically through a multi-task unit. Simulation experiments demonstrate that, compared with baseline methods, the proposed framework achieves an average 2%-4% improvement in the performance of link existence and relational sign prediction, and a significant 40%-50% reduction in edge weight prediction error."}
{"id": "2602.12872", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12872", "abs": "https://arxiv.org/abs/2602.12872", "authors": ["Shuo Ling", "Wenjun Ying", "Zhen Zhang"], "title": "Neural Evolutionary Kernel Method: A Knowledge-Guided Framework for Solving Evolutionary PDEs", "comment": null, "summary": "Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs, leveraging their approximation capabilities to handle complex domains and high-dimensional problems. Among these, operator learning has gained increasing attention by learning mappings between function spaces using DNNs. This paper proposes a novel approach, termed the Neural Evolutionary Kernel Method (NEKM), for solving a class of time-dependent partial differential equations (PDEs) via deep neural network (DNN)-based kernel representations. By integrating boundary integral techniques with operator learning, prior mathematical information of time-dependent partial differential equations (PDEs) is embedded into the design of neural network architectures for predicting their solutions, enhancing both computational efficiency and solution accuracy. Numerical experiments on the heat, wave, and Schrödinger equations demonstrate that the Neural Evolutionary Kernel Method (NEKM) achieves high accuracy and favorable computational efficiency. Furthermore, the operator learning framework inherently supports the simultaneous prediction of solutions to multiple PDEs with different coefficients, rendering its capability for solving random PDEs."}
{"id": "2602.12697", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12697", "abs": "https://arxiv.org/abs/2602.12697", "authors": ["Umair Zulfiqar"], "title": "From Data $H(jω_i)$ to Balanced Truncation Family: A Projection-based Non-intrusive Approach", "comment": null, "summary": "This paper presents data-driven implementations of balanced truncation and several of its generalizations that rely exclusively on transfer function samples on the imaginary axis. Rather than implicitly approximating the Gramians via numerical quadrature, the proposed approach approximates them implicitly through projection. This enables multiple members of the balanced truncation family to be implemented non-intrusively using practically measurable data, without requiring spectral factorizations. Using this projection-based framework, data-driven implementations are developed for standard balanced truncation, frequency-limited balanced truncation, time-limited balanced truncation, self-weighted balanced truncation, LQG balanced truncation, H-infinity balanced truncation, positive-real balanced truncation, bounded-real balanced truncation, and stochastic balanced truncation. Numerical results demonstrate that the proposed non-intrusive implementations achieve performance comparable to their intrusive counterparts and accurately capture the dominant Hankel singular values."}
{"id": "2602.13063", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.13063", "abs": "https://arxiv.org/abs/2602.13063", "authors": ["Antonin Clerc", "Ségolène Martin", "Nicolas Papadakis", "Gabriele Steidl"], "title": "Reinterpreting EMML as Mirror Descent for Constrained Maximum Likelihood Estimation", "comment": "6 pages, 3 figures, submitted at EUSIPCO 2026", "summary": "The Expectation--Maximization Maximum Likelihood (EMML) algorithm belongs to the Expectation--Maximization family and is widely used for image reconstruction problems under Poisson noise.In this paper, we reinterpret EMML as a mirror descent method applied to a reparametrized objective function. This perspective allows us to incorporate convex constraints into the algorithm through appropriately chosen Bregman projections, while preserving the multiplicative structure of the EMML updates to ensure computational efficiency. We then establish the convergence of the resulting algorithm toward a solution of the constrained maximum-likelihood problem. Numerical experiments on hyperspectral unmixing problems demonstrate that the constrained EMML converges in fewer iterations than the classical EMML."}
{"id": "2602.12767", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12767", "abs": "https://arxiv.org/abs/2602.12767", "authors": ["Yuchong Chen", "Yijun Tang"], "title": "Preparing Quantum Backflow States by Large Momentum Transfer", "comment": null, "summary": "A quantum backflow state refers to a quantum state exhibiting negative probability density flux albeit a completely positive momentum spectrum. Extending earlier work that uses single laser pulse to prepare quantum backflow state in an ultracold atomic BEC [1], we theoretical investigated flexible quantum backflow state preparation via large momentum transfer technique, which to our knowledge, has not been studied before. By combining atom interferometry theory and non-interacting BEC wave function, we solve for the evolution of a BEC wavepacket under atom interferometry sequence. Simulation results show a highly tunable backflow flux and critical density under our scheme, and can be manipulated to go beyond existing numbers."}
{"id": "2602.12697", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12697", "abs": "https://arxiv.org/abs/2602.12697", "authors": ["Umair Zulfiqar"], "title": "From Data $H(jω_i)$ to Balanced Truncation Family: A Projection-based Non-intrusive Approach", "comment": null, "summary": "This paper presents data-driven implementations of balanced truncation and several of its generalizations that rely exclusively on transfer function samples on the imaginary axis. Rather than implicitly approximating the Gramians via numerical quadrature, the proposed approach approximates them implicitly through projection. This enables multiple members of the balanced truncation family to be implemented non-intrusively using practically measurable data, without requiring spectral factorizations. Using this projection-based framework, data-driven implementations are developed for standard balanced truncation, frequency-limited balanced truncation, time-limited balanced truncation, self-weighted balanced truncation, LQG balanced truncation, H-infinity balanced truncation, positive-real balanced truncation, bounded-real balanced truncation, and stochastic balanced truncation. Numerical results demonstrate that the proposed non-intrusive implementations achieve performance comparable to their intrusive counterparts and accurately capture the dominant Hankel singular values."}
{"id": "2602.12940", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.12940", "abs": "https://arxiv.org/abs/2602.12940", "authors": ["Nitin Kumar", "Federico Pichi", "Gianluigi Rozza"], "title": "Bifurcation curve detection with deflation for multiparametric PDEs", "comment": null, "summary": "This work presents a comprehensive framework for capturing bifurcating phenomena and detecting bifurcation curves in nonlinear multiparametric partial differential equations, where the system exhibits multiple coexisting solutions for given values of the parameters. Traditional continuation methods for one-dimensional parameterizations employ the previously computed solution as the initial guess for the next parameter value. These are usually very inefficient, since small step sizes increase computational cost, while larger steps could jeopardize the method convergence jumping to a different solution branch or missing the bifurcation point. To address these challenges, we propose a novel framework that combines: (i) arclength continuation, adaptively selecting new parameter values in higher dimension, and (ii) the deflation technique, discovering multiple branches to construct complete bifurcation diagrams. In particular, the arclength continuation method is designed to handle multiparametric scenarios, where the parameter vector $λ\\in \\mathbb{R}^p$ traces a curve $g(λ)$ within a $p$-dimensional parameter space. In addition, we introduce a zigzag path-following strategy to robustly track the bifurcation curves and surfaces, respectively, for two- and three-dimensional parametric spaces. Finally, we demonstrate its performance on two benchmark problems: the Bratu equation and the Allen-Cahn equation."}
{"id": "2602.12782", "categories": ["eess.SY", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.12782", "abs": "https://arxiv.org/abs/2602.12782", "authors": ["Huang Zhenyu", "Yuan Zhao"], "title": "Empirical Validation of a Dual-Defense Mechanism Reshaping Wholesale Electricity Price Dynamics in Singapore", "comment": "This paper is submitted to Energy Policy, and it is currently under review", "summary": "While ex-ante screening and static price caps are global standards for mitigating price volatility, Singapore's electricity market employs a unique dual-defense mechanism integrating vesting contracts (VC) with a temporary price cap (TPC). Using high-frequency data from 2021 to 2024, this paper evaluates this mechanism and yields three primary findings. First, a structural trade-off exists within the VC framework: while VC quantity (VCQ) suppresses average prices, it paradoxically exacerbates instability via liquidity squeezes. Conversely, VC price (VCP) functions as a tail-risk anchor, dominating at extreme quantiles where VCQ efficacy wanes. Second, a structural break around the 2023 reform reveals a fundamental re-mapping of price dynamics; the previously positive pass-through from offer ratios to clearing prices was largely neutralized post-reform. Furthermore, diagnostics near the TPC threshold show no systematic evidence of strategic bid shading, confirming the TPC's operational integrity. Third, the dual-defense mechanism exhibits a critical synergy that resolves the volatility trade-off. The TPC reverses the volatility penalty of high VCQ, shifting the elasticity of conditional volatility from a destabilizing 0.636 to a stabilizing -0.213. This synergy enables the framework to enhance tail-risk control while eliminating liquidity-related stability costs. We conclude that this dual-defense mechanism successfully decouples price suppression from liquidity risks, thereby maximizing market stability."}
{"id": "2602.13141", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.13141", "abs": "https://arxiv.org/abs/2602.13141", "authors": ["Yixin Xie", "Jin-Peng Liu", "Cong Sun", "Ya-Xiang Yuan"], "title": "New gradient methods with 3 dimensional quadratic termination", "comment": "23 pages, 5 figures", "summary": "A new stepsize for gradient method is proposed. Combining it with the exact line search stepsizes, the gradient method achieves the optimal solution in 5 steps for 3 dimensional quadratic function minimization problem. The new stepsize is plugged in the cyclic stepsize update strategy, and a new gradient method is proposed. By applying the quadratic interpolation for Cauchy approximation, the proposed gradient method is extended to solve general unconstrained problem. With the improved GLL line search, the global convergence of the proposed method is proved. Furthermore, its sublinear convergence rate for convex problems and R-linear convergence rate for problems with quadratic functional growth property are analyzed. Numerical results show that our proposed algorithm enjoys good performances in terms of computational cost, and line search requires very few trial stepsizes."}
{"id": "2602.12773", "categories": ["quant-ph", "physics.app-ph", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2602.12773", "abs": "https://arxiv.org/abs/2602.12773", "authors": ["Oscar W. Kennedy", "Waqas Ahmad", "Robert Armstrong", "Amir Awawdeh", "Anirban Bose", "Kevin G. Crawford", "Sergey Danilin", "William D. David", "Hamid El Maazouz", "Darren J. Hayton", "George B. Long", "Alexey Lyapin", "Scott A. Manifold", "Kowsar Shahbazi", "Ryan Wesley", "Evan Wong", "Connor D. Shelly"], "title": "Design and Operation of Wafer-Scale Packages Containing >500 Superconducting Qubits", "comment": "12 page main text containing 7 figures. 7 page SI with 6 figures", "summary": "Packages capable of supporting large arrays of high-coherence superconducting qubits are vital for the realisation of fault-tolerant quantum computers and the necessary high-throughput metrology required to optimise fabrication and manufacturing processes. We present a wafer-scale packaging architecture supporting over 500 qubits on a single 3-inch die. The package is engineered to suppress parasitic RF modes, and to mitigate material loss through simulation-informed design while managing differential thermal contraction to ensure robust operation at millikelvin temperatures. System-level heat-load calculations from a large wiring payload show this package may be operated in commercial dilution refrigerators. Measurements of the qubits loaded into the package show median $T_1$, $T_{2e} \\sim 100~μ$s ($\\sim$100 qubits) alongside readout with median fidelity of 97.5% (54 qubits) and a median qubit temperature of 36 mK (54 qubits). These results validate the performance of these packages and demonstrate that large-scale integration can be achieved without compromising device performance. Finally, we highlight the utility of these packages as a tool for high throughput feedback on qubit figures of merit over large sample sizes, allowing identification of performance outliers in the tails of the coherence distribution, a critical capability for informing fabrication and manufacture of high-quality quantum qubits and quantum processors."}
{"id": "2602.12782", "categories": ["eess.SY", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.12782", "abs": "https://arxiv.org/abs/2602.12782", "authors": ["Huang Zhenyu", "Yuan Zhao"], "title": "Empirical Validation of a Dual-Defense Mechanism Reshaping Wholesale Electricity Price Dynamics in Singapore", "comment": "This paper is submitted to Energy Policy, and it is currently under review", "summary": "While ex-ante screening and static price caps are global standards for mitigating price volatility, Singapore's electricity market employs a unique dual-defense mechanism integrating vesting contracts (VC) with a temporary price cap (TPC). Using high-frequency data from 2021 to 2024, this paper evaluates this mechanism and yields three primary findings. First, a structural trade-off exists within the VC framework: while VC quantity (VCQ) suppresses average prices, it paradoxically exacerbates instability via liquidity squeezes. Conversely, VC price (VCP) functions as a tail-risk anchor, dominating at extreme quantiles where VCQ efficacy wanes. Second, a structural break around the 2023 reform reveals a fundamental re-mapping of price dynamics; the previously positive pass-through from offer ratios to clearing prices was largely neutralized post-reform. Furthermore, diagnostics near the TPC threshold show no systematic evidence of strategic bid shading, confirming the TPC's operational integrity. Third, the dual-defense mechanism exhibits a critical synergy that resolves the volatility trade-off. The TPC reverses the volatility penalty of high VCQ, shifting the elasticity of conditional volatility from a destabilizing 0.636 to a stabilizing -0.213. This synergy enables the framework to enhance tail-risk control while eliminating liquidity-related stability costs. We conclude that this dual-defense mechanism successfully decouples price suppression from liquidity risks, thereby maximizing market stability."}
{"id": "2602.13048", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.13048", "abs": "https://arxiv.org/abs/2602.13048", "authors": ["Hamid Fathi", "Alexander Skorikov", "Tristan van Leeuwen"], "title": "Data-Driven Filter Design for Flexible and Noise-Robust Tomographic Imaging", "comment": null, "summary": "While filtered back projection (FBP) is still the method of choice for fast tomographic reconstruction, its performance degrades noticeably in the presence of noise, incomplete sampling, or non-standard scan geometries. We propose a data-driven approach for learning FBP filters and projection weights directly from training data, with the goal of improving robustness without sacrificing computational efficiency. The resulting reconstructions adapt naturally to the noise level and acquisition geometry, while retaining the speed and simplicity of classical back-projection. The proposed method can be formulated as a regularized optimization problem for a linear inverse operator, which allows us to establish existence, uniqueness, and stability of the learned solution. From a spectral viewpoint, the learned filters act as data-adaptive gain functions that explicitly balance noise amplification and bias, in close analogy to a regularized pseudo-inverse. Experiments in both 2D and 3D show consistent improvements over conventional FBP and FDK in different case studies. Finally, we show that filters trained on synthetic laminography data generalize well to real-world measurements, delivering image quality comparable to advanced iterative methods without the high computational cost."}
{"id": "2602.12954", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12954", "abs": "https://arxiv.org/abs/2602.12954", "authors": ["Luisa Schuhmacher", "Hazem Sallouha", "Ihsane Gryech", "Sofie Pollin"], "title": "Data Augmentation and Attention for massive MIMO-based Indoor Localization in Changing Environments", "comment": "To be published in IEEE ICC 2026 Conference Proceedings", "summary": "The demand for high-precision indoor localization has grown significantly with the rise of smart environments, industrial automation, and location-aware applications. While massive Multiple-Input and Multiple-Output (MIMO) systems enable millimeter-level accuracy by leveraging rich Channel State Information (CSI), most existing solutions are optimized for static environments, where users or devices remain fixed during data collection and inference. Real-world applications, however, often require real-time localization in changing environments, where rapid movement, unpredictable blockages, and dynamic channel conditions pose significant challenges. To address these challenges, we introduce two data augmentation techniques designed to resemble blocked antennas, enhancing the generalizability of localization models to dynamic scenarios. Additionally, we enhance an existing Deep Learning (DL) model by incorporating attention modules, improving its ability to focus on relevant channel features and antennas. We train our model on data from a static scenario, augmented with the proposed techniques, and evaluate it on a dataset collected in changing scenarios. We investigate the performance enhancements achieved by the data augmentation techniques and the Attention modules, and observe a localization accuracy improvement from a mean error of 286 mm, when trained without Attention and without data augmentations, to 66 mm, when trained with Attention and data augmentation. This shows that high localization accuracy can be maintained in changing environments, even without training data from those scenarios."}
{"id": "2602.13157", "categories": ["math.OC", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13157", "abs": "https://arxiv.org/abs/2602.13157", "authors": ["Sean Bowerfind", "Matthew R. Kirchner", "Gary Hewer"], "title": "A Data-Driven Algorithm for Model-Free Control Synthesis", "comment": null, "summary": "Presented is an algorithm to synthesize the optimal infinite-horizon LQR feedback controller for continuous-time systems. The algorithm does not require knowledge of the system dynamics but instead uses only a finite-length sampling of arbitrary input-output data. The algorithm is based on a constrained optimization problem that enforces a necessary condition on the dynamics of the optimal value function along any trajectory. In addition to calculating the standard LQR gain matrix, a feedforward gain can be found to implement a reference tracking controller. This paper presents a theoretical justification for the method and shows several examples, including a validation test on a real scale aircraft."}
{"id": "2602.12787", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12787", "abs": "https://arxiv.org/abs/2602.12787", "authors": ["Tabitha Doicin", "Luis A. Correa", "Jonas Glatthard", "Andrew D. Armour", "Gerardo Adesso"], "title": "Equilibrium thermometry in the multilevel quantum Rabi model", "comment": "15 pages, 9 figures", "summary": "The temperature sensitivity of a probe in equilibrium can be gauged by its thermal quantum Fisher information (QFI). It is known that probes exhibiting degeneracy in their energy-level structure can achieve larger sensitivities, while probes with a more uniform spectrum may remain sensitive over a broader temperature range. Here, we study the thermometric performance of a multilevel quantum Rabi model in which two well-separated atomic manifolds of near-degenerate levels couple to a single cavity mode. We generalise the standard quantum Rabi treatment in the adiabatic regime to find an approximate closed-form expression for the thermal QFI. We then characterise two complementary limits. On the one hand, a large dark-state manifold (dark-manifold saturation) produces a robust peak in thermal sensitivity due to bright--dark population transfer. Such increase in sensitivity is further maximised at an intermediate light--matter coupling strength. Maximising instead the number of bright states (bright-manifold saturation) generates a broadband thermal response that becomes increasingly stable under random light--matter couplings as the number of levels is increased. The rich spectral structure of our cavity-QED model thus makes it a versatile and sensitive equilibrium thermometer over a broad range of temperatures."}
{"id": "2602.12954", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12954", "abs": "https://arxiv.org/abs/2602.12954", "authors": ["Luisa Schuhmacher", "Hazem Sallouha", "Ihsane Gryech", "Sofie Pollin"], "title": "Data Augmentation and Attention for massive MIMO-based Indoor Localization in Changing Environments", "comment": "To be published in IEEE ICC 2026 Conference Proceedings", "summary": "The demand for high-precision indoor localization has grown significantly with the rise of smart environments, industrial automation, and location-aware applications. While massive Multiple-Input and Multiple-Output (MIMO) systems enable millimeter-level accuracy by leveraging rich Channel State Information (CSI), most existing solutions are optimized for static environments, where users or devices remain fixed during data collection and inference. Real-world applications, however, often require real-time localization in changing environments, where rapid movement, unpredictable blockages, and dynamic channel conditions pose significant challenges. To address these challenges, we introduce two data augmentation techniques designed to resemble blocked antennas, enhancing the generalizability of localization models to dynamic scenarios. Additionally, we enhance an existing Deep Learning (DL) model by incorporating attention modules, improving its ability to focus on relevant channel features and antennas. We train our model on data from a static scenario, augmented with the proposed techniques, and evaluate it on a dataset collected in changing scenarios. We investigate the performance enhancements achieved by the data augmentation techniques and the Attention modules, and observe a localization accuracy improvement from a mean error of 286 mm, when trained without Attention and without data augmentations, to 66 mm, when trained with Attention and data augmentation. This shows that high localization accuracy can be maintained in changing environments, even without training data from those scenarios."}
{"id": "2602.13079", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.13079", "abs": "https://arxiv.org/abs/2602.13079", "authors": ["Malachi Phillips"], "title": "Multi-physics Preconditioning for Thermally Activated Batteries", "comment": "23 pages, 7 figures", "summary": "Thermal batteries, also known as molten-salt batteries, are single-use reserve power systems activated by pyrotechnic heat generation, which transitions the solid electrolyte into a molten state. The simulation of these batteries relies on multiphysics modeling to evaluate performance and behavior under various conditions. This paper presents advancements in scalable preconditioning strategies for the Thermally Activated Battery Simulator (TABS) tool, enabling efficient solutions to the coupled electrochemical systems that dominate computational costs in thermal battery simulations. We propose a hierarchical block Gauss-Seidel preconditioner implemented through the Teko package in Trilinos, which effectively addresses the challenges posed by tightly coupled physics, including charge transport, porous flow, and species diffusion. The preconditioner leverages scalable subblock solvers, including smoothed aggregation algebraic multigrid (SA-AMG) methods and domain-decomposition techniques, to achieve robust convergence and parallel scalability. Strong and weak scaling studies demonstrate the solver's ability to handle problem sizes up to 51.3 million degrees of freedom on 2048 processors, achieving near sub-second setup and solve times for the end-to-end electrochemical solve. These advancements significantly improve the computational efficiency and turnaround time of thermal battery simulations, paving the way for higher-resolution models and enabling the transition from 2D axisymmetric to full 3D simulations."}
{"id": "2602.13009", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13009", "abs": "https://arxiv.org/abs/2602.13009", "authors": ["E. Javier Olucha", "Arash Sadeghzadeh", "Amritam Das", "Roland Tóth"], "title": "Bayesian Optimization Based Grid Point Allocation for LPV and Robust Control", "comment": "Manuscript submitted to International Journal of Robust and Nonlinear Control", "summary": "This paper investigates systematic selection of optimal grid points for grid-based Linear Parameter-Varying (LPV) and robust controller synthesis. In both settings, the objective is to identify a set of local models such that the controller synthesized for these local models will satisfy global stability and performance requirements for the entire system. Here, local models correspond to evaluations of the LPV or uncertain plant at fixed values of the scheduling signal or realizations of the uncertainty set, respectively. Then, Bayesian optimization is employed to discover the most informative points that govern the closed-loop performance of the designed LPV or robust controller for the complete system until no significant further performance increase or a user specified limit is reached. Furthermore, when local model evaluations are computationally demanding or difficult to obtain, the proposed method is capable to minimize the number of evaluations and adjust the overall computational cost to the available budget. Lastly, the capabilities of the proposed method in automatically obtaining a sufficiently informative grid set are demonstrated on three case-studies: a robust controller design for an unbalanced disk, a multi-objective robust attitude controller design for a satellite with uncertain parameters and two flexible rotating solar arrays, and an LPV controller design for a robotic arm."}
{"id": "2602.13169", "categories": ["math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13169", "abs": "https://arxiv.org/abs/2602.13169", "authors": ["William Hofgard", "Asaf Cohen", "Mathieu Laurière"], "title": "Operator Learning for Families of Finite-State Mean-Field Games", "comment": "34 pages, 21 figures", "summary": "Finite-state mean-field games (MFGs) arise as limits of large interacting particle systems and are governed by an MFG system, a coupled forward-backward differential equation consisting of a forward Kolmogorov-Fokker-Planck (KFP) equation describing the population distribution and a backward Hamilton-Jacobi-Bellman (HJB) equation defining the value function. Solving MFG systems efficiently is challenging, with the structure of each system depending on an initial distribution of players and the terminal cost of the game. We propose an operator learning framework that solves parametric families of MFGs, enabling generalization without retraining for new initial distributions and terminal costs. We provide theoretical guarantees on the approximation error, parametric complexity, and generalization performance of our method, based on a novel regularity result for an appropriately defined flow map corresponding to an MFG system. We demonstrate empirically that our framework achieves accurate approximation for two representative instances of MFGs: a cybersecurity example and a high-dimensional quadratic model commonly used as a benchmark for numerical methods for MFGs."}
{"id": "2602.12823", "categories": ["quant-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.12823", "abs": "https://arxiv.org/abs/2602.12823", "authors": ["Abhijit Kundu", "Vijay Bhatt", "Arijit Sharma"], "title": "Towards Trapped-Ion Thermometry Using Cavity-Based EIT", "comment": null, "summary": "We present a technique for measuring ion temperature using cavity-based electromagnetically induced transparency (EIT) applicable for cavity-qed systems in the strong coupling regime. This method enables efficient extraction of the ion's phonon occupation number following sub-Doppler cooling close to the motional ground state. The proposed method relies on monitoring the cavity probe transmission while scanning the probe laser frequency once cavity EIT is established using the control beam, significantly simplifying the measurement procedure. We theoretically establish a model that demonstrates influence of thermal state of the trapped ion vis a vis the EIT linewidth measured. We show how the cavity EIT transmission may be used as a thermometry tool to deduce the ion temperature as well as the motional state for an ion in the sub-Doppler cooling regime. The current method can only be used for operation in the resolved-sideband regime, where individual motional states can be selectively addressed for all relevant transitions either by selecting appropriate energy levels for the three-level system or by employing strong confinement with high secular frequencies ($\\sim 10 MHz$)."}
{"id": "2602.13009", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13009", "abs": "https://arxiv.org/abs/2602.13009", "authors": ["E. Javier Olucha", "Arash Sadeghzadeh", "Amritam Das", "Roland Tóth"], "title": "Bayesian Optimization Based Grid Point Allocation for LPV and Robust Control", "comment": "Manuscript submitted to International Journal of Robust and Nonlinear Control", "summary": "This paper investigates systematic selection of optimal grid points for grid-based Linear Parameter-Varying (LPV) and robust controller synthesis. In both settings, the objective is to identify a set of local models such that the controller synthesized for these local models will satisfy global stability and performance requirements for the entire system. Here, local models correspond to evaluations of the LPV or uncertain plant at fixed values of the scheduling signal or realizations of the uncertainty set, respectively. Then, Bayesian optimization is employed to discover the most informative points that govern the closed-loop performance of the designed LPV or robust controller for the complete system until no significant further performance increase or a user specified limit is reached. Furthermore, when local model evaluations are computationally demanding or difficult to obtain, the proposed method is capable to minimize the number of evaluations and adjust the overall computational cost to the available budget. Lastly, the capabilities of the proposed method in automatically obtaining a sufficiently informative grid set are demonstrated on three case-studies: a robust controller design for an unbalanced disk, a multi-objective robust attitude controller design for a satellite with uncertain parameters and two flexible rotating solar arrays, and an LPV controller design for a robotic arm."}
{"id": "2602.12290", "categories": ["physics.comp-ph", "math.AP", "math.NA", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.12290", "abs": "https://arxiv.org/abs/2602.12290", "authors": ["Sarswati Shah", "Gerardo Hernández-Dueñas"], "title": "A new model for two-layer liquid-gas stratified flows in pipes with general cross sections", "comment": "27 pages, 8 figures", "summary": "In this work, we derive a new model for immiscible two-layer gas-liquid stratified flows in pipes with general cross sections. The bottom layer is occupied by an incompressible fluid in liquid phase with hydrodynamics based on a hydrostatic pressure, following a shallow water approximation. The top layer is occupied by a compressible gas, following an ideal gas law leading to conservation of mass, momentum and energy. The two subsystems are linked through non-conservative products, representing momentum and energy exchanges between layers. The hyperbolic properties of the resulting model are analyzed, including the derivation of entropy inequalities, and the approximations of eigenvalues of the corresponding coefficient matrix. Numerical tests are included to demonstrate the merits of the model and the numerical approximations, including well-balancedness, Riemann problems, and perturbations and convergence toward steady states at rest. Besides simulations of water and air where the density difference between layers is significant, a case where such difference is not so pronounced (like gas and liquid hydrogen) is also shown."}
{"id": "2602.13108", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13108", "abs": "https://arxiv.org/abs/2602.13108", "authors": ["J. H. Hoekstra", "B. Györök", "R. Töth", "M. Schoukens"], "title": "Encoder initialisation methods in the model augmentation setting", "comment": "Submitted to IFAC WC 2026", "summary": "Nonlinear system identification (NL-SI) has proven to be effective in obtaining accurate models for highly complex systems. Recent encoder-based methods for artificial neural network state-space (ANN-SS) models have shown state-of-the-art performance with improved computational efficiency, where the encoder is used to estimate the initial state allowing for batch optimisation methods. To address the lack of interpretability of these black-box ANN models, model augmentation approaches can be used. These combine prior available baseline models with the ANN learning components, resulting in faster convergence and more interpretable models. The combination of the encoder-based method with model augmentation has shown potential. Thus far, however, the encoder has still been treated as a black-box function in the overall estimation process, while additional information in the form of the baseline model is available to predict the model state from past input-output data. In this paper, we propose novel encoder initialisation approaches based on the available baseline model, resulting in improved noise robustness and faster convergence compared to black-box initialisation. The performance of these initialisation methods is demonstrated on a mass-spring-damper system."}
{"id": "2602.13177", "categories": ["math.OC", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13177", "abs": "https://arxiv.org/abs/2602.13177", "authors": ["Swati Gupta", "Jai Moondra", "Mohit Singh"], "title": "Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps", "comment": null, "summary": "OMD and its variants give a flexible framework for OCO where the performance depends crucially on the choice of the mirror map. While the geometries underlying OPGD and OEG, both special cases of OMD, are well understood, it remains a challenging open question on how to construct an optimal mirror map for any given constrained set and a general family of loss functions, e.g., sparse losses. Motivated by parameterizing a near-optimal set of mirror maps, we consider a simpler question: is it even possible to obtain polynomial gains in regret by using mirror maps for geometries that interpolate between $L_1$ and $L_2$, which may not be possible by restricting to only OEG ($L_1$) or OPGD ($L_2$).\n  Our main result answers this question positively. We show that mirror maps based on block norms adapt better to the sparsity of loss functions, compared to previous $L_p$ (for $p \\in [1, 2]$) interpolations. In particular, we construct a family of online convex optimization instances in $\\mathbb{R}^d$, where block norm-based mirror maps achieve a provable polynomial (in $d$) improvement in regret over OEG and OPGD for sparse loss functions. We then turn to the setting in which the sparsity level of the loss functions is unknown. In this case, the choice of geometry itself becomes an online decision problem. We first show that naively switching between OEG and OPGD can incur linear regret, highlighting the intrinsic difficulty of geometry selection. To overcome this issue, we propose a meta-algorithm based on multiplicative weights that dynamically selects among a family of uniform block norms. We show that this approach effectively tunes OMD to the sparsity of the losses, yielding adaptive regret guarantees. Overall, our results demonstrate that online mirror-map selection can significantly enhance the ability of OMD to exploit sparsity in online convex optimization."}
{"id": "2602.12831", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12831", "abs": "https://arxiv.org/abs/2602.12831", "authors": ["Alexander Popov", "Nico Meyer", "Daniel D. Scherer", "Guido Dietl"], "title": "Optimized Compilation of Logical Clifford Circuits", "comment": "This work has been submitted to the IEEE for possible publication. 8 pages, 7 figures", "summary": "Fault-tolerant quantum computing hinges on efficient logical compilation, in particular, translating high-level circuits into code-compatible implementations. Gate-by-gate compilation often yields deep circuits, requiring significant overhead to ensure fault-tolerance. As an alternative, we investigate the compilation of primitives from quantum simulation as single blocks. We focus our study on the [[n,n-2,2]] code family, which allows for the exhaustive comparison of potential compilation primitives on small circuit instances. Based upon that, we then introduce a methodology that lifts these primitives into size-invariant, depth-efficient compilation strategies. This recovers known methods for circuits with moderate Hadamard counts and yields improved realizations for sparse and dense placements. Simulations show significant error-rate reductions in the compiled circuits. We envision the approach as a core component of peephole-based compilers. Its flexibility and low hand-crafting burden make it readily extensible to other circuit structures and code families."}
{"id": "2602.13108", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13108", "abs": "https://arxiv.org/abs/2602.13108", "authors": ["J. H. Hoekstra", "B. Györök", "R. Töth", "M. Schoukens"], "title": "Encoder initialisation methods in the model augmentation setting", "comment": "Submitted to IFAC WC 2026", "summary": "Nonlinear system identification (NL-SI) has proven to be effective in obtaining accurate models for highly complex systems. Recent encoder-based methods for artificial neural network state-space (ANN-SS) models have shown state-of-the-art performance with improved computational efficiency, where the encoder is used to estimate the initial state allowing for batch optimisation methods. To address the lack of interpretability of these black-box ANN models, model augmentation approaches can be used. These combine prior available baseline models with the ANN learning components, resulting in faster convergence and more interpretable models. The combination of the encoder-based method with model augmentation has shown potential. Thus far, however, the encoder has still been treated as a black-box function in the overall estimation process, while additional information in the form of the baseline model is available to predict the model state from past input-output data. In this paper, we propose novel encoder initialisation approaches based on the available baseline model, resulting in improved noise robustness and faster convergence compared to black-box initialisation. The performance of these initialisation methods is demonstrated on a mass-spring-damper system."}
{"id": "2602.13150", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13150", "abs": "https://arxiv.org/abs/2602.13150", "authors": ["Ruiqi Wang", "Yiming Yang", "Atif Shamim"], "title": "3-D Reconfigurable Intelligent Surface: From Reflection to Transmission and From Single Hemisphere to Full 3-D Coverage", "comment": null, "summary": "Reconfigurable intelligent surfaces (RIS) are conventionally implemented as two-dimensional (2D) electromagnetic (EM) structures to steer incident waves toward desired reflection angles. This approach limits the reflection to a single hemisphere, and the beam-scanning range is relatively small. In this work, a novel three-dimensional (3D) RIS concept is proposed, where beam-scanning can be realized not only through reflection from the illuminated surface but also through controlled transmission toward adjacent surfaces, enabling near blind-spot-free coverage in the full 3D spatial domain. A cube-based 3D-RIS design operating at millimeter-wave (mm-Wave) frequencies and consisting of six interconnected RIS surfaces is presented. Each surface integrates reconfigurable receiving and reflecting arrays with orthogonal polarizations to ensure intrinsic EM isolation, while a reconfigurable feeding network supports dynamic operation. A subarray-based synthesis approach with binary amplitude gating and predefined phase offsets is developed through a unified theoretical model. This model, validated through full-wave simulations, enables efficient beam switching through a shared aperture. Based on this framework, an 8 x 12 element surface comprising six 4 x 4 subarrays is designed, with each surface covering an angular range from -30 deg to +30 deg. The experimental prototype has been characterized in the 24 to 30 GHz band, and the results demonstrate a gain enhancement of 14.7 dB for reflection, while 14.1 dB is achieved for transmission to the neighboring surface. Finally, wireless communication trials using the Pluto software-defined radio platform combined with frequency up/down converters confirm improved constellation quality and a 6-7 dB improvement in error vector magnitude (EVM) for both reflection and neighboring surface transmission scenarios."}
{"id": "2602.12472", "categories": ["quant-ph", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.12472", "abs": "https://arxiv.org/abs/2602.12472", "authors": ["Sofiane Chalal", "Nina H. Amini", "Hamed Amini", "Mathieu Laurière"], "title": "Dynamic Programming Principle and Stabilization for Mean-Field Quantum Filtering Systems", "comment": null, "summary": "Working within the quantum filtering framework, we establish a dynamic programming principle in an infinite-dimensional setting by embedding the state space into the Hilbert-Schmidt space. We then study a stabilization problem for continuously monitored Ising-coupled qubits and, in the mean-field limit, demonstrate quantum state reduction together with exponential convergence toward prescribed eigenstates under suitable feedback laws."}
{"id": "2602.12840", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12840", "abs": "https://arxiv.org/abs/2602.12840", "authors": ["Kuntal Adak", "Sakshi Kaushik", "Rahul Rana"], "title": "Airline Fleet Assignment Problems with Binary and Integer Programming models: Classical vs Quantum Annealing", "comment": null, "summary": "This research highlights the potential of quantum annealing in tackling large-scale optimization problems within the airline industry,demonstrating its efficiency for certain problem sizes while also acknowledging its current limitations. The comparative analysis provides valuable insights into the performance of advanced computational techniques, paving the way for further advancements in optimizing fleet assignments in the aviation sector."}
{"id": "2602.13150", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13150", "abs": "https://arxiv.org/abs/2602.13150", "authors": ["Ruiqi Wang", "Yiming Yang", "Atif Shamim"], "title": "3-D Reconfigurable Intelligent Surface: From Reflection to Transmission and From Single Hemisphere to Full 3-D Coverage", "comment": null, "summary": "Reconfigurable intelligent surfaces (RIS) are conventionally implemented as two-dimensional (2D) electromagnetic (EM) structures to steer incident waves toward desired reflection angles. This approach limits the reflection to a single hemisphere, and the beam-scanning range is relatively small. In this work, a novel three-dimensional (3D) RIS concept is proposed, where beam-scanning can be realized not only through reflection from the illuminated surface but also through controlled transmission toward adjacent surfaces, enabling near blind-spot-free coverage in the full 3D spatial domain. A cube-based 3D-RIS design operating at millimeter-wave (mm-Wave) frequencies and consisting of six interconnected RIS surfaces is presented. Each surface integrates reconfigurable receiving and reflecting arrays with orthogonal polarizations to ensure intrinsic EM isolation, while a reconfigurable feeding network supports dynamic operation. A subarray-based synthesis approach with binary amplitude gating and predefined phase offsets is developed through a unified theoretical model. This model, validated through full-wave simulations, enables efficient beam switching through a shared aperture. Based on this framework, an 8 x 12 element surface comprising six 4 x 4 subarrays is designed, with each surface covering an angular range from -30 deg to +30 deg. The experimental prototype has been characterized in the 24 to 30 GHz band, and the results demonstrate a gain enhancement of 14.7 dB for reflection, while 14.1 dB is achieved for transmission to the neighboring surface. Finally, wireless communication trials using the Pluto software-defined radio platform combined with frequency up/down converters confirm improved constellation quality and a 6-7 dB improvement in error vector magnitude (EVM) for both reflection and neighboring surface transmission scenarios."}
{"id": "2602.12293", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12293", "abs": "https://arxiv.org/abs/2602.12293", "authors": ["Ayrton Almada", "Laurent Pagnier", "Igal Goldshtein", "Saif R. Kazi", "Michael", "Chertkov"], "title": "Real-Time Dynamic N-1 Screening: Identifying High-Risk Lines and Transformers After Common Faults", "comment": "9 pages, 7 figures, 3 Tables, 1 Algorithm. arXiv admin note: substantial text overlap with arXiv:2510.18007", "summary": "Power system operators routinely perform N-1 contingency analysis, yet conventional tools provide limited guidance on which lines or transformers deserve heightened attention during fast post-fault transients. In particular, static screening does not reveal whether (1) the same faulted line repeatedly triggers severe downstream overloads, or (2) a specific transformer emerges as vulnerable across many distinct fault scenarios. This paper introduces a real-time dynamic N-1 screening framework that addresses this gap by estimating, for each counterfactual single-phase transmission fault, the probability of transient overcurrent on critical grid elements. The output is an operator-facing dashboard that ranks (a) faulted lines whose outages most frequently lead to dangerous transformer overloads, and (b) transformers that consistently overload across top-risk scenarios, both of which are actionable indicators for real-time situational awareness. The approach models post-fault electromechanical dynamics using a linear stochastic formulation of the swing equations with short-lived, fault-localized uncertainty, and combines analytic transient evaluation with cross-entropy based importance sampling to efficiently estimate rare but high-impact events. All N-1 contingencies are evaluated in parallel with linear computational complexity. The framework is demonstrated on the IEEE 118-bus system, where it reveals latent high-risk lines and transformers that remain invisible under deterministic dynamic or static N-1 analysis. Results show orders-of-magnitude computational speedup relative to brute-force Monte Carlo, enabling practical deployment within real-time operational cycles."}
{"id": "2602.12909", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2602.12909", "abs": "https://arxiv.org/abs/2602.12909", "authors": ["Chi Zhang", "Sara Murciano", "Nathanan Tantivasadakarn", "Ran Finkelstein"], "title": "Quantum logic control and entanglement in hybrid atom-molecule arrays", "comment": null, "summary": "Polar molecules, with their rich internal structure, offer immense potential for fundamental physics, quantum technology, and controlled chemistry. However, their utilization is currently limited because of slow and imperfect state detection and weak dipolar interaction, limiting fast and large-scale entanglement generation. We propose and analyze a scheme for quantum logic control and measurement-based state preparation in a hybrid platform of polar molecules and neutral atoms. The method leverages fast, high-fidelity atom-molecule gates and high-fidelity atomic ancilla measurements to overcome the common challenges in molecule-only platforms, while preserving their diverse structural advantages. The proposed atom-molecule controlled-phase gate is based on resonant dipole-dipole exchange between a molecular rotational transition and an atomic Rydberg transition, rendering it three orders of magnitude faster than any direct molecule-molecule entangling gate. We further study several applications of our scheme including the preparation of molecular GHZ states for quantum enhanced precision measurements, the preparation of exotic molecular qudit states with topological order, and measurement-altered criticality. Our scheme is applicable to any polar molecule. It expands the paradigm of quantum logic control and paves the way to large-scale molecular entangled states. More generally, it highlights a concrete hybrid quantum system in which each qubit is utilized in an optimal way and where the measurement-based approach can yield a significant advantage in near-term devices."}
{"id": "2602.12293", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.12293", "abs": "https://arxiv.org/abs/2602.12293", "authors": ["Ayrton Almada", "Laurent Pagnier", "Igal Goldshtein", "Saif R. Kazi", "Michael", "Chertkov"], "title": "Real-Time Dynamic N-1 Screening: Identifying High-Risk Lines and Transformers After Common Faults", "comment": "9 pages, 7 figures, 3 Tables, 1 Algorithm. arXiv admin note: substantial text overlap with arXiv:2510.18007", "summary": "Power system operators routinely perform N-1 contingency analysis, yet conventional tools provide limited guidance on which lines or transformers deserve heightened attention during fast post-fault transients. In particular, static screening does not reveal whether (1) the same faulted line repeatedly triggers severe downstream overloads, or (2) a specific transformer emerges as vulnerable across many distinct fault scenarios. This paper introduces a real-time dynamic N-1 screening framework that addresses this gap by estimating, for each counterfactual single-phase transmission fault, the probability of transient overcurrent on critical grid elements. The output is an operator-facing dashboard that ranks (a) faulted lines whose outages most frequently lead to dangerous transformer overloads, and (b) transformers that consistently overload across top-risk scenarios, both of which are actionable indicators for real-time situational awareness. The approach models post-fault electromechanical dynamics using a linear stochastic formulation of the swing equations with short-lived, fault-localized uncertainty, and combines analytic transient evaluation with cross-entropy based importance sampling to efficiently estimate rare but high-impact events. All N-1 contingencies are evaluated in parallel with linear computational complexity. The framework is demonstrated on the IEEE 118-bus system, where it reveals latent high-risk lines and transformers that remain invisible under deterministic dynamic or static N-1 analysis. Results show orders-of-magnitude computational speedup relative to brute-force Monte Carlo, enabling practical deployment within real-time operational cycles."}
{"id": "2602.13157", "categories": ["math.OC", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13157", "abs": "https://arxiv.org/abs/2602.13157", "authors": ["Sean Bowerfind", "Matthew R. Kirchner", "Gary Hewer"], "title": "A Data-Driven Algorithm for Model-Free Control Synthesis", "comment": null, "summary": "Presented is an algorithm to synthesize the optimal infinite-horizon LQR feedback controller for continuous-time systems. The algorithm does not require knowledge of the system dynamics but instead uses only a finite-length sampling of arbitrary input-output data. The algorithm is based on a constrained optimization problem that enforces a necessary condition on the dynamics of the optimal value function along any trajectory. In addition to calculating the standard LQR gain matrix, a feedforward gain can be found to implement a reference tracking controller. This paper presents a theoretical justification for the method and shows several examples, including a validation test on a real scale aircraft."}
{"id": "2602.12914", "categories": ["quant-ph", "cond-mat.other", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.12914", "abs": "https://arxiv.org/abs/2602.12914", "authors": ["Harshita Sharma", "Sayan Choudhury", "Jayendra N. Bandyopadhyay"], "title": "Quantum metrology with partially accessible chaotic sensors", "comment": "9 pages, 8 figures", "summary": "Most quantum metrology protocols harness highly entangled probe states and globally accessible measurements to surpass the standard quantum limit. However, it is challenging to satisfy these requirements in realistic many-body sensors. We demonstrate that both of these constraints can be overcome in quantum chaotic sensors. Crucially, we establish that even in the presence of partial measurement accessibility, chaotic dynamics enables initial unentangled states to exhibit Heisenberg scaling of the quantum Fisher information, $I_α$ with time. In the weakly chaotic regime, we identify spin-coherent states placed at the edge of the regular islands in the mixed classical phase space as optimal initial states for enhanced sensitivity. On the other hand, in the strongly chaotic regime, $I_α$ is insensitive to the choice of the initial state. Notably, quantum-enhanced sensitivity is achieved even when a very low fraction ($\\sim 5\\%$) of the qubits are accessible. These results establish quantum chaos as a robust resource for quantum-enhanced sensing under realistic accessibility constraints on accessibility."}
{"id": "2602.13157", "categories": ["math.OC", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13157", "abs": "https://arxiv.org/abs/2602.13157", "authors": ["Sean Bowerfind", "Matthew R. Kirchner", "Gary Hewer"], "title": "A Data-Driven Algorithm for Model-Free Control Synthesis", "comment": null, "summary": "Presented is an algorithm to synthesize the optimal infinite-horizon LQR feedback controller for continuous-time systems. The algorithm does not require knowledge of the system dynamics but instead uses only a finite-length sampling of arbitrary input-output data. The algorithm is based on a constrained optimization problem that enforces a necessary condition on the dynamics of the optimal value function along any trajectory. In addition to calculating the standard LQR gain matrix, a feedforward gain can be found to implement a reference tracking controller. This paper presents a theoretical justification for the method and shows several examples, including a validation test on a real scale aircraft."}
{"id": "2602.13006", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.13006", "abs": "https://arxiv.org/abs/2602.13006", "authors": ["Vijay Ganesh Sadhasivam", "Stuart C. Althorpe", "Venkat Kapil"], "title": "Effective classical potential for quantum statistical averages", "comment": null, "summary": "We present an effective potential that allows quantum thermal expectation values of a position-dependent observable to be estimated as a classical ensemble average of the corresponding function. We follow the approach of Feynman and Hibbs, but perform the mean-field treatment of quantum fluctuations about the path starting point rather than the path centroid. Furthermore, rather than performing a full variational optimization of the potential, we explore approximate functional forms that yield a numerical robustness. The resulting closed-form potential is exact in the classical and harmonic limits; benchmarks against exact position distributions for one-dimensional quartic, Morse, and double-well potentials, show good agreement for potentials with harmonic support."}
{"id": "2602.13026", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13026", "abs": "https://arxiv.org/abs/2602.13026", "authors": ["B. J. Alexander", "Ş. K. Özdemir", "M. S. Tame"], "title": "Weighted graph states as a resource for quantum metrology", "comment": "15 pages, 12 figures, appendix", "summary": "Quantum metrology exploits quantum mechanical effects to increase the precision of measurements of physical quantities. A wide variety of applications are currently being developed for scientific and technological purposes, however, most research relies on the use of highly entangled resource states that are challenging to generate and control in a given physical system. Here, we study the use of weighted graph states as more accessible resources for quantum metrology, which yield a favorable precision beyond the classical limit, approaching the Heisenberg limit. We find a notable robustness to variation in weights and less challenging weight requirements compared to standard graph states, which require a maximal weight at all edges. Both of these aspects reduce the practical demands in a physical setup, with the latter implying significantly less entanglement is required to gain a quantum advantage in metrology. We study the quantum Fisher information and optimized estimator variance of two identified sub classes of weighted graph states for an arbitrary number of N qubits, providing analytical forms and investigating their scaling. Our work opens up opportunities for using weakly entangled states in quantum-enhanced metrology."}
{"id": "2602.13094", "categories": ["quant-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2602.13094", "abs": "https://arxiv.org/abs/2602.13094", "authors": ["Wendy Otieno", "Alexandre Zagoskin", "Alexander G. Balanov", "Juan Totero Gongora", "Sergey E. Savel'ev"], "title": "A Quantum Reservoir Computing Approach to Quantum Stock Price Forecasting in Quantum-Invested Markets", "comment": "16 pages, 9 figures", "summary": "We present a quantum reservoir computing (QRC) framework based on a small-scale quantum system comprising at most six interacting qubits, designed for nonlinear financial time-series forecasting. We apply the model to predict future daily closing trading volumes of 20 quantum-sector publicly traded companies over the period from April 11, 2020, to April 11, 2025, as well as minute-by-minute trading volumes during out-of-market hours on July 7, 2025. Our analysis identifies optimal reservoir parameters that yield stock trend (up/down) classification accuracies exceeding $86 \\%$. Importantly, the QRC model is platform-agnostic and can be realized across diverse physical implementations of qubits, including superconducting circuits and trapped ions. These results demonstrate the expressive power and robustness of small-scale quantum reservoirs for modeling complex temporal correlations in financial data, highlighting their potential applicability to real-world forecasting tasks on near-term quantum hardware."}
{"id": "2602.13095", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.13095", "abs": "https://arxiv.org/abs/2602.13095", "authors": ["Hironobu Yoshida", "Ryusuke Hamazaki"], "title": "Theory of Steady States for Lindblad Equations beyond Time-Independence: Classification, Uniqueness and Symmetry", "comment": "25 pages, 3 figures", "summary": "We present a rigorous and comprehensive classification of the asymptotic behavior of time-quasiperiodic Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equations under the assumption of Hermitian jump operators. Our main contributions are twofold: first, we establish a criterion for the uniqueness of steady states. The criterion is formulated in terms of the algebra generated by the GKSL generators and provides a necessary and sufficient condition when the generators are analytic functions of time. We demonstrate the utility of our criterion through prototypical examples, including quantum many-body spin chains. Second, we extend the concept of strong symmetry for time-dependent GKSL equations by introducing two distinct forms, strong symmetry in the Schrödinger picture and that in the interaction picture, and completely classify the asymptotic dynamics with them. More concretely, we rigorously uncover that the strong symmetry in the interaction picture is responsible for non-trivial time-dependent steady states, such as coherent oscillations, whereas that in the Schrödinger picture controls the existence of time-independent steady states. This classification not only encompasses established mechanisms underlying non-trivial oscillatory steady states, such as strong dynamical symmetry and Floquet dynamical symmetry, but also reveals symmetry-predicted, time-dependent asymptotic dynamics in a novel class of open quantum systems. Our framework thus provides a rigorous foundation for controlling dissipative quantum systems in a time-dependent manner."}
{"id": "2602.13099", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.13099", "abs": "https://arxiv.org/abs/2602.13099", "authors": ["Riccardo Castellano", "Dmitry Grinko", "Sadra Boreiri", "Nicolas Brunner", "Jef Pauwels"], "title": "Stronger Welch Bounds and Optimal Approximate $k$-Designs", "comment": "8 + 12 pages, 2 figures", "summary": "A fundamental question asks how uniformly finite sets of pure quantum states can be distributed in a Hilbert space. The Welch bounds address this question, and are saturated by $k$-designs, i.e. sets of states reproducing the $k$-th Haar moments. However, these bounds quickly become uninformative when the number of states is below that required for an exact $k$-design. We derive strengthened Welch-type inequalities that remain sharp in this regime by exploiting rank constraints from partial transposition and spectral properties of the partially transposed Haar moment operator. We prove that the deviation from the Welch bound captures the average-case approximation error, hence characterizing a natural notion of minimum achievable error at fixed cardinality. For $k=3$, we prove that SICs and complete MUB sets saturate our bounds, making them optimal approximate 3-designs of their cardinality. This leads a natural variational criterion to rule out the existence of a complete set MUBs, which we use to obtain numerical evidence against such set in dimension $6$. As a key technical ingredient, we compute the complete spectrum of the partially transposed symmetric-subspace projector, including multiplicities and eigenvectors, which may find applications beyond the present work."}
{"id": "2602.13145", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13145", "abs": "https://arxiv.org/abs/2602.13145", "authors": ["Alireza Seif", "Moein Malekakhlagh", "Swarnadeep Majumder Luke C. G. Govia"], "title": "Single snapshot non-Markovianity of Pauli channels", "comment": "13 pages, 6 figures", "summary": "Pauli channels are widely used to describe errors in quantum computers, particularly when noise is shaped via Pauli twirling. A common assumption is that such channels admit a Markovian generator, namely a Pauli-Lindblad model with non-negative rates, but the validity of this assumption has not been systematically examined. Here, using CP-indivisibility as our criterion for non-Markovianity, we study multi-qubit Pauli channels from a single snapshot of the dynamics. We find that while the generator always has the same structure as the standard Pauli-Lindblad model, the rates may be negative or complex. We show that random Pauli channels are almost always non-Markovian, with the probability of encountering a negative rate converging doubly exponentially to unity with the number of qubits. For physically motivated noise models shaped by Pauli twirling, including single-qubit over-rotations and two-qubit amplitude damping errors, we find that negative rates are generic, even when the underlying physical noise is Markovian. We generalize probabilistic error amplification and cancellation to non-Markovian generators, and quantify the sampling overhead introduced by negative and complex rates. Experiments on superconducting qubits confirm that allowing negative rates in the learned noise model yields more accurate predictions than restricting to non-negative rates."}
{"id": "2602.13146", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13146", "abs": "https://arxiv.org/abs/2602.13146", "authors": ["Gerard McCaul"], "title": "Mean-Force Hamiltonians from Influence Functionals", "comment": null, "summary": "The Hamiltonian of mean force (HMF) provides the standard starting point for strong-coupling thermodynamics, yet explicit operator forms are known only in restricted settings. We present a quenched density framework that uses the Hubbard-Stratonovich transformation to rewrite the reduced equilibrium state as an average over local propagators in imaginary time. This approach rigorously separates the statistical definition of the environment from the algebraic structure of the system response. We apply this framework to the minimal case of a harmonic environment with a coupling commuting with the system Hamiltonian. In this scenario the correction to the HMF has an exact, closed-form expression. We validate this result against finite-bath trace-out calculations and stochastic imaginary-time sampling in a five-level projector-coupled model."}
{"id": "2602.12417", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.12417", "abs": "https://arxiv.org/abs/2602.12417", "authors": ["William Skoglund", "Elton Giacomelli", "Yiqi Yang", "Jens H. Bardarson", "Erik van Loon"], "title": "Information lattice approach to the metal-insulator transition", "comment": null, "summary": "Correlation functions and correlation lengths are frequently used to describe phase transitions in quantum systems, but they require an explicit choice of observables. The recently introduced information lattice instead provides an observable-independent way to identify where and at which scale information is contained in quantum lattice models. Here, we use it to study the difference between the metallic and insulating regime of one-dimensional tight-binding chains. We find that the information per scale follows a power law in metals at low temperature and that Friedel-like oscillations are visible in the information lattice. At high temperature or in insulators at low temperature, the information per scale decays exponentially. Thus, the information lattice is a useful tool for analyzing the metal-insulator transition."}
{"id": "2602.13050", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13050", "abs": "https://arxiv.org/abs/2602.13050", "authors": ["Gennady Y. Chitov"], "title": "Topology of the Fermi surface and universality of the metal-metal and metal-insulator transitions: $d$-dimensional Hatsugai-Kohmoto model as an example", "comment": "34 pages (18 pp main text + 3 appendices), 13 figures", "summary": "The earlier theory [1] of the quantum phase transitions related to the change of the Fermi Surface Topology (FST) is advanced. For such transitions the Fermi surface as a quantum critical manifold determined by the Lee-Yang zeros, the order parameter $\\mathcal{P}$ as the $d$-volume of the Fermi sea, and the special FST universality class were introduced in [1]. The exactly solvable Hatsugai-Kohmoto (HK) $d$-dimensional ($d=1,2,3$) model of interacting fermions is analyzed. We explore the relation between the Lee-Yang zeros, the Luttinger and the plateau (Oshikawa) theorems. The validity of the Luttinger theorem in the HK model is confirmed. It is shown that the order parameter $\\mathcal{P}$ and the FST universality class describe the transitions between metal and band/Mott insulators, as well as the Lifshitz and van Hove gapless-to-gapless transitions. The gapless phases are established to be the Landau Fermi liquids (metals). In addition to the conventional paradigm with a continuous order parameter, we apply the homology theory to analyze the FST transitions. They are critical points of the Morse function. To quantify FST we use the Euler characteristic, which is calculated for each phase of the HK model. We claim that the FST universality class is robust with respect to interactions and other model details, under the condition that the critical points are non-degenerate."}
