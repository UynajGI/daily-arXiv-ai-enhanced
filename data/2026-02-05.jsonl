{"id": "2602.03969", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03969", "abs": "https://arxiv.org/abs/2602.03969", "authors": ["Shama Magnur", "Mayank Kejriwal"], "title": "Structural shifts in institutional participation and collaboration within the AI arXiv preprint research ecosystem", "comment": "16 pages, 5 Figures, 7 Tables", "summary": "The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) from 2021 through 2025. Given the rapid pace of AI development, the preprint ecosystem has become a critical barometer for real-time scientific shifts, often preceding formal peer-reviewed publication by months or years. By employing a multi-stage data collection and enrichment pipeline in conjunction with LLM-based institution classification, we analyze the evolution of publication volumes, author team sizes, and academic--industry collaboration patterns. Our results reveal an unprecedented surge in publication output following the introduction of ChatGPT, with academic institutions continuing to provide the largest volume of research. However, we observe that academic--industry collaboration is still suppressed, as measured by a Normalized Collaboration Index (NCI) that remains significantly below the random-mixing baseline across all major subfields. These findings highlight a continuing institutional divide and suggest that the capital-intensive nature of generative AI research may be reshaping the boundaries of scientific collaboration."}
{"id": "2602.04546", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04546", "abs": "https://arxiv.org/abs/2602.04546", "authors": ["Florian Kramer", "Henrich R. Greve", "Moritz von Zahn", "Hayagreeva Rao"], "title": "Unmasking Superspreaders: Data-Driven Approaches for Identifying and Comparing Key Influencers of Conspiracy Theories on X.com", "comment": null, "summary": "Conspiracy theories can threaten society by spreading misinformation, deepening polarization, and eroding trust in democratic institutions. Social media often fuels the spread of conspiracies, primarily driven by two key actors: Superspreaders -- influential individuals disseminating conspiracy content at disproportionately high rates, and Bots -- automated accounts designed to amplify conspiracies strategically. To counter the spread of conspiracy theories, it is critical to both identify these actors and to better understand their behavior. However, a systematic analysis of these actors as well as real-world-applicable identification methods are still lacking. In this study, we leverage over seven million tweets from the COVID-19 pandemic to analyze key differences between Human Superspreaders and Bots across dimensions such as linguistic complexity, toxicity, and hashtag usage. Our analysis reveals distinct communication strategies: Superspreaders tend to use more complex language and substantive content while relying less on structural elements like hashtags and emojis, likely to enhance credibility and authority. By contrast, Bots favor simpler language and strategic cross-usage of hashtags, likely to increase accessibility, facilitate infiltration into trending discussions, and amplify reach. To counter both Human Superspreaders and Bots, we propose and evaluate 27 novel metrics for quantifying the severity of conspiracy theory spread. Our findings highlight the effectiveness of an adapted H-Index for computationally feasible identification of Human Superspreaders. By identifying behavioral patterns unique to Human Superspreaders and Bots as well as providing suitable identification methods, this study provides a foundation for mitigation strategies, including platform moderation policies, temporary and permanent account suspensions, and public awareness campaigns."}
{"id": "2602.04674", "categories": ["cs.SI", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04674", "abs": "https://arxiv.org/abs/2602.04674", "authors": ["Eun Cheol Choi", "Lindsay E. Young", "Emilio Ferrara"], "title": "Overstating Attitudes, Ignoring Networks: LLM Biases in Simulating Misinformation Susceptibility", "comment": null, "summary": "Large language models (LLMs) are increasingly used as proxies for human judgment in computational social science, yet their ability to reproduce patterns of susceptibility to misinformation remains unclear. We test whether LLM-simulated survey respondents, prompted with participant profiles drawn from social survey data measuring network, demographic, attitudinal and behavioral features, can reproduce human patterns of misinformation belief and sharing. Using three online surveys as baselines, we evaluate whether LLM outputs match observed response distributions and recover feature-outcome associations present in the original survey data. LLM-generated responses capture broad distributional tendencies and show modest correlation with human responses, but consistently overstate the association between belief and sharing. Linear models fit to simulated responses exhibit substantially higher explained variance and place disproportionate weight on attitudinal and behavioral features, while largely ignoring personal network characteristics, relative to models fit to human responses. Analyses of model-generated reasoning and LLM training data suggest that these distortions reflect systematic biases in how misinformation-related concepts are represented. Our findings suggest that LLM-based survey simulations are better suited for diagnosing systematic divergences from human judgment than for substituting it."}
{"id": "2602.04694", "categories": ["cs.SI", "cs.CR", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.04694", "abs": "https://arxiv.org/abs/2602.04694", "authors": ["Maya Le", "Paweł Prałat", "Aaron Smith", "François Théberge"], "title": "The Needle is a Thread: Finding Planted Paths in Noisy Process Trees", "comment": "15 pages, 9 figures", "summary": "Motivated by applications in cybersecurity such as finding meaningful sequences of malware-related events buried inside large amounts of computer log data, we introduce the \"planted path\" problem and propose an algorithm to find fuzzy matchings between two trees. This algorithm can be used as a \"building block\" for more complicated workflows. We demonstrate usefulness of a few of such workflows in mining synthetically generated data as well as real-world ACME cybersecurity datasets."}
{"id": "2602.04067", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.04067", "abs": "https://arxiv.org/abs/2602.04067", "authors": ["Xiaohui Zhou", "Anton S. Darmenov", "Kianoosh Yousefi"], "title": "Data Driven Air Entrainment Velocity Parameterization by Breaking Waves", "comment": null, "summary": "Wave breaking injects turbulence and bubbles into the upper ocean, modulating air-sea exchange of momentum, heat, gases, and sea-spray aerosols. These fluxes depend nonlinearly on sea state but remain poorly represented in coupled atmosphere-wave-ocean models, where air-entrainment velocity is often parameterized using wind speed or significant wave height alone. We develop a global machine-learning parameterization of Va trained on a 43-year WAVEWATCH III simulation that resolves the breaker-front distribution and associated energetics. A multilayer perceptron with seven physically motivated predictors (wind speed, wave height, wave age, steepness, direction, and depth) reproduces spectral-reference Va with high skill. The model reduces longstanding biases in bulk formulas, notably overestimation in swell-dominated low latitudes and underestimation in storm tracks. Applied globally, it improves bubble-mediated CO2 transfer velocity and sea-salt aerosol emission, reducing errors by an order of magnitude. Validation against independent HiWinGS observations supports robust deep-water performance."}
{"id": "2602.04091", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.04091", "abs": "https://arxiv.org/abs/2602.04091", "authors": ["Edson D. Leonel", "João P. C. Ferreira", "Diego F. M. Oliveira"], "title": "A phenomenological description of critical slowing down at period-doubling bifurcations", "comment": null, "summary": "We present a phenomenological description of the critical slowing down associated with period-doubling bifurcations in discrete dynamical systems. Starting from a local Taylor expansion around the fixed point and the bifurcation parameter, we derive a reduced description that captures the convergence towards stationary state both at and near criticality. At the bifurcation point, three universal critical exponents are obtained, characterising the short-time behaviour, the asymptotic decay, and the crossover between these regimes. Away from criticality, a fourth exponent governing the relaxation time is identified. We show this phenomenology, well established for one-dimensional maps, extends naturally to two-dimensional mappings. By projecting the dynamics onto the centre manifold, we demonstrate that the local normal form of a two-dimensional period-doubling bifurcation reduces to the same universal structure found in one dimension. The theoretical predictions are validated numerically using the Hénon and Ikeda maps, showing excellent agreement for all scaling laws and critical exponents."}
{"id": "2602.04146", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.04146", "abs": "https://arxiv.org/abs/2602.04146", "authors": ["Nick Polson", "Vadim Sokolov", "Daniel Zantedeschi"], "title": "Bayes, E-values and Testing", "comment": null, "summary": "This paper studies relationships between Kolmogorov complexity, Shannon entropy, Bayes factors, E-values, and exchangeability testing. The focus is on negative log marginal or predictive probabilities -- what I.J.~Good termed the ``weight of evidence'' -- as a common evidence statistic linking coding, prediction, and sequential testing. The paper reviews the relevant information-theoretic and martingale tools, and discusses exchangeability testing via conformal e-prediction."}
{"id": "2602.03919", "categories": ["cond-mat.stat-mech", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.03919", "abs": "https://arxiv.org/abs/2602.03919", "authors": ["Airton Deppman"], "title": "Tsallis Entropy derived from the Chaitin-Kolmogorov Informational Entropy", "comment": "16 pages 1 figure", "summary": "We provide a rigorous first-principle derivation of the non-additive Tsallis' entropy by employing the Chaitin-Kolmogorov algorithmic information theory. By applying non-local restrictive rules on the string formation (grammar), we show that the algorithmic cost follows a power-law of the string length, instead of the linear behaviour obtained in the classical theory. As a result, the Tsallis entropy governs the increase of information. We explore the result showing, through Landauer's limit, that the heat dissipation in systems with long-range correlations is diminished. The $Ω_q$ number, which remains incompressible, now offers the possibility of a continuous increase of complexity, measured by the parameter $q$. We show the consistency of the results by a numerical simulation, and discuss Zipf's law in light of the new findings."}
{"id": "2602.03859", "categories": ["physics.comp-ph", "math.NA", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2602.03859", "abs": "https://arxiv.org/abs/2602.03859", "authors": ["Evgeniy V. Chizhonkov", "Olga S. Rozanova"], "title": "Numerical study of loss of hyperbolicity using a cold plasma model", "comment": "16 pages, 6 figures", "summary": "We study a one-dimensional system of cold plasma equations taking into account electron-ion collisions in both relativistic and nonrelativistic cases. It is known that for a constant collision coefficient $ν$, the solution to the Cauchy problem for such a system can lose smoothness. However, if the dependence of $ν$ on the electron density $N$ is more than linear, then the solution remains globally smooth for any initial data. However, the appearance of the dependence $ν(N)$ leads to a change in the type of the system, it loses hyperbolicity, which leads to computational problems. In this paper, we propose a new implicit solution method in Euler variables that overcomes these difficulties. It can be used in both nonrelativistic and relativistic cases and is tested for the threshold case of a linear dependence $ν(N)=ν_1+ν_0 N$, when smoothness can still be lost. The computational experiments carried out are in full agreement with the available theoretical results."}
{"id": "2602.03938", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.03938", "abs": "https://arxiv.org/abs/2602.03938", "authors": ["Piper C. Wysocki", "Luke D. Burkhart", "Madeline H. Morocco", "Corey I. Ostrove", "Riley J. Murray", "Tristan Brown", "Jeffrey M. Gertler", "David K. Kim", "Nathan E. Miller", "Bethany M. Niedzielski", "Katrina M. Sliwa", "Robin Blume-Kohout", "Gabriel O. Samach", "Mollie E. Schwartz", "Kenneth M. Rudinger"], "title": "Detailed, interpretable characterization of mid-circuit measurement on a transmon qubit", "comment": null, "summary": "Mid-circuit measurements (MCMs) are critical components of the quantum error correction protocols expected to enable utility-scale quantum computing. MCMs can be modeled by quantum instruments (a type of quantum operation or process), which can be characterized self-consistently using gate set tomography. However, experimentally estimated quantum instruments are often hard to interpret or relate to device physics. We address this challenge by adapting the error generator formalism -- previously used to interpret noisy quantum gates by decomposing their error processes into physically meaningful sums of \"elementary errors\" -- to MCMs. We deploy our new analysis on a transmon qubit device to tease out and quantify error mechanisms including amplitude damping, readout error, and imperfect collapse. We examine in detail how the magnitudes of these errors vary with the readout pulse amplitude, recover the key features of dispersive readout predicted by theory, and show that these features can be modeled parsimoniously using a reduced model with just a few parameters."}
{"id": "2602.04510", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.04510", "abs": "https://arxiv.org/abs/2602.04510", "authors": ["Zhaolin Hu", "Zhiliang Wu", "Hehe Fan", "Yi Yang"], "title": "OSCAgent: Accelerating the Discovery of Organic Solar Cells with LLM Agents", "comment": null, "summary": "Organic solar cells (OSCs) hold great promise for sustainable energy, but discovering high-performance materials is time-consuming and costly. Existing molecular generation methods can aid the design of OSC molecules, but they are mostly confined to optimizing known backbones and lack effective use of domain-specific chemical knowledge, often leading to unrealistic candidates. In this paper, we introduce OSCAgent, a multi-agent framework for OSC molecular discovery that unifies retrieval-augmented design, molecular generation, and systematic evaluation into a continuously improving pipeline, without requiring additional human intervention. OSCAgent comprises three collaborative agents. The Planner retrieves knowledge from literature-curated molecules and prior candidates to guide design directions. The Generator proposes new OSC acceptors aligned with these plans. The Experimenter performs comprehensive evaluation of candidate molecules and provides feedback for refinement. Experiments show that OSCAgent produces chemically valid, synthetically accessible OSC molecules and achieves superior predicted performance compared to both traditional and large language model (LLM)-only baselines. Representative results demonstrate that some candidates achieve predicted efficiencies approaching 18\\%. The code will be publicly available."}
{"id": "2602.03939", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03939", "abs": "https://arxiv.org/abs/2602.03939", "authors": ["Chongyang Shi", "Michael Dorothy", "Jie Fu"], "title": "C-IDS: Solving Contextual POMDP via Information-Directed Objective", "comment": null, "summary": "We study the policy synthesis problem in contextual partially observable Markov decision processes (CPOMDPs), where the environment is governed by an unknown latent context that induces distinct POMDP dynamics. Our goal is to design a policy that simultaneously maximizes cumulative return and actively reduces uncertainty about the underlying context. We introduce an information-directed objective that augments reward maximization with mutual information between the latent context and the agent's observations. We develop the C-IDS algorithm to synthesize policies that maximize the information-directed objective. We show that the objective can be interpreted as a Lagrangian relaxation of the linear information ratio and prove that the temperature parameter is an upper bound on the information ratio. Based on this characterization, we establish a sublinear Bayesian regret bound over K episodes. We evaluate our approach on a continuous Light-Dark environment and show that it consistently outperforms standard POMDP solvers that treat the unknown context as a latent state variable, achieving faster context identification and higher returns."}
{"id": "2602.03857", "categories": ["physics.geo-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.03857", "abs": "https://arxiv.org/abs/2602.03857", "authors": ["Nimatullah", "Pankaj K Mishra", "Jochen Kamm", "Anand Singh"], "title": "Backend-agnostic Julia framework for 3D modeling and inversion of gravity data", "comment": "Source-code for this paper is at https://zenodo.org/records/18339626", "summary": "This paper presents a high-performance framework for three-dimensional gravity modeling and inversion implemented in Julia, addressing key challenges in geophysical modeling such as computational complexity, ill-posedness, and the non-uniqueness inherent to gravity inversion. The framework adopts a data-space inversion formulation to reduce the dimensionality of the problem, leading to significantly lower memory requirements and improved computational efficiency while maintaining inversion accuracy. Forward modeling and inversion operators are implemented within a backend-agnostic kernel abstraction, enabling execution on both multicore CPUs and GPU accelerators from a single code base. Performance analyses conducted on NVIDIA CUDA GPUs demonstrate substantial reductions in runtime relative to CPU execution, particularly for large-scale datasets involving up to approximately 3.3 million rectangular prisms, highlighting the scalability of the proposed approach. The inversion incorporates implicit model constraints through the data-space formulation and depth-weighted sensitivity, which mitigate depth-related amplitude decay and yield geologically coherent, high-resolution subsurface density models. Validation using synthetic models confirms the ability of the framework to accurately reconstruct complex subsurface structures such as vertical and dipping dykes. Application to field gravity data further demonstrates the robustness and practical utility of the GPU-accelerated framework, with the recovered models showing strong consistency with independent geological constraints and prior interpretations. Overall, this work underscores the potential of GPU-enabled computing in Julia to transform large-scale gravity inversion workflows, providing an efficient, extensible, and accurate computational solution for high-resolution geophysical studies."}
{"id": "2602.04088", "categories": ["physics.soc-ph", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.04088", "abs": "https://arxiv.org/abs/2602.04088", "authors": ["Francesca Collu", "Antonio Scala", "Emilia La Nave"], "title": "Coalition Structure and Polarization in Parliamentary Voting Networks: Evidence from the Italian Parliament", "comment": null, "summary": "Ensuring legislative accountability in multi-party systems requires quantitative tools that reveal actual voting behavior beyond formal party affiliations. We present a network-based framework for analyzing parliamentary dynamics at multiple scales, capturing coalition structure, group coherence, and individual influence. Applied to over 4 million vote expressions from the Italian Parliament across three government formations (2018-2021), the methodology combines network modularity, voting distance metrics, and betweenness centrality to map the structure of collective decision-making. Using this framework, we show that system-level polarization, as captured by network modularity, varies systematically with coalition structure rather than coalition size. Technical governments display paradoxically lower global polarization despite broader formal support, reflecting structurally mixed voting patterns rather than unified blocs. On polarizing issues such as immigration, network polarization depends strongly on the fragmentation or cohesion of the opposition, even when the governing coalition votes cohesively. Betweenness analysis reveals that mediator roles are highly concentrated, with only about 2% of parliamentarians acting as structural bridges between communities. Community detection further uncovers implicit coalitions that are not apparent from declared alliances. The framework relies exclusively on public roll-call data, enabling reproducible analysis and direct applicability to any legislature with transparent voting records. By linking individual voting behavior to emergent system-level structure, the methodology provides quantitative infrastructure for comparative analysis of legislative voting networks and coalition monitoring, enabling systematic assessment of legislative behaviour."}
{"id": "2602.04035", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.04035", "abs": "https://arxiv.org/abs/2602.04035", "authors": ["Thomas Neuner", "Henriette Padberg", "Lior Kornblum", "Eilam Yalon", "Pedram Khalili Amiri", "Shahar Kvatinsky"], "title": "A Comparative Study of Digital Memristor-Based Processing-In-Memory from a Device and Reliability Perspective", "comment": "23 pages, 12 figures, 3 tables, invited review paper, published in: https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aelm.202500348, T. Neuner and H. Padberg contributed equally to the work, S. Kvatinsky is the corresponding author", "summary": "As data-intensive applications increasingly strain conventional computing systems, processing-in-memory (PIM) has emerged as a promising paradigm to alleviate the memory wall by minimizing data transfer between memory and processing units. This review presents the recent advances in both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and magnetoresistive random-access memory (MRAM). Both experimentally demonstrated and simulated logic designs are critically examined, highlighting key challenges in reliability and the role of device-level optimization in enabling scalable and commercial viable PIM systems. The review begins with an overview of relevant logic families, memristive device types, and associated reliability metrics. Each logic family is then explored in terms of how it capitalizes on distinct device properties to implement logic techniques. A comparative table of representative device stacks and performance parameters illustrates trade-offs and quality indicators. Through this comprehensive analysis, the development of optimized, robust memristive devices for next-generation PIM applications is supported."}
{"id": "2602.04409", "categories": ["hep-lat"], "pdf": "https://arxiv.org/pdf/2602.04409", "abs": "https://arxiv.org/abs/2602.04409", "authors": ["Vaibhav Chahar", "Piotr Korcyl"], "title": "Probing Instanton Dynamics in the Pion Vector Form Factor with Wilson Flow", "comment": "Lattice2025 proceedings", "summary": "Instanton liquid model is believed to capture the main features of vacuum QCD dynamics. Recently, multiple predictions for hadron structure functions have been derived and compared with experimental measurements and lattice QCD calculations, finding a general agreement. In order to explore the precision of the instanton liquid model, one has to compare its predictions with non-perturbative simulations in a regime dominated by instanton dynamics. This has been performed for two gluon-sensitive observables: the gluon Green's function and the strong running coupling constant. In this contribution, we propose to study a fermionic observable, the pion electromagnetic form factor, for which instanton liquid model predictions have been discussed in Phys.Rev.D 109, 074029. We use the Wilson flow to single out the dominant contribution from the instantons out of a lattice QCD configuration ensemble. We describe the details of our numerical setup, and some first, preliminary results."}
{"id": "2602.04619", "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.04619", "abs": "https://arxiv.org/abs/2602.04619", "authors": ["Joshua Donald", "Ben A. Johnson", "Amir Mehrnejat", "Alex Gabbitas", "Arthur G. T. Coveney", "Alexander G. Balanov", "Sergey Savel'ev", "Pavel Borisov"], "title": "Scalable platform enabling reservoir computing with nanoporous oxide memristors for image recognition and time series prediction", "comment": null, "summary": "Typical mammal brains have some form of random connectivity between neurons. Reservoir computing, a neural network approach, uses random weights within its processing layer along with built-in recurrent connections and short-term, fading memory, and is shown to be time and training efficient in processing spatiotemporal signals. Here we prepared a niobium oxide-based thin film memristor device with intrinsic structural in-homogeneity in the form of random nanopores and performed computational tasks of XOR operations, image recognition, and time series prediction and reconstruction. For the latter task we chose a complex three-dimensional chaotic Lorenz-63 time series. By applying three temporal voltage waveforms individually across the device and training the readout layer with electrical current signals from a three-output physical reservoir, we achieved satisfactory prediction and reconstruction accuracy in comparison to the case of no reservoir. This work highlights the potential for scalable, on-chip devices using all-oxide reservoir systems, paving the way for energy-efficient neuromorphic electronics dealing with time signals."}
{"id": "2602.03977", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.03977", "abs": "https://arxiv.org/abs/2602.03977", "authors": ["Shaked Regev", "Eve Tsybina", "Slaven Peles"], "title": "A Fast Relax-and-Round Approach to Unit Commitment with Sub Hourly Mechanical and Ramp Constraints", "comment": "8 pages, 4 figures", "summary": "We propose a novel computational method for unit commitment (UC), which does not require linearized approximation and provides several orders of magnitude performance improvement over current state-of-the-art. The performance improvement is achieved by introducing a heuristic tailored for UC problems. The method can be implemented using existing continuous optimization solvers and adapted for different applications. We demonstrate value of the new method in examples of advanced UC analyses at the scale where use of current state-of-the-art tools is infeasible. We expect that the capability demonstrated in this paper will be critical to address emerging power systems challenges with more volatile large loads, such as data centers, and generation that is composed of larger number of smaller units, including significant behind-the-meter generation."}
{"id": "2602.03939", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03939", "abs": "https://arxiv.org/abs/2602.03939", "authors": ["Chongyang Shi", "Michael Dorothy", "Jie Fu"], "title": "C-IDS: Solving Contextual POMDP via Information-Directed Objective", "comment": null, "summary": "We study the policy synthesis problem in contextual partially observable Markov decision processes (CPOMDPs), where the environment is governed by an unknown latent context that induces distinct POMDP dynamics. Our goal is to design a policy that simultaneously maximizes cumulative return and actively reduces uncertainty about the underlying context. We introduce an information-directed objective that augments reward maximization with mutual information between the latent context and the agent's observations. We develop the C-IDS algorithm to synthesize policies that maximize the information-directed objective. We show that the objective can be interpreted as a Lagrangian relaxation of the linear information ratio and prove that the temperature parameter is an upper bound on the information ratio. Based on this characterization, we establish a sublinear Bayesian regret bound over K episodes. We evaluate our approach on a continuous Light-Dark environment and show that it consistently outperforms standard POMDP solvers that treat the unknown context as a latent state variable, achieving faster context identification and higher returns."}
{"id": "2602.04088", "categories": ["physics.soc-ph", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.04088", "abs": "https://arxiv.org/abs/2602.04088", "authors": ["Francesca Collu", "Antonio Scala", "Emilia La Nave"], "title": "Coalition Structure and Polarization in Parliamentary Voting Networks: Evidence from the Italian Parliament", "comment": null, "summary": "Ensuring legislative accountability in multi-party systems requires quantitative tools that reveal actual voting behavior beyond formal party affiliations. We present a network-based framework for analyzing parliamentary dynamics at multiple scales, capturing coalition structure, group coherence, and individual influence. Applied to over 4 million vote expressions from the Italian Parliament across three government formations (2018-2021), the methodology combines network modularity, voting distance metrics, and betweenness centrality to map the structure of collective decision-making. Using this framework, we show that system-level polarization, as captured by network modularity, varies systematically with coalition structure rather than coalition size. Technical governments display paradoxically lower global polarization despite broader formal support, reflecting structurally mixed voting patterns rather than unified blocs. On polarizing issues such as immigration, network polarization depends strongly on the fragmentation or cohesion of the opposition, even when the governing coalition votes cohesively. Betweenness analysis reveals that mediator roles are highly concentrated, with only about 2% of parliamentarians acting as structural bridges between communities. Community detection further uncovers implicit coalitions that are not apparent from declared alliances. The framework relies exclusively on public roll-call data, enabling reproducible analysis and direct applicability to any legislature with transparent voting records. By linking individual voting behavior to emergent system-level structure, the methodology provides quantitative infrastructure for comparative analysis of legislative voting networks and coalition monitoring, enabling systematic assessment of legislative behaviour."}
{"id": "2602.04730", "categories": ["nlin.CD", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.04730", "abs": "https://arxiv.org/abs/2602.04730", "authors": ["Christopher Amey", "Bala Sundaram", "Andrew C. Poje"], "title": "Semiclassical Structure of the Advection--Diffusion Spectrum in Mixed Phase Spaces", "comment": null, "summary": "We examine the spectral structure of the two-dimensional advection-diffusion operator in flows with mixed phase space at very large Peclet number. Using Fourier discretization combined with symmetry reduction and Krylov-Arnoldi methods, we compute on the order of one hundred leading eigenpairs reliably in the asymptotic, weak-diffusion regime. While the principal eigenvalue is asymptotically diffusive and localized on the largest regular region, the broader spectrum exhibits a rich organization controlled by local Lagrangian phase-space geometry. In particular, exponential mixing in chaotic regions rapidly suppresses correlations, whereas algebraic mixing in integrable regions generates long-lived coherent structures that dominate the slow and intermediate parts of the spectrum. We identify three distinct classes of eigenmodes: advective modes associated with transport on invariant tori, diffusive modes and, within the duffusive branch, tunneling modes arising from weak coupling between dynamically separated regular regions. Drawing on a semiclassical analogy, we assign quantum-number-like labels to these families and predict the appearance, scaling, and ordering of their sub-spectra directly from the Hamiltonian phase-space structure. The coexistence of these families implies that no uniform control of the spectral gap exists across the full spectrum: although the slowest mode is diffusive, arbitrarily small gaps arise between competing families at higher mode numbers. As a result, finite-time advection-diffusion dynamics is generically governed by persistent modal competition rather than single-mode dominance, even at asymptotically large Peclet number."}
{"id": "2602.04302", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.04302", "abs": "https://arxiv.org/abs/2602.04302", "authors": ["Rui Wang", "Guangming Pan", "Dandan Jiang"], "title": "Phase Transition of Spectral Fluctuations in Large Gram Matrices with a Variance Profile: A Unified Framework for Sparse CLTs", "comment": "25 pages, 4 figures", "summary": "We study the asymptotic spectral behavior of high-dimensional random Gram matrices with sparsity and a given variance profile, motivated by applications in wireless communication. Specifically, we consider the Gram matrices $\\mathbf S_n=\\mathbf Y_n\\mathbf Y_n^*$, where the entries of $\\mathbf Y_n$ are independent, centered, heteroscedastic, and sparse through Bernoulli masking. The sparsity level is parameterized as $s=q^2/n$, with $q$ ranging from polynomial order to order $n^{1/2}$.\n  We investigate two asymptotic regimes in a high-dimensional framework: a moderate-sparsity regime with fixed $s\\in(0,1]$, and a high-sparsity regime where $s\\to0$. In both regimes, we establish the convergence of the empirical spectral distribution of $\\mathbf S_n$ to a deterministic limit, and further derive central limit theorems for linear spectral statistics using resolvent techniques and martingale difference arguments. Our analysis reveals a phase transition in the fluctuation behavior across the two regimes. In the high-sparsity regime, the asymptotic fluctuations are governed by fourth-moment effects, with sparsity-scaled contributions being suppressed. Moreover, a mismatch between the scaling of the mean and variance, of different orders in $q$, necessitates an explicit correction in the centering of the linear spectral statistic. The theory applies to both Gaussian and non-Gaussian entries, and its statistical utility is illustrated through applications to hypothesis testing and outage probability analysis in large-scale MIMO systems."}
{"id": "2602.04200", "categories": ["cond-mat.stat-mech", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.04200", "abs": "https://arxiv.org/abs/2602.04200", "authors": ["Kevin Callahan-Coray", "Kyle Lee", "Kyle Jiang", "Kerem Y. Camsari"], "title": "Restoring Sparsity in Potts Machines via Mean-Field Constraints", "comment": null, "summary": "Ising machines and related probabilistic hardware have emerged as promising platforms for NP-hard optimization and sampling. However, many practical problems involve constraints that induce dense or all-to-all couplings, undermining scalability and hardware efficiency. We address this constraint-induced density through two complementary approaches. First, we introduce a hardware-aware native formulation for multi-state probabilistic digits (p-dits) that avoids the locally dense intra-variable couplings required by binary Ising encodings. We validate p-dit dynamics by reproducing known critical behavior of the 2D Potts model. Second, we propose mean-field constraints (MFC), a hybrid scheme that replaces dense pairwise constraint couplings with dynamically updated single-node biases. Applied to balanced graph partitioning, MFC achieves solution quality comparable to exact all-to-all constraint formulations while dramatically reducing graph density. Finally, we demonstrate the practical impact of restored sparsity by an FPGA implementation, enabling orders-of-magnitude acceleration over CPU-based solvers. Together, these results outline a pathway for scaling constrained optimization on probabilistic hardware."}
{"id": "2602.03988", "categories": ["physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.03988", "abs": "https://arxiv.org/abs/2602.03988", "authors": ["Jonathan Panuelos", "Eitan Grinspun", "David Levin"], "title": "Topology- and Geometry-Exact Coupling for Incompressible Fluids and Thin Deformables", "comment": "12 pages, 14 figures, 3 tables, 2 algorithms", "summary": "We introduce a topology-preserving discretization for coupling incompressible fluids with thin deformable structures, achieving guaranteed leakproofness through preservation of fluid domain connectivity. Our approach leverages a stitching algorithm applied to a clipped Voronoi diagram generated from Lagrangian fluid particles, in order to maintain path connectivity around obstacles. This geometric discretization naturally conforms to arbitrarily thin structures, enabling boundary conditions to be enforced exactly at fluid-solid interfaces. By discretizing the pressure projection equations on this conforming mesh, we can enforce velocity boundary conditions at the interface for the fluid while applying pressure forces directly on the solid boundary, enabling sharp two-way coupling between phases. The resulting method prevents fluid leakage through solids while permitting flow wherever a continuous path exists through the fluid domain. We demonstrate the effectiveness of our approach on diverse scenarios including flows around thin membranes, complex geometries with narrow passages, and deformable structures immersed in liquid, showcasing robust two-way coupling without artificial sealing or leakage artifacts."}
{"id": "2602.04011", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04011", "abs": "https://arxiv.org/abs/2602.04011", "authors": ["Benjamin N. Miller", "Peter K. Elgee", "Jason R. Pruitt", "Kevin C. Cox"], "title": "Approximate simulation of complex quantum circuits using sparse tensors", "comment": "10 pages, 6 figures, Submitted to APL Quantum", "summary": "The study of quantum circuit simulation using classical computers is a key research topic that helps define the boundary of verifiable quantum advantage, solve quantum many-body problems, and inform development of quantum hardware and software. Tensor networks have become forefront mathematical tools for these tasks. Here we introduce a method to approximately simulate quantum circuits using sparsely-populated tensors. We describe a sparse tensor data structure that can represent quantum states with no underlying symmetry, and outline algorithms to efficiently contract and truncate these tensors. We show that the data structure and contraction algorithm are efficient, leading to expected runtime scalings versus qubit number and circuit depth. Our results motivate future research in optimization of sparse tensor networks for quantum simulation."}
{"id": "2602.04537", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.04537", "abs": "https://arxiv.org/abs/2602.04537", "authors": ["Mihaela Chiappetta", "Massimo Carraturo", "Alexander Raßloff", "Markus Kästner", "Ferdinando Auricchio"], "title": "An Efficient Bayesian Framework for Inverse Problems via Optimization and Inversion: Surrogate Modeling, Parameter Inference, and Uncertainty Quantification", "comment": "Under review", "summary": "The present paper proposes a Bayesian framework for inverse problems that seamlessly integrates optimization and inversion to enable rapid surrogate modeling, accurate parameter inference, and rigorous uncertainty quantification. Bayesian optimization is employed to adaptively construct accurate Gaussian process surrogate models using a minimal number of high-fidelity model evaluations, strategically focusing sampling in regions of high predictive uncertainty. The trained surrogate model is then leveraged within a Bayesian inversion scheme to infer optimal parameter values by combining prior knowledge with observed quantities of interest, resulting in posterior distributions that rigorously characterize epistemic uncertainty. The framework is theoretically grounded, computationally efficient, and particularly suited for engineering applications in which high-fidelity models -- whether arising from numerical simulations or physical experiments -- are computationally expensive, analytically intractable, or difficult to replicate, and data availability is limited. Furthermore, the combined use of Bayesian optimization and inversion outperforms their separate application, highlighting the synergistic benefits of unifying the two approaches. The performance of the proposed Bayesian framework is demonstrated on a suite of one- and two-dimensional analytical benchmarks, including the Mixed Gaussian-Periodic, Lévy, Griewank, Forrester, and Rosenbrock functions, which provide a controlled setting to assess surrogate modeling accuracy, parameter inference robustness, and uncertainty quantification. The results demonstrate the framework's effectiveness in efficiently solving inverse problems while providing informative uncertainty quantification and supporting reliable engineering decision-making at reduced computational cost."}
{"id": "2602.03968", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.03968", "abs": "https://arxiv.org/abs/2602.03968", "authors": ["Hossein Rastgoftar"], "title": "Safety-Critical Reinforcement Learning with Viability-Based Action Shielding for Hypersonic Longitudinal Flight", "comment": null, "summary": "This paper presents a safety-critical reinforcement learning framework for nonlinear dynamical systems with continuous state and input spaces operating under explicit physical constraints. Hard safety constraints are enforced independently of the reward through action shielding and reachability-based admissible action sets, ensuring that unsafe behaviors are never intentionally selected during learning or execution. To capture nominal operation and recovery behavior within a single control architecture, the state space is partitioned into safe and unsafe regions based on membership in a safety box, and a mode-dependent reward is used to promote accurate tracking inside the safe region and recovery toward it when operating outside. To enable online tabular learning on continuous dynamics, a finite-state abstraction is constructed via state aggregation, and action selection and value updates are consistently restricted to admissible actions. The framework is demonstrated on a longitudinal point-mass hypersonic vehicle model with aerodynamic and propulsion couplings, using angle of attack and throttle as control inputs."}
{"id": "2602.04481", "categories": ["physics.soc-ph", "cs.GT", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2602.04481", "abs": "https://arxiv.org/abs/2602.04481", "authors": ["Mehran Noori", "Nahid Azimi-Tafreshi", "Mohammad Salahshour"], "title": "The impact of heterogeneity on the co-evolution of cooperation and epidemic spreading in complex networks", "comment": "11 pages, 8 figures", "summary": "The dynamics of herd immunity depend crucially on the interaction between collective social behavior and disease transmission, but the role of heterogeneity in this context frequently remains unclear. Here, we dissect this co-evolutionary feedback by coupling a public goods game with an epidemic model on complex networks, including multiplex and real-world networks. Our results reveals a dichotomy in how heterogeneity shapes outcomes. We demonstrate that structural heterogeneity in social networks acts as a powerful catalyst for cooperation and disease suppression. This emergent effect is driven by highly connected hubs who, facing amplified personal risk, adopt protective strategies out of self-interest. In contrast, heterogeneity in individual infection costs proves detrimental, undermining cooperation and amplifying the epidemic. This creates a ``weakest link'' problem, where individuals with low perceived risk act as persistent free-riders and disease reservoirs, degrading the collective response. Our findings establish that heterogeneity is a double-edged sword: its impact is determined by whether it creates an asymmetry of influence (leverage points) or an asymmetry of motivation (weakest links), recommending disease intervention policies that facilitate cooperative transition in hubs (strengthening the leverage point) and homogenize incentives to weakest links."}
{"id": "2602.04164", "categories": ["cs.ET", "stat.AP", "stat.CO", "stat.OT"], "pdf": "https://arxiv.org/pdf/2602.04164", "abs": "https://arxiv.org/abs/2602.04164", "authors": ["Yuan Cai", "Mustafa Demir", "Farzan Sasangohar", "Mohsen Zare"], "title": "The Dynamics of Attention across Automated and Manual Driving Modes: A Driving Simulation Study", "comment": null, "summary": "This study aims to explore the dynamics of driver attention to various zones, including the road, the central mirror, the embedded Human-Machine Interface (HMI), and the speedometer, across different driving modes in AVs. The integration of autonomous vehicles (AVs) into transportation systems has introduced critical safety concerns, particularly regarding driver re-engagement during mode transitions. Past accidents underscore the risks of overreliance on automation and highlight the need to understand dynamic attention allocation to support safety in autonomous driving. A high-fidelity driving simulation was conducted. Eye-tracking technology was used to measure fixation duration, fixation count, and time to first fixation across distinct driving modes (automated, manual, and transition), which were then used to assess how drivers allocated attention to various areas of interest (AOIs). Findings show that drivers' attention varies significantly across driving modes. In manual mode, attention consistently focuses on the road, while in automated mode, prolonged fixation on the embedded HMI was observed. During the handover and takeover phases, attention shifts dynamically between environmental and technological elements. The study reveals that driver attention allocation is mode-dependent. These findings inform the design of adaptive HMIs in AVs that align with drivers' attention patterns. By presenting relevant information according to the driving context, such systems can enhance driver-vehicle interaction, support effective transitions, and improve overall safety. Systematic analysis of visual attention dynamics across driving modes is gaining prominence, as it informs adaptive HMI designs and driver readiness interventions. The GLMM findings can be directly applied to the design of adaptive HMIs or driver training programs to enhance attention and improve safety."}
{"id": "2602.04684", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.04684", "abs": "https://arxiv.org/abs/2602.04684", "authors": ["Chenxi Wang", "Charles Emmett Maher", "Katherine A. Newhall"], "title": "Effect of Local Topological Changes on Resistance in Tunably-Disordered Networks", "comment": "12 pages 10 figures", "summary": "Disordered materials occur naturally and also provide a broader design space than ordered or crystalline structures. We investigate a two-dimensional disordered network metamaterial constructed from a Delaunay triangulation of an underlying point cloud. Small perturbations in the point cloud induce discrete topological changes. One such change we identify is a Delaunay flip, in which two neighboring Delaunay triangles that form a convex quadrilateral structure with their common edge being one of the two quadrilateral diagonals exchange this diagonal for the other diagonal. These topological changes can cause substantial jumps in the effective resistance measured diagonally across the network, when the change is located near the source or the sink node. The jumps are explained analytically by showing that the change in effective resistance from edge removal or addition depends on the voltage drop across that edge. However, Delaunay flips have less impact on global resistance measurements and in larger networks. These local topological changes are relevant for finite-sized samples and experimentally-measurable properties such as electrical transport. Global characterizations of the network disorder or topology lack the location-specificity of our observed effects on network transport, and thus may be inadequate for predicting certain experimentally measurable transport properties in disordered network metamaterials, highlighting the importance of localized regions in material design."}
{"id": "2602.04048", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04048", "abs": "https://arxiv.org/abs/2602.04048", "authors": ["Juan Pablo Vielma", "Ross Anderson", "Joey Huchette"], "title": "Convex duality contracts for production-grade mathematical optimization", "comment": null, "summary": "Deploying mathematical optimization in autonomous production systems requires precise contracts for objects returned by an optimization solver. Unfortunately, conventions on dual solution and infeasibility certificates (rays) vary widely across solvers and classes of problems. This paper presents the theoretical framework used by MathOpt (a domain-specific language developed and used at Google) to unify these notions. We propose an abstract primal-dual pair based on a simplified Fenchel duality scheme that allows for the mechanical derivation of dual problems and associated contracts for all classes of problems currently supported by MathOpt (including those with linear and quadratic objectives plus linear, conic, quadratic, and two-sided linear constraints). We also show how these contracts can improve clarity of complementary-slackness based optimality conditions for certain classes of problems."}
{"id": "2602.03968", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.03968", "abs": "https://arxiv.org/abs/2602.03968", "authors": ["Hossein Rastgoftar"], "title": "Safety-Critical Reinforcement Learning with Viability-Based Action Shielding for Hypersonic Longitudinal Flight", "comment": null, "summary": "This paper presents a safety-critical reinforcement learning framework for nonlinear dynamical systems with continuous state and input spaces operating under explicit physical constraints. Hard safety constraints are enforced independently of the reward through action shielding and reachability-based admissible action sets, ensuring that unsafe behaviors are never intentionally selected during learning or execution. To capture nominal operation and recovery behavior within a single control architecture, the state space is partitioned into safe and unsafe regions based on membership in a safety box, and a mode-dependent reward is used to promote accurate tracking inside the safe region and recovery toward it when operating outside. To enable online tabular learning on continuous dynamics, a finite-state abstraction is constructed via state aggregation, and action selection and value updates are consistently restricted to admissible actions. The framework is demonstrated on a longitudinal point-mass hypersonic vehicle model with aerodynamic and propulsion couplings, using angle of attack and throttle as control inputs."}
{"id": "2602.03874", "categories": ["q-fin.RM", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2602.03874", "abs": "https://arxiv.org/abs/2602.03874", "authors": ["Murad Farzulla", "Andrew Maksakov"], "title": "ASRI: An Aggregated Systemic Risk Index for Cryptocurrency Markets", "comment": "79 pages, 6 figures, 9 tables. Live dashboard: https://asri.dissensus.ai. Code: https://github.com/studiofarzulla/asri", "summary": "Cryptocurrency markets have grown to represent over $3 trillion in capitalization, yet no unified index exists to monitor the systemic risks arising from the interconnection between decentralized finance (DeFi) protocols and traditional financial institutions. This paper introduces the Aggregated Systemic Risk Index (ASRI), a composite measure comprising four weighted sub-indices: Stablecoin Concentration Risk (30%), DeFi Liquidity Risk (25%), Contagion Risk (25%), and Regulatory Opacity Risk (20%). We derive theoretical foundations for each component, specify quantitative formulas incorporating data from DeFi Llama, Federal Reserve FRED, and on-chain analytics, and validate the framework against historical crisis events including the Terra/Luna collapse (May 2022), the Celsius/3AC contagion (June 2022), the FTX bankruptcy (November 2022), and the SVB banking crisis (March 2023). Event study analysis detects statistically significant abnormal signals for all four crises (t-statistics 5.47-32.64, all p < 0.01), though threshold-based operational detection identifies three of four events with an average lead time of 18 days. A three-regime Hidden Markov Model identifies distinct Low Risk, Moderate, and Elevated states with regime persistence exceeding 94%. Out-of-sample specificity testing on 2024-2025 data confirms zero false positives. The ASRI framework addresses a critical gap in existing risk monitoring by capturing DeFi-specific vulnerabilities -- composability risk, flash loan exposure, and tokenized real-world asset linkages -- that traditional systemic risk measures cannot accommodate."}
{"id": "2602.04829", "categories": ["physics.soc-ph", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.04829", "abs": "https://arxiv.org/abs/2602.04829", "authors": ["Jaime L. C. da C. Filho", "Nuno Crokidakis"], "title": "Opinion dynamics under electoral shocks in competitive campaigns", "comment": "19 pages, 7 figures, to appear in IJMPC", "summary": "We propose a computational framework for modeling opinion dynamics in electoral competitions that combines two realistic features: voter memory and exogenous shocks. The population is represented by a fully-connected network of agents, each holding a binary opinion that reflects support for one of two candidates. First, inspired by the classical voter model, we introduce a memory-dependent opinion update: each agent's probability of adopting a neighbor's stance depends on how many times they agreed with that neighbor in the agent's past $m$ states, promoting inertia and resistance to change. Second, we define an electoral shock as an abrupt external influence acting uniformly over all agents during a finite interval $[t_0, t_0+Δt]$, favoring one candidate by switching opinions with probability $p_s$, representing the impact of extraordinary events such as political scandals, impactful speeches, or sudden news. We explore how the strength and duration of the shock, in conjunction with memory length, influence the transient and stationary properties of the model, as well as the candidates' advantage. Our findings reveal a rich dynamical behavior: memory slows down convergence and enhances system resilience, whereas shocks of sufficient intensity and duration can abruptly realign collective preferences, particularly when occurring close to the election date. Conversely, for long memory lengths or large election horizons, shock effects are dampened or delayed, depending on their timing. These results offer insights into why some sudden political events reshape electoral outcomes while others fade under strong individual inertia. Finally, a qualitative comparison with real electoral shocks reported in opinion polls illustrates how the model captures the competition between voter inertia and abrupt external events observed in actual elections."}
{"id": "2602.04793", "categories": ["nlin.CD", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04793", "abs": "https://arxiv.org/abs/2602.04793", "authors": ["F. J. Arranz", "F. Borondo"], "title": "Correspondence between classical and quantum resonances", "comment": null, "summary": "Bifurcations take place in molecular Hamiltonian nonlinear systems as the excitation energy increases, this leading to the appearance of different classical resonances. In this paper, we study the quantum manifestations of these classical resonances in the isomerizing system CN-Li$\\leftrightarrows$Li-CN. By using a correlation diagram of eigenenergies versus Planck constant, we show the existence of different series of avoided crossings, leading to the corresponding series of quantum resonances, which represent the quantum manifestations of the classical resonances. Moreover, the extrapolation of these series to $\\hbar=0$ unveils the correspondence between the bifurcation energy of classical resonances and the energy of the series of quantum resonances in the semiclassical limit $\\hbar\\to0$. Additionally, in order to obtain analytical expressions for our results, a semiclassical theory is developed."}
{"id": "2602.04472", "categories": ["math.ST", "cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04472", "abs": "https://arxiv.org/abs/2602.04472", "authors": ["Yanjin Xiang", "Zhihua Zhang"], "title": "Universality of General Spiked Tensor Models", "comment": "102pages", "summary": "We study the rank-one spiked tensor model in the high-dimensional regime, where the noise entries are independent and identically distributed with zero mean, unit variance, and finite fourth moment.This setting extends the classical Gaussian framework to a substantially broader class of noise distributions.Focusing on asymmetric tensors of order $d$ ($\\ge 3$), we analyze the maximum likelihood estimator of the best rank-one approximation.Under a mild assumption isolating informative critical points of the associated optimization landscape, we show that the empirical spectral distribution of a suitably defined block-wise tensor contraction converges almost surely to a deterministic limit that coincides with the Gaussian case.As a consequence, the asymptotic singular value and the alignments between the estimated and true spike directions admit explicit characterizations identical to those obtained under Gaussian noise. These results establish a universality principle for spiked tensor models, demonstrating that their high-dimensional spectral behavior and statistical limits are robust to non-Gaussian noise.\n  Our analysis relies on resolvent methods from random matrix theory, cumulant expansions valid under finite moment assumptions, and variance bounds based on Efron-Stein-type arguments. A key challenge in the proof is how to handle the statistical dependence between the signal term and the noise term."}
{"id": "2602.04308", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.04308", "abs": "https://arxiv.org/abs/2602.04308", "authors": ["Riccardo Piombo", "Dario Mazzilli", "Aurelio Patelli"], "title": "Statistical Mechanics of the Sub-Optimal Transport", "comment": "28 pages, 4 figures", "summary": "Statistical mechanics is a powerful framework for analyzing optimization yielding analytical results for matching, optimal transport, and other combinatorial problems. However, these methods typically target the zero-temperature limit, where systems collapse onto optimal configurations, a.k.a. the ground states. Real-world systems often occupy intermediate regimes where entropy and cost minimization genuinely compete, producing configurations that are structured yet sub-optimal. The Sub-Optimal Transport (SOT) model captures this competition through an ensemble of weighted bipartite graphs: a coupling parameter interpolates between entropy-dominated dense configurations and cost-dominated sparse structures. This crossover has been observed numerically but lacked analytical understanding. Here we develop a mean-field theory that characterizes this transition. We show that local fluctuations in Lagrange multipliers become sub-extensive in the thermodynamic limit, reducing the full model with strength constraints to an effective single-constraint problem admitting an exact solution in some intermediate regime. The resulting free energy is analytic in the coupling parameter, confirming a smooth crossover rather than a phase transition. We derive closed-form expressions for thermodynamic observables and weight distributions, validated against numerical simulations. These results establish the first analytical description of the SOT model, extending statistical mechanics methods beyond the zero-temperature regime."}
{"id": "2602.04014", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.04014", "abs": "https://arxiv.org/abs/2602.04014", "authors": ["Abir Hasan", "Nikhil Shukla"], "title": "At the Top of the Mountain, the World can Look Boltzmann-Like: Sampling Dynamics of Noisy Double-Well Systems", "comment": "9 pages, 2 figures", "summary": "The success of the transistor as the cornerstone of digital computation motivates analogous efforts to identify an equivalent hardware primitive, the probabilistic bit or p-bit, for the emerging paradigm of probabilistic computing. Here, we uncover a fundamental ubiquity in the stochastic dynamics of double well energy systems when initialized near the barrier top. Using a topological framework grounded in Morse theory and singularity theory, we make use of the result that all smooth, even double well potentials reduce near the saddle point to a canonical quartic normal form. Within this regime, the interplay of noise, synaptic bias, and potential curvature produces a topologically robust short time evolution characterized by a tanh like response. This enables Boltzmann like sampling that is largely independent of the detailed shape of the potential, apart from its effective temperature scaling. Analytical derivations and numerical simulations across multiple representative systems corroborate this behavior. Our work provides a unifying foundation for assessing and engineering a broad class of physical platforms, including oscillators, bistable latches, and magnetic devices, as p-bits operating within a synchronous framework for stochastic sampling and probabilistic computation."}
{"id": "2602.04061", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04061", "abs": "https://arxiv.org/abs/2602.04061", "authors": ["Sirui Chen", "Dragomir Davidović"], "title": "Correlation-Enabled Beatings in Two-Dimensional Electronic Spectroscopy", "comment": null, "summary": "Long-lived beatings in two-dimensional electronic spectroscopy (2DES) remain difficult to interpret within standard excitonic open-system models, which typically assume factorized initialization and predict rapid coherence decay. We show that persistent beatings can arise from a correlation-driven mechanism that requires both slow bath memory and ultrafast pulse sequences that propagate system-bath correlations across optical interactions. In this regime, the pulse sequence unitarily dresses the bath-memory contribution and activates nonsecular population-coherence transfer during field-free evolution, sustaining coherence signatures far beyond factorized or weak-memory descriptions. Rather than addressing what is oscillating (excitonic versus vibronic) or quantum-versus-classical semantics, this work reframes long-lived beatings as a protocol-level dynamical effect: correlation-mediated retrieval under ultrafast control."}
{"id": "2602.03987", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.03987", "abs": "https://arxiv.org/abs/2602.03987", "authors": ["Nikolaos Bousias", "George Pappas"], "title": "Towards X-embodiment safety: A control theory perspective on transferring safety certificates across dynamical systems", "comment": null, "summary": "Control barrier functions (CBFs) provide a powerful tool for enforcing safety constraints in control systems, but their direct application to complex, high-dimensional dynamics is often challenging. In many settings, safety certificates are more naturally designed for simplified or alternative system models that do not exactly match the dynamics of interest. This paper addresses the problem of transferring safety guarantees between dynamical systems with mismatched dynamics. We propose a transferred control barrier function (tCBF) framework that enables safety constraints defined on one system to be systematically enforced on another system using a simulation function and an explicit margin term. The resulting transferred barrier accounts for model mismatch and induces a safety condition that can be enforced on the target system via a quadratic-program-based safety filter. The proposed approach is general and does not require the two systems to share the same state dimension or dynamics. We demonstrate the effectiveness of the framework on a quadrotor navigation task with the transferred barrier ensuring collision avoidance for the target system, while remaining minimally invasive to a nominal controller. These results highlight the potential of transferred control barrier functions as a general mechanism for enforcing safety across heterogeneous dynamical systems."}
{"id": "2602.04507", "categories": ["physics.soc-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2602.04507", "abs": "https://arxiv.org/abs/2602.04507", "authors": ["Zhihao Qiu", "Sámuel G. Balogh", "Xinhan Liu", "Piet Van Mieghem", "Maksim Kitsak"], "title": "Geometric Organization and Inference of Shortest Path Nodes in Soft Random Geometric Graphs", "comment": null, "summary": "The shortest path problem is related to many dynamic processes on networks, ranging from routing in communication networks to signaling in molecular interaction networks. When the network is fully known, the shortest path problem can be solved precisely and in polynomial time. If, however, the network of interest is only partially observable, the shortest path problem is no longer straightforward. Inspired by the shortest path problem in partially observable networks, we investigate the geometric properties of shortest paths in {\\it Euclidean} Soft Random Geometric Graphs (SRGGs). We find that shortest paths are aligned along geodesic curves connecting shortest path endpoints. The strength of the shortest path alignment, as quantified by the average distance to geodesic from shortest path nodes and the average path stretch, is higher for larger SRGGs with short-range connections. In addition, we find that the strength of the shortest path alignment is non-monotonic with respect to the average degree of the SRGG. Based on these observations, we establish the conditions under which the alignment of shortest paths may be sufficiently strong to allow the identification of shortest path nodes based on their proximity to geodesic curves. We show that in partially observable networks with uncertain node positions, our geometric approach can outperform network-based shortest-path algorithms. In practical settings, our findings may have applications to navigation, wireless routing, and flow characterization in infrastructure networks."}
{"id": "2602.04411", "categories": ["cs.ET", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04411", "abs": "https://arxiv.org/abs/2602.04411", "authors": ["Tongtong Feng", "Xin Wang", "Wenwu Zhu"], "title": "Self-evolving Embodied AI", "comment": null, "summary": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."}
{"id": "2602.04774", "categories": ["cond-mat.dis-nn", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04774", "abs": "https://arxiv.org/abs/2602.04774", "authors": ["Blake Bordelon", "Francesco Mori"], "title": "Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model", "comment": null, "summary": "Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\\star(t) \\simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \\sim T^{-ξ}$ (2) optimal power laws $η_T(t) \\sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups."}
{"id": "2602.04122", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04122", "abs": "https://arxiv.org/abs/2602.04122", "authors": ["Zhou Wei", "He-Yi Liu", "Bo Zeng"], "title": "New Outer Approximation Algorithms for Nonsmooth Convex MINLP Problems", "comment": null, "summary": "This paper presents a novel outer approximation algorithm for nonsmooth mixed-integer nonlinear programming (MINLP) problems. The method proceeds by fixing the integer variables and solving the resulting nonlinear convex subproblem. When the subproblem is feasible, valid linear cuts are derived by computing suitable subgradients of the objective and constraint functions at the optimal solution, utilizing KKT optimality conditions. A new parameter, defined through the nonlinear constraint functions, is introduced to facilitate the generation of these cuts. For infeasible subproblems, a feasibility problem is solved, and valid linear cuts are generated via KKT-based subgradients to exclude the infeasible integer assignment.\n  By integrating both types of cuts, a mixed-integer linear programming (MILP) master problem is formulated and proven equivalent to the original MINLP. This equivalence underpins a new outer approximation algorithm, which is guaranteed to terminate after a finite number of iterations.\n  Numerical experiments on smooth convex MINLP problems demonstrate that the proposed algorithm produces tighter MILP relaxations than the classical outer approximation method. Furthermore, the approach offers an alternative mechanism for generating linear cuts, extending beyond reliance solely on first-order Taylor expansions and shows that the efficiency of outer approximation algorithm is strongly dependent on the inherent structure of the MINLP problem."}
{"id": "2602.03987", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.03987", "abs": "https://arxiv.org/abs/2602.03987", "authors": ["Nikolaos Bousias", "George Pappas"], "title": "Towards X-embodiment safety: A control theory perspective on transferring safety certificates across dynamical systems", "comment": null, "summary": "Control barrier functions (CBFs) provide a powerful tool for enforcing safety constraints in control systems, but their direct application to complex, high-dimensional dynamics is often challenging. In many settings, safety certificates are more naturally designed for simplified or alternative system models that do not exactly match the dynamics of interest. This paper addresses the problem of transferring safety guarantees between dynamical systems with mismatched dynamics. We propose a transferred control barrier function (tCBF) framework that enables safety constraints defined on one system to be systematically enforced on another system using a simulation function and an explicit margin term. The resulting transferred barrier accounts for model mismatch and induces a safety condition that can be enforced on the target system via a quadratic-program-based safety filter. The proposed approach is general and does not require the two systems to share the same state dimension or dynamics. We demonstrate the effectiveness of the framework on a quadrotor navigation task with the transferred barrier ensuring collision avoidance for the target system, while remaining minimally invasive to a nominal controller. These results highlight the potential of transferred control barrier functions as a general mechanism for enforcing safety across heterogeneous dynamical systems."}
{"id": "2602.03985", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03985", "abs": "https://arxiv.org/abs/2602.03985", "authors": ["Augustine Wigle", "Erica E. M. Moodie"], "title": "Doubly-Robust Bayesian Estimation of Optimal Individualized Treatment Rules using Network Meta-Analysis", "comment": null, "summary": "An optimal individualized treatment rule (ITR) is a function that takes a patient's characteristics, such as demographics, biomarkers, and treatment history, and outputs a treatment that is expected to give the best outcome for that patient. Major Depressive Disorder (MDD) is a common and disabling mental health condition for which an optimal ITR is of interest. Unfortunately, the power to detect treatment-covariate interactions in individual studies of MDD treatments is low. Additionally, all treatments of interest are not compared head-to-head in a single study. Network meta-analysis (NMA) is a method of synthesizing data from multiple studies to estimate the relative effects of a set of treatments. Recently, two-stage ITR NMA was proposed as a method to estimate ITRs that has the potential to improve power and simultaneously consider all relevant treatment options. In the first stage, study-specific ITRs are estimated, and in the second stage, they are pooled using a Bayesian NMA model. The existing approach is vulnerable to model misspecification and fails to address missing outcomes, which occur in the MDD data. We overcome these challenges by proposing Bayesian Bootstrap dynamic Weighted Ordinary Least Squares (BBdWOLS), a doubly-robust approach to ITR estimation that accounts for missing at random outcomes and naturally quantifies the uncertainty in estimation. We also propose an improvement to the NMA model that incorporates the full variance-covariance matrix of study-specific estimates. In a simulation study, we show that our fully Bayesian ITR NMA method is more robust and efficient than the existing approach. We apply our method to the motivating dataset consisting of three studies of pharmacological treatments for MDD, and explore how ITR NMA results can support personalized decision making in this context."}
{"id": "2602.04086", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04086", "abs": "https://arxiv.org/abs/2602.04086", "authors": ["Songchen Tan", "Oscar Smith", "Christopher Rackauckas"], "title": "Efficient Explicit Taylor ODE Integrators with Symbolic-Numeric Computing", "comment": null, "summary": "Taylor series methods show a newfound promise for the solution of non-stiff ordinary differential equations (ODEs) given the rise of new compiler-enhanced techniques for calculating high order derivatives. In this paper we detail a new Julia-based implementation that\n  has two important techniques: (1) a general purpose higher-order automatic differentiation engine for derivative evaluation with low overhead; (2) a combined symbolic-numeric approach to generate code for recursively computing the Taylor polynomial of the ODE solution. We demonstrate that the resulting software's compiler-based tooling is transparent to the user, requiring no changes from interfaces required to use standard explicit Runge-Kutta methods, while achieving better run time performance. In addition, we also developed a comprehensive adaptive time and order algorithm that uses different step size and polynomial degree across the integration period, which makes this implementation more efficient and versatile in a broad range of dynamics. We show that for codes compatible with compiler transformations, these integrators are more efficient and robust than the traditionally used explicit Runge-Kutta methods."}
{"id": "2602.03925", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.03925", "abs": "https://arxiv.org/abs/2602.03925", "authors": ["Zhi-Qiang Gao", "Yan-Qi Wang", "Ya-Hui Zhang", "Hui Yang"], "title": "Primary charge-4e superconductivity from doping a featureless Mott insulator", "comment": "4.5+7 pages, 4+4 figures", "summary": "Superconductivity is usually understood as a phase in which charge-$2e$ Cooper pairs are condensed. Charge-$4e$ superconductivity has largely been discussed as a vestigial order at finite temperature emerging from charge-$2e$ states. Primary charge-$4e$ superconducting phases at zero temperature remain scarce in both experiments and microscopic models. Here we argue that a doped featureless Mott insulator with $SU(4)$ symmetry provides a natural platform for primary charge-$4e$ superconductivity, based on perturbative renormalization group arguments and group theoretic considerations. As a concrete realization, we construct a bilayer Hubbard model with tunable onsite $SU(4)$ and $Sp(4)$ symmetries that exhibits a featureless Mott insulating phase at half filling. Its low energy physics is captured by a generalized ESD model, featuring an effective Hamiltonian that is purely kinetic within the constrained Hilbert space. Using density matrix renormalization group (DMRG) simulations, we find a primary charge-$4e$ superconducting phase in the $SU(4)$ ESD model and a conventional primary charge-$2e$ phase in the $Sp(4)$ case. We further characterize the corresponding normal states and discuss the resulting finite temperature phase diagram."}
{"id": "2602.04668", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.04668", "abs": "https://arxiv.org/abs/2602.04668", "authors": ["Oleksandr Mokliachuk"], "title": "Estimation of reliability and accuracy of models of $\\varphi$-sub-Gaussian process using generating functions of polynomial expansions", "comment": null, "summary": "Stochastic processes are often represented through orthonormal series expansions, a framework originating in the classical works of Loève and Karhunen and widely used for simulation and numerical approximation. While truncation error in such expansions has been extensively studied, practical models frequently involve an additional source of error arising from the approximation of coefficient functions when closed-form expressions are unavailable. The combined effect of these two errors remains insufficiently addressed in the literature. Building on the author's earlier work on reliability and accuracy estimates for $\\varphi$-sub-Gaussian processes, this paper extends the methodology to orthonormal polynomial systems that do not possess normalized generating functions in analytical form, including the Legendre, generalized Laguerre, and Gegenbauer families. New bounds are derived for models in $L_p(T)$ and $C([0,T])$ that simultaneously account for truncation and coefficient approximation. The resulting criteria provide practical guidance for selecting the number of series terms required to achieve prescribed levels of reliability and accuracy across a broader class of polynomial-based stochastic process models."}
{"id": "2602.04313", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04313", "abs": "https://arxiv.org/abs/2602.04313", "authors": ["Georgii Kalagov", "Nikita Lebedev"], "title": "Critical behavior of isotropic systems with strong dipole-dipole interaction from the functional renormalization group", "comment": null, "summary": "We compute the critical exponents of three-dimensional magnets with strong dipole-dipole interactions using the functional renormalization group (FRG) within the local potential approximation including the wave function renormalization (LPA$^\\prime$). The system is governed by the Aharony fixed point, which is scale-invariant but lacks conformal invariance. Our nonperturbative FRG analysis identifies this fixed point and determines its scaling behavior. The resulting critical exponents are found to be close to those of the Heisenberg $O(3)$ universality class, as computed within the same FRG/LPA$^\\prime$ framework. This proximity confirms the distinct yet numerically similar nature of the two universality classes."}
{"id": "2602.03857", "categories": ["physics.geo-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.03857", "abs": "https://arxiv.org/abs/2602.03857", "authors": ["Nimatullah", "Pankaj K Mishra", "Jochen Kamm", "Anand Singh"], "title": "Backend-agnostic Julia framework for 3D modeling and inversion of gravity data", "comment": "Source-code for this paper is at https://zenodo.org/records/18339626", "summary": "This paper presents a high-performance framework for three-dimensional gravity modeling and inversion implemented in Julia, addressing key challenges in geophysical modeling such as computational complexity, ill-posedness, and the non-uniqueness inherent to gravity inversion. The framework adopts a data-space inversion formulation to reduce the dimensionality of the problem, leading to significantly lower memory requirements and improved computational efficiency while maintaining inversion accuracy. Forward modeling and inversion operators are implemented within a backend-agnostic kernel abstraction, enabling execution on both multicore CPUs and GPU accelerators from a single code base. Performance analyses conducted on NVIDIA CUDA GPUs demonstrate substantial reductions in runtime relative to CPU execution, particularly for large-scale datasets involving up to approximately 3.3 million rectangular prisms, highlighting the scalability of the proposed approach. The inversion incorporates implicit model constraints through the data-space formulation and depth-weighted sensitivity, which mitigate depth-related amplitude decay and yield geologically coherent, high-resolution subsurface density models. Validation using synthetic models confirms the ability of the framework to accurately reconstruct complex subsurface structures such as vertical and dipping dykes. Application to field gravity data further demonstrates the robustness and practical utility of the GPU-accelerated framework, with the recovered models showing strong consistency with independent geological constraints and prior interpretations. Overall, this work underscores the potential of GPU-enabled computing in Julia to transform large-scale gravity inversion workflows, providing an efficient, extensible, and accurate computational solution for high-resolution geophysical studies."}
{"id": "2602.04072", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04072", "abs": "https://arxiv.org/abs/2602.04072", "authors": ["Junhao Song", "Ziqian Bi", "Xinliang Chia", "William Knottenbelt", "Yudong Cao"], "title": "Data Verification is the Future of Quantum Computing Copilots", "comment": "13 Pages, 20 Figures. Accepted to AAAI 2026 Workshop on AI4Research. Comments welcome", "summary": "Quantum program generation demands a level of precision that may not be compatible with the statistical reasoning carried out in the inference of large language models (LLMs). Hallucinations are mathematically inevitable and not addressable by scaling, which leads to infeasible solutions. We argue that architectures prioritizing verification are necessary for quantum copilots and AI automation in domains governed by constraints. Our position rests on three key points: verified training data enables models to internalize precise constraints as learned structures rather than statistical approximations; verification must constrain generation rather than filter outputs, as valid designs occupy exponentially shrinking subspaces; and domains where physical laws impose correctness criteria require verification embedded as architectural primitives. Early experiments showed LLMs without data verification could only achieve a maximum accuracy of 79% in circuit optimization. Our positions are formulated as quantum computing and AI4Research community imperatives, calling for elevating verification from afterthought to architectural foundation in AI4Research."}
{"id": "2602.04056", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04056", "abs": "https://arxiv.org/abs/2602.04056", "authors": ["Joonkyung Kim", "Wenxi Chen", "Davood Soleymanzadeh", "Yi Ding", "Xiangbo Gao", "Zhengzhong Tu", "Ruqi Zhang", "Fan Fei", "Sushant Veer", "Yiwei Lyu", "Minghui Zheng", "Yan Gu"], "title": "Modular Safety Guardrails Are Necessary for Foundation-Model-Enabled Robots in the Real World", "comment": null, "summary": "The integration of foundation models (FMs) into robotics has accelerated real-world deployment, while introducing new safety challenges arising from open-ended semantic reasoning and embodied physical action. These challenges require safety notions beyond physical constraint satisfaction. In this paper, we characterize FM-enabled robot safety along three dimensions: action safety (physical feasibility and constraint compliance), decision safety (semantic and contextual appropriateness), and human-centered safety (conformance to human intent, norms, and expectations). We argue that existing approaches, including static verification, monolithic controllers, and end-to-end learned policies, are insufficient in settings where tasks, environments, and human expectations are open-ended, long-tailed, and subject to adaptation over time. To address this gap, we propose modular safety guardrails, consisting of monitoring (evaluation) and intervention layers, as an architectural foundation for comprehensive safety across the autonomy stack. Beyond modularity, we highlight possible cross-layer co-design opportunities through representation alignment and conservatism allocation to enable faster, less conservative, and more effective safety enforcement. We call on the community to explore richer guardrail modules and principled co-design strategies to advance safe real-world physical AI deployment."}
{"id": "2602.04829", "categories": ["physics.soc-ph", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.04829", "abs": "https://arxiv.org/abs/2602.04829", "authors": ["Jaime L. C. da C. Filho", "Nuno Crokidakis"], "title": "Opinion dynamics under electoral shocks in competitive campaigns", "comment": "19 pages, 7 figures, to appear in IJMPC", "summary": "We propose a computational framework for modeling opinion dynamics in electoral competitions that combines two realistic features: voter memory and exogenous shocks. The population is represented by a fully-connected network of agents, each holding a binary opinion that reflects support for one of two candidates. First, inspired by the classical voter model, we introduce a memory-dependent opinion update: each agent's probability of adopting a neighbor's stance depends on how many times they agreed with that neighbor in the agent's past $m$ states, promoting inertia and resistance to change. Second, we define an electoral shock as an abrupt external influence acting uniformly over all agents during a finite interval $[t_0, t_0+Δt]$, favoring one candidate by switching opinions with probability $p_s$, representing the impact of extraordinary events such as political scandals, impactful speeches, or sudden news. We explore how the strength and duration of the shock, in conjunction with memory length, influence the transient and stationary properties of the model, as well as the candidates' advantage. Our findings reveal a rich dynamical behavior: memory slows down convergence and enhances system resilience, whereas shocks of sufficient intensity and duration can abruptly realign collective preferences, particularly when occurring close to the election date. Conversely, for long memory lengths or large election horizons, shock effects are dampened or delayed, depending on their timing. These results offer insights into why some sudden political events reshape electoral outcomes while others fade under strong individual inertia. Finally, a qualitative comparison with real electoral shocks reported in opinion polls illustrates how the model captures the competition between voter inertia and abrupt external events observed in actual elections."}
{"id": "2602.04495", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.04495", "abs": "https://arxiv.org/abs/2602.04495", "authors": ["Maher Harb", "Nader Foroughi", "Matt Stehman", "Bob Lutz", "Nati Erez", "Erik Garcell"], "title": "Quantum-Based Resilient Routing in Networks: Minimizing Latency Under Dual-Link Failures", "comment": "15 pages, 4 figures", "summary": "Network optimization problems represent large combinatorial search spaces that grow exponentially with network size, making them computationally intensive to solve. This paper addresses the latency-resilient Layer 3 routing optimization problem in telecommunications networks with predefined Layer 1 optical links. We formulate this problem as a graph-based optimization problem with the objective of minimizing latency, creating vertex-disjoint paths from each site to the internet backbone, and maximizing overall resiliency by limiting the impact of dual-link failures. By framing the problem as finding two disjoint shortest paths, coupled together with a resiliency component to the objective function, we establish a single formulation to produce optimal path design. The mathematical formulation was adapted to solve the problem using quantum approximate optimization algorithm (QAOA) executed over both quantum simulator and quantum hardware. QAOA was tested on a toy graph topology with 5 vertices and 7 edges and considering two limiting scenarios respectively representing independent (uncorrelated) link failures and highly correlated failure for one pair of edges. Both explored scenarios produced the optimal network design-corresponding to the valid solution with highest frequency of occurrence and minimum energy state, hence, validating the proposed formulation for optimizing Layer 3 routing on quantum systems of the future."}
{"id": "2602.04308", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.04308", "abs": "https://arxiv.org/abs/2602.04308", "authors": ["Riccardo Piombo", "Dario Mazzilli", "Aurelio Patelli"], "title": "Statistical Mechanics of the Sub-Optimal Transport", "comment": "28 pages, 4 figures", "summary": "Statistical mechanics is a powerful framework for analyzing optimization yielding analytical results for matching, optimal transport, and other combinatorial problems. However, these methods typically target the zero-temperature limit, where systems collapse onto optimal configurations, a.k.a. the ground states. Real-world systems often occupy intermediate regimes where entropy and cost minimization genuinely compete, producing configurations that are structured yet sub-optimal. The Sub-Optimal Transport (SOT) model captures this competition through an ensemble of weighted bipartite graphs: a coupling parameter interpolates between entropy-dominated dense configurations and cost-dominated sparse structures. This crossover has been observed numerically but lacked analytical understanding. Here we develop a mean-field theory that characterizes this transition. We show that local fluctuations in Lagrange multipliers become sub-extensive in the thermodynamic limit, reducing the full model with strength constraints to an effective single-constraint problem admitting an exact solution in some intermediate regime. The resulting free energy is analytic in the coupling parameter, confirming a smooth crossover rather than a phase transition. We derive closed-form expressions for thermodynamic observables and weight distributions, validated against numerical simulations. These results establish the first analytical description of the SOT model, extending statistical mechanics methods beyond the zero-temperature regime."}
{"id": "2602.04123", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04123", "abs": "https://arxiv.org/abs/2602.04123", "authors": ["Junhao Wu", "Shaoze Li", "Cheng Lu", "Zhibin Deng", "Shu-Cherng Fang"], "title": "Variable Aggregation-based Perspective Reformulation for Mixed-Integer Convex Optimization with Symmetry", "comment": null, "summary": "This paper addresses the challenging issue of symmetry in mixed-integer convex optimization problems, which frequently arise in real-world applications such as the unit commitment problem. Although variable aggregation techniques have been employed to mitigate symmetry, their impact on tightening the corresponding continuous relaxation has not been thoroughly investigated. In this work, we propose a new formulation that integrates the perspective reformulation method into the variable aggregation framework, yielding a tighter continuous relaxation for mixed-integer convex optimization problems with symmetric structures. We prove that, in the presence of symmetry, the convex hull of the feasible region associated with each set of aggregated variables can be exactly characterized. These results demonstrate the effectiveness of the proposed reformulation and establish new theoretical foundations for achieving tightness in variable aggregation-based mixed-integer programming formulations."}
{"id": "2602.04056", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04056", "abs": "https://arxiv.org/abs/2602.04056", "authors": ["Joonkyung Kim", "Wenxi Chen", "Davood Soleymanzadeh", "Yi Ding", "Xiangbo Gao", "Zhengzhong Tu", "Ruqi Zhang", "Fan Fei", "Sushant Veer", "Yiwei Lyu", "Minghui Zheng", "Yan Gu"], "title": "Modular Safety Guardrails Are Necessary for Foundation-Model-Enabled Robots in the Real World", "comment": null, "summary": "The integration of foundation models (FMs) into robotics has accelerated real-world deployment, while introducing new safety challenges arising from open-ended semantic reasoning and embodied physical action. These challenges require safety notions beyond physical constraint satisfaction. In this paper, we characterize FM-enabled robot safety along three dimensions: action safety (physical feasibility and constraint compliance), decision safety (semantic and contextual appropriateness), and human-centered safety (conformance to human intent, norms, and expectations). We argue that existing approaches, including static verification, monolithic controllers, and end-to-end learned policies, are insufficient in settings where tasks, environments, and human expectations are open-ended, long-tailed, and subject to adaptation over time. To address this gap, we propose modular safety guardrails, consisting of monitoring (evaluation) and intervention layers, as an architectural foundation for comprehensive safety across the autonomy stack. Beyond modularity, we highlight possible cross-layer co-design opportunities through representation alignment and conservatism allocation to enable faster, less conservative, and more effective safety enforcement. We call on the community to explore richer guardrail modules and principled co-design strategies to advance safe real-world physical AI deployment."}
{"id": "2602.04010", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04010", "abs": "https://arxiv.org/abs/2602.04010", "authors": ["Arijit Pyne"], "title": "Robust Nonparametric Two-Sample Tests via Mutual Information using Extended Bregman Divergence", "comment": null, "summary": "We introduce a generalized formulation of mutual information (MI) based on the extended Bregman divergence, a framework that subsumes the generalized S-Bregman (GSB) divergence family. The GSB divergence unifies two important classes of statistical distances, namely the S-divergence and the Bregman exponential divergence (BED), thereby encompassing several widely used subfamilies, including the power divergence (PD), density power divergence (DPD), and S-Hellinger distance (S-HD). In parametric inference, minimum divergence estimators are well known to balance robustness with high asymptotic efficiency relative to the maximum likelihood estimator. However, nonparametric tests based on such statistical distances have been relatively less explored. In this paper, we construct a class of consistent and robust nonparametric two-sample tests for the equality of two absolutely continuous distributions using the generalized MI. We establish the asymptotic normality of the proposed test statistics under the null and contiguous alternatives. The robustness properties of the generalized MI are rigorously studied through the influence function and the breakdown point, demonstrating that stability of the generalized MI translates into stability of the associated tests. Extensive simulation studies show that divergences beyond the PD family often yield superior robustness under contamination while retaining high asymptotic power. A data-driven scheme for selecting optimal tuning parameters is also proposed. Finally, the methodology is illustrated with applications to real data."}
{"id": "2602.04207", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04207", "abs": "https://arxiv.org/abs/2602.04207", "authors": ["Guanqiu Ma", "Hongxia Guo", "Guanghui Hu"], "title": "A frequency-domain method to inverse moving source problem with unknown radiating moment", "comment": null, "summary": "This paper introduces a multi-frequency factorization method for imaging a time-dependent source, specifically to recover its spatial support and the associated excitation instants. Using far-field data from two opposite directions, we establish a computational criterion that characterizes both the unknown pulse moments and the narrowest strip (perpendicular to the direction) enclosing the source support. Central to our inversion scheme is the construction of indicator functions, defined pointwise over the spatial and temporal sampling variables. The proposed inversion scheme permits the recovery of the $Θ$-convex support domain from far-field data at sparse observation directions. Uniqueness in determining the convex hull of the support and the excitation instants-using all observation directions-is also established as a direct consequence of the factorization method. The effectiveness and feasibility of the approach are examined through comprehensive numerical simulations in two and three dimensions."}
{"id": "2602.03936", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.03936", "abs": "https://arxiv.org/abs/2602.03936", "authors": ["Amanda A. Konieczna", "P. Peter Stavropoulos", "Roser Valentí"], "title": "Revealing the microscopic origin of the magnetization plateau in Na$_3$Ni$_2$BiO$_6$", "comment": "11 pages, 8 figures", "summary": "Recent experimental studies of the spin-1 honeycomb antiferromagnet Na$_3$Ni$_2$BiO$_6$ have revealed a pronounced one-third magnetization plateau under applied magnetic fields, highlighting the presence of strong magnetic frustration and anisotropy in this material. Such behavior has been attributed to substantial bond-dependent Kitaev interactions in combination with single-ion anisotropy, placing Na$_3$Ni$_2$BiO$_6$ among honeycomb compounds of interest for unconventional magnetic phases. Motivated by these observations, we present a first-principles-based analysis of the magnetic interactions in Na$_3$Ni$_2$BiO$_6$. By combining density-functional calculations with microscopic modeling, we extract the relevant exchange parameters and construct an effective spin model that quantitatively reproduces both the elastic neutron-scattering spectra and the magnetization curve. The model captures the experimentally observed zero-field zigzag magnetic order, and proposes a $\\textit{double-zigzag}$ state at intermediate magnetic fields, realizing the 1/3-magnetization plateau in a simpler way than suggested in previous works. Crucially, we show that the one-third magnetization plateau does not require Kitaev interactions; instead, it arises from the interplay of strong out-of-plane single-ion anisotropy and competing ferromagnetic nearest-neighbor ($J_1$) and antiferromagnetic third-neighbor ($J_3$) Heisenberg couplings. These results establish a consistent microscopic description of Na$_3$Ni$_2$BiO$_6$ and clarify the origin of its field-induced plateau phase."}
{"id": "2602.04708", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.04708", "abs": "https://arxiv.org/abs/2602.04708", "authors": ["Anton Tiepner", "Mathias Trabs", "Eric Ziebell"], "title": "Statistical inference for the stochastic wave equation based on discrete observations", "comment": "44 pages, 6 figures", "summary": "The wave speed of a stochastic wave equation driven by Riesz noise on the unbounded multidimensional spatial domain is estimated based on discrete measurements. Central limit theorems for second-order variations of the observations in space, time, and space-time are established. Under general assumptions on the spatial and temporal sampling frequencies, the resulting method-of-moments estimators are asymptotically normally distributed. The covariance structure of the discrete increments admits a closed-form representation involving two different Fejér-type kernels, enabling a precise analysis of the interplay between spatial and temporal contributions."}
{"id": "2602.04342", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04342", "abs": "https://arxiv.org/abs/2602.04342", "authors": ["Vicenç Méndez", "Rosa Flaquer-Galmés", "Javier Cristín"], "title": "Area under subdiffusive random walks", "comment": null, "summary": "We study the statistical properties of the area and the absolute area under the trajectories of subdiffusive random walks. Using different frameworks to describe subdiffusion (as the scaled Brownian motion, fractional Brownian motion, the continuous-time random walk or the Brownian motion in heterogeneous media), we compute the first two moments, the ergodicity breaking parameter for the absolute area and infer a general scaling for the probability density functions of these functionals. We discuss the differences between the statistical properties of the area and the absolute area for the different subdiffusion models and illustrate the experimental interest of our results. Our theoretical findings are supported by Monte Carlo simulations showing an excellent agreement."}
{"id": "2602.04173", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04173", "abs": "https://arxiv.org/abs/2602.04173", "authors": ["Ying-Qiu He", "Yu-Yan Zhang", "Dong Ding", "Ting Gao", "Feng-Li Yan"], "title": "Symmetric joint measurement as a complement to the elegant joint measurement", "comment": "8 pages, 4 figures", "summary": "Traditional Bell state measurement (BSM) and product basis measurements (PBM) have been integral to nearly the entire development of quantum computing. Unlike the BSM and the PBM, a recently proposed two-qubit joint measurement called the elegant joint measurement (EJM) exhibits novel tetrahedral symmetry in its single-qubit reduced states. In [Phys.Rev.Lett.126:220401], a parameterized two-qubit iso-entangled basis was proposed, with concurrence between 1/2 and 1, perfectly spanning the original EJM and conventional BSM. We present a two-qubit symmetric joint measurement having concurrence from 0 to 1/2, which is complementary to [Phys.Rev.Lett.126:220401] and contains the PBM and the original EJM. We investigate the symmetry of the current structure and its application in triangular networks. The results indicate that the reduction vectors of the current basis states exhibit rotational symmetry, rather than the aforementioned mirror symmetry; moreover, the output probability distributions of three parties in the network explicitly demonstrate the expected permutation symmetry. Furthermore, we generalize the two-qubit symmetric joint measurement to the multiqubit systems with an even number of qubits."}
{"id": "2602.04132", "categories": ["eess.SY", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04132", "abs": "https://arxiv.org/abs/2602.04132", "authors": ["Dhruv S. Kushwaha", "Zoleikha A. Biron"], "title": "Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking", "comment": "12 pages, 7 Figures, submitted to IEEE RA-L", "summary": "Reinforcement Learning (RL) has achieved remarkable success in solving complex sequential decision-making problems. However, its application to safety-critical physical systems remains constrained by the lack of stability guarantees. Standard RL algorithms prioritize reward maximization, often yielding policies that may induce oscillations or unbounded state divergence. There has significant work in incorporating Lyapunov-based stability guarantees in RL algorithms with key challenges being selecting a candidate Lyapunov function, computational complexity by using excessive function approximators and conservative policies by incorporating stability criterion in the learning process. In this work we propose a novel Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm using Koopman operator theory. We propose use of extended dynamic mode decomposition (EDMD) to produce a linear approximation of the system and use this approximation to derive a closed form solution for candidate Lyapunov function. This derived Lyapunov function is incorporated in the SAC algorithm to further provide guarantees for a policy that stabilizes the nonlinear system. The results are evaluated trajectory tracking of a 2D Quadrotor environment based on safe-control-gym. The proposed algorithm shows training convergence and decaying violations for Lyapunov stability criterion compared to baseline vanilla SAC algorithm. GitHub Repository: https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking"}
{"id": "2602.04200", "categories": ["cond-mat.stat-mech", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.04200", "abs": "https://arxiv.org/abs/2602.04200", "authors": ["Kevin Callahan-Coray", "Kyle Lee", "Kyle Jiang", "Kerem Y. Camsari"], "title": "Restoring Sparsity in Potts Machines via Mean-Field Constraints", "comment": null, "summary": "Ising machines and related probabilistic hardware have emerged as promising platforms for NP-hard optimization and sampling. However, many practical problems involve constraints that induce dense or all-to-all couplings, undermining scalability and hardware efficiency. We address this constraint-induced density through two complementary approaches. First, we introduce a hardware-aware native formulation for multi-state probabilistic digits (p-dits) that avoids the locally dense intra-variable couplings required by binary Ising encodings. We validate p-dit dynamics by reproducing known critical behavior of the 2D Potts model. Second, we propose mean-field constraints (MFC), a hybrid scheme that replaces dense pairwise constraint couplings with dynamically updated single-node biases. Applied to balanced graph partitioning, MFC achieves solution quality comparable to exact all-to-all constraint formulations while dramatically reducing graph density. Finally, we demonstrate the practical impact of restored sparsity by an FPGA implementation, enabling orders-of-magnitude acceleration over CPU-based solvers. Together, these results outline a pathway for scaling constrained optimization on probabilistic hardware."}
{"id": "2602.04452", "categories": ["cond-mat.str-el", "cond-mat.dis-nn", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.04452", "abs": "https://arxiv.org/abs/2602.04452", "authors": ["E. Yu. Beliayev", "Y. K. Mishra", "I. A. Chichibaba", "I. G. Mirzoiev", "V. A. Horielyi", "A. V. Terekhov"], "title": "Emergent Coherence at the Edge of Magnetism: Low-Doped La2-xSrxCuO4+delta Revisited", "comment": "27 pages, 6 figures. Review article", "summary": "The La2-xSrxCuO4+delta (LSCO) system provides a unique experimental setting for exploring how magnetism, superconductivity, and disorder jointly shape charge transport in a doped Mott insulator. Transport measurements in lightly doped and oxygen-enriched LSCO reveal a strongly insulating normal state governed by variable-range hopping, accompanied by pronounced nonlinear current-voltage characteristics and, at low temperatures, current-induced negative differential resistance. With increasing carrier concentration, these features evolve into regimes characterized by granular and percolative superconductivity near the threshold of bulk superconductivity and, eventually, into a homogeneous strange-metal state close to optimal doping. Throughout this evolution, the transport response shows marked sensitivity to disorder, electronic inhomogeneity, and external control parameters, such as bias current and magnetic field. Rather than reflecting a sequence of sharply distinct phases, the observed transport regimes form a continuous crossover from a localization-dominated insulating state to granular superconductivity and further to a coherent metallic state. This crossover is driven primarily by the progressive enhancement of electronic screening, inter-region coupling, and superconducting connectivity, rather than by abrupt changes in the underlying microscopic scattering mechanisms. Taken together, the available transport data provide a coherent experimental basis for understanding how disorder and mesoscale electronic inhomogeneity organize charge transport and superconductivity across the LSCO phase diagram, underscoring the central role of percolation and nonequilibrium effects in underdoped cuprates."}
{"id": "2602.04143", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04143", "abs": "https://arxiv.org/abs/2602.04143", "authors": ["Zeying Gao", "Xiangkai Sun", "Liang He"], "title": "Inertial dynamical systems and accelerated algorithms with implicit Hessian-driven damping for nonconvex optimization", "comment": null, "summary": "This paper is devoted to the investigation of inertial dynamical systems with implicit Hessian-driven damping for strongly quasiconvex optimization which is a specific class of nonconvex optimization problems. We first establish exponential convergence rate properties for this system without requiring Lipschitz continuity of the gradient on the function. Then, we obtain an inertial accelerated algorithm for minimizing strongly quasiconvex functions through natural explicit time discretization to the dynamical system. Meanwhile, we consider an exogenous additive perturbation term to this dynamical system and obtain the corresponding algorithm. By utilizing the Lyapunov method, we establish convergence rates of iterative sequences and their function values. Furthermore, we conduct numerical experiments to illustrate the theoretical results."}
{"id": "2602.04132", "categories": ["eess.SY", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04132", "abs": "https://arxiv.org/abs/2602.04132", "authors": ["Dhruv S. Kushwaha", "Zoleikha A. Biron"], "title": "Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking", "comment": "12 pages, 7 Figures, submitted to IEEE RA-L", "summary": "Reinforcement Learning (RL) has achieved remarkable success in solving complex sequential decision-making problems. However, its application to safety-critical physical systems remains constrained by the lack of stability guarantees. Standard RL algorithms prioritize reward maximization, often yielding policies that may induce oscillations or unbounded state divergence. There has significant work in incorporating Lyapunov-based stability guarantees in RL algorithms with key challenges being selecting a candidate Lyapunov function, computational complexity by using excessive function approximators and conservative policies by incorporating stability criterion in the learning process. In this work we propose a novel Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm using Koopman operator theory. We propose use of extended dynamic mode decomposition (EDMD) to produce a linear approximation of the system and use this approximation to derive a closed form solution for candidate Lyapunov function. This derived Lyapunov function is incorporated in the SAC algorithm to further provide guarantees for a policy that stabilizes the nonlinear system. The results are evaluated trajectory tracking of a 2D Quadrotor environment based on safe-control-gym. The proposed algorithm shows training convergence and decaying violations for Lyapunov stability criterion compared to baseline vanilla SAC algorithm. GitHub Repository: https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking"}
{"id": "2602.04124", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04124", "abs": "https://arxiv.org/abs/2602.04124", "authors": ["Monika Hu", "Matthew R. Williams", "Terrance D. Savitsky"], "title": "Privacy Amplification for Synthetic data using Range Restriction", "comment": "25 pages, 20 figures", "summary": "We introduce a new class of range restricted formal data privacy standards that condition on owner beliefs about sensitive data ranges. By incorporating this additional information, we can provide a stronger privacy guarantee (e.g. an amplification). The range restricted formal privacy standards protect only a subset (or ball) of data values and exclude ranges (or balls) believed to be already publicly known. The privacy standards are designed for the risk-weighted pseudo posterior (model) mechanism (PPM) used to generate synthetic data under an asymptotic Differential (aDP) privacy guarantee. The PPM downweights the likelihood contribution for each record proportionally to its disclosure risk. The PPM is adapted under inclusion of beliefs by adjusting the risk-weighted pseudo likelihood. We introduce two alternative adjustments. The first expresses data owner knowledge of the sensitive range as a probability, $λ$, that a datum value drawn from the underlying generating distribution lies outside the ball or subspace of values that are sensitive. The portion of each datum likelihood contribution deemed sensitive is then $(1-λ) \\leq 1$ and is the only portion of the likelihood subject to risk down-weighting. The second adjustment encodes knowledge as the difference in probability masses $P(R) \\leq 1$ between the edges of the sensitive range, $R$. We use the resulting conditional (pseudo) likelihood for a sensitive record, which boosts its worst case tail values away from 0. We compare privacy and utility properties for the PPM under the aDP and range restricted privacy standards."}
{"id": "2602.04235", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04235", "abs": "https://arxiv.org/abs/2602.04235", "authors": ["Xihao Zhang", "Hengguang Li", "Nianyu Yi", "Peimeng Yin"], "title": "Towards $C^0$ finite element methods for fourth-order elliptic equation. Part I: general boundary conditions", "comment": "23 pages, 13 figures", "summary": "This paper is part of a series developing $C^0$ finite element methods for fourth-order elliptic equations on polygonal domains. Here, we investigate how boundary conditions influence the design of effective $C^0$ schemes, specifically focusing on equations without lower-order terms, namely the biharmonic equation. We propose a modified mixed formulation that decomposes the problem into a system of Poisson equations, where the number of equations depends on both the largest interior angle and the boundary conditions on its two adjacent sides. In contrast to the naive mixed formulation, which involves only two Poisson problems, the proposed approach guarantees convergence to the true solution for arbitrary polygonal domains and general boundary conditions, including Navier, Neumann, and mixed boundary conditions. $C^0$ finite element algorithms are developed, rigorous error estimates are established, and numerical experiments are presented to demonstrate the well-posedness and effectiveness of the proposed method."}
{"id": "2602.04002", "categories": ["cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.04002", "abs": "https://arxiv.org/abs/2602.04002", "authors": ["Rodrigo Corso"], "title": "Boundary and Symmetry Breaking in a Deformed Toric Code", "comment": "20 pages, 8 figues, comments are welcome", "summary": "This work explores a deformation of the Kitaev toric code that induces a phase transition out of the topologically ordered phase. By placing the model on a cylinder, the bulk global 1-form symmetries separate into distinct boundary operators, allowing us to show that the transition is accompanied by the breaking of one higher-form symmetry. Using a holographic $(1+1)$-dimensional boundary Hamiltonian, we extract an effective central charge and find a pronounced suppression near $β_c$, followed by its restoration at strong coupling, indicating sensitivity to bulk criticality rather than topological order."}
{"id": "2602.04823", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.04823", "abs": "https://arxiv.org/abs/2602.04823", "authors": ["Claudio Durastanti"], "title": "Adaptive estimation of Sobolev-type energy functionals on the sphere", "comment": "26 pages, 3 figures", "summary": "We study the estimation of quadratic Sobolev-type integral functionals of an unknown density on the unit sphere. The functional is defined through fractional powers of the Laplace--Beltrami operator and provides a global measure of smoothness and spectral energy. Our approach relies on spherical needlet frames, which yield a localized multiscale decomposition while preserving tight frame properties in the natural square-integrable function space on the sphere.\n  We construct unbiased estimators of suitably truncated versions of the functional and derive sharp oracle risk bounds through an explicit bias--variance analysis. When the smoothness of the density is unknown, we propose a Lepski-type data-driven selection of the resolution level. The resulting adaptive estimator achieves minimax-optimal rates over Sobolev classes, without resorting to nonlinear or sparsity-based methods."}
{"id": "2602.04357", "categories": ["cond-mat.stat-mech", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.04357", "abs": "https://arxiv.org/abs/2602.04357", "authors": ["Yuta Yanagibashi", "Kazumasa A. Takeuchi"], "title": "Population dynamics simulations of large deviations for three subclasses of the Kardar-Parisi-Zhang universality class", "comment": "9 pages, 5 figures", "summary": "Recent theoretical studies have gradually deepened our understanding of the one-dimensional (1D) Kardar-Parisi-Zhang (KPZ) universality class even in the large deviation regime, but numerical methods for studying KPZ large deviations remain limited. Here we implement a method based on the population dynamics algorithm for studying large deviations of time-integrated local currents in the totally asymmetric simple exclusion process (TASEP), which is a pragmatic model in the 1D KPZ class. Carrying out simulations for the three representative initial conditions, namely step, flat, and stationary ones, we not only confirm theoretical predictions available for the step case, but also characterize large deviations for the flat and stationary cases which have not been investigated before. We reveal in particular an unexpected robustness of the deeply negative large deviation regime with respect to different initial conditions. We attribute this robustness to the spontaneous formation of a wedge shape in interface profile. Our population dynamics approach may serve as a versatile method for studying large deviations in the KPZ class numerically and, potentially, even experimentally."}
{"id": "2602.04199", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04199", "abs": "https://arxiv.org/abs/2602.04199", "authors": ["Saveetha Harikrishnan", "Tim Byrnes", "Chandrashekar Radhakrishnan"], "title": "Influence of Noninertial Dynamics on Static Quantum Resource Theories", "comment": "12 pages", "summary": "The effect of noninertial dynamics on static quantum resource theories is investigated. To this end, we first show the equivalence between noninertial effects and a completely positive, trace-preserving (CPTP) map. In this formulation, the Unruh effect is equivalent to a bosonic amplifier channel. The effect of this map on a generic quantum resource is investigated by studying the role of the CPTP map on the three core ingredients of a resource theory, namely, the free states, the free operations and the resource quantifiers. We show several general statements can be made about these three components of a resource theory in the presence of noninertial motion."}
{"id": "2602.04221", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04221", "abs": "https://arxiv.org/abs/2602.04221", "authors": ["Jaekeun Lee", "Jae-Jung Jung", "Shenghui Cui"], "title": "Mitigation of Structural Harmonic Instability in Virtual Admittance-Based Grid-Forming Inverters via Mimicking Skin Effect", "comment": null, "summary": "The virtual admittance-current controller (VA-CC) scheme is widely employed to emulate an equivalent inductance in front of the internal voltage source of grid-forming inverters. However, recent studies have reported harmonic instabilities associated with VA-CC, motivating the need for a more physically interpretable understanding of their origin. This letter identifies a delay-independent structural mechanism of harmonic instability in the VA-CC scheme, wherein the interaction between the filter and virtual inductances introduces a non-passive second-order transfer-function term exhibiting negative resistance. To address this issue, a simple yet effective modification is proposed by integrating a parallel virtual resistor into the VA structure. This reconfiguration enhances the passivity of VA-CC scheme across the harmonic range by mimicking the skin effect which augments damping in high-frequency range, without altering the wellestablished current controller or voltage feedforward control. Experimental results validate that the proposed method achieves robust harmonic stability, whereas the conventional approach fails under identical grid conditions."}
{"id": "2602.04161", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04161", "abs": "https://arxiv.org/abs/2602.04161", "authors": ["Xinming Wu", "Zi Xu", "Huiling Zhang"], "title": "Restart-Free (Accelerated) Gradient Sliding Methods for Strongly Convex Composite Optimization", "comment": null, "summary": "In this paper, we study a class of composite optimization problems whose objective function is given by the summation of a general smooth and nonsmooth component, together with a relatively simple nonsmooth term. While restart strategies are commonly employed in first-order methods to achieve optimal convergence under strong convexity, they introduce structural complexity and practical overhead, making algorithm design and nesting cumbersome. To address this, we propose a \\emph{restart-free} stochastic gradient sliding algorithm that eliminates the need for explicit restart phases when the simple nonsmooth component is strongly convex. Through a novel and carefully designed parameter selection strategy, we prove that the proposed algorithm achieves an $ε$-solution with only $\\mathcal{O}(\\log(\\frac{1}ε))$ gradient evaluations for the smooth component and $\\mathcal{O}(\\frac{1}ε)$ stochastic subgradient evaluations for the nonsmooth component, matching the optimal complexity of existing multi-phase (restart-based) methods. Moreover, for the case where the nonsmooth component is structured, allowing the overall problem to be reformulated as a bilinear saddle-point problem, we develop a restart-free accelerated stochastic gradient sliding algorithm. We show that the resulting method requires only $\\mathcal{O}(\\log(\\frac{1}ε))$ gradient computations for the smooth component while preserving an overall iteration complexity of $\\mathcal{O}(\\frac{1}{\\sqrtε})$ for solving the corresponding saddle-point problems. Our work thus provides simpler, restart-f"}
{"id": "2602.04221", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04221", "abs": "https://arxiv.org/abs/2602.04221", "authors": ["Jaekeun Lee", "Jae-Jung Jung", "Shenghui Cui"], "title": "Mitigation of Structural Harmonic Instability in Virtual Admittance-Based Grid-Forming Inverters via Mimicking Skin Effect", "comment": null, "summary": "The virtual admittance-current controller (VA-CC) scheme is widely employed to emulate an equivalent inductance in front of the internal voltage source of grid-forming inverters. However, recent studies have reported harmonic instabilities associated with VA-CC, motivating the need for a more physically interpretable understanding of their origin. This letter identifies a delay-independent structural mechanism of harmonic instability in the VA-CC scheme, wherein the interaction between the filter and virtual inductances introduces a non-passive second-order transfer-function term exhibiting negative resistance. To address this issue, a simple yet effective modification is proposed by integrating a parallel virtual resistor into the VA structure. This reconfiguration enhances the passivity of VA-CC scheme across the harmonic range by mimicking the skin effect which augments damping in high-frequency range, without altering the wellestablished current controller or voltage feedforward control. Experimental results validate that the proposed method achieves robust harmonic stability, whereas the conventional approach fails under identical grid conditions."}
{"id": "2602.04178", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.04178", "abs": "https://arxiv.org/abs/2602.04178", "authors": ["Qi Xu", "Jing Lei", "Kathryn Roeder"], "title": "Sparse group principal component analysis via double thresholding with application to multi-cellular programs", "comment": null, "summary": "Multi-cellular programs (MCPs) are coordinated patterns of gene expression across interacting cell types that collectively drive complex biological processes such as tissue development and immune responses. While MCPs are typically estimated from high-dimensional gene expression data using methods like sparse principal component analysis or latent factor models, these approaches often suffer from high computational costs and limited statistical power. In this work, we propose Sparse Group Principal Component Analysis (SGPCA) to estimate MCPs by leveraging their inherent group and individual sparsity. We introduce an efficient double-thresholding algorithm based on power iteration. In each iteration, a group thresholding step first identifies relevant gene groups, followed by an individual thresholding step to select active cell types. This algorithm achieves a linear computational complexity of $O(np)$, making it highly efficient and scalable for large-scale genomic analyses. We establish theoretical guarantees for SGPCA, including statistical consistency and a convergence rate that surpasses competing methods. Through extensive simulations, we demonstrate that SGPCA achieves superior estimation accuracy and improved statistical power for signal detection. Furthermore, We apply SGPCA to a Lupus study, discovering differentially expressed MCPs distinguishing Lupus patients from normal subjects."}
{"id": "2602.04348", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04348", "abs": "https://arxiv.org/abs/2602.04348", "authors": ["Erin Claire Carson"], "title": "Balancing Inexactness in Mixed Precision Matrix Computations", "comment": "12 pages", "summary": "Support for arithmetic in multiple precisions and number formats is becoming increasingly common in emerging high-performance architectures. From a computational scientist's perspective, our goal is to determine how and where we can safely exploit mixed precision computation in our codes to improve performance. One case where the use of low precision is natural, common in computational science, is when there are already other significant sources of ``inexactness'' present, e.g., discretization error, measurement error, or algorithmic approximation error. In such instances, analyzing the interaction of these different sources of inexactness can give insight into how the precisions of various computations should be chosen in order to ``balance'' the errors, potentially improving performance without a noticeable decrease in accuracy. We present a few recent examples of this approach which demonstrate the potential for the use of mixed precision in numerical linear algebra and matrix computations."}
{"id": "2602.04452", "categories": ["cond-mat.str-el", "cond-mat.dis-nn", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.04452", "abs": "https://arxiv.org/abs/2602.04452", "authors": ["E. Yu. Beliayev", "Y. K. Mishra", "I. A. Chichibaba", "I. G. Mirzoiev", "V. A. Horielyi", "A. V. Terekhov"], "title": "Emergent Coherence at the Edge of Magnetism: Low-Doped La2-xSrxCuO4+delta Revisited", "comment": "27 pages, 6 figures. Review article", "summary": "The La2-xSrxCuO4+delta (LSCO) system provides a unique experimental setting for exploring how magnetism, superconductivity, and disorder jointly shape charge transport in a doped Mott insulator. Transport measurements in lightly doped and oxygen-enriched LSCO reveal a strongly insulating normal state governed by variable-range hopping, accompanied by pronounced nonlinear current-voltage characteristics and, at low temperatures, current-induced negative differential resistance. With increasing carrier concentration, these features evolve into regimes characterized by granular and percolative superconductivity near the threshold of bulk superconductivity and, eventually, into a homogeneous strange-metal state close to optimal doping. Throughout this evolution, the transport response shows marked sensitivity to disorder, electronic inhomogeneity, and external control parameters, such as bias current and magnetic field. Rather than reflecting a sequence of sharply distinct phases, the observed transport regimes form a continuous crossover from a localization-dominated insulating state to granular superconductivity and further to a coherent metallic state. This crossover is driven primarily by the progressive enhancement of electronic screening, inter-region coupling, and superconducting connectivity, rather than by abrupt changes in the underlying microscopic scattering mechanisms. Taken together, the available transport data provide a coherent experimental basis for understanding how disorder and mesoscale electronic inhomogeneity organize charge transport and superconductivity across the LSCO phase diagram, underscoring the central role of percolation and nonequilibrium effects in underdoped cuprates."}
{"id": "2602.03919", "categories": ["cond-mat.stat-mech", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.03919", "abs": "https://arxiv.org/abs/2602.03919", "authors": ["Airton Deppman"], "title": "Tsallis Entropy derived from the Chaitin-Kolmogorov Informational Entropy", "comment": "16 pages 1 figure", "summary": "We provide a rigorous first-principle derivation of the non-additive Tsallis' entropy by employing the Chaitin-Kolmogorov algorithmic information theory. By applying non-local restrictive rules on the string formation (grammar), we show that the algorithmic cost follows a power-law of the string length, instead of the linear behaviour obtained in the classical theory. As a result, the Tsallis entropy governs the increase of information. We explore the result showing, through Landauer's limit, that the heat dissipation in systems with long-range correlations is diminished. The $Ω_q$ number, which remains incompressible, now offers the possibility of a continuous increase of complexity, measured by the parameter $q$. We show the consistency of the results by a numerical simulation, and discuss Zipf's law in light of the new findings."}
{"id": "2602.04363", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04363", "abs": "https://arxiv.org/abs/2602.04363", "authors": ["R. Goutham", "R. Rajesh", "V. Subashri", "Oleg Zaboronski"], "title": "Probabilities of rare events in product kernel aggregation: An exact formula and phase diagram", "comment": "17 pages, 12 figures", "summary": "We present an exact method for calculating the large deviation function describing rare fluctuations in the number of particles for product-kernel aggregation. Starting from the master equation, we derive an exact integral representation for the probability $P(M,N,t)$ of observing $N$ particles at time $t$ starting from $M$ monomers for any finite $M, N, t$. From this, we obtain an exact expression for the exponential moment $\\langle p^N\\rangle$ for integer $p$. Employing a replica conjecture -- numerically validated by finite-$M$ scaling -- we extend this result to real $p \\geq 0$. The convex envelope of the large deviation function, obtained via a Legendre-Fenchel transform of the exponential moment, shows singular behavior. The singular structure allows us to construct the full phase diagram of product-kernel aggregation, which contains a tricritical point, separating continuous and discontinuous transitions. We also compute the asymptotic form of the LDF for small $N/M$."}
{"id": "2602.04239", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04239", "abs": "https://arxiv.org/abs/2602.04239", "authors": ["Vanshaj Kerni", "Abdelrahman E. Ahmed", "Syed Ali Asghar"], "title": "Benchmarking Quantum and Classical Algorithms for the 1D Burgers Equation: QTN, HSE, and PINN", "comment": "8 pages, 11 figures, submitted to IEEE CMISI 2026 Conference", "summary": "We present a comparative benchmark of Quantum Tensor Networks (QTN), the Hydrodynamic Schrödinger Equation (HSE), and Physics-Informed Neural Networks (PINN) for simulating the 1D Burgers' equation. Evaluating these emerging paradigms against classical GMRES and Spectral baselines, we analyse solution accuracy, runtime scaling, and resource overhead across grid resolutions ranging from $N=4$ to $N=128$. Our results reveal a distinct performance hierarchy. The QTN solver achieves superior precision ($L_2 \\sim 10^{-7}$) with remarkable near-constant runtime scaling, effectively leveraging entanglement compression to capture shock fronts. In contrast, while the Finite-Difference HSE implementation remains robust, the Spectral HSE method suffers catastrophic numerical instability at high resolutions, diverging significantly at $N=128$. PINNs demonstrate flexibility as mesh-free solvers but stall at lower accuracy tiers ($L_2 \\sim 10^{-1}$), limited by spectral bias compared to grid-based methods. Ultimately, while quantum methods offer novel representational advantages for low-resolution fluid dynamics, this study confirms they currently yield no computational advantage over classical solvers without fault tolerance or significant algorithmic breakthroughs in handling non-linear feedback."}
{"id": "2602.04262", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04262", "abs": "https://arxiv.org/abs/2602.04262", "authors": ["Haokun Yu", "Jingyuan Zhou", "Kaidi Yang"], "title": "Parameter Privacy-Preserving Data Sharing: A Particle-Belief MDP Formulation", "comment": "17 pages, 10 figures", "summary": "This paper investigates parameter-privacy-preserving data sharing in continuous-state dynamical systems, where a data owner designs a data-sharing policy to support downstream estimation and control while preventing adversarial inference of a sensitive parameter. This data-sharing problem is formulated as an optimization problem that trades off privacy leakage and the impact of data sharing on the data owner's utility, subject to a data-usability constraint. We show that this problem admits an equivalent belief Markov decision process (MDP) formulation, which provides a simplified representation of the optimal policy. To efficiently characterize information-theoretic privacy leakage in continuous state and action spaces, we propose a particle-belief MDP formulation that tracks the parameter posterior via sequential Monte Carlo, yielding a tractable belief-state approximation that converges asymptotically as the number of particles increases. We further derive a tractable closed-form upper bound on particle-based MI via Gaussian mixture approximations, which enables efficient optimization of the particle-belief MDP. Experiments on a mixed-autonomy platoon show that the learned continuous policy substantially impedes inference attacks on human-driving behavior parameters while maintaining data usability and system performance."}
{"id": "2602.04219", "categories": ["math.OC", "eess.SY", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2602.04219", "abs": "https://arxiv.org/abs/2602.04219", "authors": ["Chung-Han Hsieh"], "title": "Sampled-Data Wasserstein Distributionally Robust Control of Multiplicative Systems: A Convex Relaxation with Performance Guarantees", "comment": "Submitted for possible publication", "summary": "This paper investigates the robust optimal control of sampled-data stochastic systems with multiplicative noise and distributional ambiguity. We consider a class of discrete-time optimal control problems where the controller \\emph{jointly} selects a feedback policy and a sampling period to maximize the worst-case expected concave utility of the inter-sample growth factor. Modeling uncertainty via a Wasserstein ambiguity set, we confront the structural obstacle of~``concave-max'' geometry arising from maximizing a concave utility against an adversarial distribution. Unlike standard convex loss minimization, the dual reformulation here requires a minimax interchange within the semi-infinite constraints, where the utility's concavity precludes exact strong duality. To address this, we utilize a general minimax inequality to derive a tractable convex relaxation. Our approach yields a rigorous lower bound that functions as a probabilistic performance guarantee. We establish an explicit, non-asymptotic bound on the resulting duality gap, proving that the approximation error is uniformly controlled by the Lipschitz-smoothness of the stage reward and the diameter of the disturbance support. Furthermore, we introduce necessary and sufficient conditions for \\emph{robust viability}, ensuring state positivity invariance across the entire ambiguity set. Finally, we bridge the gap between static optimization and dynamic performance, proving that the optimal value of the relaxation serves as a rigorous deterministic floor for the asymptotic average utility rate almost surely. The framework is illustrated on a log-optimal portfolio control problem, which serves as a canonical instance of multiplicative stochastic control."}
{"id": "2602.04262", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04262", "abs": "https://arxiv.org/abs/2602.04262", "authors": ["Haokun Yu", "Jingyuan Zhou", "Kaidi Yang"], "title": "Parameter Privacy-Preserving Data Sharing: A Particle-Belief MDP Formulation", "comment": "17 pages, 10 figures", "summary": "This paper investigates parameter-privacy-preserving data sharing in continuous-state dynamical systems, where a data owner designs a data-sharing policy to support downstream estimation and control while preventing adversarial inference of a sensitive parameter. This data-sharing problem is formulated as an optimization problem that trades off privacy leakage and the impact of data sharing on the data owner's utility, subject to a data-usability constraint. We show that this problem admits an equivalent belief Markov decision process (MDP) formulation, which provides a simplified representation of the optimal policy. To efficiently characterize information-theoretic privacy leakage in continuous state and action spaces, we propose a particle-belief MDP formulation that tracks the parameter posterior via sequential Monte Carlo, yielding a tractable belief-state approximation that converges asymptotically as the number of particles increases. We further derive a tractable closed-form upper bound on particle-based MI via Gaussian mixture approximations, which enables efficient optimization of the particle-belief MDP. Experiments on a mixed-autonomy platoon show that the learned continuous policy substantially impedes inference attacks on human-driving behavior parameters while maintaining data usability and system performance."}
{"id": "2602.04230", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.04230", "abs": "https://arxiv.org/abs/2602.04230", "authors": ["Albert Tan", "Sadegh Shirani", "James Nordlund", "Mohsen Bayati"], "title": "Validating Causal Message Passing Against Network-Aware Methods on Real Experiments", "comment": null, "summary": "Estimating total treatment effects in the presence of network interference typically requires knowledge of the underlying interaction structure. However, in many practical settings, network data is either unavailable, incomplete, or measured with substantial error. We demonstrate that causal message passing, a methodology that leverages temporal structure in outcome data rather than network topology, can recover total treatment effects comparable to network-aware approaches. We apply causal message passing to two large-scale field experiments where a recently developed bipartite graph methodology, which requires network knowledge, serves as a benchmark. Despite having no access to the interaction network, causal message passing produces effect estimates that match the network-aware approach in direction across all metrics and in statistical significance for the primary decision metric. Our findings validate the premise of causal message passing: that temporal variation in outcomes can serve as an effective substitute for network observation when estimating spillover effects. This has important practical implications: practitioners facing settings where network data is costly to collect, proprietary, or unreliable can instead exploit the temporal dynamics of their experimental data."}
{"id": "2602.04359", "categories": ["math.NA", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.04359", "abs": "https://arxiv.org/abs/2602.04359", "authors": ["Ahsan Kaleem", "Cristian Gebhardt", "Ignacio Romero"], "title": "On the pure traction problem of linear elasticity: a regularized formulation and its robust approximation", "comment": "43 pages, 18 figures", "summary": "The pure traction problem of elasticity appears frequently in engineering applications, and its complexity stems from the fact that its solution is unique only up to (infinitesimal) rigid body motions. When finite elements are employed to approximate this problem, one solution is typically singled out by applying carefully selected boundary conditions on the discrete model or by imposing global constraints on the deformation. However, neither of these strategies is both simple and computationally efficient. In this work, we propose a new approach to solving the pure traction problem that overcomes existing limitations. Our method builds on a regularized form of the problem whose solution is shown to be unique, converges to the original solution of minimal norm, and can be approximated with finite elements in a straightforward way, without additional degrees of freedom. Additionally, we analyze the situation in which the approximation of the solution domain renders the loading of the discretized problem non-equilibrated, making the problem ill-posed. In this case, we propose a regularized predictor--corrector finite element formulation that handles the incompatibilities of the loading, providing a solution that converges to that of the original Neumann problem as the mesh size and the regularizing parameter tend to zero. Numerical examples illustrate the effectiveness of the proposed approach for representative problems in mechanics where pure traction boundary conditions appear."}
{"id": "2602.04781", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.04781", "abs": "https://arxiv.org/abs/2602.04781", "authors": ["Panchlal Prabhat", "Amit Gupta"], "title": "Magneto-optical transport in type-II Weyl semimetals in the presence of orbital magnetic moment", "comment": null, "summary": "The magneto-optical transport of gapless type-I tilted single Weyl semimetals(WSMs) exhibits suppression of total magnetoconductivities in the presence of orbital magnetic moment(OMM) in linear and nonlinear responses (Yang Gao et al., Phys. Rev. B {\\bf 105}, 165307 (2022)). In this work, we extend our study to investigate magnetoconductivities in gapless type-II Weyl semimetals within the semiclassical Boltzmann approach and show the differences that arise compared to type-I Weyl semimetals."}
{"id": "2602.04318", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04318", "abs": "https://arxiv.org/abs/2602.04318", "authors": ["Yoshiki Kinoshita", "Aya Shinozaki", "Toshinari Kamakura"], "title": "Accurate and Efficient Approximation of the Null Distribution of Rao's Spacing Test", "comment": "10 pages", "summary": "Rao's spacing test is a widely used nonparametric method for assessing uniformity on the circle. However, its broader applicability in practical settings has been limited because the null distribution is not easily calculated. As a result, practitioners have traditionally depended on pre-tabulated critical values computed for a limited set of sample sizes, which restricts the flexibility and generality of the method. In this paper, we address this limitation by recursively computing higher-order moments of the Rao's spacing test statistic and employing the Gram-Charlier expansion to derive an accurate approximation to its null distribution. This approach allows for the efficient and direct computation of p-values for arbitrary sample sizes, thereby eliminating the dependency on existing critical value tables. Moreover, we confirm that our method remains accurate and effective even for large sample sizes that are not represented in current tables, thus overcoming a significant practical limitation. Comparative evaluations with published critical values and saddlepoint approximations demonstrate that our method achieves a high degree of accuracy across a wide range of sample sizes. These findings greatly improve the practicality and usability of Rao's spacing test in both theoretical investigations and applied statistical analyses."}
{"id": "2602.04560", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04560", "abs": "https://arxiv.org/abs/2602.04560", "authors": ["Marco Baldovin", "Alessandro Manacorda"], "title": "Control protocols for harmonically confined run-and-tumble particles", "comment": "13 pages, 9 figures", "summary": "Run-and-tumble particles constitute one of the simplest models of self-propelled active matter, and provide an ideal playground to the understanding of out-of-equilibrium systems. We consider an idealized setup where one such particle is subject to a harmonic confining potential, and an external agent can vary in time the tumbling rate and the strength of the trap. We search for time-dependent control protocols steering the system between assigned end states, in a prescribed time interval. To this aim, we propose a description of the dynamics, alternative to the usual ones, in the form of an infinite set of ordinary differential equations. Solutions based on a suitable closure of such hierarchy, which we expect to hold true in the limit of long protocol duration, are discussed and compared with numerical simulations. We also look for the protocol completing the task with the minimal work, on average: the problem can be tackled analytically, again in the regime of slow (but not quasi-static) transformations. The solution provides insightful intuition on the optimal strategies for the control of active matter systems."}
{"id": "2602.04253", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.04253", "abs": "https://arxiv.org/abs/2602.04253", "authors": ["Runhong He", "Xin Hong", "Qiaozhen Chai", "Ji Guan", "Junyuan Zhou", "Arapat Ablimit", "Guolong Cui", "Shenggang Ying"], "title": "Constructing Compact ADAPT Unitary Coupled-Cluster Ansatz with Parameter-Based Criterion", "comment": "12 pages, 8 figures", "summary": "The adaptive derivative-assembled pseudo-trotter variational quantum eigensolver (ADAPT-VQE) is a promising hybrid quantum-classical algorithm for molecular ground state energy calculation, yet its practical scalability is hampered by redundant excitation operators and excessive measurement costs. To address these challenges, we propose Param-ADAPT-VQE, a novel improved algorithm that selects excitation operators based on a parameter-based criterion instead of the traditional gradient-based metric. This strategy effectively eludes redundant operators. We further develop a sub-Hamiltonian technique and integrate a hot-start VQE optimization strategy, achieving a significant reduction in measurement costs. Numerical experiments on typical molecular systems demonstrate that Param-ADAPT-VQE outperforms the original ADAPT-VQE in computational accuracy, ansatz size, and measurement costs. Furthermore, our scheme retains the fundamental framework of ADAPT-VQE and is thus fully compatible with its various modified versions, enabling further performance improvements in specific aspects. This work presents an efficient and scalable enhancement to ADAPT-VQE, mitigating the core obstacles that impede its practical implementation in the field of molecular quantum chemistry."}
{"id": "2602.04568", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04568", "abs": "https://arxiv.org/abs/2602.04568", "authors": ["Axel Stafström", "Daniel Arnström", "Adam Miksits", "David Umsonst"], "title": "Peak Bounds for the Estimation Error under Sensor Attacks", "comment": "7 pages, 3 figures, accepted at the American Control Conference 2026", "summary": "This paper investigates bounds on the estimation error of a linear system affected by norm-bounded disturbances and full sensor attacks. The system is equipped with a detector that evaluates the norm of the innovation signal to detect faults, and the attacker wants to avoid detection. We utilize induced $L_\\infty$ system norms, also called \\emph{peak-to-peak} norms, to compare the estimation error bounds under nominal operations and under attack. This leads to a sufficient condition for when the bound on the estimation error is smaller during an attack than during nominal operation. This condition is independent of the attack strategy and depends only on the attacker's desire to remain undetected and (indirectly) the observer gain. Therefore, we investigate both an observer design method, that seeks to reduce the error bound under attack while keeping the nominal error bound low, and detector threshold tuning. As a numerical illustration, we show how a sensor attack can deactivate a robust safety filter based on control barrier functions if the attacked error bound is larger than the nominal one. We also statistically evaluate our observer design method and the effect of the detector threshold."}
{"id": "2602.04237", "categories": ["math.OC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04237", "abs": "https://arxiv.org/abs/2602.04237", "authors": ["ZeYu Li", "Te Qi", "TieYong Zeng"], "title": "An Improved Boosted DC Algorithm for Nonsmooth Functions with Applications in Image Recovery", "comment": null, "summary": "We propose a new approach to perform the boosted difference of convex functions algorithm (BDCA) on non-smooth and non-convex problems involving the difference of convex (DC) functions. The recently proposed BDCA uses an extrapolation step from the point computed by the classical DC algorithm (DCA) via a line search procedure in a descent direction to get an additional decrease of the objective function and accelerate the convergence of DCA. However, when the first function in DC decomposition is non-smooth, the direction computed by BDCA can be ascent and a monotone line search cannot be performed. In this work, we proposed a monotone improved boosted difference of convex functions algorithm (IBDCA) for certain types of non-smooth DC programs, namely those that can be formulated as the difference of a possibly non-smooth function and a smooth one. We show that any cluster point of the sequence generated by IBDCA is a critical point of the problem under consideration and that the corresponding objective value is monotonically decreasing and convergent. We also present the global convergence and the convergent rate under the Kurdyka-Lojasiewicz property. The applications of IBDCA in image recovery show the effectiveness of our proposed method. The corresponding numerical experiments demonstrate that our IBDCA outperforms DCA and other state-of-the-art DC methods in both computational time and number of iterations."}
{"id": "2602.04568", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04568", "abs": "https://arxiv.org/abs/2602.04568", "authors": ["Axel Stafström", "Daniel Arnström", "Adam Miksits", "David Umsonst"], "title": "Peak Bounds for the Estimation Error under Sensor Attacks", "comment": "7 pages, 3 figures, accepted at the American Control Conference 2026", "summary": "This paper investigates bounds on the estimation error of a linear system affected by norm-bounded disturbances and full sensor attacks. The system is equipped with a detector that evaluates the norm of the innovation signal to detect faults, and the attacker wants to avoid detection. We utilize induced $L_\\infty$ system norms, also called \\emph{peak-to-peak} norms, to compare the estimation error bounds under nominal operations and under attack. This leads to a sufficient condition for when the bound on the estimation error is smaller during an attack than during nominal operation. This condition is independent of the attack strategy and depends only on the attacker's desire to remain undetected and (indirectly) the observer gain. Therefore, we investigate both an observer design method, that seeks to reduce the error bound under attack while keeping the nominal error bound low, and detector threshold tuning. As a numerical illustration, we show how a sensor attack can deactivate a robust safety filter based on control barrier functions if the attacked error bound is larger than the nominal one. We also statistically evaluate our observer design method and the effect of the detector threshold."}
{"id": "2602.04318", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04318", "abs": "https://arxiv.org/abs/2602.04318", "authors": ["Yoshiki Kinoshita", "Aya Shinozaki", "Toshinari Kamakura"], "title": "Accurate and Efficient Approximation of the Null Distribution of Rao's Spacing Test", "comment": "10 pages", "summary": "Rao's spacing test is a widely used nonparametric method for assessing uniformity on the circle. However, its broader applicability in practical settings has been limited because the null distribution is not easily calculated. As a result, practitioners have traditionally depended on pre-tabulated critical values computed for a limited set of sample sizes, which restricts the flexibility and generality of the method. In this paper, we address this limitation by recursively computing higher-order moments of the Rao's spacing test statistic and employing the Gram-Charlier expansion to derive an accurate approximation to its null distribution. This approach allows for the efficient and direct computation of p-values for arbitrary sample sizes, thereby eliminating the dependency on existing critical value tables. Moreover, we confirm that our method remains accurate and effective even for large sample sizes that are not represented in current tables, thus overcoming a significant practical limitation. Comparative evaluations with published critical values and saddlepoint approximations demonstrate that our method achieves a high degree of accuracy across a wide range of sample sizes. These findings greatly improve the practicality and usability of Rao's spacing test in both theoretical investigations and applied statistical analyses."}
{"id": "2602.04368", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04368", "abs": "https://arxiv.org/abs/2602.04368", "authors": ["P. A. Gazca-Orozco", "A. Kaltenbach"], "title": "A Priori and A Posteriori Error Identities for Vectorial Problems via Convex Duality", "comment": null, "summary": "Convex duality has been leveraged in recent years to derive a posteriori error estimates and identities for a wide range of non-linear and non-smooth scalar problems. By employing remarkable compatibility properties of the Crouzeix-Raviart and Raviart-Thomas elements, optimal convergence of non-conforming discretisations and flux reconstruction formulas have also been established. This paper aims to extend these results to the vectorial setting, focusing on the archetypical problems of incompressible Stokes and Navier-Lamé. Moreover, unlike most previous results, we consider inhomogeneous mixed boundary conditions and loads in the topological dual of the energy space. At the discrete level, we derive error identities and estimates that enable to prove quasi-optimal error estimates for a Crouzeix-Raviart discretisation with minimal regularity assumptions and no data oscillation terms."}
{"id": "2602.04400", "categories": ["stat.ME", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04400", "abs": "https://arxiv.org/abs/2602.04400", "authors": ["F. A. Shiha"], "title": "Unit Shiha Distribution and its Applications to Engineering and Medical Data", "comment": null, "summary": "There is a growing need for flexible statistical distributions that can accurately model data defined on the unit interval. This paper introduces a new unit distribution, termed the unit Shiha (USh) distribution, which is derived from the original Shiha (Sh) distribution through an inverse exponential transformation. The probability density function of the USh distribution is sufficiently flexible to model both left- and right-skewed data, while its hazard rate function is capable of capturing various failure-rate patterns, including increasing, bathtub-shaped, and J-shaped forms. Several statistical properties of the proposed distribution are investigated, including moments and related measures, the quantile function, entropy, and stress-strength reliability. Parameter estimation is carried out using the maximum likelihood method, and its performance is evaluated through a simulation study. The practical usefulness of the USh distribution is demonstrated using four real-life data sets, and its performance is compared with several well-known competing unit distributions. The comparative results indicate that the proposed model fits the data better than the competitive models applied in this study."}
{"id": "2602.04593", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.04593", "abs": "https://arxiv.org/abs/2602.04593", "authors": ["Nitesh Jaiswal", "S. Shankaranarayanan"], "title": "Emergent Hawking Radiation and Quantum Sensing in a Quenched Chiral Spin Chain", "comment": "23 pages, 7 figures. Comments are welcome", "summary": "We investigate the emergence and detection of Hawking radiation (HR) in a 1D chiral spin chain model, where the gravitational collapse is simulated by a sudden quantum quench that triggers a horizon-inducing phase transition. While our previous work Jaiswal [2025] established that this model mimics BH formation conditions even when the Hoop conjecture is seemingly violated, we here focus on the resulting stationary radiation spectrum and its detectability. By mapping the spin chain dynamics to a Dirac fermion in a curved (1 + 1)-dimensional spacetime, we analyze the radiation using two complementary approaches: field-theoretic modes and operational quantum sensors. First, using localized Gaussian wave packets to model realistic detectors, we find that the radiation spectrum exhibits deviations from the ideal Planckian form, analogous to frequency-dependent greybody factors, while retaining robust Poissonian statistics that signal the loss of formation-scale information. Second, we introduce a qubit coupled to the chain as a stationary Unruh-DeWitt detector. We demonstrate that the qubit functions as a faithful quantum sensor of the Hawking temperature only in the weak-coupling regime, where its population dynamics are governed solely by the bath spectral density. In the strong-coupling limit, the probe thermalizes with the global environment, obscuring the horizon-induced thermal signature. These results provide a clear operational protocol for distinguishing genuine analog HR from environmental noise in quantum simulation platforms."}
{"id": "2602.04295", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04295", "abs": "https://arxiv.org/abs/2602.04295", "authors": ["Alexandre Delattre", "Eddy Collin"], "title": "Canonical Quantization of Cylindrical Waveguides: A Gauge-Based Approach", "comment": null, "summary": "We present a canonical quantization of electromagnetic modes in cylindrical waveguides, extending a gauge-based formalism previously developed for Cartesian geometries [1]. By introducing the two field quadratures $X,Y$ of TEM (transverse electric-magnetic), but also of TM (transverse magnetic) and TE (transverse electric) traveling modes, we identify for each a characteristic one-dimensional scalar field (a generalized flux $\\varphi$) governed by a Klein-Gordon type equation. The associated Hamiltonian is derived explicitly from Maxwell's equations, allowing the construction of bosonic ladder operators. The generalized flux is directly deduced from the electromagnetic potentials $A,V$ by a proper gauge choice, generalizing Devoret's approach [2]. Our analysis unifies the treatment of cylindrical and Cartesian guided modes under a consistent and generic framework, ensuring both theoretical insight and experimental relevance. We derive mode-specific capacitance and inductance from the field profiles and express voltage and current in terms of the canonical field variables. Measurable quantities are therefore properly defined from the mode quantum operators, especially for the non-trivial TM and TE ones. The formalism shall extend in future works to any other type of waveguides, especially on-chip coplanar geometries particularly relevant to quantum technologies."}
{"id": "2602.04578", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04578", "abs": "https://arxiv.org/abs/2602.04578", "authors": ["Meng Yuan", "Ye Wang", "Xinghuo Yu", "Torsten Wik", "Changfu Zou"], "title": "Reinforcement Learning-based Home Energy Management with Heterogeneous Batteries and Stochastic EV Behaviour", "comment": null, "summary": "The widespread adoption of photovoltaic (PV), electric vehicles (EVs), and stationary energy storage systems (ESS) in households increases system complexity while simultaneously offering new opportunities for energy regulation. However, effectively coordinating these resources under uncertainties remains challenging. This paper proposes a novel home energy management framework based on deep reinforcement learning (DRL) that can jointly minimise energy expenditure and battery degradation while guaranteeing occupant comfort and EV charging requirements. Distinct from existing studies, we explicitly account for the heterogeneous degradation characteristics of stationary and EV batteries in the optimisation, alongside stochastic user behaviour regarding arrival time, departure time, and driving distance. The energy scheduling problem is formulated as a constrained Markov decision process (CMDP) and solved using a Lagrangian soft actor-critic (SAC) algorithm. This approach enables the agent to learn optimal control policies that enforce physical constraints, including indoor temperature bounds and target EV state of charge upon departure, despite stochastic uncertainties. Numerical simulations over a one-year horizon demonstrate the effectiveness of the proposed framework in satisfying physical constraints while eliminating thermal oscillations and achieving significant economic benefits. Specifically, the method reduces the cumulative operating cost substantially compared to two standard rule-based baselines while simultaneously decreasing battery degradation costs by 8.44%."}
{"id": "2602.04310", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04310", "abs": "https://arxiv.org/abs/2602.04310", "authors": ["Léa Ninite", "Adrien Banse", "Guillaume O. Berger", "Raphaël M. Jungers"], "title": "A Path-Complete Approach for Optimal Control of Switched Systems", "comment": null, "summary": "We study the problem of estimating the value function of discrete-time switched systems under arbitrary switching. Unlike the switched LQR problem, where both inputs and mode sequences are optimized, we consider the case where switching is exogenous. For such systems, the number of possible mode sequences grows exponentially with time, making the exact computation of the value function intractable. This motivates the development of tractable bounds that approximate it. We propose a novel framework, based on path-complete graphs, for constructing computable upper bounds on the value function. In this framework, multiple quadratic functions are combined through a directed graph that encodes dynamic programming inequalities, yielding convex and sound formulations. For example, for switched linear systems with quadratic cost, we derive tractable LMI-based formulations and provide computational complexity bounds. We further establish approximation guarantees for the upper bounds and show asymptotic non-conservativeness using concepts from graph theory. Finally, we extend the approach to controller synthesis for systems with affine control inputs and demonstrate its effectiveness on numerical examples."}
{"id": "2602.04578", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04578", "abs": "https://arxiv.org/abs/2602.04578", "authors": ["Meng Yuan", "Ye Wang", "Xinghuo Yu", "Torsten Wik", "Changfu Zou"], "title": "Reinforcement Learning-based Home Energy Management with Heterogeneous Batteries and Stochastic EV Behaviour", "comment": null, "summary": "The widespread adoption of photovoltaic (PV), electric vehicles (EVs), and stationary energy storage systems (ESS) in households increases system complexity while simultaneously offering new opportunities for energy regulation. However, effectively coordinating these resources under uncertainties remains challenging. This paper proposes a novel home energy management framework based on deep reinforcement learning (DRL) that can jointly minimise energy expenditure and battery degradation while guaranteeing occupant comfort and EV charging requirements. Distinct from existing studies, we explicitly account for the heterogeneous degradation characteristics of stationary and EV batteries in the optimisation, alongside stochastic user behaviour regarding arrival time, departure time, and driving distance. The energy scheduling problem is formulated as a constrained Markov decision process (CMDP) and solved using a Lagrangian soft actor-critic (SAC) algorithm. This approach enables the agent to learn optimal control policies that enforce physical constraints, including indoor temperature bounds and target EV state of charge upon departure, despite stochastic uncertainties. Numerical simulations over a one-year horizon demonstrate the effectiveness of the proposed framework in satisfying physical constraints while eliminating thermal oscillations and achieving significant economic benefits. Specifically, the method reduces the cumulative operating cost substantially compared to two standard rule-based baselines while simultaneously decreasing battery degradation costs by 8.44%."}
{"id": "2602.04322", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04322", "abs": "https://arxiv.org/abs/2602.04322", "authors": ["Vincent Runge", "Anica Kostic", "Alexandre Combeau", "Gaetano Romano"], "title": "Exact Multiple Change-Point Detection Via Smallest Valid Partitioning", "comment": null, "summary": "We introduce smallest valid partitioning (SVP), a segmentation method for multiple change-point detection in time-series. SVP relies on a local notion of segment validity: a candidate segment is retained only if it passes a user-chosen validity test (e.g., a single change-point test). From the collection of valid segments, we propose a coherent aggregation procedure that constructs a global segmentation which is the exact solution of an optimization problem. Our main contribution is the use of a lexicographic order for the optimization problem that prioritizes parsimony. We analyze the computational complexity of the resulting procedure, which ranges from linear to cubic time depending on the chosen cost and validity functions, the data regime and the number of detected changes. Finally, we assess the quality of SVP through comparisons with standard optimal partitioning algorithms, showing that SVP yields competitive segmentations while explicitly enforcing segment validity. The flexibility of SVP makes it applicable to a broad class of problems; as an illustration, we demonstrate robust change-point detection by encoding robustness in the validity criterion."}
{"id": "2602.04490", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04490", "abs": "https://arxiv.org/abs/2602.04490", "authors": ["Johannes Storn"], "title": "Randomized Projection Operators onto Piecewise Polynomial Spaces", "comment": null, "summary": "We introduce computable projection operators onto piecewise polynomial spaces, defined via sampling and discrete least-squares polynomial approximations. The resulting mappings exhibit (almost) optimal approximation properties in $L^2$ and $H^{-1}$. As smoothers for incomplete or rough data, they yield computable finite element discretizations with optimal convergence rates."}
{"id": "2602.04550", "categories": ["quant-ph", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04550", "abs": "https://arxiv.org/abs/2602.04550", "authors": ["Cristina Butucea", "Jan Johannes", "Henning Stein"], "title": "Locally Gentle State Certification for High Dimensional Quantum Systems", "comment": null, "summary": "Standard approaches to quantum statistical inference rely on measurements that induce a collapse of the wave function, effectively consuming the quantum state to extract information. In this work, we investigate the fundamental limits of \\emph{locally-gentle} quantum state certification, where the learning algorithm is constrained to perturb the state by at most $α$ in trace norm, thereby allowing for the reuse of samples. We analyze the hypothesis testing problem of distinguishing whether an unknown state $ρ$ is equal to a reference $ρ_0$ or $ε$-far from it. We derive the minimax sample complexity for this problem, quantifying the information-theoretic price of non-destructive measurements. Specifically, by constructing explicit measurement operators, we show that the constraint of $α$-gentleness imposes a sample size penalty of $\\frac{d}{α^2}$, yielding a total sample complexity of $n = Θ(\\frac{d^3}{ε^2 α^2})$. Our results clarify the trade-off between information extraction and state disturbance, and highlight deep connections between physical measurement constraints and privacy mechanisms in quantum learning. Crucially, we find that the sample size penalty incurred by enforcing $α$-gentleness scales linearly with the Hilbert-space dimension $d$ rather than the number of parameters $d^2-1$ typical for high-dimensional private estimation."}
{"id": "2602.04626", "categories": ["cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.04626", "abs": "https://arxiv.org/abs/2602.04626", "authors": ["Fabio Deelan Cunden", "Noemi Cuppone", "Giovanni Gramegna", "Pierpaolo Vivo"], "title": "The Most Dispersed Subset of Random Points in $\\mathbb{R}^d$", "comment": "33 pages, 7 figures", "summary": "Consider a population of $N$ individuals, each having $d\\geq 1$ different traits, and an additive measure, called dispersion, which rewards large pairwise separations between traits. The goal is to select $M\\leq N$ individuals such that their traits are as dispersed as possible. We compute analytically the full statistics (including large deviation tails) of the maximally achievable dispersion among sub-populations of size $M$ when the traits are independent and identically distributed. Two complementary approaches are developed, one based on a mean-field theory for order statistics, and the other on the replica method from the field of disordered systems. In all dimensions $d$, and for rotationally symmetric distributions, the optimal subset for large populations consists of all points lying outside a $d$-dimensional ball whose radius is determined self-consistently. For a single trait ($d=1$), the statistics of the maximal dispersion can be tackled for finite $N,M$ as well. The formulae we obtained are corroborated by numerical simulations on small instances and by heuristic algorithms that find near-optimal solutions."}
{"id": "2602.04345", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04345", "abs": "https://arxiv.org/abs/2602.04345", "authors": ["Daria Gaidukevich"], "title": "Does the entropy of systems with larger internal entanglement grow stronger?", "comment": "27 pages, 8 figures", "summary": "It is known that when a system interacts with its environment, the entanglement contained in the system is redistributed since parts of the system entangle with the environment. On the other hand, the entanglement of a system with its environment is closely related to the entropy of the system. However, does this imply that the entropy of systems with larger internal entanglement will grow stronger? We study the issue using the simplest model as an example: a system of qubits interacts with the environment described by the quantum harmonic oscillator. The answer to the posed question is ambiguous. However, the study of the situation on average (using the simulation of a set of random states) reveals certain patterns and we can say that the answer is affirmative. At the same time, the choice of states satisfying certain conditions in some cases can change the dependence to the opposite. Additionally, we show that the entanglement depth also makes a small contribution to entropy growth."}
{"id": "2602.04656", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04656", "abs": "https://arxiv.org/abs/2602.04656", "authors": ["Yun Jiang", "Ji Wang"], "title": "Safe Adaptive Control of Parabolic PDE-ODE Cascades", "comment": null, "summary": "In this paper, we propose a safe adaptive boundary control strategy for a class of parabolic partial differential equation-ordinary differential equation (PDE-ODE) cascaded systems with parametric uncertainties in both the PDE and ODE subsystems. The proposed design is built upon an adaptive Control Barrier Function (aCBF) framework that incorporates high-relative-degree CBFs together with a batch least-squares identification (BaLSI)-based adaptive control that guarantees exact parameter identification in finite time. The proposed control law ensures that: (i) if the system output state initially lies within a prescribed safe set, safety is maintained for all time; otherwise, the output is driven back into the safe region within a preassigned finite time; and (ii) convergence to zero of all plant states is achieved. Numerical simulations are provided to demonstrate the effectiveness of the proposed approach."}
{"id": "2602.04378", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04378", "abs": "https://arxiv.org/abs/2602.04378", "authors": ["Jannis Halbey", "Daniel Deza", "Max Zimmer", "Christophe Roux", "Bartolomeo Stellato", "Sebastian Pokutta"], "title": "Lower Bounds for Frank-Wolfe on Strongly Convex Sets", "comment": null, "summary": "We present a constructive lower bound of $Ω(1/\\sqrt{\\varepsilon})$ for Frank-Wolfe (FW) when both the objective and the constraint set are smooth and strongly convex, showing that the known uniform $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ guarantees in this regime are tight. It is known that under additional assumptions on the position of the optimizer, FW can converge linearly. However, it remained unclear whether strong convexity of the set can yield rates uniformly faster than $\\mathcal{O}(1/\\sqrt{\\varepsilon})$, i.e., irrespective of the position of the optimizer. To investigate this question, we focus on a simple yet representative problem class: minimizing a strongly convex quadratic over the Euclidean unit ball, with the optimizer on the boundary. We analyze the dynamics of FW for this problem in detail and develop a novel computational approach to construct worst-case FW trajectories, which is of independent interest. Guided by these constructions, we develop an analytical proof establishing the lower bound."}
{"id": "2602.04656", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04656", "abs": "https://arxiv.org/abs/2602.04656", "authors": ["Yun Jiang", "Ji Wang"], "title": "Safe Adaptive Control of Parabolic PDE-ODE Cascades", "comment": null, "summary": "In this paper, we propose a safe adaptive boundary control strategy for a class of parabolic partial differential equation-ordinary differential equation (PDE-ODE) cascaded systems with parametric uncertainties in both the PDE and ODE subsystems. The proposed design is built upon an adaptive Control Barrier Function (aCBF) framework that incorporates high-relative-degree CBFs together with a batch least-squares identification (BaLSI)-based adaptive control that guarantees exact parameter identification in finite time. The proposed control law ensures that: (i) if the system output state initially lies within a prescribed safe set, safety is maintained for all time; otherwise, the output is driven back into the safe region within a preassigned finite time; and (ii) convergence to zero of all plant states is achieved. Numerical simulations are provided to demonstrate the effectiveness of the proposed approach."}
{"id": "2602.04400", "categories": ["stat.ME", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04400", "abs": "https://arxiv.org/abs/2602.04400", "authors": ["F. A. Shiha"], "title": "Unit Shiha Distribution and its Applications to Engineering and Medical Data", "comment": null, "summary": "There is a growing need for flexible statistical distributions that can accurately model data defined on the unit interval. This paper introduces a new unit distribution, termed the unit Shiha (USh) distribution, which is derived from the original Shiha (Sh) distribution through an inverse exponential transformation. The probability density function of the USh distribution is sufficiently flexible to model both left- and right-skewed data, while its hazard rate function is capable of capturing various failure-rate patterns, including increasing, bathtub-shaped, and J-shaped forms. Several statistical properties of the proposed distribution are investigated, including moments and related measures, the quantile function, entropy, and stress-strength reliability. Parameter estimation is carried out using the maximum likelihood method, and its performance is evaluated through a simulation study. The practical usefulness of the USh distribution is demonstrated using four real-life data sets, and its performance is compared with several well-known competing unit distributions. The comparative results indicate that the proposed model fits the data better than the competitive models applied in this study."}
{"id": "2602.04603", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.04603", "abs": "https://arxiv.org/abs/2602.04603", "authors": ["Abdessadek Rifqui", "Ahmed Ratnani", "Stefano Serra-Capizzano"], "title": "Domain decomposition methods and preconditioning strategies using generalized locally Toepltiz tools: proposals, analysis, and numerical validation", "comment": null, "summary": "In the current work we present a spectral analysis of the additive and multiplicative Schwarz methods within the framework of domain decomposition techniques, by investigating the spectral properties of these classical Schwarz preconditioning matrix-sequences, with emphasis on their convergence behavior and on the effect of transmission operators. In particular, after a general presentation of various options, we focus on restricted variants of the Schwarz methods aimed at improving parallel efficiency, while preserving their convergence features. In order to rigorously describe and analyze the convergence behavior, we employ the theory of generalized locally Toeplitz (GLT) sequences, which provides a robust framework for studying the asymptotic spectral distribution of the discretized operators arising from Schwarz iterations. By associating each operator sequence with the appropriate GLT symbol, we derive explicit expressions for the GLT symbols of the convergence factors, for both additive and multiplicative Schwarz methods. The GLT-based spectral approach offers a unified and systematic understanding of how the spectrum evolves with mesh refinement and overlap size (in the algebraic case). Our analysis not only deepens the theoretical understanding of classical Schwarz methods, but also establishes a foundation for examining future restricted or hybrid Schwarz variants using symbolic spectral tools. These results enable the prediction of the remarkable efficiency of block Jacobi/Gauss--Seidel and block additive/multiplicative Schwarz preconditioners for GLT sequences, as further illustrated through a wide choice of numerical experiments."}
{"id": "2602.04818", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04818", "abs": "https://arxiv.org/abs/2602.04818", "authors": ["Bishnu Bhowmik", "Sayantan Mitra", "Robert M. Ziff", "Ankur Sensharma"], "title": "Site and bond percolation in linearly distorted triangular and square lattices", "comment": "13 pages, 13 figures", "summary": "We investigate site and bond percolation in triangular and square lattices subjected to linear distortion. In contrast to previously studied distortion schemes that preserve lattice geometry, linear distortion dislocates regular lattice sites along a fixed direction. Nearest-neighbors of a regular lattice need to satisfy a distance-based connection criterion to remain neighbors in the linearly distorted lattice. Using extensive Monte Carlo simulations and finite-size scaling analyses, we examine how site and bond percolation thresholds vary with the distortion parameter and the connection threshold. For triangular lattices, we observe pronounced directional dependence of both site and bond percolation thresholds, as well as of the critical connection threshold. This arises from the distortion-induced anisotropic modification of nearest-neighbor separations. In particular, bond percolation exhibits nontrivial behavior that cannot be explained solely in terms of changes in the average coordination number. In contrast, square lattices remain effectively isotropic under linear distortion, resulting in identical percolation thresholds for distortions applied along different directions. Percolation thresholds in the thermodynamic limit, evaluated for a selected set of values of distortion parameter and connection threshold, confirm that the results for large finite lattices provide reliable estimates of the infinite-system behavior."}
{"id": "2602.04350", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04350", "abs": "https://arxiv.org/abs/2602.04350", "authors": ["Chiara Vercellino", "Giacomo Vitali", "Paolo Viviani", "Alberto Scionti", "Olivier Terzo", "Bartolomeo Montrucchio", "Pascal Jahan Elahi", "Ugo Varetto"], "title": "Quantum-Assisted Design of Space-Terrestrial Integrated Networks", "comment": "16 pages, 6 figures, 2 tables", "summary": "Achieving ubiquitous global connectivity requires integrating satellite and terrestrial networks, particularly to serve remote and underserved regions. In this work, we investigate the design and optimization of Space-Terrestrial Integrated Networks (STINs) using a hybrid quantum-classical approach. We formalize three key combinatorial optimization problems: the Satellite Selection Problem (SSP), the Gateway Selection Problem (GSP), and the Spectrum Assignment Problem (SAP), each capturing critical aspects of network deployment and operation. Leveraging neutral-atom quantum processors, we map the SSP onto a Maximum Weight Independent Set problem, embedding it onto the Aquila platform and solving it via the Quantum Adiabatic Algorithm (QAA). Postprocessing ensures feasible solutions that guide downstream GSP and SAP optimization. Benchmarking across 165 realistic remote regions shows that QAA solutions closely match classical exact solvers and outperform greedy heuristics, while subsequent GSP and SAP outcomes remain largely robust to differences in initial satellite selection. These results demonstrate that quantum optimization achieves performance broadly comparable to classical approaches for end-to-end STIN design, with rare instances where it can even surpass state-of-the-art solvers. This suggests that, while not yet consistently superior, quantum methods may offer competitive advantages for larger or more complex instances of the underlying combinatorial subproblems."}
{"id": "2602.04744", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04744", "abs": "https://arxiv.org/abs/2602.04744", "authors": ["Joscha F. Bongard", "Valentin L. Krieger", "Boris Lohmann"], "title": "Dynamic Constraint Tightening for Nonlinear MPC for Autonomous Racing via Contraction Analysis", "comment": "8 pages", "summary": "This work develops a robust nonlinear Model Predictive Control (MPC) framework for path tracking in autonomous vehicles operating at the limits of handling utilizing a Control Contraction Metric (CCM) derived from a perturbed dynamic single track model. We first present a nonlinear MPC scheme for autonomous vehicles. Building on this nominal scheme, we assume limited uncertainty in tire parameters as well as bounded force disturbances in both lateral and longitudinal directions. By simplifying the perturbed model, we optimize a CCM for the uncertain model, which is validated through simulations at the dynamic limits of vehicle performance. This CCM is subsequently employed to parameterize a homothetic tube used for constraint tightening within the MPC formulation. The resulting robust nonlinear MPC is computationally more efficient than competing methods, as it introduces only a single additional state variable into the prediction model compared to the nominal scheme. Simulation results demonstrate that the homothetic tube expands most significantly in regions where the nominal scheme would otherwise violate constraints, illustrating its ability to capture all uncertain trajectories while avoiding unnecessary conservatism."}
{"id": "2602.04479", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04479", "abs": "https://arxiv.org/abs/2602.04479", "authors": ["Demyan Yarmoshik", "Nhat Trung Nguyen", "Alexander Rogozin", "Alexander Gasnikov"], "title": "Decentralized Optimization with Mixed Affine Constraints", "comment": null, "summary": "This paper considers decentralized optimization of convex functions with mixed affine equality constraints involving both local and global variables. Constraints on global variables may vary across different nodes in the network, while local variables are subject to coupled and node-specific constraints. Such problem formulations arise in machine learning applications, including federated learning and multi-task learning, as well as in resource allocation and distributed control. We analyze this problem under smooth and non-smooth assumptions, considering both strongly convex and general convex objective functions. Our main contribution is an optimal algorithm for the smooth, strongly convex regime, whose convergence rate matches established lower complexity bounds. We further provide near-optimal methods for the remaining cases."}
{"id": "2602.04744", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04744", "abs": "https://arxiv.org/abs/2602.04744", "authors": ["Joscha F. Bongard", "Valentin L. Krieger", "Boris Lohmann"], "title": "Dynamic Constraint Tightening for Nonlinear MPC for Autonomous Racing via Contraction Analysis", "comment": "8 pages", "summary": "This work develops a robust nonlinear Model Predictive Control (MPC) framework for path tracking in autonomous vehicles operating at the limits of handling utilizing a Control Contraction Metric (CCM) derived from a perturbed dynamic single track model. We first present a nonlinear MPC scheme for autonomous vehicles. Building on this nominal scheme, we assume limited uncertainty in tire parameters as well as bounded force disturbances in both lateral and longitudinal directions. By simplifying the perturbed model, we optimize a CCM for the uncertain model, which is validated through simulations at the dynamic limits of vehicle performance. This CCM is subsequently employed to parameterize a homothetic tube used for constraint tightening within the MPC formulation. The resulting robust nonlinear MPC is computationally more efficient than competing methods, as it introduces only a single additional state variable into the prediction model compared to the nominal scheme. Simulation results demonstrate that the homothetic tube expands most significantly in regions where the nominal scheme would otherwise violate constraints, illustrating its ability to capture all uncertain trajectories while avoiding unnecessary conservatism."}
{"id": "2602.04457", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04457", "abs": "https://arxiv.org/abs/2602.04457", "authors": ["Qianyi Chen", "Anpeng Wu", "Bo Li", "Lu Deng", "Yong Wang"], "title": "Journey to the Centre of Cluster: Harnessing Interior Nodes for A/B Testing under Network Interference", "comment": "ICLR 2026", "summary": "A/B testing on platforms often faces challenges from network interference, where a unit's outcome depends not only on its own treatment but also on the treatments of its network neighbors. To address this, cluster-level randomization has become standard, enabling the use of network-aware estimators. These estimators typically trim the data to retain only a subset of informative units, achieving low bias under suitable conditions but often suffering from high variance. In this paper, we first demonstrate that the interior nodes - units whose neighbors all lie within the same cluster - constitute the vast majority of the post-trimming subpopulation. In light of this, we propose directly averaging over the interior nodes to construct the mean-in-interior (MII) estimator, which circumvents the delicate reweighting required by existing network-aware estimators and substantially reduces variance in classical settings. However, we show that interior nodes are often not representative of the full population, particularly in terms of network-dependent covariates, leading to notable bias. We then augment the MII estimator with a counterfactual predictor trained on the entire network, allowing us to adjust for covariate distribution shifts between the interior nodes and full population. By rearranging the expression, we reveal that our augmented MII estimator embodies an analytical form of the point estimator within prediction-powered inference framework. This insight motivates a semi-supervised lens, wherein interior nodes are treated as labeled data subject to selection bias. Extensive and challenging simulation studies demonstrate the outstanding performance of our augmented MII estimator across various settings."}
{"id": "2602.03859", "categories": ["physics.comp-ph", "math.NA", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2602.03859", "abs": "https://arxiv.org/abs/2602.03859", "authors": ["Evgeniy V. Chizhonkov", "Olga S. Rozanova"], "title": "Numerical study of loss of hyperbolicity using a cold plasma model", "comment": "16 pages, 6 figures", "summary": "We study a one-dimensional system of cold plasma equations taking into account electron-ion collisions in both relativistic and nonrelativistic cases. It is known that for a constant collision coefficient $ν$, the solution to the Cauchy problem for such a system can lose smoothness. However, if the dependence of $ν$ on the electron density $N$ is more than linear, then the solution remains globally smooth for any initial data. However, the appearance of the dependence $ν(N)$ leads to a change in the type of the system, it loses hyperbolicity, which leads to computational problems. In this paper, we propose a new implicit solution method in Euler variables that overcomes these difficulties. It can be used in both nonrelativistic and relativistic cases and is tested for the threshold case of a linear dependence $ν(N)=ν_1+ν_0 N$, when smoothness can still be lost. The computational experiments carried out are in full agreement with the available theoretical results."}
{"id": "2602.04792", "categories": ["quant-ph", "cond-mat.other", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04792", "abs": "https://arxiv.org/abs/2602.04792", "authors": ["Sukrut Mondkar", "Priya Ghosh", "Ujjwal Sen"], "title": "Dynamical Quantum Phase Transitions in Boundary Time Crystals", "comment": "10 pages, 6 figures, 3 tables", "summary": "We demonstrate the existence of a dynamical quantum phase transition (DQPT) in a dissipative collective-spin model that exhibits the boundary time crystal (BTC) phase. We initialize the system in the ground state of the Hamiltonian in either the BTC or the non-BTC phase, and drive it across the BTC transition. The driving is done by an abrupt quench or by a finite-time linear ramp of a Hamiltonian control parameter under Markovian Lindblad dynamics. We diagnose DQPTs through zeros of the fidelity-based Loschmidt echo between the initial state and the evolving mixed state, which induce nonanalytic cusp-like features in the associated rate function. For quenches into the BTC phase, the Loschmidt echo exhibits repeated zeros due to the emergent time-periodic steady state, whereas for quenches into the non-BTC phase, the overlap vanishes and remains zero once the dynamics relaxes to a stationary state. We further show that the DQPT persists under the ramp protocol followed by unitary evolution with the final Hamiltonian. Finally, we analyze the finite-size scaling of the first critical time and find convergence to a constant in the thermodynamic limit, with distinct power-law approaches for the quench and the ramp protocols."}
{"id": "2602.04351", "categories": ["quant-ph", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.04351", "abs": "https://arxiv.org/abs/2602.04351", "authors": ["Antonio Falcó", "Hermann G. Matthies"], "title": "Vistas of Algebraic Probability: Quantum Computation and Information", "comment": null, "summary": "Kolmogorov's foundation of probability takes measure spaces, $σ$-algebras, and probability measures as basic objects. It is, however, widely recognized that this classical framework is inadequate for random phenomena involving quantum effects, and more generally for \\emph{quantum-like} situations. A broader formulation is provided by an algebraic viewpoint: one starts from an algebra of random variables equipped with a distinguished linear functional -- the \\emph{state} -- interpreted as expectation. In this sense, the approach can also be viewed as a modern reading of ideas already implicit in early probability (e.g., the Bernoullis), while its contemporary form has been developed and used extensively in quantum physics.\n  The algebraic framework accommodates both classical and quantum-like behaviours, yet it remains underused in classical probability and uncertainty quantification, where it can nevertheless open new perspectives and clarify structural features. Although the language carries a physics flavor, the subject is purely probabilistic. The key distinction between classical and quantum-like behaviour is \\emph{commutativity}: its failure produces the characteristic effects of quantum-like situations. The rise of quantum computing is a prominent setting in which such behaviour may become relevant even for practitioners in computational science. Here we focus on the purely algebraic core of the approach. By restricting attention to finite-dimensional algebras, we avoid many analytical subtleties while retaining the main ideas, their classical limit, and their applicability to quantum-like models and quantum computation."}
{"id": "2602.04756", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04756", "abs": "https://arxiv.org/abs/2602.04756", "authors": ["Joscha F. Bongard", "Boris Lohmann"], "title": "Control Lyapunov Functions for Optimality in Sontag-Type Control", "comment": "6 pages", "summary": "Given a Control Lyapunov Function (CLF), Sontag's famous Formula provides a nonlinear state-feedback guaranteeing asymptotic stability of the setpoint. At the same time, a cost function that depends on the CLF is minimized. While there exist methods to construct CLFs for certain classes of systems, the impact on the resulting performance is unclear. This article aims to make two contributions to this problem: (1) We show that using the value function of an LQR design as CLF, the resulting Sontag-type controller minimizes a classical quadratic cost around the setpoint and a CLF-dependent cost within the domain where the CLF condition holds. We also show that the closed-loop system is stable within a local region at least as large as that generated by the LQR. (2) We show a related CLF design for feedback-linearizable systems resulting in a global CLF in a straight-forward manner; The Sontag design then guarantees global asymptotic stability while minimizing a quadratic cost at the setpoint and a CLF-dependent cost in the whole state-space. Both designs are constructive and easily applicable to nonlinear multi-input systems under mild assumptions."}
{"id": "2602.04551", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.04551", "abs": "https://arxiv.org/abs/2602.04551", "authors": ["Xiang Meng", "Ryan Lucas", "Rahul Mazumder"], "title": "A GPU-accelerated Nonlinear Branch-and-Bound Framework for Sparse Linear Models", "comment": null, "summary": "We study exact sparse linear regression with an $\\ell_0-\\ell_2$ penalty and develop a branch-and-bound (BnB) algorithm explicitly designed for GPU execution. Starting from a perspective reformulation, we derive an interval relaxation that can be solved by ADMM with closed-form, coordinate-wise updates. We structure these updates so that the main work at each BnB node reduces to batched matrix-vector operations with a shared data matrix, enabling fine-grained parallelism across coordinates and coarse-grained parallelism across many BnB nodes on a single GPU. Feasible solutions (upper bounds) are generated by a projected gradient method on the active support, implemented in a batched fashion so that many candidate supports are updated in parallel on the GPU. We discuss practical design choices such as memory layout, batching strategies, and load balancing across nodes that are crucial for obtaining good utilization on modern GPUs. On synthetic and real high-dimensional datasets, our GPU-based approach achieves clear runtime improvements over a CPU implementation of our method, an existing specialized BnB method, and commercial MIP solvers."}
{"id": "2602.04756", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04756", "abs": "https://arxiv.org/abs/2602.04756", "authors": ["Joscha F. Bongard", "Boris Lohmann"], "title": "Control Lyapunov Functions for Optimality in Sontag-Type Control", "comment": "6 pages", "summary": "Given a Control Lyapunov Function (CLF), Sontag's famous Formula provides a nonlinear state-feedback guaranteeing asymptotic stability of the setpoint. At the same time, a cost function that depends on the CLF is minimized. While there exist methods to construct CLFs for certain classes of systems, the impact on the resulting performance is unclear. This article aims to make two contributions to this problem: (1) We show that using the value function of an LQR design as CLF, the resulting Sontag-type controller minimizes a classical quadratic cost around the setpoint and a CLF-dependent cost within the domain where the CLF condition holds. We also show that the closed-loop system is stable within a local region at least as large as that generated by the LQR. (2) We show a related CLF design for feedback-linearizable systems resulting in a global CLF in a straight-forward manner; The Sontag design then guarantees global asymptotic stability while minimizing a quadratic cost at the setpoint and a CLF-dependent cost in the whole state-space. Both designs are constructive and easily applicable to nonlinear multi-input systems under mild assumptions."}
{"id": "2602.04594", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04594", "abs": "https://arxiv.org/abs/2602.04594", "authors": ["Wen Zhang", "Liping Zhu", "Songshan Yang"], "title": "Distributed Convoluted Rank Regression for Non-Shareable Data under Non-Additive Losses", "comment": null, "summary": "We study high-dimensional rank regression when data are distributed across multiple machines and the loss is a non-additive U-statistic, as in convoluted rank regression (CRR). Classical communication-efficient surrogate likelihood (CSL) methods crucially rely on the additivity of the empirical loss and therefore break down for CRR, whose global loss couples all sample pairs across machines. We propose a distributed convoluted rank regression (DCRR) framework that constructs a similar surrogate loss and demonstrate its validity under the non-additive losses. We show that this surrogate shares the same population minimizer as the full-data CRR loss and yields estimators that are statistically equivalent to centralized CRR. Building on this, we develop a two-stage sparse DCRR procedure -- an iterative $\\ell_1$-penalized stage followed by a folded-concave refinement -- and establish non-asymptotic error bounds, a distributed strong oracle property, and a DHBIC-type criterion for consistent model selection. A scaling result shows that the number of machines may diverge as $M = o({N/(s^2\\log p)})$ while achieving centralized oracle rates with only $O(\\log N)$ communication rounds. Simulations and a large-scale real data example demonstrate substantial gains over naive divide-and-conquer, particularly under heavy-tailed errors."}
{"id": "2602.04366", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04366", "abs": "https://arxiv.org/abs/2602.04366", "authors": ["A. García-Velo", "R. Puebla", "Y. Ban", "E. Torrontegui", "M. Paraschiv"], "title": "Low resource entanglement classification from neural network interpretability", "comment": "12 pages, 10 figures + 4 appendices (13 pages, 17 figures, 6 tables)", "summary": "Entanglement is a central resource in quantum information and quantum technologies, yet its characterization remains challenging due to both theoretical complexity and measurement requirements. Machine learning has emerged as a promising alternative, enabling entanglement characterization from incomplete measurement data, however model interpretability remains a challenge. In this work, we introduce a unified and interpretable framework for SLOCC entanglement classification of two- and three-qubit states, encompassing both pure and mixed states. We train dense and convolutional neural networks on Pauli-measurement outcomes, provide design guidelines for each architecture, and systematically compare their performance across types of states. To interpret the models, we compute Shapley values to quantify the contribution of each measurement, analyze measurement-importance patterns across different systems, and use these insights to guide a measurement-reduction scheme. Accuracy-versus-measurement curves and comparisons with analytical entanglement criteria demonstrate the minimal resources required for reliable classification and highlight both the capabilities and limitations of Shapley-based interpretability when using machine learning models for entanglement detection and classification."}
{"id": "2602.04801", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04801", "abs": "https://arxiv.org/abs/2602.04801", "authors": ["Lamberto Vazquez-Soqui", "Fatima Oliva-Palomo", "Diego Mercado-Ravell", "Pedro Castillo"], "title": "SQP-Based Cable-Tension Allocation for Multi-Drone Load Transport", "comment": null, "summary": "Multi-Agent Aerial Load Transport Systems (MAATS) offer greater payload capacity and fault tolerance than single-drone solutions. However, they have an underdetermined tension allocation problem that leads to uneven energy distribution, cable slack, or collisions between drones and cables. This paper presents a real-time optimization layer that improves a hierarchical load-position-attitude controller by incorporating a Sequential Quadratic Programming (SQP) algorithm. The SQP formulation minimizes the sum of squared cable tensions while imposing a cable-alignment penalty that discourages small inter-cable angles, thereby preventing tether convergence without altering the reference trajectory. We tested the method under nominal conditions by running numerical simulations of four quadrotors. Computational experiments based on numerical simulations demonstrate that the SQP routine runs in a few milliseconds on standard hardware, indicating feasibility for real-time use. A sensitivity analysis confirms that the gain of the cable-alignment penalty can be tuned online, enabling a controllable trade-off between safety margin and energy consumption with no measurable degradation of tracking performance in simulation. This framework provides a scalable path to safe and energy-balanced cooperative load transport in practical deployments."}
{"id": "2602.04262", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04262", "abs": "https://arxiv.org/abs/2602.04262", "authors": ["Haokun Yu", "Jingyuan Zhou", "Kaidi Yang"], "title": "Parameter Privacy-Preserving Data Sharing: A Particle-Belief MDP Formulation", "comment": "17 pages, 10 figures", "summary": "This paper investigates parameter-privacy-preserving data sharing in continuous-state dynamical systems, where a data owner designs a data-sharing policy to support downstream estimation and control while preventing adversarial inference of a sensitive parameter. This data-sharing problem is formulated as an optimization problem that trades off privacy leakage and the impact of data sharing on the data owner's utility, subject to a data-usability constraint. We show that this problem admits an equivalent belief Markov decision process (MDP) formulation, which provides a simplified representation of the optimal policy. To efficiently characterize information-theoretic privacy leakage in continuous state and action spaces, we propose a particle-belief MDP formulation that tracks the parameter posterior via sequential Monte Carlo, yielding a tractable belief-state approximation that converges asymptotically as the number of particles increases. We further derive a tractable closed-form upper bound on particle-based MI via Gaussian mixture approximations, which enables efficient optimization of the particle-belief MDP. Experiments on a mixed-autonomy platoon show that the learned continuous policy substantially impedes inference attacks on human-driving behavior parameters while maintaining data usability and system performance."}
{"id": "2602.04801", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04801", "abs": "https://arxiv.org/abs/2602.04801", "authors": ["Lamberto Vazquez-Soqui", "Fatima Oliva-Palomo", "Diego Mercado-Ravell", "Pedro Castillo"], "title": "SQP-Based Cable-Tension Allocation for Multi-Drone Load Transport", "comment": null, "summary": "Multi-Agent Aerial Load Transport Systems (MAATS) offer greater payload capacity and fault tolerance than single-drone solutions. However, they have an underdetermined tension allocation problem that leads to uneven energy distribution, cable slack, or collisions between drones and cables. This paper presents a real-time optimization layer that improves a hierarchical load-position-attitude controller by incorporating a Sequential Quadratic Programming (SQP) algorithm. The SQP formulation minimizes the sum of squared cable tensions while imposing a cable-alignment penalty that discourages small inter-cable angles, thereby preventing tether convergence without altering the reference trajectory. We tested the method under nominal conditions by running numerical simulations of four quadrotors. Computational experiments based on numerical simulations demonstrate that the SQP routine runs in a few milliseconds on standard hardware, indicating feasibility for real-time use. A sensitivity analysis confirms that the gain of the cable-alignment penalty can be tuned online, enabling a controllable trade-off between safety margin and energy consumption with no measurable degradation of tracking performance in simulation. This framework provides a scalable path to safe and energy-balanced cooperative load transport in practical deployments."}
{"id": "2602.04682", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04682", "abs": "https://arxiv.org/abs/2602.04682", "authors": ["Emma G Crenshaw", "Yuhua Zhang", "Jukka-Pekka Onnela"], "title": "Covariate Selection for Joint Latent Space Modeling of Sparse Network Data", "comment": null, "summary": "Network data are increasingly common in the social sciences and infectious disease epidemiology. Analyses often link network structure to node-level covariates, but existing methods falter with sparse networks and high-dimensional node features. We propose a joint latent space modeling framework for sparse networks with high-dimensional binary node covariates that performs covariate selection while accounting for uncertainty in estimated latent positions. Building on joint latent space models that couple edges and node variables through shared latent positions, we introduce a group lasso screening step and incorporate a measurement-error-aware stabilization term to mitigate bias from using estimated latent positions as predictors. We establish prediction error rates for the covariate component both when latent positions are treated as observed and when they are estimated with bounded error; under uniform control across $q$ covariates and $n$ nodes, the rate is of order $O(\\log q / n)$ up to an additional term due to latent position estimation error. Our method addresses three challenges: (1) incorporating information from isolated nodes, which are common in sparse networks but often ignored; (2) selecting relevant covariates from high-dimensional spaces; and (3) accounting for uncertainty in estimated latent positions. Simulations show predictive performance remains stable as covariate sparsity grows, while naive approaches degrade. We illustrate how the method can support efficient study design using household social networks from 75 Indian villages, where an emulated pilot study screens a large covariate battery and substantially reduces required subsequent data collection without sacrificing network predictive accuracy."}
{"id": "2602.04370", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04370", "abs": "https://arxiv.org/abs/2602.04370", "authors": ["Rasmus Vesterager Gothelf", "Lars Bojer Madsen", "Christian Saugbjerg Lange"], "title": "Limitations of an approximative phase-space description in strong-field quantum optics", "comment": null, "summary": "In recent years, strong-field processes such as high-order harmonic generation (HHG) and above-threshold ionization driven by nonclassical states of light have become an increasingly popular field of study. The theoretical modeling of these processes often applies an approximate phase-space expansion of the nonclassical driving field in terms of coherent states, which has been shown to accurately predict the harmonic spectrum. However, its accuracy for the computation of quantum optical observables like the degree of squeezing and photon statistics has not been thoroughly considered. In this work, we introduce this approximative phase-space description and discuss its accuracy, and we find that it mischaracterizes the quantum optical properties of the driving laser by making it an incoherent mixture of classical states. We further show that this error in the driving field description maps onto the light emitted from HHG, as neither sub-Poissonian photon statistics nor quadrature squeezing below vacuum fluctuations can be captured by the approximative phase-space description. Lastly, to benchmark the approximative phase-space description, we consider the quantum HHG from a one-band model, which yields an exact analytical solution. Using the approximative phase-space representation with this specific model, we find a small quantitative error in the quadrature variance of the emitted field that scales with pulse duration and emitter density. Our results show that using this approximative phase-space description can mischaracterize quantum optical observables. Attributing physical meaning to such results should therefore be accompanied by a quantitative analysis of the error."}
{"id": "2602.04219", "categories": ["math.OC", "eess.SY", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2602.04219", "abs": "https://arxiv.org/abs/2602.04219", "authors": ["Chung-Han Hsieh"], "title": "Sampled-Data Wasserstein Distributionally Robust Control of Multiplicative Systems: A Convex Relaxation with Performance Guarantees", "comment": "Submitted for possible publication", "summary": "This paper investigates the robust optimal control of sampled-data stochastic systems with multiplicative noise and distributional ambiguity. We consider a class of discrete-time optimal control problems where the controller \\emph{jointly} selects a feedback policy and a sampling period to maximize the worst-case expected concave utility of the inter-sample growth factor. Modeling uncertainty via a Wasserstein ambiguity set, we confront the structural obstacle of~``concave-max'' geometry arising from maximizing a concave utility against an adversarial distribution. Unlike standard convex loss minimization, the dual reformulation here requires a minimax interchange within the semi-infinite constraints, where the utility's concavity precludes exact strong duality. To address this, we utilize a general minimax inequality to derive a tractable convex relaxation. Our approach yields a rigorous lower bound that functions as a probabilistic performance guarantee. We establish an explicit, non-asymptotic bound on the resulting duality gap, proving that the approximation error is uniformly controlled by the Lipschitz-smoothness of the stage reward and the diameter of the disturbance support. Furthermore, we introduce necessary and sufficient conditions for \\emph{robust viability}, ensuring state positivity invariance across the entire ambiguity set. Finally, we bridge the gap between static optimization and dynamic performance, proving that the optimal value of the relaxation serves as a rigorous deterministic floor for the asymptotic average utility rate almost surely. The framework is illustrated on a log-optimal portfolio control problem, which serves as a canonical instance of multiplicative stochastic control."}
{"id": "2602.04568", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04568", "abs": "https://arxiv.org/abs/2602.04568", "authors": ["Axel Stafström", "Daniel Arnström", "Adam Miksits", "David Umsonst"], "title": "Peak Bounds for the Estimation Error under Sensor Attacks", "comment": "7 pages, 3 figures, accepted at the American Control Conference 2026", "summary": "This paper investigates bounds on the estimation error of a linear system affected by norm-bounded disturbances and full sensor attacks. The system is equipped with a detector that evaluates the norm of the innovation signal to detect faults, and the attacker wants to avoid detection. We utilize induced $L_\\infty$ system norms, also called \\emph{peak-to-peak} norms, to compare the estimation error bounds under nominal operations and under attack. This leads to a sufficient condition for when the bound on the estimation error is smaller during an attack than during nominal operation. This condition is independent of the attack strategy and depends only on the attacker's desire to remain undetected and (indirectly) the observer gain. Therefore, we investigate both an observer design method, that seeks to reduce the error bound under attack while keeping the nominal error bound low, and detector threshold tuning. As a numerical illustration, we show how a sensor attack can deactivate a robust safety filter based on control barrier functions if the attacked error bound is larger than the nominal one. We also statistically evaluate our observer design method and the effect of the detector threshold."}
{"id": "2602.04219", "categories": ["math.OC", "eess.SY", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2602.04219", "abs": "https://arxiv.org/abs/2602.04219", "authors": ["Chung-Han Hsieh"], "title": "Sampled-Data Wasserstein Distributionally Robust Control of Multiplicative Systems: A Convex Relaxation with Performance Guarantees", "comment": "Submitted for possible publication", "summary": "This paper investigates the robust optimal control of sampled-data stochastic systems with multiplicative noise and distributional ambiguity. We consider a class of discrete-time optimal control problems where the controller \\emph{jointly} selects a feedback policy and a sampling period to maximize the worst-case expected concave utility of the inter-sample growth factor. Modeling uncertainty via a Wasserstein ambiguity set, we confront the structural obstacle of~``concave-max'' geometry arising from maximizing a concave utility against an adversarial distribution. Unlike standard convex loss minimization, the dual reformulation here requires a minimax interchange within the semi-infinite constraints, where the utility's concavity precludes exact strong duality. To address this, we utilize a general minimax inequality to derive a tractable convex relaxation. Our approach yields a rigorous lower bound that functions as a probabilistic performance guarantee. We establish an explicit, non-asymptotic bound on the resulting duality gap, proving that the approximation error is uniformly controlled by the Lipschitz-smoothness of the stage reward and the diameter of the disturbance support. Furthermore, we introduce necessary and sufficient conditions for \\emph{robust viability}, ensuring state positivity invariance across the entire ambiguity set. Finally, we bridge the gap between static optimization and dynamic performance, proving that the optimal value of the relaxation serves as a rigorous deterministic floor for the asymptotic average utility rate almost surely. The framework is illustrated on a log-optimal portfolio control problem, which serves as a canonical instance of multiplicative stochastic control."}
{"id": "2602.04691", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04691", "abs": "https://arxiv.org/abs/2602.04691", "authors": ["Subhodeep Dey", "Gopal K. Basak", "Samarjit Das"], "title": "Linear Regression: Inference Based on Cluster Estimates", "comment": null, "summary": "This article proposes a novel estimator for regression coefficients in clustered data that explicitly accounts for within-cluster dependence. We study the asymptotic properties of the proposed estimator under both finite and infinite cluster sizes. The analysis is then extended to a standard random coefficient model, where we derive asymptotic results for the average (common) parameters and develop a Wald-type test for general linear hypotheses. We also investigate the performance of the conventional pooled ordinary least squares (POLS) estimator within the random coefficients framework and show that it can be unreliable across a wide range of empirically relevant settings. Furthermore, we introduce a new test for parameter stability at a higher (superblock; Tier 2, Tier 3,...) level, assuming that parameters are stable across clusters within that level. Extensive simulation studies demonstrate the effectiveness of the proposed tests, and an empirical application illustrates their practical relevance."}
{"id": "2602.04394", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04394", "abs": "https://arxiv.org/abs/2602.04394", "authors": ["Michal Natan", "Saar Levin", "Avi Pe'er"], "title": "Squeezing Enhanced Sagnac Sensing based on SU(1,1) Quantum Interference", "comment": null, "summary": "We present a simple and robust design for a squeezing-enhanced Sagnac interferometer that employs the concept of SU(1,1) interference to significantly surpass the classical sensitivity limit (shot-noise limit - SNL) in rotational sensing. By strategically placing an optical parametric amplifier (OPA) inside the Sagnac loop, light is automatically squeezed in both forward and backward directions of the loop, which enhances the detectability of a small phase. For measuring the squeezed quadrature, we explore two approaches: Direct detection of the output intensity, which is simple, but requires a high-efficiency photo-detector; and parametric homodyne with an additional OPA, which accepts practical detectors with no efficiency limitation, but is technically more complex. Our analysis demonstrates super-classical sensitivity under most realistic conditions of loss and detector inefficiency, thereby leveraging the resources of squeezing and the principles of SU(1,1) interference, while maintaining compatibility with standard Sagnac configurations."}
{"id": "2602.04310", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04310", "abs": "https://arxiv.org/abs/2602.04310", "authors": ["Léa Ninite", "Adrien Banse", "Guillaume O. Berger", "Raphaël M. Jungers"], "title": "A Path-Complete Approach for Optimal Control of Switched Systems", "comment": null, "summary": "We study the problem of estimating the value function of discrete-time switched systems under arbitrary switching. Unlike the switched LQR problem, where both inputs and mode sequences are optimized, we consider the case where switching is exogenous. For such systems, the number of possible mode sequences grows exponentially with time, making the exact computation of the value function intractable. This motivates the development of tractable bounds that approximate it. We propose a novel framework, based on path-complete graphs, for constructing computable upper bounds on the value function. In this framework, multiple quadratic functions are combined through a directed graph that encodes dynamic programming inequalities, yielding convex and sound formulations. For example, for switched linear systems with quadratic cost, we derive tractable LMI-based formulations and provide computational complexity bounds. We further establish approximation guarantees for the upper bounds and show asymptotic non-conservativeness using concepts from graph theory. Finally, we extend the approach to controller synthesis for systems with affine control inputs and demonstrate its effectiveness on numerical examples."}
{"id": "2602.04310", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04310", "abs": "https://arxiv.org/abs/2602.04310", "authors": ["Léa Ninite", "Adrien Banse", "Guillaume O. Berger", "Raphaël M. Jungers"], "title": "A Path-Complete Approach for Optimal Control of Switched Systems", "comment": null, "summary": "We study the problem of estimating the value function of discrete-time switched systems under arbitrary switching. Unlike the switched LQR problem, where both inputs and mode sequences are optimized, we consider the case where switching is exogenous. For such systems, the number of possible mode sequences grows exponentially with time, making the exact computation of the value function intractable. This motivates the development of tractable bounds that approximate it. We propose a novel framework, based on path-complete graphs, for constructing computable upper bounds on the value function. In this framework, multiple quadratic functions are combined through a directed graph that encodes dynamic programming inequalities, yielding convex and sound formulations. For example, for switched linear systems with quadratic cost, we derive tractable LMI-based formulations and provide computational complexity bounds. We further establish approximation guarantees for the upper bounds and show asymptotic non-conservativeness using concepts from graph theory. Finally, we extend the approach to controller synthesis for systems with affine control inputs and demonstrate its effectiveness on numerical examples."}
{"id": "2602.04788", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.04788", "abs": "https://arxiv.org/abs/2602.04788", "authors": ["Louise Alamichel", "Julyan Arbel", "Guillaume Kon Kam King", "Igor Prünster"], "title": "Species Sensitivity Distribution revisited: a Bayesian nonparametric approach", "comment": null, "summary": "We present a novel approach to ecological risk assessment by recasting the Species Sensitivity Distribution (SSD) method within a Bayesian nonparametric (BNP) framework. Widely mandated by environmental regulatory bodies globally, SSD has faced criticism due to its historical reliance on parametric assumptions when modeling species variability. By adopting nonparametric mixture models, we address this limitation, establishing a statistically robust foundation for SSD. Our BNP approach offers several advantages, including its efficacy in handling small datasets or censored data, which are common in ecological risk assessment, and its ability to provide principled uncertainty quantification alongside simultaneous density estimation and clustering. We utilize a specific nonparametric prior as the mixing measure, chosen for its robust clustering properties, a crucial consideration given the lack of strong prior beliefs about the number of components. Through simulation studies and analysis of real datasets, we demonstrate the superiority of our BNP-SSD over classical SSD methods. We also provide a BNP-SSD Shiny application, making our methodology available to the Ecotoxicology community. Moreover, we exploit the inherent clustering structure of the mixture model to explore patterns in species sensitivity. Our findings underscore the effectiveness of the proposed approach in improving ecological risk assessment methodologies."}
{"id": "2602.04443", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04443", "abs": "https://arxiv.org/abs/2602.04443", "authors": ["Mourad Halla"], "title": "Qudit Twisted-Torus Codes in the Bivariate Bicycle Framework", "comment": null, "summary": "We study finite-length qudit quantum low-density parity-check (LDPC) codes from translation-invariant CSS constructions on two-dimensional tori with twisted boundary conditions. Recent qubit work [PRX Quantum 6, 020357 (2025)] showed that, within the bivariate-bicycle viewpoint, twisting generalized toric patterns can significantly improve finite-size performance as measured by $k d^{2}/n$. Here $n$ denotes the number of physical qudits, $k$ the number of logical qudits, and $d$ the code distance. Building on this insight, we extend the search to qudit codes over finite fields. Using algebraic methods, we compute the number of logical qudits and identify compact codes with favorable rate--distance tradeoffs. Overall, for the finite sizes explored, twisted-torus qudit constructions typically achieve larger distances than their untwisted counterparts and outperform previously reported twisted qubit instances. The best new codes are tabulated."}
{"id": "2602.04831", "categories": ["quant-ph", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04831", "abs": "https://arxiv.org/abs/2602.04831", "authors": ["Hiu Yung Wong"], "title": "Review of Superconducting Qubit Devices and Their Large-Scale Integration", "comment": null, "summary": "The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration."}
{"id": "2602.04831", "categories": ["quant-ph", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04831", "abs": "https://arxiv.org/abs/2602.04831", "authors": ["Hiu Yung Wong"], "title": "Review of Superconducting Qubit Devices and Their Large-Scale Integration", "comment": null, "summary": "The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration."}
{"id": "2602.04798", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04798", "abs": "https://arxiv.org/abs/2602.04798", "authors": ["Wenbin Zhou", "Liyan Xie", "Shixiang Zhu"], "title": "Score-Based Change-Point Detection and Region Localization for Spatio-Temporal Point Processes", "comment": null, "summary": "We study sequential change-point detection for spatio-temporal point processes, where actionable detection requires not only identifying when a distributional change occurs but also localizing where it manifests in space. While classical quickest change detection methods provide strong guarantees on detection delay and false-alarm rates, existing approaches for point-process data predominantly focus on temporal changes and do not explicitly infer affected spatial regions. We propose a likelihood-free, score-based detection framework that jointly estimates the change time and the change region in continuous space-time without assuming parametric knowledge of the pre- or post-change dynamics. The method leverages a localized and conditionally weighted Hyvärinen score to quantify event-level deviations from nominal behavior and aggregates these scores using a spatio-temporal CUSUM-type statistic over a prescribed class of spatial regions. Operating sequentially, the procedure outputs both a stopping time and an estimated change region, enabling real-time detection with spatial interpretability. We establish theoretical guarantees on false-alarm control, detection delay, and spatial localization accuracy, and demonstrate the effectiveness of the proposed approach through simulations and real-world spatio-temporal event data."}
{"id": "2602.04444", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04444", "abs": "https://arxiv.org/abs/2602.04444", "authors": ["G. A. Bochkin", "E. B. Fel'dman", "E. I. Kuznetsova", "E. I. Shipulya"], "title": "Influence of environment on quantum correlations in two-spin systems with dipole-dipole interactions", "comment": "12 pages, 4 figures", "summary": "An influence of environment on quantum correlations (entanglement and quantum discord) is studied in a two-spin-1/2 system with dipole-dipole interactions on the basis of Lindblad equation. We consider the simplest case when the environment causes only dephasing of system spins. The dependencies of entanglement and the quantum discord on the relaxation rate are obtained. We compare the influence of the environment on entanglement and quantum discord."}
{"id": "2602.04855", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04855", "abs": "https://arxiv.org/abs/2602.04855", "authors": ["Suchismita Roy", "Alexander A. Fisher", "Jason Xu"], "title": "Marginal Likelihood Inference for Fitting Dynamical Survival Analysis Models to Epidemic Count Data", "comment": "25 pages, 2 figures and 6 tables", "summary": "Stochastic compartmental models are prevalent tools for describing disease spread, but inference under these models is challenging for many types of surveillance data when the marginal likelihood function becomes intractable due to missing information. To address this, we develop a closed-form likelihood for discretely observed incidence count data under the dynamical survival analysis (DSA) paradigm. The method approximates the stochastic population-level hazard by a large population limit while retaining a count-valued stochastic model, and leads to survival analytic inferential strategies that are both computationally efficient and flexible to model generalizations. Through simulation, we show that parameter estimation is competitive with recent exact but computationally expensive likelihood-based methods in partially observed settings. Previous work has shown that the DSA approximation is generalizable, and we show that the inferential developments here also carry over to models featuring individual heterogeneity, such as frailty models. We consider case studies of both Ebola and COVID-19 data on variants of the model, including a network-based epidemic model and a model with distributions over susceptibility, demonstrating its flexibility and practical utility on real, partially observed datasets."}
{"id": "2602.04480", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04480", "abs": "https://arxiv.org/abs/2602.04480", "authors": ["JunDong Zhong", "ZhaoMing Wang"], "title": "Optimal Control Design Guided by Adam Algorithm and LSTM-Predicted Open Quantum System Dynamics", "comment": null, "summary": "The realization of high-fidelity quantum control is crucial for quantum information processing, particularly in noisy environments where control strategies must simultaneously achieve precise manipulation and effective noise suppression. Conventional optimal control designs typically requires numerical calculations of the system dynamics. Recent studies have demonstrated that long short-term memory neural networks (LSTM-NNs) can accurately predict the time evolution of open quantum systems. Based on LSTM-NN predicted dynamics, we propose an optimal control framework for rapid and efficient optimal control design in open quantum systems. As an exemplary example, we apply our scheme to design an optimal control for the adiabatic speedup in a two-level system under a non-Markovian environment. Our optimization procedure entails two steps: driving trajectory optimization and zero-area pulse optimization. Fidelity improvement for both steps have been obtained, showing the effectiveness of the scheme. Our optimal control design scheme utilizes predicted dynamics to generate optimized controls, offering broad application potential in quantum computing, communication, and sensing."}
{"id": "2602.04508", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04508", "abs": "https://arxiv.org/abs/2602.04508", "authors": ["Javier Navarro", "Mateo Casariego", "Gabriel Molina-Terriza", "Íñigo Luis Egusquiza", "Mikel Sanz"], "title": "Squeezing-Enhanced Rotational Doppler Metrology", "comment": null, "summary": "A rotating surface can induce a frequency shift in incident light by changing its angular momentum, a phenomenon known as the rotational Doppler effect. This effect provides a means to estimate the angular velocity of the rotating surface. In this work, we develop a continuous-variable quantum protocol for estimating the angular velocity of a rotating surface via the rotational Doppler effect. Our approach exploits squeezed and displaced Laguerre-Gaussian modes as quantum resources, which interact with a rotating metallic disc with surface roughness. The frequency shift induced by the rotational Doppler effect is then measured using a homodyne detection scheme. By analyzing the Fisher information, we demonstrate that the proposed squeezing-enhanced protocol achieves Heisenberg scaling in the ideal noiseless regime. Furthermore, we investigate the influence of noise and consider different surface models to assess their impact on the protocol's performance. While Heisenberg scaling is degraded in the presence of noise, we show that optimizing the energy allocation ratio between displacement and squeezing of the probe ensures that the quantum strategy consistently outperforms its classical counterpart."}
{"id": "2602.04524", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04524", "abs": "https://arxiv.org/abs/2602.04524", "authors": ["Eric Tesse"], "title": "A simple means for deriving quantum mechanics", "comment": "21 pages", "summary": "A type of mechanics will be presented that possesses some distinctive properties. On the one hand, its physical description & rules of operation are readily comprehensible & intuitively clear. On the other, it fully satisfies all observable predictions of non-relativistic quantum mechanics. Within it, particles exist at points in space, follow continuous, piecewise differentiable paths, and their linear momentum is equal to their mass times their velocity along their path. Yet the probabilities for position and momentum, conditioned on the state of the particle's environment, follow the rules of quantum theory. Indeed, all observable consequences of quantum theory are satisfied; particles can be entangled, have intrinsic spin, this spin is not local to the particle, particle identity can effect probabilities, and so forth. All the rules of quantum mechanics are obeyed, and all arise in a straightforward fashion. After this is established, connections will be drawn out between this type of mechanics and other types of quantum worlds; those that obey Bohmian mechanics, stochastic mechanics, the many worlds interpretation, and physical collapse. In the final section, a relativistic version of the mechanics will be presented."}
{"id": "2602.04538", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04538", "abs": "https://arxiv.org/abs/2602.04538", "authors": ["Ferdi Altintas"], "title": "Thermodynamic Cost of Regeneration in a Quantum Stirling Cycle", "comment": "Comments are welcome", "summary": "We study the regenerative quantum Stirling heat engine cycle within the standard weak-coupling, Markovian open quantum system framework. We point out that the regeneration process is not thermodynamically free in a reduced open-system description, and we treat the required work input as an explicit regeneration cost by modifying the cycle efficiency accordingly. We consider two working substances--a single spin-$1/2$ and a pair of interacting spin-$1/2$ particles--and investigate the cycle performance by taking the regeneration cost at its minimum value set by the Carnot heat-pump limit. For comparison, we also analyze the conventional Stirling cycle without regeneration under the same conditions. The super-Carnot efficiencies reported under the cost-free regeneration assumption disappear once the regeneration cost is included: the modified efficiency stays below the Carnot bound, while still remaining higher than the efficiency of the conventional Stirling cycle. For the conventional Stirling cycle, we provide a rigorous Carnot bound using quantum relative entropy, whereas for the regenerative cycle we derive a sufficient lower bound on the regeneration cost that guarantees thermodynamic consistency. Finally, we suggest three candidate quantum regenerator models for future work."}
{"id": "2602.04543", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04543", "abs": "https://arxiv.org/abs/2602.04543", "authors": ["Daria Gaidukevich"], "title": "Effect of initial intrasystem entanglement on entropy growth in generalized Jaynes-Cummings models", "comment": "9 pages, 5 figures", "summary": "We investigate how initial intrasystem entanglement influences the entropy generated in atomic systems interacting with a photonic environment in several generalizations of the Jaynes-Cummings model with two or more subsystems. Since the initial entanglement does not uniquely determine the final entropy, we focus on ensemble-averaged behavior. We consider ensembles of initial system states including pure and mixed Haar-random states, ensembles with fixed average energy or fixed mixedness, and varying initial photon numbers in the environment. In all cases, we observe a positive correlation between the initial entanglement and the entropy growth, although the fractional contribution of the initial entanglement varies. Our results emphasize the role of intrasystem correlations as a factor contributing to entropy growth in quantum informational processes."}
{"id": "2602.04550", "categories": ["quant-ph", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04550", "abs": "https://arxiv.org/abs/2602.04550", "authors": ["Cristina Butucea", "Jan Johannes", "Henning Stein"], "title": "Locally Gentle State Certification for High Dimensional Quantum Systems", "comment": null, "summary": "Standard approaches to quantum statistical inference rely on measurements that induce a collapse of the wave function, effectively consuming the quantum state to extract information. In this work, we investigate the fundamental limits of \\emph{locally-gentle} quantum state certification, where the learning algorithm is constrained to perturb the state by at most $α$ in trace norm, thereby allowing for the reuse of samples. We analyze the hypothesis testing problem of distinguishing whether an unknown state $ρ$ is equal to a reference $ρ_0$ or $ε$-far from it. We derive the minimax sample complexity for this problem, quantifying the information-theoretic price of non-destructive measurements. Specifically, by constructing explicit measurement operators, we show that the constraint of $α$-gentleness imposes a sample size penalty of $\\frac{d}{α^2}$, yielding a total sample complexity of $n = Θ(\\frac{d^3}{ε^2 α^2})$. Our results clarify the trade-off between information extraction and state disturbance, and highlight deep connections between physical measurement constraints and privacy mechanisms in quantum learning. Crucially, we find that the sample size penalty incurred by enforcing $α$-gentleness scales linearly with the Hilbert-space dimension $d$ rather than the number of parameters $d^2-1$ typical for high-dimensional private estimation."}
{"id": "2602.04552", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.04552", "abs": "https://arxiv.org/abs/2602.04552", "authors": ["Hao Xu"], "title": "Restoring Landauer's Principle for Unitarily Transformed Thermal Reservoirs", "comment": "6+2 pages", "summary": "Landauer's principle, a cornerstone of quantum information and thermodynamics, appears to be violated when the thermal reservoir is replaced by a squeezed thermal state (STS). We introduce a formal extension of the principle to such unitarily transformed thermal states. By defining an effective Hamiltonian, we rigorously establish a generalized Landauer inequality, which naturally reduces to the standard case for an ordinary thermal reservoir as a special instance. The framework further yields a consistent definition of entropy production and a proof of its non-negativity. We illustrate its utility by studying an arbitrarily moving Unruh-DeWitt detector coupled to a quantum field initially prepared in the STS. Using perturbation theory, we compute the entropy production explicitly, confirming its positivity. As a result of the symmetry breaking induced by the unitary transformation, it depends on both the proper time interval and the absolute spacetime position. Our work resolves the apparent violation of Landauer's principle with STSs. It also provides a robust tool for analyzing quantum thermodynamics in non-equilibrium and relativistic settings, with potential implications for quantum thermal machines and information protocols."}
{"id": "2602.04588", "categories": ["quant-ph", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.04588", "abs": "https://arxiv.org/abs/2602.04588", "authors": ["Francisco Ferreira da Silva", "Stephanie Wehner"], "title": "Entanglement improves coordination in distributed systems", "comment": "8 pages, 3 figures + 18 pages, 1 table appendix", "summary": "Coordination in distributed systems is often hampered by communication latency, which degrades performance. Quantum entanglement offers fundamentally stronger correlations than classically achievable without communication. Crucially, these correlations manifest instantaneously upon measurement, irrespective of the physical distance separating the systems. We investigate the application of shared entanglement to a dual-work optimization problem in a distributed system comprising two servers. The system must process both a continuously available, preemptible baseline task and incoming customer requests arriving in pairs. System performance is characterized by the trade-off between baseline task throughput and customer waiting time. We present a rigorous analytical model demonstrating that when the baseline task throughput function is strictly convex, rewarding longer uninterrupted processing periods, entanglement-assisted routing strategies achieve Pareto-superior performance compared to optimal communication-free classical strategies. We prove this advantage through queueing-theoretic analysis, non-local game formulation, and computational certification of classical bounds. Our results identify distributed scheduling and coordination as a novel application domain for near-term entanglement-based quantum networks."}
{"id": "2602.04627", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.04627", "abs": "https://arxiv.org/abs/2602.04627", "authors": ["Daniel Eyles", "Emmanuel Lassalle", "Adam Stokes", "Ahsan Nazir", "Ramón Paniagua-Domínguez"], "title": "Dicke Superradiance in Extended 2D Quantum Arrays Coupled to Metasurface Bound States in the Continuum", "comment": "14 pages, 7 figures, including supplementary material", "summary": "Dicke superradiance is a collective phenomenon where the emission from ensembles of quantum emitters is coherently enhanced beyond the sum of each emitter's independent emission. Here, we propose a platform that exploits the delocalised nature of a high-Q, non-local mode supported by a dielectric metasurface (a so-called bound-state-in-the-continuum or BIC) to induce superradiant behaviour within an extended two-dimensional array of distant quantum emitters. We show that these BIC-mediated emitter interactions can span several wavelengths, thus overcoming the traditional subwavelength separation between emitters required in free space. We further show that reaching the idealised Dicke limit is possible in this system, provided that the emitters are coupled to the BIC mode efficiently enough, as quantified through the $β$-factor. Moreover, we demonstrate its experimental viability by analysing its robustness to realistic experimental imperfections. This work puts forward optical metasurfaces supporting BICs as a physically viable platform for realising the upper limits of cooperative emission in physically extended quantum emitter arrays."}
{"id": "2602.04633", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04633", "abs": "https://arxiv.org/abs/2602.04633", "authors": ["Xuan Du Trinh", "Ismaël Septembre", "Hai-Chau Nguyen"], "title": "On the emergence of classical stochasticity", "comment": "Comments are welcome!", "summary": "We examine the logical structure of the emergence of classical stochasticity for a quantum system governed by a Pauli-type master equation. It is well-known that while such equations describe the evolution of probabilities, they do not automatically justify classical reasoning based on the assumption that the system exists in a definite state at intermediate times. On the other hand, we show that this assumption is crucial for the standard calculation of stochastic times such as the persistent time and the time of first arrivals. We then consider examples of single particles, bosons, and fermions in the so-called ultradecoherence limit to illustrate how classical stochasticity may emerge from quantum mechanics."}
{"id": "2602.04646", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04646", "abs": "https://arxiv.org/abs/2602.04646", "authors": ["Xavier Barcons Planas", "Helen M. Chrzanowski", "Janik Wolters"], "title": "Pure narrowband photon-pair generation in a monolithic cavity", "comment": null, "summary": "Photonic quantum technologies require efficient sources of pure single photons. Here we present a heralded SPDC single-photon source in a monolithic cavity optimized for high spectral and spatial purity. The source heralds single-photons at a wavelength of 1540 nm and a spectral bandwidth of 168 MHz with a maximum heralding efficiency of 84%, while keeping the multi-photon contamination below 3%. The cavity enhancement generates photons mainly in the central cavity mode with 96.2% spectral purity."}
{"id": "2602.04676", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04676", "abs": "https://arxiv.org/abs/2602.04676", "authors": ["Baptiste Anselme Martin", "Thomas Ayral"], "title": "Pre-optimization of quantum circuits, barren plateaus and classical simulability: tensor networks to unlock the variational quantum eigensolver", "comment": null, "summary": "Variational quantum algorithms are practical approaches to prepare ground states, but their potential for quantum advantage remains unclear. Here, we use differentiable 2D tensor networks (TN) to optimize parameterized quantum circuits that prepare the ground state of the transverse field Ising model (TFIM). Our method enables the preparation of states with high energy accuracy, even for large systems beyond 1D. We show that TN pre-optimization can mitigate the barren plateau issue by giving access to enhanced gradient zones that do not shrink exponentially with system size. We evaluate the classical simulation cost evaluating energies at these warm-starts, and identify regimes where quantum hardware offers better scaling than TN simulations."}
{"id": "2602.04700", "categories": ["quant-ph", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.04700", "abs": "https://arxiv.org/abs/2602.04700", "authors": ["Sebastian Alberto Grillo", "Bernardo Daniel Dávalos", "Rodney Fabian Franco Torres", "Franklin de Lima Marquezino", "Edgar López Pezoa"], "title": "Quantum Advantage in Decision Trees: A Weighted Graph and $L_1$ Norm Approach", "comment": "24 pages, 3 figures", "summary": "The analysis of the computational power of single-query quantum algorithms is important because they must extract maximal information from one oracle call, revealing fundamental limits of quantum advantage and enabling optimal, resource-efficient quantum computation. This paper proposes a formulation of single-query quantum decision trees as weighted graphs. This formulation has the advantage that it facilitates the analysis of the $L_1$ spectral norm of the algorithm output. This advantage is based on the fact that a high $L_1$ spectral norm of the output of a quantum decision tree is a necessary condition to outperform its classical counterpart. We propose heuristics for maximizing the $L_{1}$ spectral norm, show how to combine weighted graphs to generate sequences with strictly increasing norm, and present functions exhibiting exponential quantum advantage. Finally, we establish a necessary condition linking single-query quantum advantage to the asymptotic growth of measurement projector dimensions."}
{"id": "2602.04719", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04719", "abs": "https://arxiv.org/abs/2602.04719", "authors": ["Laurin E. Fischer"], "title": "Enabling large-scale digital quantum simulations with superconducting qubits", "comment": "187 pages", "summary": "Quantum computing promises to revolutionize several scientific and technological domains through fundamentally new ways of processing information. Among its most compelling applications is digital quantum simulation, where quantum computers are used to replicate the behavior of other quantum systems. This could enable the study of problems that are otherwise intractable on classical computers, transforming fields such as quantum chemistry, condensed matter physics, and materials science. Despite this potential, realizations of practical quantum advantage for relevant problems are hindered by imperfections of current devices. This also affects quantum hardware based on superconducting circuits which is among the most advanced and scalable platforms. The envisaged long-term solution of fault-tolerant quantum computers that correct their own errors remains out of reach mainly due to the associated qubit number overhead. As a result, the field has developed strategies that combine quantum and classical resources, exploit hardware-native operations, and employ error mitigation techniques to extract meaningful results from noisy data. This doctoral thesis contributes to this broader effort by exploring methods for advancing quantum simulation across the full computational stack, including hardware-level innovations, refined techniques for noise modeling and error mitigation, and algorithmic improvements enabled by efficient measurement processing."}
{"id": "2602.04721", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04721", "abs": "https://arxiv.org/abs/2602.04721", "authors": ["Hongze Ding", "Jiuqing Liang"], "title": "Ising-Induced Spectral Broadening Resolves the Relaxation Bottleneck in Superradiant Masers", "comment": "5 pages, 1 figure", "summary": "The recent observation of self-induced superradiant masing [[W. Kersten et al., Nat. Phys. 22, 158 (2026)]] revealed a collective relaxation timescale significantly slower than predicted by standard coherent transport models. Here, we elucidate the microscopic origin of this ``relaxation bottleneck.'' We show that in the high-density regime relevant to the experiment, diagonal Ising interactions -- often treated as perturbative -- generate profound inhomogeneous broadening that exceeds the intrinsic single-particle dephasing. This intense diagonal disorder suppresses resonant flip-flop exchange, effectively renormalizing the density of states available for spectral diffusion. Our parameter-free analytic theory quantitatively reproduces the experimentally observed microsecond dynamics, identifying Ising-induced broadening as the governing mechanism for energy transport in dense solid-state spin ensembles."}
{"id": "2602.04740", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04740", "abs": "https://arxiv.org/abs/2602.04740", "authors": ["Felip Pellicer", "Juan José García-Ripoll", "Alan C. Santos"], "title": "Resource-Efficient Digitized Adiabatic Quantum Factorization", "comment": "13 pages, 3 figures and 1 table. Comments are welcome", "summary": "Digitized adiabatic quantum factorization is a hybrid algorithm that exploits the advantage of digitized quantum computers to implement efficient adiabatic algorithms for factorization through gate decompositions of analog evolutions. In this paper, we harness the flexibility of digitized computers to derive a digitized adiabatic algorithm able to reduce the gate-demanding costs of implementing factorization. To this end, we propose a new approach for adiabatic factorization by encoding the solution of the problem in the kernel subspace of the problem Hamiltonian, instead of using ground-state encoding considered in the standard adiabatic factorization proposed by Peng $et$ $al$. [Phys. Rev. Lett. 101, 220405 (2008)]. Our encoding enables the design of adiabatic factorization algorithms belonging to the class of Quadratic Unconstrained Binary Optimization (QUBO) methods, instead the Polinomial Unconstrained Binary Optimization (PUBO) used by standard adiabatic factorization. We illustrate the performance of our QUBO algorithm by implementing the factorization of integers $N$ up to 8 bits. The results demonstrate a substantial improvement over the PUBO formulation, both in terms of reduced circuit complexity and increased fidelity in identifying the correct solution."}
{"id": "2602.04747", "categories": ["quant-ph", "math-ph", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2602.04747", "abs": "https://arxiv.org/abs/2602.04747", "authors": ["Bijan Bagchi", "Anindya Ghose-Choudhury"], "title": "Generalized quantum theory for accessing nonlinear systems: the case of Levinson-Smith equations", "comment": "8 pages, 2 figures", "summary": "Motivated by a recently developed generalized scheme of quantum mechanics, we touch upon connections with Levinson-Smith classes of nonlinear systems that contain as a particular case the Liénard family of differential equations. The latter, which has coefficients of odd and odd symmetry, admits a closed form solution when converted to the Abel form. Analysis of the governing condition shows that one of the nontrivial equilibrium points is stable in character. Other classes of differential equations that we encounter speak of solutions involving Jacobi elliptic functions for a certain combination of underlying parameters, while, for a different set, relevance to position-dependent mass systems is shown. In addition, an interesting off-shoot of our results is the emergence of solitonic-like solutions from the condition of the level surface in the system."}
{"id": "2602.04760", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04760", "abs": "https://arxiv.org/abs/2602.04760", "authors": ["Francois Payn", "Michele Minervini", "Davide Girolami"], "title": "Quantifying the Operational Cost of Multipartite Entanglement", "comment": "9 pages", "summary": "Multipartite entanglement determines the strength and range of interactions in many-body quantum systems. Yet, it is hard to evaluate it, due to the complex structures of quantum states. Here, we introduce a generic method to quantify the k <= N-partite entanglement of an N-particle system, by maximizing an arbitrary bipartite entanglement measure within subsystems of size up to k. The resulting classification of multipartite states captures their experimental cost: creating a k-partite entangled state requires at least k-1 two-particle entangling gates. Further, we analytically calculate the newly defined k-partite entanglement of formation, which generalizes an important bipartite entanglement measure, in several classes of states, including the W states of any dimension."}
{"id": "2602.04792", "categories": ["quant-ph", "cond-mat.other", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04792", "abs": "https://arxiv.org/abs/2602.04792", "authors": ["Sukrut Mondkar", "Priya Ghosh", "Ujjwal Sen"], "title": "Dynamical Quantum Phase Transitions in Boundary Time Crystals", "comment": "10 pages, 6 figures, 3 tables", "summary": "We demonstrate the existence of a dynamical quantum phase transition (DQPT) in a dissipative collective-spin model that exhibits the boundary time crystal (BTC) phase. We initialize the system in the ground state of the Hamiltonian in either the BTC or the non-BTC phase, and drive it across the BTC transition. The driving is done by an abrupt quench or by a finite-time linear ramp of a Hamiltonian control parameter under Markovian Lindblad dynamics. We diagnose DQPTs through zeros of the fidelity-based Loschmidt echo between the initial state and the evolving mixed state, which induce nonanalytic cusp-like features in the associated rate function. For quenches into the BTC phase, the Loschmidt echo exhibits repeated zeros due to the emergent time-periodic steady state, whereas for quenches into the non-BTC phase, the overlap vanishes and remains zero once the dynamics relaxes to a stationary state. We further show that the DQPT persists under the ramp protocol followed by unitary evolution with the final Hamiltonian. Finally, we analyze the finite-size scaling of the first critical time and find convergence to a constant in the thermodynamic limit, with distinct power-law approaches for the quench and the ramp protocols."}
{"id": "2602.04831", "categories": ["quant-ph", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04831", "abs": "https://arxiv.org/abs/2602.04831", "authors": ["Hiu Yung Wong"], "title": "Review of Superconducting Qubit Devices and Their Large-Scale Integration", "comment": null, "summary": "The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration."}
{"id": "2602.04859", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04859", "abs": "https://arxiv.org/abs/2602.04859", "authors": ["Pradeep Niroula", "Minzhao Liu", "Sivaprasad Omanakuttan", "David Amaro", "Shouvanik Chakrabarti", "Soumik Ghosh", "Zichang He", "Yuwei Jin", "Fatih Kaleoglu", "Steven Kordonowy", "Rohan Kumar", "Michael A. Perlin", "Akshay Seshadri", "Matthew Steinberg", "Joseph Sullivan", "Jacob Watkins", "Henry Yuen", "Ruslan Shaydulin"], "title": "Digital signatures with classical shadows on near-term quantum computers", "comment": null, "summary": "Quantum mechanics provides cryptographic primitives whose security is grounded in hardness assumptions independent of those underlying classical cryptography. However, existing proposals require low-noise quantum communication and long-lived quantum memory, capabilities which remain challenging to realize in practice. In this work, we introduce a quantum digital signature scheme that operates with only classical communication, using the classical shadows of states produced by random circuits as public keys. We provide theoretical and numerical evidence supporting the conjectured hardness of learning the private key (the circuit) from the public key (the shadow). A key technical ingredient enabling our scheme is an improved state-certification primitive that achieves higher noise tolerance and lower sample complexity than prior methods. We realize this certification by designing a high-rate error-detecting code tailored to our random-circuit ensemble and experimentally generating shadows for 32-qubit states using circuits with $\\geq 80$ logical ($\\geq 582$ physical) two-qubit gates, attaining 0.90 $\\pm$ 0.01 fidelity. With increased number of measurement samples, our hardware-demonstrated primitives realize a proof-of-principle quantum digital signature, demonstrating the near-term feasibility of our scheme."}
{"id": "2602.04869", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04869", "abs": "https://arxiv.org/abs/2602.04869", "authors": ["Soubhadra Maiti", "Guus Avis", "Sounak Kar", "Stephanie Wehner"], "title": "Requirements for Teleportation in an Intercity Quantum Network", "comment": "72 pages, 9 figures", "summary": "We investigate the hardware requirements for quantum teleportation in an intercity-scale network topology consisting of two metropolitan-scale networks connected via a long-distance backbone link. Specifically, we identify the minimal improvements required beyond the state-of-the-art to achieve an end-to-end expected teleportation fidelity of $2/3$, which represents the classical limit. To this end, we formulate the hardware requirements computation as optimisation problems, where the hardware parameters representing the underlying device capabilities serve as decision variables. Assuming a simplified noise model, we derive closed-form analytical expressions for the teleportation fidelity and rate when the network is realised using heterogeneous quantum hardware, including a quantum repeater chain with a memory cut-off. Our derivations are based on events defined by the order statistics of link generation durations in both the metropolitan networks and the backbone, and the resulting expressions are validated through simulations on the NetSquid platform. The analytical expressions facilitate efficient exploration of the optimisation parameter space without resorting to computationally intensive simulations. We then apply this framework to a representative realisation in which the metropolitan nodes are based on trapped-ion processors and the backbone is composed of ensemble-based quantum memories. Our results suggest that teleportation across metropolitan distances is already achievable with state-of-the-art hardware when the data qubit is prepared after end-to-end entanglement has already been established, whereas extending teleportation to intercity scales requires additional, though plausibly achievable, improvements in hardware performance."}
{"id": "2602.04878", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04878", "abs": "https://arxiv.org/abs/2602.04878", "authors": ["Manuel S. Rudolph", "Armando Angrisani", "Andrew Wright", "Iwo Sanderski", "Ricard Puig", "Zoë Holmes"], "title": "Thermal State Simulation with Pauli and Majorana Propagation", "comment": "34 pages, 5 figues", "summary": "We introduce a propagation-based approach to thermal state simulation by adapting Pauli and Majorana propagation to imaginary-time evolution in the Schrödinger picture. Our key observation is that high-temperature states can be sparse in the Pauli or Majorana bases, approaching the identity at infinite temperature. By formulating imaginary-time evolution directly in these operator bases and evolving from the maximally mixed state, we access a continuum of temperatures where the state remains efficiently representable. We provide analytic guarantees for small-coefficient truncation and Pauli-weight (Majorana-length) truncation strategies by quantifying the error growth and the impact of backflow. Large-scale numerics on the 1D J1-J2 model (energies) and the triangular-lattice Hubbard model (static correlations) validate efficiency at high temperatures."}
{"id": "2602.04484", "categories": ["physics.hist-ph", "hep-ph", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04484", "abs": "https://arxiv.org/abs/2602.04484", "authors": ["Roberto Casalbuoni", "Daniele Dominici"], "title": "From Florence to Fermions: a historical reconstruction of the origins of Fermi's statistics one hundred years later", "comment": "19 pages, 8 figures", "summary": "Aim of this paper is to retrace the path that led the young Enrico Fermi to write his paper on the statistics of an ideal monatomic gas. This discovery originated in his interest, which he had shown since his formative years, in the absolute entropy constant and in the problems he highlighted in Sommerfeld's quantization in the case of identical particle systems. The fundamental step taken by Fermi in writing his work on statistics was to apply the Exclusion Principle, formulated for electrons in an atom and which could therefore have been a pure effect due to dynamics, to a system of non-interacting particles."}
{"id": "2602.04793", "categories": ["nlin.CD", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04793", "abs": "https://arxiv.org/abs/2602.04793", "authors": ["F. J. Arranz", "F. Borondo"], "title": "Correspondence between classical and quantum resonances", "comment": null, "summary": "Bifurcations take place in molecular Hamiltonian nonlinear systems as the excitation energy increases, this leading to the appearance of different classical resonances. In this paper, we study the quantum manifestations of these classical resonances in the isomerizing system CN-Li$\\leftrightarrows$Li-CN. By using a correlation diagram of eigenenergies versus Planck constant, we show the existence of different series of avoided crossings, leading to the corresponding series of quantum resonances, which represent the quantum manifestations of the classical resonances. Moreover, the extrapolation of these series to $\\hbar=0$ unveils the correspondence between the bifurcation energy of classical resonances and the energy of the series of quantum resonances in the semiclassical limit $\\hbar\\to0$. Additionally, in order to obtain analytical expressions for our results, a semiclassical theory is developed."}
