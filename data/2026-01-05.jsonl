{"id": "2601.00011", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2601.00011", "abs": "https://arxiv.org/abs/2601.00011", "authors": ["Jiawei Du", "Yi Hong"], "title": "Ultimate Forward Rate Prediction and its Application to Bond Yield Forecasting: A Machine Learning Perspective", "comment": null, "summary": "This study focuses on forecasting the ultimate forward rate (UFR) and developing a UFRbased bond yield prediction model using data from Chinese treasury bonds and macroeconomic variables spanning from December 2009 to December 2024. The de Kort-Vellekooptype methodology is applied to estimate the UFR, incorporating the optimal turning parameter determination technique proposed in this study, which helps mitigate anomalous fluctuations. In addition, both linear and nonlinear machine learning techniques are employed to forecast the UFR and ultra-long-term bond yields. The results indicate that nonlinear machine learning models outperform their linear counterparts in forecasting accuracy. Incorporating macroeconomic variables, particularly price index-related variables, significantly improves the accuracy of predictions. Finally, a novel UFR-based bond yield forecasting model is developed, demonstrating superior performance across different bond maturities."}
{"id": "2601.00395", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2601.00395", "abs": "https://arxiv.org/abs/2601.00395", "authors": ["Kundan Mukhia", "Imran Ansari", "S R Luwang", "Md Nurujjaman"], "title": "Core-Periphery Dynamics in Market-Conditioned Financial Networks: A Conditional P-Threshold Mutual Information Approach", "comment": null, "summary": "This study investigates how financial market structure reorganizes during the COVID-19 crash using a conditional p-threshold mutual information (MI) based Minimum Spanning Tree (MST) framework. We analyze nonlinear dependencies among the largest stocks from four diverse QUAD countries: the US, Japan, Australia, and India. Crashes are identified using the Hellinger distance and Hilbert spectrum; a crash occurs when HD = mu\\_H + 2*sigma\\_H, segmenting data into pre-crash, crash, and post-crash periods. Conditional p-threshold MI filters out common market effects and applies permutation-based significance testing. Resulting validated dependencies are used to construct MST networks for comparison across periods. Networks become more integrated during the crash, with shorter path lengths, higher centrality, and lower algebraic connectivity, indicating fragility. Core-periphery structure declines, with increased periphery vulnerability, and disassortative mixing facilitates shock transmission. Post-crash networks show only partial recovery. Aftershock analysis using the Gutenberg-Richter law indicates higher relative frequency of large volatility events following the crash. Results are consistent across all markets, highlighting the conditional p-threshold MI framework for capturing nonlinear interdependencies and systemic vulnerability."}
{"id": "2601.00221", "categories": ["eess.SY", "cs.DC", "cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00221", "abs": "https://arxiv.org/abs/2601.00221", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Impact of Clustering on the Observability and Controllability of Complex Networks", "comment": "Cluster Computing Journal", "summary": "The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering."}
{"id": "2601.00221", "categories": ["eess.SY", "cs.DC", "cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00221", "abs": "https://arxiv.org/abs/2601.00221", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Impact of Clustering on the Observability and Controllability of Complex Networks", "comment": "Cluster Computing Journal", "summary": "The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering."}
{"id": "2601.00147", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00147", "abs": "https://arxiv.org/abs/2601.00147", "authors": ["Debjoy Thakur", "Soumendra N. Lahiri"], "title": "Multi-Resolution Analysis of Variable Selection for Road Safety in St. Louis and Its Neighboring Area", "comment": null, "summary": "Generally, Lasso, Adaptive Lasso, and SCAD are standard approaches in variable selection in the presence of a large number of predictors. In recent years, during intensity function estimation for spatial point processes with a diverging number of predictors, many researchers have considered these penalized methods. But we have discussed a multi-resolution perspective for the variable selection method for spatial point process data. Its advantage is twofold: it not only efficiently selects the predictors but also provides the idea of which points are liable for selecting a predictor at a specific resolution. Actually, our research is motivated by the crime and accident occurrences in St. Louis and its neighborhoods. It is more relevant to select predictors at the local level, and thus we get the idea of which set of predictors is relevant for the occurrences of crime or accident in which parts of St. Louis. We describe the simulation results to justify the accuracy of local-level variable selection during intensity function estimation."}
{"id": "2601.00220", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00220", "abs": "https://arxiv.org/abs/2601.00220", "authors": ["Bibek Saha", "Sthitadhi Roy"], "title": "Anderson localisation in spatially structured random graphs", "comment": "18 pages, 12 figures", "summary": "We study Anderson localisation on high-dimensional graphs with spatial structure induced by long-ranged but distance-dependent hopping. To this end, we introduce a class of models that interpolate between the short-range Anderson model on a random regular graph and fully connected models with statistically uniform hopping, by embedding a random regular graph into a complete graph and allowing hopping amplitudes to decay exponentially with graph distance. The competition between the exponentially growing number of neighbours with graph distance and the exponentially decaying hopping amplitude positions our models effectively as power-law hopping generalisation of the Anderson model on random regular graphs. Using a combination of numerical exact diagonalisation and analytical renormalised perturbation theory, we establish the resulting localisation phase diagram emerging from the interplay of the lengthscale associated to the hopping range and the onsite disorder strength. We find that increasing the hopping range shifts the localisation transition to stronger disorder, and that beyond a critical range the localised phase ceases to exist even at arbitrarily strong disorder. Our results indicate a direct Anderson transition between delocalised and localised phases, with no evidence for an intervening multifractal phase, for both deterministic and random hopping models. A scaling analysis based on inverse participation ratios reveals behaviour consistent with a Kosterlitz-Thouless-like transition with two-parameter scaling, in line with Anderson transitions on high-dimensional graphs. We also observe distinct critical behaviour in average and typical correlation functions, reflecting the different scaling properties of generalised inverse participation ratios."}
{"id": "2601.00265", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.00265", "abs": "https://arxiv.org/abs/2601.00265", "authors": ["Prem Talwai", "Rene Caldentey", "Avi Giloni", "Clifford Hurvich", "David Simchi-Levi", "Yichen Zhang"], "title": "Designing Information Delays in Supply Chains", "comment": null, "summary": "This paper studies how a downstream retailer in a decentralized two-tier supply chain can implicitly transmit demand information to an upstream supplier through the structure of its order stream in the absence of an explicit information-sharing mechanism. We distinguish our work from prior work by introducing the notion of information delay and by linking optimal implicit information sharing to the group delay of the retailer's ordering transfer function. We show that pure delay is strictly suboptimal, while fractional-delay mechanisms can reshape the order autocorrelation to improve supplier forecastability and reduce system-wide inventory costs. Using Hardy-space factorization, we develop a tractable family of invertible ARMA policies that approximates the theoretically optimal (but non-rational) limiting filter derived by Caldentey et al. (2025) and preserves its informational delay properties. This construction yields sharp guidance on how policy complexity, as measured by the degrees of the ARMA policies, impacts supply chain costs. We further extend the analysis to memory-constrained suppliers and characterize how the complexity of the retailer's policy should scale with the supplier's finite forecasting window, highlighting when, perhaps counterintuitively, increasing policy complexity can become counterproductive."}
{"id": "2601.00168", "categories": ["physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2601.00168", "abs": "https://arxiv.org/abs/2601.00168", "authors": ["Jonathon Sendall"], "title": "From Grounding to Stabilisation: Adequacy as a Criterion for Scientific Explanation", "comment": "14 pages", "summary": "This paper develops a process-based account of scientific explanation that reconceives grounding in terms of stabilisation. Grounding theories capture hierarchical dependence but lack criteria for when explanations remain adequate under model updates, perturbations, and theory change. Stabilisation is formally defined by a schema \\(C \\to P(I)\\), where explanatory relations are sufficient when they preserve specified relational invariants under admissible transformations. This replaces the search for ultimate foundations with operational adequacy tests indexed to measurable invariance, resolving infinite regress worries while preserving a modest scientific realism. Applications show unifying power: theory change becomes an empirical question about structural continuity; quantum measurement becomes apparatus-dependent pattern selection; the effectiveness of mathematics reflects convergence on transformation-invariant descriptions; and emergence versus reduction reduces to stability of cross-level mappings. The black hole event horizon illustrates how ontologically identical states can diverge in admissible evolution, revealing process as explanatorily fundamental. Companion work develops apparatus-dependent adequacy protocols, including pointer-basis rotation and coupling-spectra methods, turning the framework into a falsifiable research programme across quantum, thermodynamic, and relativistic domains."}
{"id": "2601.00091", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00091", "abs": "https://arxiv.org/abs/2601.00091", "authors": ["Manuel Sáenz", "Pragya Sur"], "title": "Characterizing Finite-Dimensional Posterior Marginals in High-Dimensional GLMs via Leave-One-Out", "comment": null, "summary": "We investigate Bayes posterior distributions in high-dimensional generalized linear models (GLMs) under the proportional asymptotics regime, where the number of features and samples diverge at a comparable rate. Specifically, we characterize the limiting behavior of finite-dimensional marginals of the posterior. We establish that the posterior does not contract in this setting. Yet, the finite-dimensional posterior marginals converge to Gaussian tilts of the prior, where the mean of the Gaussian depends on the true signal coordinates of interest. Notably, the effect of the prior survives even in the limit of large samples and dimensions. We further characterize the behavior of the posterior mean and demonstrate that the posterior mean can strictly outperform the maximum likelihood estimate in mean-squared error in natural examples. Importantly, our results hold regardless of the sparsity level of the underlying signal. On the technical front, we introduce leave-one-out strategies for studying these marginals that may be of independent interest for analyzing low-dimensional functionals of high-dimensional signals in other Bayesian inference problems."}
{"id": "2601.00496", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2601.00496", "abs": "https://arxiv.org/abs/2601.00496", "authors": ["Jan Rawa", "Julian Sienkiewicz"], "title": "Quantifying correlations between information overload and fake news during COVID-19 pandemic: a Reddit study with BERT model approach", "comment": "10 pages, 2 figures, 2 tables", "summary": "Information overload (IOL) is a well-known and devastating phenomenon that alters the performance of carrying out all types of tasks. It has been shown that in the media space, IOL can contribute to news fatigue and news avoidance, which often leads to the proliferation of fake news posts on social networks. However, there is a lack of automatic methods that can be used to track IOL in large datasets. In this study, we investigate whether the Gini index calculated from the distribution of topics obtained via the BERTopic model can be considered a proxy for IOL. We test our assumptions on a set of Reddit communities related to the COVID-19 pandemic and obtain a significant global correlation between the Gini index and the fraction of fake news detected by the FakeBERT classifier. However, at the community level, the correlation analysis results are ambiguous."}
{"id": "2601.00018", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.00018", "abs": "https://arxiv.org/abs/2601.00018", "authors": ["V. A. Buryachenko"], "title": "New RVE concept in thermoelasticity of periodic composites subjected to compact support loading", "comment": "Manuscript submitted for journal publication. 42pp, 85 refs", "summary": "This paper introduces an advanced Computational Analytical Micromechanics (CAM) framework for linear thermoelastic composites (CMs) with periodic microstructures. The approach is based on an exact new Additive General Integral Equation (AGIE), formulated for compactly supported loading conditions, such as body forces and localized thermal effects (for example laser heating). In addition, new general integral equations (GIEs) are established for arbitrary mechanical and thermal loading. A unified iterative scheme is developed for solving the static AGIEs, where the compact support of loading serves as a new fundamental training parameter. At the core of the methodology lies a generalized Representative Volume Element (RVE) concept that extends Hill classical definition of the RVE. Unlike conventional RVEs, this generalized RVE is not fixed geometrically but emerges naturally from the characteristic scale of localized loading, thereby reducing the analysis of an infinite periodic medium to a finite, data-driven domain. This formulation automatically filters out nonrepresentative subsets of effective parameters while eliminating boundary effects, edge artifacts, and finite-size sample dependencies. Furthermore, the AGIE-based CAM framework integrates seamlessly with machine learning (ML) and neural network (NN) architectures, supporting the development of accurate, physics-informed surrogate nonlocal operators."}
{"id": "2601.00013", "categories": ["physics.soc-ph", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2601.00013", "abs": "https://arxiv.org/abs/2601.00013", "authors": ["Madalina I. Sas", "Fernando E. Rosas", "Hardik Rajpal", "Daniel Bor", "Henrik J. Jensen", "Pedro A. M. Mediano"], "title": "Improved estimators of causal emergence for large systems", "comment": "12 pages, 6 figures", "summary": "A central challenge in the study of complex systems is the quantification of emergence -- understood as the ability of the system to exhibit collective behaviours that cannot be traced down to the individual components. While recent work has proposed practical measures to detect emergence, these approaches tend to double-count the contribution of shared components, which substantially hinders their capability to effectively study large systems. In this work, we introduce a family of improved information-theoretic measures of emergence that iteratively correct for double-counted terms. Our approach is computationally efficient and provides a controllable trade-off between computational load and sensitivity, leading to more accurate and versatile estimates of emergence. The benefits of the proposed approach are demonstrated by successfully detecting emergence in both simulated and real-world data related to flocking behaviour."}
{"id": "2601.00317", "categories": ["cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.00317", "abs": "https://arxiv.org/abs/2601.00317", "authors": ["Estefanía Recayte"], "title": "On the Error Floor Evaluation of NOMA-Irregular Repetition Slotted ALOHA", "comment": null, "summary": "In this work, we provide a simple yet tight analytical approximation of the packet loss rate in the error floor region for a non-orthogonal multiple access (NOMA)-based irregular repetition slotted ALOHA (IRSA) scheme. Considering an Internet of Things (IoT) scenario, users randomly select both the number of replicas based on a designed degree distribution and the transmission power from predetermined levels, while successive interference cancellation (SIC) is performed at the receiver. Our derived packet loss rate expression in the finite length regime is promptly evaluated. Its accuracy is validated through Monte-Carlo simulations, demonstrating a strong match across channel loads, including those beyond the low load regime"}
{"id": "2601.00103", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.00103", "abs": "https://arxiv.org/abs/2601.00103", "authors": ["Ari Stern", "Enrico Zampa"], "title": "Finite element exterior calculus for time-dependent Hamiltonian partial differential equations", "comment": "42 pages, 4 figures", "summary": "The success of symplectic integrators for Hamiltonian ODEs has led to a decades-long program of research seeking analogously structure-preserving numerical methods for Hamiltonian PDEs. In this paper, we construct a large class of such methods by combining finite element exterior calculus (FEEC) for spatial semidiscretization with symplectic integrators for time discretization. The resulting methods satisfy a local multisymplectic conservation law in space and time, which generalizes the symplectic conservation law of Hamiltonian ODEs, and which carries finer information about Hamiltonian structure than other approaches based on global function spaces. We give particular attention to conforming FEEC methods and hybridizable discontinuous Galerkin (HDG) methods. The theory and methods are illustrated by application to the semilinear Hodge wave equation."}
{"id": "2601.00010", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00010", "abs": "https://arxiv.org/abs/2601.00010", "authors": ["Luigi E. Picasso"], "title": "On the measurement problem in quantum mechanics: a simple proposal", "comment": "8 pages, 0 figures", "summary": "Some of the problems connected with the interpretation of quantum mechanics are enumerated, in particular those related to some well known paradoxes and, above all, to the measurement process. We then show how the so called \"Physics Laboratory Assumption\" introduced in [1], which considers as \"observables'' only the self-adjoint operators corresponding to existing measuring instruments, can propose a new perspective on the aforementioned problems and can replace the wavefunction collapse postulate.\n  [1] Luigi E. Picasso, \"On the Concept of State in Quantum Mechanics: Another Way to Decoherence?'' Int. J. Theor. Phys. 62 (2), (2023)"}
{"id": "2601.00206", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.00206", "abs": "https://arxiv.org/abs/2601.00206", "authors": ["Johannes Lawen", "George Salman", "Akshita Bhardwaj"], "title": "Stratosphere Model Verification with Manufactured Geometry", "comment": null, "summary": "We propose an exact solution for a stratosphere dynamical core formulated in geopotential/pressure coordinates with a time-evolving lower boundary supplied by the troposphere. Rather than constraining the stratospheric circulation via specified dynamics (``nudging'') to a reanalysis, we treat the tropopause as a moving geometric boundary. The stratospheric domain thus expands, contracts, and undulates in response to tropospheric variability while preserving familiar hybrid $σ$--$p$ structure and pressure-gradient calculations. The approach integrates naturally with arbitrary Lagrangian--Eulerian (ALE) updates and conservative remap to maintain positive layer thickness and tracer monotonicity. We outline the formulation, highlight analytical properties (well-posedness, energetics, wave propagation), and sketch a verification/validation path based on modified standard test cases and reanalysis-driven experiments."}
{"id": "2601.00197", "categories": ["cs.CE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00197", "abs": "https://arxiv.org/abs/2601.00197", "authors": ["Shaswat Mohanty"], "title": "StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices", "comment": "14 pages, 5 figures", "summary": "Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility. Building on our earlier recurrent neural network framework, we present an enhanced StockBot architecture that systematically evaluates modern attention-based, convolutional, and recurrent time-series forecasting models within a unified experimental setting. While attention-based and transformer-inspired models offer increased modeling flexibility, extensive empirical evaluation reveals that a carefully constructed vanilla LSTM consistently achieves superior predictive accuracy and more stable buy/sell decision-making when trained under a common set of default hyperparameters. These results highlight the robustness and data efficiency of recurrent sequence models for financial time-series forecasting, particularly in the absence of extensive hyperparameter tuning or the availability of sufficient data when discretized to single-day intervals. Additionally, these results underscore the importance of architectural inductive bias in data-limited market prediction tasks."}
{"id": "2601.00707", "categories": ["nlin.AO", "math.DS", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2601.00707", "abs": "https://arxiv.org/abs/2601.00707", "authors": ["Frederik J. Thomsen", "Johan L. A. Dubbeldam", "Rudolf Hanel"], "title": "Gardner volumes and self-organization in a minimal model of complex ecosystems", "comment": "21 pages, 10 figures", "summary": "We study self-organization in a minimally nonlinear model of large random ecosystems. Populations evolve over time according to a piecewise linear system of ordinary differential equations subject to a non-negativity constraint resulting in discrete time extinction and revival events. The dynamics are generated by a random elliptic community matrix with tunable correlation strength. We show that, independent of the correlation strength, solutions of the system are confined to subsets of the phase space that can be cast as time-varying Gardner volumes from the theory of learning in neural networks. These volumes decrease with the diversity (i.e. the fraction of extant species) and become exponentially small in the long-time limit. Using standard results from random matrix theory, the changing diversity is then linked to a sequence of contractions and expansions in the spectrum of the community matrix over time, resulting in a sequence of May-type stability problems determining whether the total population evolves toward complete extinction or unbounded growth. In the case of unbounded growth, we show the model allows for a particularly simple nonlinear extension in which the solutions instead evolve towards a new attractor."}
{"id": "2601.00009", "categories": ["q-fin.CP", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00009", "abs": "https://arxiv.org/abs/2601.00009", "authors": ["Lucas Arenstein", "Michael Kastoryano"], "title": "Full grid solution for multi-asset options pricing with tensor networks", "comment": null, "summary": "Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power."}
{"id": "2601.00257", "categories": ["eess.SY", "cs.AI", "cs.CV", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.00257", "abs": "https://arxiv.org/abs/2601.00257", "authors": ["Aly Sabri Abdalla", "Vuk Marojevic"], "title": "Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective", "comment": "This article has been accepted for publication in the IEEE Wireless Communications Magazine", "summary": "Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs."}
{"id": "2601.00257", "categories": ["eess.SY", "cs.AI", "cs.CV", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.00257", "abs": "https://arxiv.org/abs/2601.00257", "authors": ["Aly Sabri Abdalla", "Vuk Marojevic"], "title": "Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective", "comment": "This article has been accepted for publication in the IEEE Wireless Communications Magazine", "summary": "Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs."}
{"id": "2601.00154", "categories": ["stat.ME", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00154", "abs": "https://arxiv.org/abs/2601.00154", "authors": ["Qianqian Qi", "Zhongming Chen", "Peter G. M. van der Heijden"], "title": "Unmixing highly mixed grain size distribution data via maximum volume constrained end member analysis", "comment": null, "summary": "End member analysis (EMA) unmixes grain size distribution (GSD) data into a mixture of end members (EMs), thus helping understand sediment provenance and depositional regimes and processes. In highly mixed data sets, however, many EMA algorithms find EMs which are still a mixture of true EMs. To overcome this, we propose maximum volume constrained EMA (MVC-EMA), which finds EMs as different as possible. We provide a uniqueness theorem and a quadratic programming algorithm for MVC-EMA. Experimental results show that MVC-EMA can effectively find true EMs in highly mixed data sets."}
{"id": "2601.00062", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2601.00062", "abs": "https://arxiv.org/abs/2601.00062", "authors": ["Haowei Fan", "Vladimir Fal'ko", "Xiao Li"], "title": "Classical vs quantum dynamics and the onset of chaos in a macrospin system", "comment": "32 pages, 17 figures. Comments are welcome", "summary": "We study a periodically driven macrospin system with anisotropic long-range interactions and collective dissipation, described by a Lindblad master equation. In the thermodynamic limit ($N\\to\\infty$), a mean-field treatment yields classical equations of motion, whose dynamics are characterized via the maximal Lyapunov exponent (MLE). Focusing on the thermodynamic limit, we map out chaotic, quasiperiodic, and periodic phases via bifurcation diagrams, MLEs, and Fourier spectra of evolved observables, identifying classic period-doubling bifurcations and fractal boundaries in the regions of attractors. Finite-size quantum simulations in the Dicke basis reveal that while both quantum and classical systems exhibit diverse dynamical phases, finite-size effects suppress some behaviors present in the thermodynamic limit. The sign of $λ_{\\mathrm{max}}$ serves as a key indicator of convergence between quantum and classical dynamics, which agree over timescales up to the Lyapunov time. Analysis of the density matrix shows that convergence occurs only when its nonzero elements are sharply localized. However, the nonconvergence does not imply a fundamental difference between quantum and classical dynamics: in chaotic regimes, although the evolution orbits of quantum and classical systems show significant differences, quantum evolution becomes mixed and diffusively explores the Hilbert space, signaling quantum chaos, which can be confirmed by the delocalized nature of the density matrix."}
{"id": "2601.00350", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.00350", "abs": "https://arxiv.org/abs/2601.00350", "authors": ["Liang Hong"], "title": "The true detection probability versus the subjective detection probability of a uniformly optimal search plan", "comment": null, "summary": "This article investigates the difference between the true detection probability and the subjective probability of a uniformly optimal search plan. Its main contributions are multi-fold. First, it provides a set of examples to show that, in terms of the true detection probability, the uniformly optimal search plan may or may not be optimal. Secondly, it establishes that the true detection probability of the uniformly optimal search plan based on a composite prior can be less than that of the composite uniformly search plan based on different priors. Next, it argues that an open problem is unsolvable. Finally, it shows that the true detection probability of the uniformly optimal search plan converges to one as the search time approaches infinity."}
{"id": "2601.00515", "categories": ["physics.hist-ph", "physics.bio-ph", "q-bio.MN", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2601.00515", "abs": "https://arxiv.org/abs/2601.00515", "authors": ["Leroy Cronin", "Sara I. Walker"], "title": "The Physics of Causation", "comment": "56 pages, 7 Figures, 64 references", "summary": "Assembly theory (AT) introduces a concept of causation as a material property, constitutive of a metrology of evolution and selection. The physical scale for causation is quantified with the assembly index, defined as the minimum number of steps necessary for a distinguishable object to exist, where steps are assembled recursively. Observing countable copies of high assembly index objects indicates that a mechanism to produce them is persistent, such that the object's environment builds a memory that traps causation within a contingent chain. Copy number and assembly index underlie the standardized metrology for detecting causation (assembly index), and evidence of contingency (copy number). Together, these allow the precise definition of a selective threshold in assembly space, understood as the set of all causal possibilities. This threshold demarcates life (and its derivative agential, intelligent and technological forms) as structures with persistent copies beyond the threshold. In introducing a fundamental concept of material causation to explain and measure life, AT represents a departure from prior theories of causation, such as interventional ones, which have so far proven incompatible with fundamental physics. We discuss how AT's concept of causation provides the foundation for a theory of physics where novelty, contingency and the potential for open-endedness are fundamental, and determinism is emergent along assembled lineages."}
{"id": "2601.00239", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.00239", "abs": "https://arxiv.org/abs/2601.00239", "authors": ["Ioannis Papastathopoulos", "Jennifer Wadsworth"], "title": "Geometric extremal graphical models and coefficients of extremal dependence on block graphs", "comment": "20 pages, 5 figures", "summary": "We introduce the concept of geometric extremal graphical models, which are defined through the gauge function of the limit set obtained from suitably scaled random vectors in light-tailed margins. For block graphs, we prove results relating to the propagation of various extremal dependence coefficients along the graph. A particular focus is placed on coefficients that link to the framework of conditional extreme value theory, which are especially interesting when variables do not all attain their most extreme values simultaneously. We also consider results related to the case when variables do exhibit joint extreme behaviour. Through the recent translation of the geometric approach for multivariate extremes to a statistical modelling framework, geometric extremal graphical models, and results relating to them, pave the way for an approach to modelling of high dimensional extremes with complex extremal dependence structures."}
{"id": "2601.00221", "categories": ["eess.SY", "cs.DC", "cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00221", "abs": "https://arxiv.org/abs/2601.00221", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Impact of Clustering on the Observability and Controllability of Complex Networks", "comment": "Cluster Computing Journal", "summary": "The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering."}
{"id": "2601.00019", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.00019", "abs": "https://arxiv.org/abs/2601.00019", "authors": ["Valeriy A. Buryachenko"], "title": "Additive general integral equations in thermoelastic micromechanics of composites", "comment": "Manuscript submitted for journal publication. 42pp, 94 refs", "summary": "This work presents an enhanced Computational Analytical Micromechanics (CAM) framework for the analysis of linear thermoelastic composite materials (CMs) with random microstructure. The proposed approach is grounded in an exact Additive General Integral Equation (AGIE), specifically formulated for compactly supported loading, including both body forces and localized thermal changes (such as those from laser heating). New general integral equations (GIEs) for arbitrary mechanical and thermal loading are proposed. A unified iterative solution strategy is developed for the static AGIE, applicable to CMs with both perfectly and imperfectly bonded interfaces, where the compact support of loading is introduced as a new fundamental training parameter. Central to this methodology is a generalized Representative Volume Element (RVE) concept, which extends Hill classical definition. The resulting RVE is not predefined geometrically, but rather emerges from the characteristic scale of the localized loading, effectively reducing the analysis of an infinite, randomly heterogeneous medium to a finite, data-driven domain. This generalized RVE approach enables automatic exclusion of unrepresentative subsets of effective parameters, while inherently eliminating boundary effects, edge artifacts, and finite size limitations. Moreover, the AGIE-based CAM framework is naturally compatible with machine learning (ML) and neural network (NN) architectures, facilitating the construction of accurate and physically informed surrogate nonlocal operators."}
{"id": "2601.00330", "categories": ["physics.soc-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00330", "abs": "https://arxiv.org/abs/2601.00330", "authors": ["Xiangrong Wang", "Xin Yu", "Zongze Wu", "Yamir Moreno"], "title": "Effective Graph Resistance as Cumulative Heat Dissipation", "comment": "APS one-column style. 32 pages and 10 figures", "summary": "Effective graph resistance is a fundamental structural metric in network science, widely used to quantify global connectivity, compare network architectures, and assess robustness in flow-based systems. Despite its importance, current formulations rely mainly on spectral or pseudo-inverse Laplacian representations, offering limited physical insight into how structural features shape this quantity or how it can be efficiently optimized. Here, we establish an exact and physically transparent relationship between effective graph resistance and the cumulative heat dissipation generated by Laplacian diffusion dynamics. We show that the total heat dissipated during relaxation to equilibrium precisely equals the effective graph resistance. This dynamical viewpoint uncovers a natural multi-scale decomposition of the Laplacian spectrum: early-time dissipation is governed by degree-based local structure, intermediate times isolate eigenvalues below the spectral mean, and long times are dominated by the algebraic connectivity. These multi-scale properties yield continuous and interpretable strategies for modifying network structure and constructing optimized ensembles, enabling improvements that are otherwise NP-hard to achieve via combinatorial methods. Our results unify structural and dynamical perspectives on network connectivity and provide new tools for analyzing, comparing, and optimizing complex networks across domains."}
{"id": "2601.00343", "categories": ["cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.00343", "abs": "https://arxiv.org/abs/2601.00343", "authors": ["Estefanía Recayte", "Leonardo Badia", "Andrea Munari"], "title": "Two-Step Interference Cancellation for Energy Saving in Irregular Repetition Slotted ALOHA", "comment": null, "summary": "We evaluate a modification of irregular repetition slotted ALOHA (IRSA) involving intermediate decoding and early transmission termination by some nodes, upon their decoding success. This is meant to avoid unnecessary transmissions, thereby reducing energy consumption. We expect this to be particularly useful at low loads, where most transmissions can be avoided as they do not often result in a collision and are therefore redundant. To validate this proposal, we observe that most of the literature related to IRSA considers an asymptotic heavily loaded regime; thus, we also present a model of energy consumption and success probability for frames of limited length and low offered loads. Thanks to our analysis, also confirmed by simulation, we are able to show that the proposed technique is able to reduce IRSA energy consumption by minimizing transmissions, while preserving performance gains over standard ALOHA. For example, we are able to get a 33% energy saving at offered loads around 10% without affecting throughput."}
{"id": "2601.00107", "categories": ["math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.00107", "abs": "https://arxiv.org/abs/2601.00107", "authors": ["Deepyaman Chakraborty", "Ruben Harris", "Rupert Klein", "Guillermo Olicón-Méndez", "Sebastian Reich", "Claudia Schillings"], "title": "Affine Invariant Langevin Dynamics for rare-event sampling", "comment": null, "summary": "We introduce an affine invariant Langevin dynamics (ALDI) framework for the efficient estimation of rare events in nonlinear dynamical systems. Rare events are formulated as Bayesian inverse problems through a nonsmooth limit-state function whose zero level set characterises the event of interest. To overcome the nondifferentiability of this function, we propose a smooth approximation that preserves the failure set and yields a posterior distribution satisfying the small-noise limit. The resulting potential is sampled by ALDI, a (derivative-free) interacting particle system whose affine invariance allows it to adapt to the local anisotropy of the posterior.\n  We demonstrate the performance of the method across a hierarchy of benchmarks, namely two low-dimensional examples (an algebraic problem with convex geometry and a dynamical problem of saddle-type instability) and a point-vortex model for atmospheric blockings. In all cases, ALDI concentrates near the relevant near-critical sets and provides accurate proposal distributions for self-normalised importance sampling. The framework is computationally robust, potentially gradient-free, and well-suited for complex forward models with strong geometric anisotropy. These results highlight ALDI as a promising tool for rare-event estimation in unstable regimes of dynamical systems."}
{"id": "2601.00062", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2601.00062", "abs": "https://arxiv.org/abs/2601.00062", "authors": ["Haowei Fan", "Vladimir Fal'ko", "Xiao Li"], "title": "Classical vs quantum dynamics and the onset of chaos in a macrospin system", "comment": "32 pages, 17 figures. Comments are welcome", "summary": "We study a periodically driven macrospin system with anisotropic long-range interactions and collective dissipation, described by a Lindblad master equation. In the thermodynamic limit ($N\\to\\infty$), a mean-field treatment yields classical equations of motion, whose dynamics are characterized via the maximal Lyapunov exponent (MLE). Focusing on the thermodynamic limit, we map out chaotic, quasiperiodic, and periodic phases via bifurcation diagrams, MLEs, and Fourier spectra of evolved observables, identifying classic period-doubling bifurcations and fractal boundaries in the regions of attractors. Finite-size quantum simulations in the Dicke basis reveal that while both quantum and classical systems exhibit diverse dynamical phases, finite-size effects suppress some behaviors present in the thermodynamic limit. The sign of $λ_{\\mathrm{max}}$ serves as a key indicator of convergence between quantum and classical dynamics, which agree over timescales up to the Lyapunov time. Analysis of the density matrix shows that convergence occurs only when its nonzero elements are sharply localized. However, the nonconvergence does not imply a fundamental difference between quantum and classical dynamics: in chaotic regimes, although the evolution orbits of quantum and classical systems show significant differences, quantum evolution becomes mixed and diffusively explores the Hilbert space, signaling quantum chaos, which can be confirmed by the delocalized nature of the density matrix."}
{"id": "2601.00628", "categories": ["physics.ao-ph", "physics.comp-ph", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00628", "abs": "https://arxiv.org/abs/2601.00628", "authors": ["Cédric Goeury", "Thierry Fouquet", "Maria Teles", "Michel Benoit"], "title": "Bayesian optimization for re-analysis and calibration of extreme sea state events simulated with a spectral third-generation wave model", "comment": null, "summary": "Accurate hindcasting of extreme sea state events is essential for coastal engineering, risk assessment, and climate studies. However, the reliability of numerical wave models remains limited by uncertainties in physical parameterizations and model inputs. This study presents a novel calibration framework based on Bayesian Optimization (BO), leveraging the Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters, specifically bottom friction dissipation, depth induced breaking, and wave dissipation from strong opposing currents, in the ANEMOC-3 hindcast wave model. The proposed method enables joint optimization of continuous parameters and discrete model structures, significantly reducing discrepancies between model outputs and observations. Applied to a one month period encompassing multiple intense storm events along the French Atlantic coast, the calibrated model demonstrates improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index relative to the default sea$-$state solver configuration. The results highlight the potential of BO to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to a wide range of geophysical modeling problems. Future extensions include multi-objective optimization, uncertainty quantification, and integration of additional observational datasets."}
{"id": "2601.00471", "categories": ["cs.CE", "cond-mat.mtrl-sci", "physics.app-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2601.00471", "abs": "https://arxiv.org/abs/2601.00471", "authors": ["L. Castro", "Y. Navidtehrani. C. Betegón", "E. Martínez-Pañeda"], "title": "Coupled thermo-chemo-mechanical phase field-based modelling of hydrogen-assisted cracking in girth welds", "comment": null, "summary": "A new computational framework is presented to predict the structural integrity of welds in hydrogen transmission pipelines. The framework combines: (i) a thermo-mechanical weld process model, and (ii) a coupled deformation-diffusion-fracture phase field-based model that accounts for plasticity and hydrogen trapping, considering multiple trap types, with stationary and evolving trap densities. This enables capturing, for the first time, the interplay between residual stresses, trap creation, hydrogen transport, and fracture. The computational framework is particularised and applied to the study of weld integrity in X80 pipeline steel. The focus is on girth welds, as they are more complex due to their multi-pass nature. The weld process model enables identifying the dimensions and characteristics of the three weld regions: base metal, heat-affected zone, and weld metal, and these are treated distinctively. This is followed by virtual fracture experiments, which reveal a very good agreement with laboratory studies. Then, weld pipeline integrity is assessed, estimating critical failure pressures for a wide range of scenarios. Of particular interest is to assess the structural integrity implications of welding defects present in existing natural gas pipelines under consideration for hydrogen transport: pores, lack of penetration, imperfections, lack of fusion, root contraction, and undercutting. The results obtained in hydrogen-containing environments reveal an important role of the weld microstructure and the detrimental effect of weld defects that are likely to be present in existing natural gas pipelines, as they are considered safe in gas pipeline standards."}
{"id": "2601.00013", "categories": ["physics.soc-ph", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2601.00013", "abs": "https://arxiv.org/abs/2601.00013", "authors": ["Madalina I. Sas", "Fernando E. Rosas", "Hardik Rajpal", "Daniel Bor", "Henrik J. Jensen", "Pedro A. M. Mediano"], "title": "Improved estimators of causal emergence for large systems", "comment": "12 pages, 6 figures", "summary": "A central challenge in the study of complex systems is the quantification of emergence -- understood as the ability of the system to exhibit collective behaviours that cannot be traced down to the individual components. While recent work has proposed practical measures to detect emergence, these approaches tend to double-count the contribution of shared components, which substantially hinders their capability to effectively study large systems. In this work, we introduce a family of improved information-theoretic measures of emergence that iteratively correct for double-counted terms. Our approach is computationally efficient and provides a controllable trade-off between computational load and sensitivity, leading to more accurate and versatile estimates of emergence. The benefits of the proposed approach are demonstrated by successfully detecting emergence in both simulated and real-world data related to flocking behaviour."}
{"id": "2601.00289", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00289", "abs": "https://arxiv.org/abs/2601.00289", "authors": ["S. Gokul Krishnan", "Mohd. Asim Aftab", "Nabil Mohammed", "Shehab Ahmed", "Charalambos Konstantinou"], "title": "Impact Assessment of Heterogeneous Grid Support Functions in Smart Inverter Deployments", "comment": null, "summary": "The decarbonization of the energy sector has led to a significant high penetration of distributed energy resources (DERs), particularly photovoltaic (PV) systems, in low-voltage (LV) distribution networks. To maintain grid stability, recent standards (e.g., IEEE 1547-2018) mandate DERs to provide grid-support functionalities through smart inverters (SIs), which typically operate autonomously based on local measurements. However, as DER penetration increases, uncoordinated control modes of SIs can lead to adverse interactions, compromising system efficiency, voltage regulation, and overall stability. While previous studies have demonstrated the benefits of coordinated inverter control and optimal dispatch strategies, the system-wide impacts of heterogeneous SI groups operating under different control modes remain largely unexamined. This paper addresses this gap by assessing the dynamic interactions among multiple SI groups with varying control strategies, namely: Constant Power Factor (CPF), Volt-VAR, and Volt-Watt modes. Furthermore, the analysis covers both resistive and inductive feeder types. The validation is performed using a real-time setup. The CIRGE low-voltage (LV) distribution network is simulated in the Opal-RT platform as the test network, enabling realistic and high-fidelity evaluation of SI control interactions under practical grid conditions."}
{"id": "2601.00289", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00289", "abs": "https://arxiv.org/abs/2601.00289", "authors": ["S. Gokul Krishnan", "Mohd. Asim Aftab", "Nabil Mohammed", "Shehab Ahmed", "Charalambos Konstantinou"], "title": "Impact Assessment of Heterogeneous Grid Support Functions in Smart Inverter Deployments", "comment": null, "summary": "The decarbonization of the energy sector has led to a significant high penetration of distributed energy resources (DERs), particularly photovoltaic (PV) systems, in low-voltage (LV) distribution networks. To maintain grid stability, recent standards (e.g., IEEE 1547-2018) mandate DERs to provide grid-support functionalities through smart inverters (SIs), which typically operate autonomously based on local measurements. However, as DER penetration increases, uncoordinated control modes of SIs can lead to adverse interactions, compromising system efficiency, voltage regulation, and overall stability. While previous studies have demonstrated the benefits of coordinated inverter control and optimal dispatch strategies, the system-wide impacts of heterogeneous SI groups operating under different control modes remain largely unexamined. This paper addresses this gap by assessing the dynamic interactions among multiple SI groups with varying control strategies, namely: Constant Power Factor (CPF), Volt-VAR, and Volt-Watt modes. Furthermore, the analysis covers both resistive and inductive feeder types. The validation is performed using a real-time setup. The CIRGE low-voltage (LV) distribution network is simulated in the Opal-RT platform as the test network, enabling realistic and high-fidelity evaluation of SI control interactions under practical grid conditions."}
{"id": "2601.00188", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.00188", "abs": "https://arxiv.org/abs/2601.00188", "authors": ["Landon Hurley"], "title": "An exact unbiased semi-parametric L2 quasi-likelihood framework, complete in the presence of ties", "comment": null, "summary": "Maximum likelihood style estimators possesses a number of ideal characteristics, but require prior identification of the distribution of errors to ensure exact unbiasedness. Independent of the focus of the primary statistical analysis, the estimation of a covariance matrix \\(S^{P \\times P}\\approx Σ^{P \\times P}\\) must possess a specific structure and regularity constraints. The need to estimate a linear Gaussian covariance models appear in various applications as a formal precondition for scientific investigation and predictive analytics. In this work, we construct an \\(\\ell_{2}\\)-norm based quasi-likelihood framework, identified by binomial comparisons between all pairs \\(X_{n},Y_{n}, \\forall {n}\\). Our work here focuses upon the quasi-likelihood basis for estimation of an exactly unbiased linear regression Hájek projection, within which the Kemeny metric space is operationalised via Whitney embedding to obtain exact unbiased minimum variance multivariate covariance estimators upon both discrete and continuous random variables (i.e., exact unbiased identification in the presence of ties upon finite samples). While the covariance estimator is inherently useful, expansion of the Wilcoxon rank-sum testing framework to handle multiple covariates with exact unbiasedness upon finite samples is a currently unresolved research problem, as it maintains identification in the presence of linear surjective mappings onto common points: this model space, by definition, expands our likelihood framework into a consistent non-parametric form of the standard general linear model, which we extend to address both unknown heterogeneity and the problem of weak inferential instruments."}
{"id": "2601.00375", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.00375", "abs": "https://arxiv.org/abs/2601.00375", "authors": ["Haibin Chen", "Hong Yan", "Guanglu Zhou"], "title": "Completely Positive Reformulations of Polynomial Optimization Problems with Linear Inequality Constraints", "comment": null, "summary": "Polynomial optimization encompasses a broad class of problems in which both the objective function and constraints are polynomial functions of the decision variables. In recent years, a substantial body of research has focused on reformulating polynomial optimization problems (POPs) as conic programs over the cone of completely positive tensors (CPTs). In this article, we propose several new completely positive reformulations for a class of POPs with linear inequality constraints. Our approach begins by lifting these problems into a novel convex optimization framework, wherein the variables are represented as combinations of symmetric rank-one tensors. Based on this lifted formulation, we present a general characterization of POPs with linear inequality constraints that can be reformulated as conic programs over the CPT cone. Additionally, we construct the dual formulations of the resulting completely positive programs. Under mild assumptions, we prove that these dual problems are strictly feasible and strong duality holds."}
{"id": "2601.00377", "categories": ["math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00377", "abs": "https://arxiv.org/abs/2601.00377", "authors": ["Sijia Xia", "Michael K. Ng", "Xiongjun Zhang"], "title": "Sparse Tucker Decomposition and Graph Regularization for High-Dimensional Time Series Forecasting", "comment": null, "summary": "Existing methods of vector autoregressive model for multivariate time series analysis make use of low-rank matrix approximation or Tucker decomposition to reduce the dimension of the over-parameterization issue. In this paper, we propose a sparse Tucker decomposition method with graph regularization for high-dimensional vector autoregressive time series. By stacking the time-series transition matrices into a third-order tensor, the sparse Tucker decomposition is employed to characterize important interactions within the transition third-order tensor and reduce the number of parameters. Moreover, the graph regularization is employed to measure the local consistency of the response, predictor and temporal factor matrices in the vector autoregressive model.The two proposed regularization techniques can be shown to more accurate parameters estimation. A non-asymptotic error bound of the estimator of the proposed method is established, which is lower than those of the existing matrix or tensor based methods. A proximal alternating linearized minimization algorithm is designed to solve the resulting model and its global convergence is established under very mild conditions. Extensive numerical experiments on synthetic data and real-world datasets are carried out to verify the superior performance of the proposed method over existing state-of-the-art methods."}
{"id": "2601.00742", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00742", "abs": "https://arxiv.org/abs/2601.00742", "authors": ["Turab Lookman", "YuJie Liu", "Zhibin Gao"], "title": "Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI", "comment": "44 pages, 14 figures", "summary": "This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by \"human-out-of-the-loop\" discovery processes."}
{"id": "2601.00433", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.00433", "abs": "https://arxiv.org/abs/2601.00433", "authors": ["Fredrik Jansson"], "title": "Modelling cultural evolution", "comment": "Chapter in Johan Lind and Anna Jon-And, eds.: Cultural Evolution from Minimal Principles, Cambridge University Press", "summary": "Formal modelling provides a toolkit for understanding cultural dynamics, from individual decisions to recurring patterns of change. This chapter explains what models are and why they matter. Using a precise, shared language, they aid thinking and communication by turning fuzzy assumptions into clear, comparable, testable claims. The chapter describes the modelling process, trading explanatory clarity against predictive specificity. Four families of models are surveyed, from the micro-level with optimising agents to macro-level dynamics with heuristic or even implicit agents, covering reasoning (Bayesian inference, game theory), adaptive updating (reinforcement learning, evolutionary games), mean-field approaches (compartmental models, population dynamics), and complex systems (agent-based models, social networks). Building on these, a general template for modelling cultural evolution is outlined that connects system states, cognitive processes, behaviour, and macro-level outcomes in dynamic loops, linking individuals, groups, institutions, and their environments. Taken together, these tools support a pluralist but coherent understanding of cultural change."}
{"id": "2601.00711", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.00711", "abs": "https://arxiv.org/abs/2601.00711", "authors": ["Ali Abbassi", "Yann Dujardin", "Eric Gourdin", "Philippe Lacomme", "Caroline Prodhon"], "title": "Assessing Quantum Annealing to Solve the Minimum Vertex Multicut", "comment": "Published in Codit 2025", "summary": "Cybersecurity in telecommunication networks often leads to hard combinatorial optimization problems that are challenging to solve with classical methods. This work investigates the practical feasibility of using quantum annealing to address the Restricted Vertex Minimum Multicut Problem. The problem is formulated as a Quadratic Unconstrained Binary Optimization model and implemented on D-Wave s quantum annealer. Rather than focusing on solution quality alone, we analyze key aspects of the quantum workflow including minor embedding techniques, chain length, topology constraints, chain strength selection, unembedding procedures, and postprocessing. Our results show that quantum annealing faces substantial hardware-level constraints limitations in embedding and scalability, especially for large instances, while hybrid quantum-classical solvers provide improved feasibility. This study offers a realistic assessment of the D-Wave system s current capabilities and identifies crucial parameters that govern the success of quantum optimization in cybersecurity-related network problems."}
{"id": "2601.00161", "categories": ["math.NA", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2601.00161", "abs": "https://arxiv.org/abs/2601.00161", "authors": ["Jiuyang Liang", "Libin Lu", "Shidong Jiang"], "title": "Fast Ewald Summation with Prolates for Charged Systems in the NPT Ensemble", "comment": "31 pages, 9 figures", "summary": "We present an NPT extension of Ewald summation with prolates (ESP), a spectrally accurate and scalable particle-mesh method for molecular dynamics simulations of periodic, charged systems. Building on the recently introduced ESP framework, this work focuses on rigorous and thermodynamically consistent pressure/stress evaluation in the isothermal--isobaric ensemble. ESP employs prolate spheroidal wave functions as both splitting and spreading kernels, reducing the Fourier grid size needed to reach a prescribed pressure accuracy compared with current widely used mesh-Ewald methods based on Gaussian splitting and B-spline spreading. We derive a unified pressure-tensor formulation applicable to isotropic, semi-isotropic, anisotropic, and fully flexible cells, and show that the long-range pressure can be evaluated with a single forward FFT followed by diagonal scaling, whereas force evaluation requires both forward and inverse transforms. We provide production implementations in LAMMPS and GROMACS and validate pressure and force accuracy on bulk water, LiTFSI ionic liquids, and a transmembrane system. Benchmarks on up to $3\\times 10^3$ CPU cores demonstrate strong scaling and reduced communication cost at matched accuracy, particularly for NPT pressure evaluation."}
{"id": "2601.00064", "categories": ["quant-ph", "cond-mat.str-el", "hep-th", "math.QA"], "pdf": "https://arxiv.org/pdf/2601.00064", "abs": "https://arxiv.org/abs/2601.00064", "authors": ["Yitao Feng", "Hanyu Xue", "Ryohei Kobayashi", "Po-Shen Hsin", "Yu-An Chen"], "title": "Pauli stabilizer formalism for topological quantum field theories and generalized statistics", "comment": "48 pages, 2 figures", "summary": "Topological quantum field theory (TQFT) provides a unifying framework for describing topological phases of matter and for constructing quantum error-correcting codes, playing a central role across high-energy physics, condensed matter, and quantum information. A central challenge is to formulate topological order on the lattice and to extract the properties of topological excitations from microscopic Hamiltonians. In this work, we construct new classes of lattice gauge theories as Pauli stabilizer models, realizing a wide range of TQFTs in general spacetime dimensions. We develop a lattice description of the resulting extended excitations and systematically determine their generalized statistics.\n  Our main example is the $(4+1)$D \\emph{fermionic-loop toric code}, obtained by condensing the $e^2 m^2$-loop in the $(4+1)$D $\\mathbb{Z}_4$ toric code. We show that the loop excitation exhibits fermionic loop statistics: the 24-step loop-flipping process yields a phase of $-1$. Our Pauli stabilizer models realize all twisted 2-form gauge theories in $(4+1)$D, the higher-form Dijkgraaf-Witten TQFT classified by $H^{5}(B^{2}G, U(1))$. % Beyond $(4+1)$D, the fermionic-loop toric codes form a family of $\\mathbb{Z}_2$ topological orders in arbitrary dimensions featuring fermionic loop excitations, realized as explicit Pauli stabilizer codes using $\\mathbb{Z}_4$ qudits. % Finally, we develop a Pauli-based framework that defines generalized statistics for extended excitations in any dimension, yielding computable lattice unitary processes to detect nontrivial generalized statistics. For example, we propose anyonic membrane statistics in $(6+1)$D, as well as fermionic membrane and volume statistics in arbitrary dimensions. We construct new families of $\\mathbb{Z}_2$ topological orders: the \\emph{fermionic-membrane toric code} and the \\emph{fermionic-volume toric code}."}
{"id": "2601.00637", "categories": ["physics.ao-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.00637", "abs": "https://arxiv.org/abs/2601.00637", "authors": ["K. Shri Vignesh", "Ambedkar Sanket Sukdeo", "P. V. Sruthibhai", "Aishwarya Singh", "Srikrishna Sahu", "Swetaprovo Chaudhari", "Amit K. Patra", "T. Narayana Rao", "Rama Govindarajan", "Sachin S. Gunthe", "R. I. Sujith"], "title": "Turbulence is ineffective in causing raindrop growth in polluted clouds", "comment": "19 pages, 2 figures corresponding to the main text and 6 figures corresponding to the extended data", "summary": "Aerosol-cloud interactions represent the largest uncertainty in climate-change assessment, and while cloud turbulence is considered crucial for droplet growth, its precise role remains unclear. Our laboratory-controlled studies show that turbulence does not always enhance collision and coalescence; instead, its influence emerges only when droplets have a sufficiently broad size distribution. The dissipative-scale droplet behaviour underscores the importance of improved parameterisations to accurately model cloud microphysics."}
{"id": "2601.00491", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2601.00491", "abs": "https://arxiv.org/abs/2601.00491", "authors": ["Shuwei Zhou", "Christian Haeffner", "Shuancheng Wang", "Sophie Stebner", "Zhen Liao", "Bing Yang", "Zhichao Wei", "Sebastian Muenstermann"], "title": "Transfer-learned Kolosov-Muskhelishvili Informed Neural Networks for Fracture Mechanics", "comment": "30 pages, 12 figures", "summary": "Physics-informed neural networks have been widely applied to solid mechanics problems. However, balancing the governing partial differential equations and boundary conditions remains challenging, particularly in fracture mechanics, where accurate predictions strongly depend on refined sampling near crack tips. To overcome these limitations, a Kolosov-Muskhelishvili informed neural network with Williams enrichment is developed in this study. Benefiting from the holomorphic representation, the governing equations are satisfied by construction, and only boundary points are required for training. Across a series of benchmark problems, the Kolosov-Muskhelishvili informed neural network shows excellent agreement with analytical and finite element method references, achieving average relative errors below 1\\% and $R^2$ above 0.99 for both mode I and mode II loadings. Furthermore, three crack propagation criteria (maximum tangential stress, maximum energy release rate, and principle of local symmetry) are integrated into the framework using a transfer learning strategy to predict crack propagation directions. The predicted paths are nearly identical across all criteria, and the transfer learning strategy reduces the required training time by more than 70\\%. Overall, the developed framework provides a unified, mesh-free, and physically consistent approach for accurate and efficient crack propagation analysis."}
{"id": "2601.00494", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00494", "abs": "https://arxiv.org/abs/2601.00494", "authors": ["Marc Seidel", "Mahathi Anand", "Frank Allgöwer"], "title": "Safety for Weakly-Hard Control Systems via Graph-Based Barrier Functions", "comment": "Submitted to IEEE TAC", "summary": "Despite significant advancement in technology, communication and computational failures are still prevalent in safety-critical engineering applications. Often, networked control systems experience packet dropouts, leading to open-loop behavior that significantly affects the behavior of the system. Similarly, in real-time control applications, control tasks frequently experience computational overruns and thus occasionally no new actuator command is issued. This article addresses the safety verification and controller synthesis problem for a class of control systems subject to weakly-hard constraints, i.e., a set of window-based constraints where the number of failures are bounded within a given time horizon. The results are based on a new notion of graph-based barrier functions that are specifically tailored to the considered system class, offering a set of constraints whose satisfaction leads to safety guarantees despite communication failures. Subsequent reformulations of the safety constraints are proposed to alleviate conservatism and improve computational tractability, and the resulting trade-offs are discussed. Finally, several numerical case studies demonstrate the effectiveness of the proposed approach."}
{"id": "2601.00494", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00494", "abs": "https://arxiv.org/abs/2601.00494", "authors": ["Marc Seidel", "Mahathi Anand", "Frank Allgöwer"], "title": "Safety for Weakly-Hard Control Systems via Graph-Based Barrier Functions", "comment": "Submitted to IEEE TAC", "summary": "Despite significant advancement in technology, communication and computational failures are still prevalent in safety-critical engineering applications. Often, networked control systems experience packet dropouts, leading to open-loop behavior that significantly affects the behavior of the system. Similarly, in real-time control applications, control tasks frequently experience computational overruns and thus occasionally no new actuator command is issued. This article addresses the safety verification and controller synthesis problem for a class of control systems subject to weakly-hard constraints, i.e., a set of window-based constraints where the number of failures are bounded within a given time horizon. The results are based on a new notion of graph-based barrier functions that are specifically tailored to the considered system class, offering a set of constraints whose satisfaction leads to safety guarantees despite communication failures. Subsequent reformulations of the safety constraints are proposed to alleviate conservatism and improve computational tractability, and the resulting trade-offs are discussed. Finally, several numerical case studies demonstrate the effectiveness of the proposed approach."}
{"id": "2601.00284", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00284", "abs": "https://arxiv.org/abs/2601.00284", "authors": ["Neda Mohammadi", "Soham Sarkar", "Piotr Kokoszka"], "title": "Deep learning estimation of the spectral density of functional time series on large domains", "comment": null, "summary": "We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \\times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \\sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images."}
{"id": "2601.00449", "categories": ["math.OC", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.00449", "abs": "https://arxiv.org/abs/2601.00449", "authors": ["Jonas Christoffer Villumsen", "Yusuke Sugita"], "title": "Quadratic Unconstrained Binary Optimisation for Training and Regularisation of Binary Neural Networks", "comment": "32 pages, 12 figures", "summary": "Advances in artificial intelligence (AI) and deep learning have raised concerns about its increasing energy consumption, while demand for deploying AI in mobile devices and machines at the edge is growing. Binary neural networks (BNNs) have recently gained attention as energy and memory efficient models suitable for resource constrained environments; however, training BNNs exactly is computationally challenging because of its discrete characteristics. Recent work proposing a framework for training BNNs based on quadratic unconstrained binary optimisation (QUBO) and progress in the design of Ising machines for solving QUBO problems suggest a potential path to efficiently optimising discrete neural networks. In this work, we extend existing QUBO models for training BNNs to accommodate arbitrary network topologies and propose two novel methods for regularisation. The first method maximises neuron margins biasing the training process toward parameter configurations that yield larger pre-activation magnitudes. The second method employs a dropout-inspired iterative scheme in which reduced subnetworks are trained and used to adjust linear penalties on network parameters. We apply the proposed QUBO formulation to a small binary image classification problem and conduct computational experiments on a GPU-based Ising machine. The numerical results indicate that the proposed regularisation terms modify training behaviour and yield improvements in classification accuracy on data not present in the training set."}
{"id": "2601.00507", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.00507", "abs": "https://arxiv.org/abs/2601.00507", "authors": ["Junhyung Park", "Fanny Yang", "Thomas Icard"], "title": "Counterfactual Spaces", "comment": null, "summary": "We mathematically axiomatise the stochastics of counterfactuals, by introducing two related frameworks, called counterfactual probability spaces and counterfactual causal spaces, which we collectively term counterfactual spaces. They are, respectively, probability and causal spaces whose underlying measurable spaces are products of world-specific measurable spaces. In contrast to more familiar accounts of counterfactuals founded on causal models, we do not view interventions as a necessary component of a theory of counterfactuals. As an alternative to Pearl's celebrated ladder of causation, we view counterfactuals and interventions are orthogonal concepts, respectively mathematised in counterfactual probability spaces and causal spaces. The two concepts are then combined to form counterfactual causal spaces. At the heart of our theory is the notion of shared information between the worlds, encoded completely within the probability measure and causal kernels, and whose extremes are characterised by independence and synchronisation of worlds. Compared to existing frameworks, counterfactual spaces enable the mathematical treatment of a strictly broader spectrum of counterfactuals."}
{"id": "2601.00009", "categories": ["q-fin.CP", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00009", "abs": "https://arxiv.org/abs/2601.00009", "authors": ["Lucas Arenstein", "Michael Kastoryano"], "title": "Full grid solution for multi-asset options pricing with tensor networks", "comment": null, "summary": "Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power."}
{"id": "2601.00440", "categories": ["physics.soc-ph", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.00440", "abs": "https://arxiv.org/abs/2601.00440", "authors": ["Fredrik Jansson"], "title": "The dynamics of cultural systems", "comment": "Chapter in Johan Lind and Anna Jon-And, eds.: Cultural Evolution from Minimal Principles, Cambridge University Press", "summary": "Culture is not just traits but a dynamic system of interdependent beliefs, practices and artefacts embedded in cognitive, social and material structures. Culture evolves as these entities interact, generating path dependence, attractor states and tension, with long-term stability punctuated by rapid systemic transformations. Cultural learning and creativity is modelled as coherence-seeking information processing: individuals filter, transform and recombine input in light of prior acquisitions and dissonance reduction, thereby creating increasingly structured worldviews. Higher-order traits such as goals, skills, norms and cognitive gadgets act as emergent metafilters that regulate subsequent selection by defining what counts as coherent. Together, these filtering processes self-organise into epistemic niches, echo chambers, polarised groups and institutions that channel information flows and constrain future evolution. In this view, LLMs and recommender algorithms are products of cultural embeddings that now act back on cultural systems by automated filtering and recombination of information, reshaping future dynamics of cultural systems."}
{"id": "2601.00193", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.00193", "abs": "https://arxiv.org/abs/2601.00193", "authors": ["Lisen Ding", "Xiangyi Peng", "Dongling Wang"], "title": "Temporal Two-Grid Compact Difference Scheme for Benjamin-Bona-Mahony-Burgers Equation", "comment": null, "summary": "This paper proposes a temporal two-grid compact difference (TTCD) scheme for solving the Benjamin-Bona-Mahony-Burgers (BBMB) equation with initial and periodic boundary conditions. The method consists of three main steps: first, solving a nonlinear system on a coarse time grid of size $τ_c$; then obtaining a coarse approximation on the fine time grid of size $τ_f$ via linear Lagrange interpolation; and finally solving a linearized scheme on the fine grid to obtain the corrected solution. The TTCD scheme reduces computational cost without sacrificing accuracy. Moreover, using the energy method, we rigorously prove the conservation property, unique solvability, convergence, and stability of the proposed scheme. It is shown that the method achieves convergence of order $\\mathcal{O}(τ_c^2 + τ_f^2 + h^4)$ in the maximum norm, where $h$ is space step size. Finally, some numerical experiments are provided to demonstrate the effectiveness and feasibility of the proposed strategy."}
{"id": "2601.00077", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00077", "abs": "https://arxiv.org/abs/2601.00077", "authors": ["Tailan S. Sarubi", "Santiago Zamora", "Moisés Alves", "Vinícius F. Alves", "Gandhi Viswanathan", "Rafael Chaves"], "title": "Detection Efficiency Bounds in (Semi-)Device-Independent Scenarios", "comment": null, "summary": "This article provides a comprehensive review of the critical role of detection efficiency in demonstrating non-classicality across various device-independent and semi-device-independent scenarios. The central focus is the detection loophole, a challenge in which imperfect detectors can allow classical hidden variable models to mimic quantum correlations, thus masking genuine non-classicality. As a review, the article revisits the paradigmatic Bell scenario, detailing the efficiency requirements for the CHSH inequality, such as the 2/3 threshold for symmetric efficiencies, and traces the historical trajectory toward the first loophole-free tests. The analysis extends to other causal structures to explore how efficiency requirements are affected in different contexts. These include the instrumental scenario, which for binary variables has recently been shown to follow the same inefficiency bounds as the bipartite dichotomic Bell scenario; the prepare-and-measure scenario, where inefficiencies impact the certification of a quantum system's dimension and create security breaches in protocols such as Quantum Key Distribution (QKD); and the bilocality scenario, which exemplifies how employing multiple independent sources can significantly relax the required efficiencies to certify non-classical correlations."}
{"id": "2601.00354", "categories": ["nlin.CD", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.00354", "abs": "https://arxiv.org/abs/2601.00354", "authors": ["Ioannis Diamantis"], "title": "A Topological Framework for Atmospheric River Interaction Using Framed Braids", "comment": "33 pages, 23 figures", "summary": "Atmospheric Rivers (ARs) are filamentary moisture pathways responsible for a large fraction of extreme precipitation and often occur as interacting filament bundles within the same synoptic regime. Existing diagnostics typically analyze ARs in isolation, despite the frequent coexistence and interaction of multiple filaments. We introduce a topological framework for AR analysis based on framed braids and framed braidoids, which encodes both the geometric interaction of AR centroids and the internal evolution of moisture transport.\n  In this approach, AR filaments are represented as strands whose time-ordered crossings form braid words, while moisture-based framing captures internal intensification or weakening along each filament. Applying this framework to reanalysis-derived Atmospheric River track data, we construct braid and framed braid representations over sliding time windows and analyze a strongly interacting multi-filament AR episode in the North Pacific. The results show that braid-based indicators capture structural reorganizations and moisture intensification episodes that are not apparent from centroid geometry or IVT magnitude alone, offering a complementary structural perspective on atmospheric moisture transport."}
{"id": "2601.00505", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2601.00505", "abs": "https://arxiv.org/abs/2601.00505", "authors": ["Mario de Lucio", "Pavlos P. Vlachos", "Hector Gomez"], "title": "Effect of Electric Charge on Biotherapeutic Transport, Binding and Absorption: A Computational Study", "comment": "27 pages, 13 figures", "summary": "This study explores the effects of electric charge on the dynamics of drug transport and absorption in subcutaneous injections of monoclonal antibodies (mAbs). We develop a novel mathematical and computational model, based on the Nernst-Planck equations and porous media flow theory, to investigate the complex interactions between mAbs and charged species in subcutaneous tissue. The model enables us to study short-term transport dynamics and long-term binding and absorption for two mAbs with different electric properties. We examine the influence of buffer pH, body mass index, injection depth, and formulation concentration on drug distribution and compare our numerical results with experimental data from the literature."}
{"id": "2601.00085", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.00085", "abs": "https://arxiv.org/abs/2601.00085", "authors": ["Asha Ann Abraham", "Anjali Kumari", "Md Aktar Hossain", "Sanjoy Kr Mahatha", "Saikat Das", "A. K. Bera", "Soham Manni"], "title": "Probing the magnetic ground state and magnetoelastic coupling in double perovskite ruthenate: Ca2ScRuO6", "comment": "11 pages, including appendix", "summary": "Ruthenates, materials with a single magnetic Ruthenium (Ru) atom, often display an exotic array of ground states ranging from superconductivity to altermagnetism. In this work, we investigated the magnetic ground state of a least explored member of the 4d3 double perovskite ruthenate series A2ScRuO6 (A = Ca, Sr, Ba): Ca2ScRuO6. Interestingly, temperature-dependent bulk susceptibility curve shows ferrimagnetic-like behaviour above the magnetic ordering at around 40 K, which were corroborated by the identification of the mixed valence states, Ru5+ and Ru4+ via X-ray absorption spectroscopy. Structural analysis further revealed atomic-site exchange between the Ru and Sc sites, which results in the Ru mixed valence states. Neutron powder diffraction measurements detected the presence of magnetic Bragg peaks at a low temperature near 4 K and a moderate magnetoelastic coupling near the ordering temperature of 40 K. However, the corresponding symmetry analysis shows a weak Type I antiferromagnetic ground state with a reduced magnetic moment of 1.1μB/Ru atom. Our findings establish an unusual magnetic ground state in the Mott insulating Ca2ScRuO6, where a long range ordered antiferromagnet coexists with small magnetic clusters, which manifests a ferrimagnetic-like high temperature inverse magnetic susceptibility. This system presents a unique platform to study long-range magnetic order in the presence of antisite disorder."}
{"id": "2601.00293", "categories": ["q-fin.RM"], "pdf": "https://arxiv.org/pdf/2601.00293", "abs": "https://arxiv.org/abs/2601.00293", "authors": ["Pengpeng Li", "Shi-Dong Liang"], "title": "Option Pricing beyond Black-Scholes Model:Quantum Mechanics Approach", "comment": "12 pages, 5 figures", "summary": "Based on the analog between the stochastic dynamics and quantum harmonic oscillator, we propose a market force driving model to generalize the Black-Scholes model in finance market. We give new schemes of option pricing, in which we can take various unexpected market behaviors into account to modify the option pricing. As examples, we present several market forces to analyze their effects on the option pricing. These results provide us two practical applications. One is to be used as a new scheme of option pricing when we can predict some hidden market forces or behaviors emerging. The other implies the existence of some risk premium when some unexpected forces emerge."}
{"id": "2601.00137", "categories": ["physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.00137", "abs": "https://arxiv.org/abs/2601.00137", "authors": ["Rachel Middleton", "Ute Herzfeld", "Thomas Trantow"], "title": "Mapping Supraglacial Water as a Window into Surge Hydrology: Linking Surface Water, Drainage Efficiency, and Surge Dynamics on Negribreen, Svalbard", "comment": "53 pages, 16 figures, figures after references", "summary": "We analyze the dynamics of Negribreen Glacier System, a polythermal glacier in Svalbard, during its ongoing surge and investigate the role of supraglacial (surface) water as both an indicator of ice-dynamic processes and a driver of surge evolution. We identify three distinct surge phases: the initial acceleration phase, mature phase, and return to quiescence. Comparing the quiescent supraglacial hydrological state to each of the surge phases, we observe a sudden increase in hydrological connectivity between the glacier surface and base during initial acceleration, followed by a gradual return to quiescent water extent. In the mature surge phase, emergent water-filled crevasses coincide with regions of compressive forcing and extensive deformation, follow local accelerations, and preceded smaller, secondary accelerations. Additionaly, rapid drainage of surface ponds is observed in the mature surge. A data-fusion approach, using Maxar WorldView(c) imagery, ICESat-2 altimetry, and Sentinel-1 Synthetic Aperture Radar, is taken to create a time series of supraglacial water maps, water volumes, surface velocity changes, and spatial ice surface roughness. These observations provide a qualitative (process understanding) and quantitative (water time series) basis for supraglacial water sources as a driver and indicator of surge activity for Arctic glaciers."}
{"id": "2601.00521", "categories": ["eess.SY", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00521", "abs": "https://arxiv.org/abs/2601.00521", "authors": ["Cameron Hickert", "Sirui Li", "Zhengbing He", "Cathy Wu"], "title": "Probability-Aware Parking Selection", "comment": "10 pages, 6 figures, 3 tables. To be published in IEEE Transactions on Intelligent Transportation Systems", "summary": "Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates."}
{"id": "2601.00521", "categories": ["eess.SY", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00521", "abs": "https://arxiv.org/abs/2601.00521", "authors": ["Cameron Hickert", "Sirui Li", "Zhengbing He", "Cathy Wu"], "title": "Probability-Aware Parking Selection", "comment": "10 pages, 6 figures, 3 tables. To be published in IEEE Transactions on Intelligent Transportation Systems", "summary": "Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates."}
{"id": "2601.00287", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00287", "abs": "https://arxiv.org/abs/2601.00287", "authors": ["Kohei Yoshikawa", "Shuichi Kawano"], "title": "Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach", "comment": "20 pages, 3 figures", "summary": "The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method."}
{"id": "2601.00476", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.00476", "abs": "https://arxiv.org/abs/2601.00476", "authors": ["Trivikram Satharasi", "Tochukwu E. Ogri", "Muzaffar Qureshi", "Kyle Volle", "Rushikesh Kamalapurkar"], "title": "Safe Adaptive Feedback Control via Barrier States", "comment": "Submission in review for IFAC conference", "summary": "This paper presents a safe feedback control framework for nonlinear control-affine systems with parametric uncertainty by leveraging adaptive dynamic programming (ADP) with barrier-state augmentation. The developed ADP-based controller enforces control invariance by optimizing a value function that explicitly penalizes the barrier state, thereby embedding safety directly into the Bellman structure. The near-optimal control policy computed using model-based reinforcement learning is combined with a concurrent learning estimator to identify the unknown parameters and guarantee uniform convergence without requiring persistency of excitation. Using a barrier-state Lyapunov function, we establish boundedness of the barrier dynamics and prove closed-loop stability and safety. Numerical simulations on an optimal obstacle-avoidance problem validate the effectiveness of the developed approach."}
{"id": "2601.00541", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.00541", "abs": "https://arxiv.org/abs/2601.00541", "authors": ["Falong Tan", "Shan Tang", "Lixing Zhu"], "title": "Asymptotic Distribution-Free Tests for Ultra-high Dimensional Parametric Regressions via Projected Empirical Processes and $p$-value Combination", "comment": null, "summary": "This paper develops a novel methodology for testing the goodness-of-fit of sparse parametric regression models based on projected empirical processes and p-value combination, where the covariate dimension may substantially exceed the sample size. In such ultra-high dimensional settings, traditional empirical process-based tests often fail due to the curse of dimensionality or their reliance on the asymptotic linearity and normality of parameter estimators--properties that may not hold under ultra-high dimensional scenarios. To overcome these challenges, we first extend the classic martingale transformation to ultra-high dimensional settings under mild conditions and construct a Cramer-von Mises type test based on a martingale-transformed, projected residual-marked empirical process for any projection on the unit sphere. The martingale transformation renders this projected test asymptotically distribution-free and enables us to derive its limiting distribution using only standard convergence rates of parameter estimators. While the projected test is consistent for almost all projections on the unit sphere under mild conditions, it may still suffer from power loss for specific projections. Therefore, we further employ powerful p-value combination procedures, such as the Cauchy combination, to aggregate p-values across multiple projections, thereby enhancing overall robustness. Furthermore, recognizing that empirical process-based tests excel at detecting low-frequency signals while local smoothing tests are generally superior for high-frequency alternatives, we propose a novel hybrid test that aggregates both approaches using Cauchy combination. The resulting hybrid test is powerful against both low-frequency and high-frequency alternatives. $\\cdots$"}
{"id": "2601.00458", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00458", "abs": "https://arxiv.org/abs/2601.00458", "authors": ["Rik S. Breebaart", "Peter G. Bolhuis"], "title": "Combining multiple interface set path ensembles with MBAR reweighting", "comment": null, "summary": "We introduce a method to compute the reweighted path ensemble by combining transition interface sampling simulations conditioned on different collective variables. The approach is based on the Multistate Bennett Acceptance Ratio (MBAR) methodology applied to entire trajectories. Illustrating the technique with simple 2D potential models and a more complex host-guest system, we show that the statistics can significantly improve compared to a straightforward combination."}
{"id": "2601.00460", "categories": ["physics.soc-ph", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2601.00460", "abs": "https://arxiv.org/abs/2601.00460", "authors": ["Jnanajyoti Bhaumik", "Naoki Masuda"], "title": "Non-dilemmatic social dynamics promote cooperation in multilayer networks", "comment": "18 pages, 6 figures, together with Supplementary Information", "summary": "Various theoretical and empirical studies have accounted for why humans cooperate in competitive environments. Although prior work has revealed that network structure and multiplex interactions can promote cooperation, most theory assumes that individuals play similar dilemma games in all social contexts. However, real-world agents may participate in a diversity of interactions, not all of which present dilemmas. We develop an evolutionary game model on multilayer networks in which one layer supports the prisoner's dilemma game, while the other follows constant-selection dynamics, representing biased but non-dilemmatic competition, akin to opinion or fad spreading. Our theoretical analysis reveals that coupling a social dilemma layer to a non-dilemmatic constant-selection layer robustly enhances cooperation in many cases, across different multilayer networks, updating rules, and payoff schemes. These findings suggest that embedding individuals within diverse networked settings -- even those unrelated to direct social dilemmas -- can be a principled approach to engineering cooperation in socio-ecological and organizational systems."}
{"id": "2601.00301", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.00301", "abs": "https://arxiv.org/abs/2601.00301", "authors": ["Allal Guessab", "Federico Nudo"], "title": "Spectral Schur analysis of structured moment matrices for quadratic histopolation", "comment": null, "summary": "In this paper we study parameter-dependent structured moment matrices with a canonical block form arising from weighted quadratic histopolation on simplicial meshes. For a strictly positive density on a simplex, we construct compatible face densities and an orthogonal decomposition of the quadratic polynomial space into face and interior components, which induces a natural face-interior block structure. A reduced Schur complement is identified that fully characterizes enrichment and well-posedness and provides a sharp spectral stability result. We show that this quantity coincides with the square root of the smallest eigenvalue of a low-dimensional symmetric positive definite operator. This matrix-based viewpoint yields simple spectral criteria for the invertibility of local moment systems and motivates spectrally preferable choices of face and interior bases with improved conditioning. Using the resulting degrees of freedom together with density and scaling parameters as design variables, we formulate a small eigenvalue optimization problem aimed at improving stability and reducing the condition number of the global reconstruction system. Three-dimensional experiments on uniform and quasi-uniform simplicial meshes illustrate the predicted stability, conditioning, and convergence behaviour of the enriched quadratic reconstruction."}
{"id": "2601.00078", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00078", "abs": "https://arxiv.org/abs/2601.00078", "authors": ["Nicolas Zapata", "Najmeh Etehadi Abari", "Mitchell Field", "Patrick Winkel", "Simon Geisert", "Soeren Ihssen", "Anja Metelmann", "Ioan M. Pop"], "title": "Double-Pumped Kerr Parametric Amplifier Beyond the Gain-Bandwidth Limit", "comment": null, "summary": "Superconducting standing$-$wave parametric amplifiers are crucial for the readout of microwave quantum devices. Despite significant improvements in recent years, the need to operate near an instability point imposes a fundamental constraint: the instantaneous bandwidth decreases with increasing amplifier gain. Here we show that it is possible to obtain parametric amplification without instability by using two simultaneous drives that activate phase-preserving gain and frequency conversion. Realized in a granular aluminum dimer with Kerr nonlinearity, our method demonstrates a sixfold bandwidth increase at 20 dB gain, surpasses the conventional gain$-$bandwidth scaling up to 25 dB, and remains near the quantum limit."}
{"id": "2601.00546", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2601.00546", "abs": "https://arxiv.org/abs/2601.00546", "authors": ["Chenxi Zhao", "Dong Wu", "Weiyi Kong", "Oskar J. Haidn", "Xiangyu Hu"], "title": "Toward Efficient FSI Modeling in Patient-Specific Arteries: SPH Simulation of Blood Flow in Thin Deformable Vessels", "comment": "60 pages, 38 figures and 2 tables", "summary": "Accurate simulation of blood flow in deformable vessels is critical in cardiovascular research for understanding disease progression and informing clinical decision-making. However, due to the thin-walled nature of arteries, traditional smoothed particle hydrodynamics (SPH) approaches based on full-dimensional volume modeling often require extremely fine particle spacing to ensure numerical convergence for the solid mechanics. This, in turn, leads to redundant resolution in the fluid domain to maintain sufficient kernel support near the fluid-solid interface in fluid-structure interaction (FSI) simulations. To address this limitation, we propose an efficient reduced-dimensional shell-based SPH method for modeling thin-walled deformable arteries, and conduct FSI for capturing hemodynamics and arterial wall mechanics. Through a series of validation cases, the proposed shell model demonstrates comparable accuracy in fluid dynamics to the volume model, while achieving faster convergence in solid mechanics and reduced computational cost. We further investigate the influence of wall compliance on flow transitions and key hemodynamic indices, highlighting the necessity of FSI modeling over rigid-wall assumptions. Finally, the method is applied to two patient-specific vascular geometries, i.e. the carotid artery and the aorta, which demonstrates its robustness, efficiency and physiological relevance in realistic cardiovascular simulations."}
{"id": "2601.00101", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.00101", "abs": "https://arxiv.org/abs/2601.00101", "authors": ["T. Poulis", "G. Mani", "J. Sturt", "W. J. Duncan", "H. Thoma", "V. Hutanu", "B. Ouladdiaf", "I. Kibalin", "M. H. Lemee", "P. Manuel", "A. Neubauer", "C. Pfleiderer", "F. M. Grosche", "P. G. Niklowitz"], "title": "Spin-density wave of ferrimagnetic building blocks masking the ferromagnetic quantum-critical point in NbFe2", "comment": "6 pages", "summary": "In the metallic magnet NbFe2, the low temperature threshold of ferromagnetism can be investigated by varying the Fe concentration within a narrow homogeneity range. NbFe2 is one of a number of compounds where modulated order is found to mask the ferromagnetic quantum critical point. However, here we report the rare case where the masking modulated magnetic order has been fully refined. Spherical neutron polarimetry and high-intensity single-crystal neutron diffraction reveal the first case of a longitudinal spin-density wave masking the ferromagnetic quantum critical point. The spin-density wave is characterised by a large-wavelength incommensurate modulation of its low average moment. It is formed from ferrimagnetic building blocks with antiparallel ferromagnetic sheets. The existence of ferromagnetic sheets and cancellation of the magnetisation only over mesoscopic length scales show local similarity between the spin-density wave and the ferromagnetic parent phase and indicate the spin-density wave's unconventional nature as emerging from underlying ferromagnetic quantum criticality."}
{"id": "2601.00478", "categories": ["q-fin.RM", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2601.00478", "abs": "https://arxiv.org/abs/2601.00478", "authors": ["Zongxiao Wu", "Ran Liu", "Jiang Dai", "Dan Luo"], "title": "Multimodal Insights into Credit Risk Modelling: Integrating Climate and Text Data for Default Prediction", "comment": null, "summary": "Credit risk assessment increasingly relies on diverse sources of information beyond traditional structured financial data, particularly for micro and small enterprises (mSEs) with limited financial histories. This study proposes a multimodal framework that integrates structured credit variables, climate panel data, and unstructured textual narratives within a unified learning architecture. Specifically, we use long short-term memory (LSTM), the gated recurrent unit (GRU), and transformer models to analyse the interplay between these data modalities. The empirical results demonstrate that unimodal models based on climate or text data outperform those relying solely on structured data, while the integration of multiple data modalities yields significant improvements in credit default prediction. Using SHAP-based explainability methods, we find that physical climate risks play an important role in default prediction, with water-logging by rain emerging as the most influential factor. Overall, this study demonstrates the potential of multimodal approaches in AI-enabled decision-making, which provides robust tools for credit risk assessment while contributing to the broader integration of environmental and textual insights into predictive analytics."}
{"id": "2601.00629", "categories": ["physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.00629", "abs": "https://arxiv.org/abs/2601.00629", "authors": ["Rasmus E. Benestad"], "title": "Artificial intelligence and downscaling global climate model future projections", "comment": "9 pagers, 1 figure", "summary": "A critical review of artificial intelligence and deep machine learning (AI/ML) applied to downscaling of global climate model simulations provides some words of caution, based on past experiences and well-established principles. Recent papers tend to ignore more subtle successes with statistics and mathematical based downscaling, and there are examples of inappropriate evaluation strategies and incomplete accounts of the scientific progress when it comes to climate downscaling. An incomplete description state-of-the-art and a dogmatic approach to evaluation may give a deceiving impression that AI/ML is superior to more statistics and mathematics based methods."}
{"id": "2601.00178", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2601.00178", "abs": "https://arxiv.org/abs/2601.00178", "authors": ["Kaiming Luo"], "title": "Controlling synchronization dynamics via physics-informed neural networks", "comment": null, "summary": "Synchronization control in networked dynamical systems requires regulating not only whether coherence is achieved, but also when and to what extent it emerges. We propose a physics-informed neural network (PINN) framework for continuous-time synchronization regulation, in which system trajectories and control inputs are jointly parameterized and constrained by the governing dynamics. Macroscopic synchronization objectives are imposed directly at the trajectory level by enforcing persistence conditions on the order parameter after a prescribed target time. This formulation enables simultaneous control of synchronization time and coherence level without assuming any explicit feedback law or solving a strict optimal control problem. Numerical studies on networked Kuramoto oscillators demonstrate smooth synchronization with reduced transient control effort and competitive cumulative cost relative to analytical baselines. The framework remains effective in non-gradient and frustrated dynamics, highlighting physics-informed neural control as a flexible trajectory-level approach to synchronization regulation."}
{"id": "2601.00548", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00548", "abs": "https://arxiv.org/abs/2601.00548", "authors": ["Kooktae Lee"], "title": "Optimal Transport-Based Decentralized Multi-Agent Distribution Matching", "comment": null, "summary": "This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner."}
{"id": "2601.00548", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00548", "abs": "https://arxiv.org/abs/2601.00548", "authors": ["Kooktae Lee"], "title": "Optimal Transport-Based Decentralized Multi-Agent Distribution Matching", "comment": null, "summary": "This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner."}
{"id": "2601.00310", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00310", "abs": "https://arxiv.org/abs/2601.00310", "authors": ["Manganaw N'Daam", "Tchilabalo Abozou Kpanzou", "Edoh Katchekpele"], "title": "Asymptotic distribution of a robust wavelet-based NKK periodogram", "comment": "19 pages, 4 figures", "summary": "This paper investigates the asymptotic distribution of a wavelet-based NKK periodogram constructed from least absolute deviations (LAD) harmonic regression at a fixed resolution level. Using a wavelet representation of the underlying time series, we analyze the probabilistic structure of the resulting periodogram under long-range dependence. It is shown that, under suitable regularity conditions, the NKK periodogram converges in distribution to a nonstandard limit characterized as a quadratic form in a Gaussian random vector, whose covariance structure depends on the memory properties of the process and on the chosen wavelet filters. This result establishes a rigorous theoretical foundation for the use of robust wavelet-based periodograms in the spectral analysis of long-memory time series with heavy-tailed inovations."}
{"id": "2601.00632", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.00632", "abs": "https://arxiv.org/abs/2601.00632", "authors": ["Giacomo Borghi", "José A. Carrillo"], "title": "Variational inference via Gaussian interacting particles in the Bures-Wasserstein geometry", "comment": null, "summary": "Motivated by variational inference methods, we propose a zeroth-order algorithm for solving optimization problems in the space of Gaussian probability measures. The algorithm is based on an interacting system of Gaussian particles that stochastically explore the search space and self-organize around global minima via a consensus-based optimization (CBO) mechanism. Its construction relies on the Linearized Bures-Wasserstein (LBW) space, a novel parametrization of Gaussian measures we introduce for efficient computations. LBW is inspired by linearized optimal transport and preserves key geometric features while enabling computational tractability. We establish well-posedness and study the convergence properties of the particle dynamics via a mean-field approximation. Numerical experiments on variational inference tasks demonstrate the algorithm's robustness and superior performance with respect to gradient-based method in presence of non log-concave targets."}
{"id": "2601.00188", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.00188", "abs": "https://arxiv.org/abs/2601.00188", "authors": ["Landon Hurley"], "title": "An exact unbiased semi-parametric L2 quasi-likelihood framework, complete in the presence of ties", "comment": null, "summary": "Maximum likelihood style estimators possesses a number of ideal characteristics, but require prior identification of the distribution of errors to ensure exact unbiasedness. Independent of the focus of the primary statistical analysis, the estimation of a covariance matrix \\(S^{P \\times P}\\approx Σ^{P \\times P}\\) must possess a specific structure and regularity constraints. The need to estimate a linear Gaussian covariance models appear in various applications as a formal precondition for scientific investigation and predictive analytics. In this work, we construct an \\(\\ell_{2}\\)-norm based quasi-likelihood framework, identified by binomial comparisons between all pairs \\(X_{n},Y_{n}, \\forall {n}\\). Our work here focuses upon the quasi-likelihood basis for estimation of an exactly unbiased linear regression Hájek projection, within which the Kemeny metric space is operationalised via Whitney embedding to obtain exact unbiased minimum variance multivariate covariance estimators upon both discrete and continuous random variables (i.e., exact unbiased identification in the presence of ties upon finite samples). While the covariance estimator is inherently useful, expansion of the Wilcoxon rank-sum testing framework to handle multiple covariates with exact unbiasedness upon finite samples is a currently unresolved research problem, as it maintains identification in the presence of linear surjective mappings onto common points: this model space, by definition, expands our likelihood framework into a consistent non-parametric form of the standard general linear model, which we extend to address both unknown heterogeneity and the problem of weak inferential instruments."}
{"id": "2601.00628", "categories": ["physics.ao-ph", "physics.comp-ph", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00628", "abs": "https://arxiv.org/abs/2601.00628", "authors": ["Cédric Goeury", "Thierry Fouquet", "Maria Teles", "Michel Benoit"], "title": "Bayesian optimization for re-analysis and calibration of extreme sea state events simulated with a spectral third-generation wave model", "comment": null, "summary": "Accurate hindcasting of extreme sea state events is essential for coastal engineering, risk assessment, and climate studies. However, the reliability of numerical wave models remains limited by uncertainties in physical parameterizations and model inputs. This study presents a novel calibration framework based on Bayesian Optimization (BO), leveraging the Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters, specifically bottom friction dissipation, depth induced breaking, and wave dissipation from strong opposing currents, in the ANEMOC-3 hindcast wave model. The proposed method enables joint optimization of continuous parameters and discrete model structures, significantly reducing discrepancies between model outputs and observations. Applied to a one month period encompassing multiple intense storm events along the French Atlantic coast, the calibrated model demonstrates improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index relative to the default sea$-$state solver configuration. The results highlight the potential of BO to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to a wide range of geophysical modeling problems. Future extensions include multi-objective optimization, uncertainty quantification, and integration of additional observational datasets."}
{"id": "2601.00399", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.00399", "abs": "https://arxiv.org/abs/2601.00399", "authors": ["Chunmei Wang", "Shangyou Zhang"], "title": "A weak Galerkin least squares finite element method for linear convection equations in non-divergence form", "comment": "16 pages, 14 tables, 4 figures", "summary": "This article develops a weak Galerkin least-squares (WG--LS) finite element method for first-order linear convection equations in non-divergence form. The method is formulated using discontinuous finite element functions and does not require any coercivity assumption on the convection vector or reaction coefficient. The resulting discrete problem leads to a symmetric and positive definite linear system and is applicable to general polygonal and polyhedral meshes. Under minimal regularity assumptions on the coefficients, optimal-order error estimates are established for the WG--LS approximation in a suitable energy norm. Numerical experiments are presented to confirm the theoretical convergence results and to demonstrate the accuracy and efficiency of the proposed method."}
{"id": "2601.00111", "categories": ["quant-ph", "cond-mat.other", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00111", "abs": "https://arxiv.org/abs/2601.00111", "authors": ["J. Eisert"], "title": "A compellingly simple proof of the speed of sound for interacting bosons", "comment": "4 pages, 1 figure", "summary": "On physical grounds, one expects locally interacting quantum many-body systems to feature a finite group velocity. This intuition is rigorously underpinned by Lieb-Robinson bounds that state that locally interacting Hamiltonians with finite-dimensional constituents on suitably regular lattices always exhibit such a finite group velocity. This also implies that causality is always respected by the dynamics of quantum lattice models. It had been a long-standing open question whether interacting bosonic systems also feature finite speeds of sound in information and particle propagation, which was only recently resolved. This work proves a strikingly simple such bound for particle propagation - shown in literally a few elementary, yet not straightforward, lines - for generalized Bose-Hubbard models defined on general lattices, proving that appropriately locally perturbed stationary states feature a finite speed of sound in particle numbers."}
{"id": "2601.00770", "categories": ["cs.CE", "cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.00770", "abs": "https://arxiv.org/abs/2601.00770", "authors": ["Simon Paquette-Greenbaum", "Jiangbo Yu"], "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization", "comment": null, "summary": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported."}
{"id": "2601.00184", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00184", "abs": "https://arxiv.org/abs/2601.00184", "authors": ["Xue-Jia Yu", "Limei Xu", "Hai-Qing Lin"], "title": "Topological physics in quantum critical systems", "comment": "Invited review, published version. 56 pages", "summary": "Topology forms a cornerstone in modern condensed matter and statistical physics, offering a new framework to classify the phases and phase transitions beyond the traditional Landau paradigm. However, it is widely believed that topological properties are destroyed when the bulk energy gap closes, making it highly nontrivial to consider topology in gapless quantum critical systems. To address these challenges, recent advancements have sought to generalize the notion of topology to systems without a bulk energy gap, including quantum critical points and critical phases, collectively referred to as gapless symmetry-protected topological states. Extending topology to gapless quantum critical systems challenges the traditional belief in condensed matter physics that topological edge states are typically tied to the presence of a bulk energy gap. Furthermore, it suggests that topology plays a crucial role in classifying quantum phase transitions even if they belong to the same universality class, fundamentally enriching the textbook understanding of phase transitions. Given its importance, here we give a pedagogical review of the current progress of topological physics in quantum critical systems. We introduce the topological properties of quantum critical points and generalize them to stable critical phases, both for noninteracting and interacting systems. Additionally, we discuss further generalizations and future directions, including higher dimensions, nonequilibrium phase transitions, and realizations in modern experiments."}
{"id": "2601.00710", "categories": ["physics.geo-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.00710", "abs": "https://arxiv.org/abs/2601.00710", "authors": ["Mohammad Nooraiepour", "Mohammad Masoudi", "Helge Hellevang"], "title": "Carbon mineralization in CO2-seawater-basalt systems: Reactive transport dynamics and vesicular pore architecture controls", "comment": null, "summary": "Carbon mineralization in basaltic rocks may offer rapid, permanent \\ce{CO2} storage, yet fundamental controls on reactive transport and precipitation patterns remain poorly understood. This study integrates flow-through experiments at 80\\degree C using \\ce{CO2}-acidified seawater with geochemical simulation and multi-scale pore imaging to elucidate mineralization dynamics in basaltic glass. Results reveal that carbonate precipitation is nucleation-controlled and stochastic rather than growth-controlled and deterministic, with isolated accumulations forming randomly despite continuous supersaturation. Residence time exerts primary control: reducing flow rate from 0.05 to 0.005\\,mL/min proved necessary for visible precipitation. Post-experiment analyses identified calcium carbonate and smectite phases. Multi-scale characterization of three basalt facies revealed that connected porosity fractions (1.3--32\\%) differ significantly from total porosity (18--42\\%), demonstrating that network topology controls permeability. Micro-CT analysis revealed that pore coordination numbers in basalts (modal = 2) were notably lower than those in reservoir sandstones, creating serial flow paths that are vulnerable to catastrophic permeability loss from modest precipitation. Precipitation-induced clogging scenarios were proposed, where distributed small precipitates cause more severe permeability degradation than large accumulations. The use of seawater complicates geochemistry and reduces mineralization efficiency compared to freshwater. Findings emphasize the need for probabilistic reactive transport modeling frameworks and realistic pore topologies, which are fundamentally different from conventional CCS operations."}
{"id": "2601.00354", "categories": ["nlin.CD", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.00354", "abs": "https://arxiv.org/abs/2601.00354", "authors": ["Ioannis Diamantis"], "title": "A Topological Framework for Atmospheric River Interaction Using Framed Braids", "comment": "33 pages, 23 figures", "summary": "Atmospheric Rivers (ARs) are filamentary moisture pathways responsible for a large fraction of extreme precipitation and often occur as interacting filament bundles within the same synoptic regime. Existing diagnostics typically analyze ARs in isolation, despite the frequent coexistence and interaction of multiple filaments. We introduce a topological framework for AR analysis based on framed braids and framed braidoids, which encodes both the geometric interaction of AR centroids and the internal evolution of moisture transport.\n  In this approach, AR filaments are represented as strands whose time-ordered crossings form braid words, while moisture-based framing captures internal intensification or weakening along each filament. Applying this framework to reanalysis-derived Atmospheric River track data, we construct braid and framed braid representations over sliding time windows and analyze a strongly interacting multi-filament AR episode in the North Pacific. The results show that braid-based indicators capture structural reorganizations and moisture intensification episodes that are not apparent from centroid geometry or IVT magnitude alone, offering a complementary structural perspective on atmospheric moisture transport."}
{"id": "2601.00056", "categories": ["cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00056", "abs": "https://arxiv.org/abs/2601.00056", "authors": ["Nikolay V. Gnezdilov", "Andrei I. Pavlov"], "title": "Thermalization in a closed quantum system from randomized dynamics", "comment": "8 pages, 6 figures", "summary": "The emergence of statistical mechanics from quantum dynamics is a central problem in quantum many-body physics. Deriving observables aligned with the prediction of the canonical ensemble for a quantum system relies on the presence of a bath provided either as an external environment or as a larger part of a closed system. We demonstrate that thermal (canonical) observables for a whole closed quantum system of finite size can arise in the absence of a bath. These thermal observables stem from classical averaging over randomized unitary evolutions for a few-body system. The temperature in the canonical ensemble appears as a global constraint on the total energy of the system, determined by the choice of the initial state. From averaging randomized evolutions, we derive spin-spin correlation functions for a finite spin chain and show that they exhibit a temperature-dependent finite correlation length, in agreement with the prediction of the canonical ensemble. This establishes a method for computing thermal observables in a closed, finite-size system from real-time propagation without a bath. An implementation of this thermalization approach on a quantum computer can be utilized for thermal state preparation."}
{"id": "2601.00587", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00587", "abs": "https://arxiv.org/abs/2601.00587", "authors": ["Junyue Huang", "Shaoyuan Li", "Xiang Yin"], "title": "Stability Verification for Switched Systems using Neural Multiple Lyapunov Functions", "comment": null, "summary": "Stability analysis of switched systems, characterized by multiple operational modes and switching signals, is challenging due to their nonlinear dynamics. While frameworks such as multiple Lyapunov functions (MLF) provide a foundation for analysis, their computational applicability is limited for systems without favorable structure. This paper investigates stability analysis for switched systems under state-dependent switching conditions. We propose neural multiple Lyapunov functions (NMLF), a unified framework that combines the theoretical guarantees of MLF with the computational efficiency of neural Lyapunov functions (NLF). Our approach leverages a set of tailored loss functions and a counter-example guided inductive synthesis (CEGIS) scheme to train neural networks that rigorously satisfy MLF conditions. Through comprehensive simulations and theoretical analysis, we demonstrate NMLF's effectiveness and its potential for practical deployment in complex switched systems."}
{"id": "2601.00587", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00587", "abs": "https://arxiv.org/abs/2601.00587", "authors": ["Junyue Huang", "Shaoyuan Li", "Xiang Yin"], "title": "Stability Verification for Switched Systems using Neural Multiple Lyapunov Functions", "comment": null, "summary": "Stability analysis of switched systems, characterized by multiple operational modes and switching signals, is challenging due to their nonlinear dynamics. While frameworks such as multiple Lyapunov functions (MLF) provide a foundation for analysis, their computational applicability is limited for systems without favorable structure. This paper investigates stability analysis for switched systems under state-dependent switching conditions. We propose neural multiple Lyapunov functions (NMLF), a unified framework that combines the theoretical guarantees of MLF with the computational efficiency of neural Lyapunov functions (NLF). Our approach leverages a set of tailored loss functions and a counter-example guided inductive synthesis (CEGIS) scheme to train neural networks that rigorously satisfy MLF conditions. Through comprehensive simulations and theoretical analysis, we demonstrate NMLF's effectiveness and its potential for practical deployment in complex switched systems."}
{"id": "2601.00499", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00499", "abs": "https://arxiv.org/abs/2601.00499", "authors": ["Marcio A. Diniz", "Hulya Kocyigit", "Erin Moshier", "Madhu Mazumdar", "Deukwoo Kwon"], "title": "Continuous monitoring of delayed outcomes in basket trials", "comment": "28 pages, 8 figures, 6 tables", "summary": "Precision medicine has led to a paradigm shift allowing the development of targeted drugs that are agnostic to the tumor location. In this context, basket trials aim to identify which tumor types - or baskets - would benefit from the targeted therapy among patients with the same molecular marker or mutation. We propose the implementation of continuous monitoring for basket trials to increase the likelihood of early identification of non-promising baskets. Although the current Bayesian trial designs available in the literature can incorporate more than one interim analysis, most of them have high computational cost, and none of them handle delayed outcomes that are expected for targeted treatments such as immunotherapies. We leverage the Bayesian empirical approach proposed by Fujiwara et al., which has low computational cost. We also extend ideas of Cai et al to address the practical challenge of performing interim analysis with delayed outcomes using multiple imputation. Operating characteristics of four different strategies to handle delayed outcomes in basket trials are compared in an extensive simulation study with the benchmark strategy where trial accrual is put on hold until complete data is observed to make a decision. The optimal handling of missing data at interim analyses is trial-dependent. With slow accrual, missingness is minimal even with continuous monitoring, favoring simpler approaches over computationally intensive methods. Although individual sample-size savings are small, multiple imputation becomes more appealing when sample size savings scale with the number of baskets and agents tested."}
{"id": "2601.00732", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.00732", "abs": "https://arxiv.org/abs/2601.00732", "authors": ["Michalis Ramp", "Andreas Kasis", "Stelios Timotheou"], "title": "Stability of vehicular admission control schemes in urban traffic networks under modelling uncertainty", "comment": null, "summary": "Urban transportation networks face significant challenges due to traffic congestion, leading to adverse environmental and socioeconomic impacts. Vehicular admission control (VAC) strategies have emerged as a promising solution to alleviate congestion. By leveraging information and communication technologies, VAC strategies regulate vehicle entry into the network to optimize different traffic metrics of interest over space and time. Despite the significant development of VAC strategies, their stability at the presence of modelling uncertainty remains under-explored. This paper investigates the stability properties of a class of decentralized VAC schemes under modelling uncertainty. Specifically, we consider large-scale, heterogeneous urban traffic networks characterised by nonlinear dynamics and concave macroscopic fundamental diagrams with bounded uncertainty between flow, density, and speed. In this context, we examine a broad class of decentralized VAC dynamics, described by general nonlinear forms. Using passivity theory, we derive scalable, locally verifiable conditions on the design of VAC schemes, that enable stability guarantees in the presence of modelling uncertainty. Several examples are presented to illustrate the applicability of the proposed design framework. Our analytical results are validated through numerical simulations on a 6-region system, demonstrating their effectiveness and practical relevance."}
{"id": "2601.00284", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00284", "abs": "https://arxiv.org/abs/2601.00284", "authors": ["Neda Mohammadi", "Soham Sarkar", "Piotr Kokoszka"], "title": "Deep learning estimation of the spectral density of functional time series on large domains", "comment": null, "summary": "We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \\times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \\sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images."}
{"id": "2601.00404", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.00404", "abs": "https://arxiv.org/abs/2601.00404", "authors": ["T. Chaumont-Frelet"], "title": "Guaranteed stability bounds for second-order PDE problems satisfying a Garding inequality", "comment": null, "summary": "We propose an algorithm to numerically determined whether a second-order linear PDE problem satisfying a Garding inequality is well-posed. This algorithm further provides a lower bound to the inf-sup constant of the weak formulation, which may in turn be used for a posteriori error estimation purposes. Our numerical lower bound is based on two discrete singular value problems involving a Lagrange finite element discretization coupled with an a posteriori error estimator based on flux reconstruction techniques. We show that if the finite element discretization is sufficiently rich, our lower bound underestimates the optimal constant only by a factor roughly equal to two."}
{"id": "2601.00128", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.00128", "abs": "https://arxiv.org/abs/2601.00128", "authors": ["T. Rick Perche"], "title": "(PhD Thesis) The Information Locally Stored in Quantum Fields: From Entanglement to Gravity", "comment": "271 pages + appendices", "summary": "This is an updated version of my PhD thesis, defended at the University of Waterloo on the 2nd of April 2025, uploaded to the ArXiv with the goal of reaching a wider audience. The thesis is divided into 5 chapters, respectively containing (I) a brief introduction to local quantum field theory (QFT), (II) a description of local probes in QFT, (III) a discussion of entanglement in QFT and how to probe it, (IV) a description of the regimes where QFT interactions can be approximated by direct interactions, and (V) a discussion the information about the geometry of spacetime contained in quantum fields. The partial goal of this thesis is to serve as a guide for students aiming to tackle these different research programs. If the reader is interested in pursuing one or more research projects detailed here, they are encouraged to contact me for collaboration in these topics."}
{"id": "2601.00749", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.00749", "abs": "https://arxiv.org/abs/2601.00749", "authors": ["Y. Alexanian", "E. Lhotel", "J. Robert", "S. Petit", "E. Lefrançois", "P. Lejay", "A. Hadj-Azzem", "F. Damay", "J. Ollivier", "B. Fåk", "R. Ballou", "S. De Brion", "V. Simonet"], "title": "About the origin of the magnetic ground state of Tb$_{2}$Ir$_{2}$O$_{7}$", "comment": "16 pages", "summary": "Magnetic-rare-earth pyrochlore iridates exhibit a rich variety of unconventional phases, driven by the complex interactions within and between the rare-earth and the iridium sublattices. In this study, we investigate the peculiar magnetic state of Tb$_{2}$Ir$_{2}$O$_{7}$, where a component of the Tb$^{3+}$ moment orders perpendicular to its local Ising anisotropy axis. By means of neutron diffraction and inelastic neutron scattering down to dilution temperatures, complemented by specific heat measurements, we show that this intriguing magnetic state is fully established at 1.5 K and we characterize its excitation spectrum across a broad range of energies. Our calculations reveal that bilinear interactions between Tb$^{3+}$ ions subjected to the Ir molecular field capture several key features of the experiments, but need to be supplemented to fully reproduce the observed behavior."}
{"id": "2601.00294", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00294", "abs": "https://arxiv.org/abs/2601.00294", "authors": ["Bo-Ting Chen", "Yu-Ping Wang", "Biao Lian"], "title": "Bridging Commutant and Polynomial Methods for Hilbert Space Fragmentation", "comment": "17 pages", "summary": "A quantum model exhibits Hilbert space fragmentation (HSF) if its Hilbert space decomposes into exponentially many dynamically disconnected subspaces, known as Krylov subspaces. A model may however have different HSFs depending on the method for identifying them. Here we establish a connection between two vastly distinct methods recently proposed for identifying HSF: the commutant algebra (CA) method and integer characteristic polynomial factorization (ICPF) method. For a Hamiltonian consisting of operators admitting rational number matrix representations, we prove a theorem that, if its center of commutant algebra have all eigenvalues being rational, the HSF from the ICPF method must be equal to or finer than that from the CA method. We show that this condition is satisfied by most known models exhibiting HSF, for which we demonstrate the validity of our theorem. We further discuss representative models for which ICPF and CA methods yield different HSFs. Our results may facilitate the exploration of a unified definition of HSF."}
{"id": "2601.00755", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00755", "abs": "https://arxiv.org/abs/2601.00755", "authors": ["Mayuranath SureshKumar", "Hanumanthrao Kannan"], "title": "A formal theory on problem space as a semantic world model in systems engineering", "comment": "Submitted to Wiley Systems Engineering Journal", "summary": "Classic problem-space theory models problem solving as a navigation through a structured space of states, operators, goals, and constraints. Systems Engineering (SE) employs analogous constructs (functional analysis, operational analysis, scenarios, trade studies), yet still lacks a rigorous systems-theoretic representation of the problem space itself. In current practice, reasoning often proceeds directly from stakeholder goals to prescriptive artifacts. This makes foundational assumptions about the operational environment, admissible interactions, and contextual conditions implicit or prematurely embedded in architectures or requirements. This paper addresses that gap by formalizing the problem space as an explicit semantic world model containing theoretical constructs that are defined prior to requirements and solution commitments. These constructs along with the developed axioms, theorems and corollary establish a rigorous criterion for unambiguous boundary semantics, context-dependent interaction traceability to successful stakeholder goal satisfaction, and sufficiency of problem-space specification over which disciplined reasoning can occur independent of solution design. It offers a clear distinction between what is true of the problem domain and what is chosen as a solution. The paper concludes by discussing the significance of the theory on practitioners and provides a dialogue-based hypothetical case study between a stakeholder and an engineer, demonstrating how the theory guides problem framing before designing any prescriptive artifacts."}
{"id": "2601.00755", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00755", "abs": "https://arxiv.org/abs/2601.00755", "authors": ["Mayuranath SureshKumar", "Hanumanthrao Kannan"], "title": "A formal theory on problem space as a semantic world model in systems engineering", "comment": "Submitted to Wiley Systems Engineering Journal", "summary": "Classic problem-space theory models problem solving as a navigation through a structured space of states, operators, goals, and constraints. Systems Engineering (SE) employs analogous constructs (functional analysis, operational analysis, scenarios, trade studies), yet still lacks a rigorous systems-theoretic representation of the problem space itself. In current practice, reasoning often proceeds directly from stakeholder goals to prescriptive artifacts. This makes foundational assumptions about the operational environment, admissible interactions, and contextual conditions implicit or prematurely embedded in architectures or requirements. This paper addresses that gap by formalizing the problem space as an explicit semantic world model containing theoretical constructs that are defined prior to requirements and solution commitments. These constructs along with the developed axioms, theorems and corollary establish a rigorous criterion for unambiguous boundary semantics, context-dependent interaction traceability to successful stakeholder goal satisfaction, and sufficiency of problem-space specification over which disciplined reasoning can occur independent of solution design. It offers a clear distinction between what is true of the problem domain and what is chosen as a solution. The paper concludes by discussing the significance of the theory on practitioners and provides a dialogue-based hypothetical case study between a stakeholder and an engineer, demonstrating how the theory guides problem framing before designing any prescriptive artifacts."}
{"id": "2601.00508", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00508", "abs": "https://arxiv.org/abs/2601.00508", "authors": ["Simon Rudkin", "Wanling Rudkin"], "title": "ballmapper: Applying Topological Data Analysis Ball Mapper in Stata", "comment": "The accompanying GitHub is https://github.com/srudkin12/statabm", "summary": "Topological Data Analysis Ball Mapper (TDABM) offers a model-free visualization of multivariate data which does not necessitate the information loss associated with dimensionality reduction. TDABM Dlotko (2019) produces a cover of a multidimensional point cloud using equal size balls, the radius of the ball is the only parameter. A TDABM visualization retains the full structure of the data. The graphs produced by TDABM can convey coloration according to further variables, model residuals, or variables within the multivariate data. An expanding literature makes use of the power of TDABM across Finance, Economics, Geography, Medicine and Chemistry amongst others. We provide an introduction to TDABM and the \\texttt{ballmapper} package for Stata."}
{"id": "2601.00154", "categories": ["stat.ME", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00154", "abs": "https://arxiv.org/abs/2601.00154", "authors": ["Qianqian Qi", "Zhongming Chen", "Peter G. M. van der Heijden"], "title": "Unmixing highly mixed grain size distribution data via maximum volume constrained end member analysis", "comment": null, "summary": "End member analysis (EMA) unmixes grain size distribution (GSD) data into a mixture of end members (EMs), thus helping understand sediment provenance and depositional regimes and processes. In highly mixed data sets, however, many EMA algorithms find EMs which are still a mixture of true EMs. To overcome this, we propose maximum volume constrained EMA (MVC-EMA), which finds EMs as different as possible. We provide a uniqueness theorem and a quadratic programming algorithm for MVC-EMA. Experimental results show that MVC-EMA can effectively find true EMs in highly mixed data sets."}
{"id": "2601.00672", "categories": ["math.NA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00672", "abs": "https://arxiv.org/abs/2601.00672", "authors": ["Seungchan Ko", "Jiyeon Kim", "Dongwook Shin"], "title": "Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs", "comment": null, "summary": "In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction."}
{"id": "2601.00157", "categories": ["quant-ph", "physics.app-ph", "physics.atom-ph", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2601.00157", "abs": "https://arxiv.org/abs/2601.00157", "authors": ["Sean Lourette", "Andrey Jarmola", "Jabir Chathanathil", "Victor M. Acosta", "A. Glen Birdwell", "Peter Blümler", "Dmitry Budker", "Sebastián C. Carrasco", "Tony G. Ivanov", "Shimon Kolkowitz", "Vladimir S. Malinovsky"], "title": "Towards a temperature-insensitive composite diamond clock", "comment": "14 pages, 6 figures", "summary": "Frequency references based on solid state spins promise simplicity, compactness, robustness, multifunctionality, ease of integration, and high densities of emitters. Nitrogen-vacancy (NV) centers in diamond are a natural candidate, but the electronic zero-field splitting exhibits a large fractional temperature dependence, which has precluded its use as a stable clock transition. Here we show that this limitation can be overcome by forming a composite frequency reference that combines measurements of the electronic splitting D with the nuclear quadrupole splitting of the $^{14}$N nuclear spin intrinsic to the NV center. We further benchmark this composite approach against alternative strategies for mitigating temperature sensitivity. By implementing a specially designed pulse sequence with an eight-phase control scheme that suppresses pulse imperfections, we interleave measurements of D and Q in a high-density NV ensemble and demonstrate a temperature-compensated composite frequency reference. The stability of this composite diamond clock is characterized over a 10-day period at room temperature through a comparison to a Rb vapor-cell clock, yielding a fractional instability below $5 \\times 10^{-9}$ for an averaging time of $τ= 200$ s and below $1 \\times 10^{-8}$ at $τ= 2 \\times 10^5$ s, corresponding to measured improvements by a factor of 4 and 200, respectively, over a clock based purely on the single frequency D for the same periods. By characterizing the residual sensitivity to magnetic fields, optical power, and radio-frequency drive amplitudes, we find that temperature is no longer the dominant source of instability. These results establish complementary electron- and nuclear-spin transitions in diamond as a viable route to thermally robust frequency metrology, providing a pathway toward compact, multifunctional solid-state clocks and quantum sensors."}
{"id": "2601.00064", "categories": ["quant-ph", "cond-mat.str-el", "hep-th", "math.QA"], "pdf": "https://arxiv.org/pdf/2601.00064", "abs": "https://arxiv.org/abs/2601.00064", "authors": ["Yitao Feng", "Hanyu Xue", "Ryohei Kobayashi", "Po-Shen Hsin", "Yu-An Chen"], "title": "Pauli stabilizer formalism for topological quantum field theories and generalized statistics", "comment": "48 pages, 2 figures", "summary": "Topological quantum field theory (TQFT) provides a unifying framework for describing topological phases of matter and for constructing quantum error-correcting codes, playing a central role across high-energy physics, condensed matter, and quantum information. A central challenge is to formulate topological order on the lattice and to extract the properties of topological excitations from microscopic Hamiltonians. In this work, we construct new classes of lattice gauge theories as Pauli stabilizer models, realizing a wide range of TQFTs in general spacetime dimensions. We develop a lattice description of the resulting extended excitations and systematically determine their generalized statistics.\n  Our main example is the $(4+1)$D \\emph{fermionic-loop toric code}, obtained by condensing the $e^2 m^2$-loop in the $(4+1)$D $\\mathbb{Z}_4$ toric code. We show that the loop excitation exhibits fermionic loop statistics: the 24-step loop-flipping process yields a phase of $-1$. Our Pauli stabilizer models realize all twisted 2-form gauge theories in $(4+1)$D, the higher-form Dijkgraaf-Witten TQFT classified by $H^{5}(B^{2}G, U(1))$. % Beyond $(4+1)$D, the fermionic-loop toric codes form a family of $\\mathbb{Z}_2$ topological orders in arbitrary dimensions featuring fermionic loop excitations, realized as explicit Pauli stabilizer codes using $\\mathbb{Z}_4$ qudits. % Finally, we develop a Pauli-based framework that defines generalized statistics for extended excitations in any dimension, yielding computable lattice unitary processes to detect nontrivial generalized statistics. For example, we propose anyonic membrane statistics in $(6+1)$D, as well as fermionic membrane and volume statistics in arbitrary dimensions. We construct new families of $\\mathbb{Z}_2$ topological orders: the \\emph{fermionic-membrane toric code} and the \\emph{fermionic-volume toric code}."}
{"id": "2601.00410", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00410", "abs": "https://arxiv.org/abs/2601.00410", "authors": ["Simone Franchini"], "title": "Constructive Cavity Method", "comment": "8 pages. The research presented in this work was conducted in the period 2016-2020 within the LoTGlasSy project (ERC Grant 694925). Replaces arXiv:1909.06594v1", "summary": "We show that the functional appearing in the celebrated Parisi formula for the free energy of the Sherrington-Kirkpatrick model can be found from the incremental free energy obtained by Cavity Method if one assumes that the state is a product of independent Random Energy models."}
{"id": "2601.00531", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00531", "abs": "https://arxiv.org/abs/2601.00531", "authors": ["Raphael C. Kim", "Rachel C. Nethery", "Kevin L. Chen", "Falco J. Bargagli-Stoffi"], "title": "Fair Policy Learning under Bipartite Network Interference: Learning Fair and Cost-Effective Environmental Policies", "comment": null, "summary": "Numerous studies have shown the harmful effects of airborne pollutants on human health. Vulnerable groups and communities often bear a disproportionately larger health burden due to exposure to airborne pollutants. Thus, there is a need to design policies that effectively reduce the public health burdens while ensuring cost-effective policy interventions. Designing policies that optimally benefit the population while ensuring equity between groups under cost constraints is a challenging statistical and causal inference problem. In the context of environmental policy this is further complicated by the fact that interventions target emission sources but health impacts occur in potentially distant communities due to atmospheric pollutant transport -- a setting known as bipartite network interference (BNI). To address these issues, we propose a fair policy learning approach under BNI. Our approach allows to learn cost-effective policies under fairness constraints even accounting for complex BNI data structures. We derive asymptotic properties and demonstrate finite sample performance via Monte Carlo simulations. Finally, we apply the proposed method to a real-world dataset linking power plant scrubber installations to Medicare health records for more than 2 million individuals in the U.S. Our method determine fair scrubber allocations to reduce mortality under fairness and cost constraints."}
{"id": "2601.00221", "categories": ["eess.SY", "cs.DC", "cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00221", "abs": "https://arxiv.org/abs/2601.00221", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Impact of Clustering on the Observability and Controllability of Complex Networks", "comment": "Cluster Computing Journal", "summary": "The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering."}
{"id": "2601.00729", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.00729", "abs": "https://arxiv.org/abs/2601.00729", "authors": ["Mohamed El Guide", "Alaa El Ichi", "Khalide Jbilou", "Lothar Reichel", "Hessah Alqahtani"], "title": "A Unified Trace-Optimization Framework for Multidimensionality Reduction", "comment": null, "summary": "This paper presents a comprehensive overview of several multidimensional reduction methods focusing on Multidimensional Principal Component Analysis (MPCA), Multilinear Orthogonal Neighborhood Preserving Projection (MONPP), Multidimensional Locally Linear Embedding (MLLE), and Multidimensional Laplacian Eigenmaps (MLE). These techniques are formulated within a unified framework based on trace optimization, where the dimensionality reduction problem is expressed as maximization or minimization problems. In addition to the linear MPCA and MONPP approaches, kernel-based extensions of these methods also are presented. The latter methods make it possible to capture nonlinear relations between high-dimensional data. A comparative analysis highlights the theoretical foundations, assumptions, and computational efficiency of each method, as well as their practical applicability. The study provides insights and guidelines for selecting an appropriate dimensionality reduction technique suited to the application at hand."}
{"id": "2601.00198", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00198", "abs": "https://arxiv.org/abs/2601.00198", "authors": ["Keyi Huang", "Qi Zhang", "Xiangjing Liu", "Ruiqing Li", "Xinyue Long", "Hongfeng Liu", "Xiangyu Wang", "Yu-ang Fan", "Yuxuan Zheng", "Yufang Feng", "Yu Zhou", "Jack Ng", "Xinfang Nie", "Zhong-Xiao Man", "Dawei Lu"], "title": "Reversing Heat Flow by Coherence in a Multipartite Quantum System", "comment": "6+18 pages, Comments are welcome!", "summary": "The second law of thermodynamics dictates that heat flows spontaneously from a high-temperature entity to a lower-temperature one. Yet, recent advances have demonstrated that quantum correlations between a system and its thermal environment can induce a reversal of heat flow, challenging classical thermodynamic expectations. Here, we experimentally demonstrate that internal quantum coherence in a multipartite spin system can also reverse heat flow, without relying on initial correlations with the environment. Under the collision model with cascade interaction, we verify that both the strength and the phase of the coherence term determine the direction and magnitude of energy transfer. These results enable precise control of heat flow using only local quantum properties."}
{"id": "2601.00294", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00294", "abs": "https://arxiv.org/abs/2601.00294", "authors": ["Bo-Ting Chen", "Yu-Ping Wang", "Biao Lian"], "title": "Bridging Commutant and Polynomial Methods for Hilbert Space Fragmentation", "comment": "17 pages", "summary": "A quantum model exhibits Hilbert space fragmentation (HSF) if its Hilbert space decomposes into exponentially many dynamically disconnected subspaces, known as Krylov subspaces. A model may however have different HSFs depending on the method for identifying them. Here we establish a connection between two vastly distinct methods recently proposed for identifying HSF: the commutant algebra (CA) method and integer characteristic polynomial factorization (ICPF) method. For a Hamiltonian consisting of operators admitting rational number matrix representations, we prove a theorem that, if its center of commutant algebra have all eigenvalues being rational, the HSF from the ICPF method must be equal to or finer than that from the CA method. We show that this condition is satisfied by most known models exhibiting HSF, for which we demonstrate the validity of our theorem. We further discuss representative models for which ICPF and CA methods yield different HSFs. Our results may facilitate the exploration of a unified definition of HSF."}
{"id": "2601.00458", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00458", "abs": "https://arxiv.org/abs/2601.00458", "authors": ["Rik S. Breebaart", "Peter G. Bolhuis"], "title": "Combining multiple interface set path ensembles with MBAR reweighting", "comment": null, "summary": "We introduce a method to compute the reweighted path ensemble by combining transition interface sampling simulations conditioned on different collective variables. The approach is based on the Multistate Bennett Acceptance Ratio (MBAR) methodology applied to entire trajectories. Illustrating the technique with simple 2D potential models and a more complex host-guest system, we show that the statistics can significantly improve compared to a straightforward combination."}
{"id": "2601.00773", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00773", "abs": "https://arxiv.org/abs/2601.00773", "authors": ["Sinan Acemoglu", "Christian Kleiber", "Jörg Urban"], "title": "Variable Importance in Generalized Linear Models -- A Unifying View Using Shapley Values", "comment": "33 pages, 3 figures", "summary": "Variable importance in regression analyses is of considerable interest in a variety of fields. There is no unique method for assessing variable importance. However, a substantial share of the available literature employs Shapley values, either explicitly or implicitly, to decompose a suitable goodness-of-fit measure, in the linear regression model typically the classical $R^2$. Beyond linear regression, there is no generally accepted goodness-of-fit measure, only a variety of pseudo-$R^2$s. We formulate and discuss the desirable properties of goodness-of-fit measures that enable Shapley values to be interpreted in terms of relative, and even absolute, importance. We suggest to use a pseudo-$R^2$ based on the Kullback-Leibler divergence, the Kullback-Leibler $R^2$, which has a convenient form for generalized linear models and permits to unify and extend previous work on variable importance for linear and nonlinear models. Several examples are presented, using data from public health and insurance."}
{"id": "2601.00009", "categories": ["q-fin.CP", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00009", "abs": "https://arxiv.org/abs/2601.00009", "authors": ["Lucas Arenstein", "Michael Kastoryano"], "title": "Full grid solution for multi-asset options pricing with tensor networks", "comment": null, "summary": "Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power."}
{"id": "2601.00214", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00214", "abs": "https://arxiv.org/abs/2601.00214", "authors": ["Yecheng Xue", "Rui Yang", "Zhiding Liang", "Tongyang Li"], "title": "DC-MBQC: A Distributed Compilation Framework for Measurement-Based Quantum Computing", "comment": "14 pages, 10 figures. To appear in the IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2026", "summary": "Distributed quantum computing (DQC) is a promising technique for scaling up quantum systems. While significant progress has been made in DQC for quantum circuit models, there exists much less research on DQC for measurement-based quantum computing (MBQC), which is a universal quantum computing model that is essentially different from the circuit model and particularly well-suited to photonic quantum platforms. In this paper, we propose DC-MBQC, the first distributed quantum compilation framework tailored for MBQC. We identify and address two key challenges in enabling DQC for MBQC. First, for task allocation among quantum processing units (QPUs), we develop an adaptive graph partitioning algorithm that preserves the structure of the graph state while balancing the workload across QPUs. Second, for inter-QPU communication, we introduce the layer scheduling problem and propose an algorithm to solve it. Regrading realistic hardware requirements, we optimize the execution time of running quantum programs and the corresponding required photon lifetime to avoid fatal failures caused by photon loss. Our experiments demonstrate a $7.46\\times$ improvement on required photon lifetime and $6.82\\times$ speedup with 8 fully-connected QPUs, which further confirm the advantage of distributed quantum computing in photonic systems. The source code is publicly available at https://github.com/qfcwj/DC-MBQC."}
{"id": "2508.21012", "categories": ["physics.comp-ph", "cond-mat.soft", "cond-mat.stat-mech", "nlin.CD", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2508.21012", "abs": "https://arxiv.org/abs/2508.21012", "authors": ["Magnus F Ivarsen"], "title": "Kinetic Turing Instability and Emergent Spectral Scaling in Chiral Active Turbulence", "comment": "8 pages, 6 figures", "summary": "The spontaneous emergence of coherent structures from chaotic backgrounds is a hallmark of active biological swarms. We investigate this self-organization by simulating an ensemble of polar chiral active agents that couple locally via a Kuramoto interaction. We demonstrate that the system's transition from chaos to active turbulence is characterized by quantized loop phase currents and coherent clustering, and that this transition is strictly governed by a kinetic Turing instability. By deriving the continuum kinetic theory for the model, we identify that the competition between local phase-locking and active agent motility selects a critical structural wavenumber. The instability then drives the system into a state of developed, active turbulence that exhibits stable, robust power-laws in spectral density, suggestive of universality and consistent with observations from a broad range of turbulent phenomena. Our results bridge the gap between discrete chimera states and continuous fluid turbulence, suggesting that the statistical scaling laws of active turbulence can arise from fundamental kinetic instability criteria."}
{"id": "2601.00091", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00091", "abs": "https://arxiv.org/abs/2601.00091", "authors": ["Manuel Sáenz", "Pragya Sur"], "title": "Characterizing Finite-Dimensional Posterior Marginals in High-Dimensional GLMs via Leave-One-Out", "comment": null, "summary": "We investigate Bayes posterior distributions in high-dimensional generalized linear models (GLMs) under the proportional asymptotics regime, where the number of features and samples diverge at a comparable rate. Specifically, we characterize the limiting behavior of finite-dimensional marginals of the posterior. We establish that the posterior does not contract in this setting. Yet, the finite-dimensional posterior marginals converge to Gaussian tilts of the prior, where the mean of the Gaussian depends on the true signal coordinates of interest. Notably, the effect of the prior survives even in the limit of large samples and dimensions. We further characterize the behavior of the posterior mean and demonstrate that the posterior mean can strictly outperform the maximum likelihood estimate in mean-squared error in natural examples. Importantly, our results hold regardless of the sparsity level of the underlying signal. On the technical front, we introduce leave-one-out strategies for studying these marginals that may be of independent interest for analyzing low-dimensional functionals of high-dimensional signals in other Bayesian inference problems."}
{"id": "2601.00242", "categories": ["quant-ph", "cs.AI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00242", "abs": "https://arxiv.org/abs/2601.00242", "authors": ["Yotam Peled", "David Zenati", "Eliya Nachmani"], "title": "Neural Minimum Weight Perfect Matching for Quantum Error Codes", "comment": null, "summary": "Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching."}
{"id": "2601.00184", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00184", "abs": "https://arxiv.org/abs/2601.00184", "authors": ["Xue-Jia Yu", "Limei Xu", "Hai-Qing Lin"], "title": "Topological physics in quantum critical systems", "comment": "Invited review, published version. 56 pages", "summary": "Topology forms a cornerstone in modern condensed matter and statistical physics, offering a new framework to classify the phases and phase transitions beyond the traditional Landau paradigm. However, it is widely believed that topological properties are destroyed when the bulk energy gap closes, making it highly nontrivial to consider topology in gapless quantum critical systems. To address these challenges, recent advancements have sought to generalize the notion of topology to systems without a bulk energy gap, including quantum critical points and critical phases, collectively referred to as gapless symmetry-protected topological states. Extending topology to gapless quantum critical systems challenges the traditional belief in condensed matter physics that topological edge states are typically tied to the presence of a bulk energy gap. Furthermore, it suggests that topology plays a crucial role in classifying quantum phase transitions even if they belong to the same universality class, fundamentally enriching the textbook understanding of phase transitions. Given its importance, here we give a pedagogical review of the current progress of topological physics in quantum critical systems. We introduce the topological properties of quantum critical points and generalize them to stable critical phases, both for noninteracting and interacting systems. Additionally, we discuss further generalizations and future directions, including higher dimensions, nonequilibrium phase transitions, and realizations in modern experiments."}
{"id": "2601.00247", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00247", "abs": "https://arxiv.org/abs/2601.00247", "authors": ["Martin Plesch", "Martin Friák", "Ijaz Ahamed Mohammad"], "title": "Efficient implementation of single particle Hamiltonians in exponentially reduced qubit space", "comment": "12 pages, 2 figures", "summary": "Current and near-term quantum hardware is constrained by limited qubit counts, circuit depth, and the high cost of repeated measurements. We address these challenges for solid state Hamiltonians by introducing a logarithmic-qubit encoding that maps a system with $N$ physical sites onto only $\\lceil \\log_2 N \\rceil$ qubits while maintaining a clear correspondence with the underlying physical model. Within this reduced register, we construct a compatible variational circuit and a Gray-code-inspired measurement strategy whose number of global settings grows only logarithmically with system size. To quantify the overall hardware load, we introduce a volumetric efficiency metric that combines the number of qubit, circuit depth, and the number of measurement settings into a single measure, expressing the overall computation costs. Using this metric, we show that the total space-time-sampling volume required in a variational loop can be reduced dramatically from $N^2$ to $(logN)^3$ for hardware efficient ansatz, allowing an exponential reduction in time and size of the quantum hardware. These results demonstrate that large, structured solid-state Hamiltonians can be simulated on substantially smaller quantum registers with controlled sampling overhead and manageable circuit complexity, extending the reach of variational quantum algorithms on near-term devices."}
{"id": "2601.00220", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00220", "abs": "https://arxiv.org/abs/2601.00220", "authors": ["Bibek Saha", "Sthitadhi Roy"], "title": "Anderson localisation in spatially structured random graphs", "comment": "18 pages, 12 figures", "summary": "We study Anderson localisation on high-dimensional graphs with spatial structure induced by long-ranged but distance-dependent hopping. To this end, we introduce a class of models that interpolate between the short-range Anderson model on a random regular graph and fully connected models with statistically uniform hopping, by embedding a random regular graph into a complete graph and allowing hopping amplitudes to decay exponentially with graph distance. The competition between the exponentially growing number of neighbours with graph distance and the exponentially decaying hopping amplitude positions our models effectively as power-law hopping generalisation of the Anderson model on random regular graphs. Using a combination of numerical exact diagonalisation and analytical renormalised perturbation theory, we establish the resulting localisation phase diagram emerging from the interplay of the lengthscale associated to the hopping range and the onsite disorder strength. We find that increasing the hopping range shifts the localisation transition to stronger disorder, and that beyond a critical range the localised phase ceases to exist even at arbitrarily strong disorder. Our results indicate a direct Anderson transition between delocalised and localised phases, with no evidence for an intervening multifractal phase, for both deterministic and random hopping models. A scaling analysis based on inverse participation ratios reveals behaviour consistent with a Kosterlitz-Thouless-like transition with two-parameter scaling, in line with Anderson transitions on high-dimensional graphs. We also observe distinct critical behaviour in average and typical correlation functions, reflecting the different scaling properties of generalised inverse participation ratios."}
{"id": "2601.00259", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00259", "abs": "https://arxiv.org/abs/2601.00259", "authors": ["Rohit Kumar Shukla", "Amikam Levy"], "title": "First appearance of quasiprobability negativity in quantum many-body dynamics", "comment": "16 pages, 7 figures", "summary": "Quasiprobability distributions capture aspects of quantum dynamics that have no classical counterpart, yet the dynamical emergence of their negativity in many-body systems remains largely unexplored. We introduce the \\emph{first-time negativity} (FTN) of the Margenau-Hill quasiprobability as a dynamical indicator of when local measurement sequences in an interacting quantum system begin to exhibit genuinely nonclassical behavior. Using the Ising chain, we show that FTN discriminates clearly between interaction-dominated and field-dominated regimes, is systematically reshaped by temperature, and responds sensitively to the breaking of integrability. When measurements are performed on different sites, FTN reveals a characteristic spatio-temporal structure that reflects the finite-time spreading of operator incompatibility across the lattice. We further compare the numerical onset of negativity with a recently proposed quantum speed limit (QSL) for quasiprobabilities, which provides a geometric benchmark for the observed dynamics. Our results identify FTN as a practical and experimentally accessible probe of real-time quantum coherence and contextuality, directly suited to current platforms capable of sequential weak and strong measurements."}
{"id": "2601.00266", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00266", "abs": "https://arxiv.org/abs/2601.00266", "authors": ["Wai-Keong Mok", "Tobias Haug", "Wen Wei Ho", "John Preskill"], "title": "Nature is stingy: Universality of Scrooge ensembles in quantum many-body systems", "comment": "16 pages, 5 figures + Appendices", "summary": "Recent advances in quantum simulators allow direct experimental access to the ensemble of pure states generated by measuring part of an isolated quantum many-body system. These projected ensembles encode fine-grained information beyond thermal expectation values and provide a new window into quantum thermalization. In chaotic dynamics, projected ensembles exhibit universal statistics, a phenomenon known as deep thermalization. While infinite-temperature systems generate Haar-random ensembles, realistic physical constraints such as finite temperature or conservation laws require a more general framework. It has been proposed that deep thermalization is governed in general by the emergence of Scrooge ensembles, maximally entropic distributions of pure states consistent with the underlying constraints. Here we provide rigorous arguments supporting this proposal. To characterize this universal behavior, we invoke Scrooge $k$-designs, which approximate Scrooge ensembles, and identify three physically distinct mechanisms for their emergence. First, global Scrooge designs can arise from long-time chaotic unitary dynamics alone, without the need for measurements. Second, if the global state is highly scrambled, a local Scrooge design is induced when the complementary subsystem is measured. Third, a local Scrooge ensemble arises from an arbitrary entangled state when the complementary system is measured in a highly scrambled basis. Numerical simulations across a range of many-body systems identify coherence, entanglement, non-stabilizerness, and information scrambling as essential resources for the emergence of Scrooge-like behavior. Taken together, our results establish a unified theoretical framework for the emergence of maximally entropic, information-stingy randomness in quantum many-body systems."}
{"id": "2601.00266", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00266", "abs": "https://arxiv.org/abs/2601.00266", "authors": ["Wai-Keong Mok", "Tobias Haug", "Wen Wei Ho", "John Preskill"], "title": "Nature is stingy: Universality of Scrooge ensembles in quantum many-body systems", "comment": "16 pages, 5 figures + Appendices", "summary": "Recent advances in quantum simulators allow direct experimental access to the ensemble of pure states generated by measuring part of an isolated quantum many-body system. These projected ensembles encode fine-grained information beyond thermal expectation values and provide a new window into quantum thermalization. In chaotic dynamics, projected ensembles exhibit universal statistics, a phenomenon known as deep thermalization. While infinite-temperature systems generate Haar-random ensembles, realistic physical constraints such as finite temperature or conservation laws require a more general framework. It has been proposed that deep thermalization is governed in general by the emergence of Scrooge ensembles, maximally entropic distributions of pure states consistent with the underlying constraints. Here we provide rigorous arguments supporting this proposal. To characterize this universal behavior, we invoke Scrooge $k$-designs, which approximate Scrooge ensembles, and identify three physically distinct mechanisms for their emergence. First, global Scrooge designs can arise from long-time chaotic unitary dynamics alone, without the need for measurements. Second, if the global state is highly scrambled, a local Scrooge design is induced when the complementary subsystem is measured. Third, a local Scrooge ensemble arises from an arbitrary entangled state when the complementary system is measured in a highly scrambled basis. Numerical simulations across a range of many-body systems identify coherence, entanglement, non-stabilizerness, and information scrambling as essential resources for the emergence of Scrooge-like behavior. Taken together, our results establish a unified theoretical framework for the emergence of maximally entropic, information-stingy randomness in quantum many-body systems."}
{"id": "2601.00330", "categories": ["physics.soc-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00330", "abs": "https://arxiv.org/abs/2601.00330", "authors": ["Xiangrong Wang", "Xin Yu", "Zongze Wu", "Yamir Moreno"], "title": "Effective Graph Resistance as Cumulative Heat Dissipation", "comment": "APS one-column style. 32 pages and 10 figures", "summary": "Effective graph resistance is a fundamental structural metric in network science, widely used to quantify global connectivity, compare network architectures, and assess robustness in flow-based systems. Despite its importance, current formulations rely mainly on spectral or pseudo-inverse Laplacian representations, offering limited physical insight into how structural features shape this quantity or how it can be efficiently optimized. Here, we establish an exact and physically transparent relationship between effective graph resistance and the cumulative heat dissipation generated by Laplacian diffusion dynamics. We show that the total heat dissipated during relaxation to equilibrium precisely equals the effective graph resistance. This dynamical viewpoint uncovers a natural multi-scale decomposition of the Laplacian spectrum: early-time dissipation is governed by degree-based local structure, intermediate times isolate eigenvalues below the spectral mean, and long times are dominated by the algebraic connectivity. These multi-scale properties yield continuous and interpretable strategies for modifying network structure and constructing optimized ensembles, enabling improvements that are otherwise NP-hard to achieve via combinatorial methods. Our results unify structural and dynamical perspectives on network connectivity and provide new tools for analyzing, comparing, and optimizing complex networks across domains."}
{"id": "2601.00337", "categories": ["quant-ph", "cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.00337", "abs": "https://arxiv.org/abs/2601.00337", "authors": ["Daniel Alabi", "Theshani Nuradha"], "title": "When Does Quantum Differential Privacy Compose?", "comment": "36 pages, 1 figure", "summary": "Composition is a cornerstone of classical differential privacy, enabling strong end-to-end guarantees for complex algorithms through composition theorems (e.g., basic and advanced). In the quantum setting, however, privacy is defined operationally against arbitrary measurements, and classical composition arguments based on scalar privacy-loss random variables no longer apply. As a result, it has remained unclear when meaningful composition guarantees can be obtained for quantum differential privacy (QDP).\n  In this work, we clarify both the limitations and possibilities of composition in the quantum setting. We first show that classical-style composition fails in full generality for POVM-based approximate QDP: even quantum channels that are individually perfectly private can completely lose privacy when combined through correlated joint implementations. We then identify a setting in which clean composition guarantees can be restored. For tensor-product channels acting on product neighboring inputs, we introduce a quantum moments accountant based on an operator-valued notion of privacy loss and a matrix moment-generating function. Although the resulting Rényi-type divergence does not satisfy a data-processing inequality, we prove that controlling its moments suffices to bound measured Rényi divergence, yielding operational privacy guarantees against arbitrary measurements. This leads to advanced-composition-style bounds with the same leading-order behavior as in the classical theory.\n  Our results demonstrate that meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements, and provide a principled framework for understanding which classical ideas do and do not extend to the quantum setting."}
{"id": "2601.00511", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00511", "abs": "https://arxiv.org/abs/2601.00511", "authors": ["Anton Kapustin", "Daniil Radamovich"], "title": "Chaos and thermalization in Clifford-Floquet dynamics", "comment": "20 pages, 6 figures", "summary": "We study the ergodic properties of a unitary Floquet dynamics arising from the repeated application of a translationally-invariant Clifford Quantum Cellular Automata to an infinite system of qubits in d dimensions. One expects that if the QCA does not exhibit any periodicity, a generic initial state of qubits will thermalize, that is, approach the infinite-temperature state. We show that this is true for many classes of states, both pure and mixed. In particular, this is true for all initial states that are short-range entangled and close to the equilibrium state. We also point out a subtle distinction between weak and strong thermalization."}
{"id": "2601.00383", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00383", "abs": "https://arxiv.org/abs/2601.00383", "authors": ["Xian Shi"], "title": "Probabilistic Entanglement Distillation and Cost under Approximately Nonentangling and Dually Nonentangling Instruments", "comment": null, "summary": "Entanglement distillation and entanglement cost are fundamental tasks in quantum entanglement theory. This work studies both in the probabilistic setting and focuses on the asymptotic error exponent of probabilistic entanglement distillation when the operational model is $δ$-approximately nonentangling(ANE) and $δ$-approximately dually nonentangling(ADNE) quantum instruments. While recent progress has clarified limitations of probabilistic transformations in general resource theories, an analytic formula for the error exponent of probabilistic entanglement distillation under approximately (dually) nonentangling operations has remained unavailable.\n  Building on the framework of postselected quantum hypothesis testing, we establish a direct connection between probabilistic distillation and postselected hypothesis testing against the set of separable states. In particular, we derive an analytical characterization of the distillation error exponent under ANE. Besides, we relate the exponent to postselected hypothesis testing with measurements restricted to be separable. We further investigate probabilistic entanglement dilution and establish a relation between probabilistic entanglement costs under approximately nonentangling and approximately dually nonentangling instruments, together with a bound on the probabilistic entanglement cost under nonentangling instruments"}
{"id": "2601.00761", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00761", "abs": "https://arxiv.org/abs/2601.00761", "authors": ["Zhenyu Xiao", "Shinsei Ryu"], "title": "Exponentially Accelerated Sampling of Pauli Strings for Nonstabilizerness", "comment": null, "summary": "Quantum magic, quantified by nonstabilizerness, measures departures from stabilizer structure and underlies potential quantum speedups. We introduce an efficient classical algorithm that exactly computes stabilizer Rényi entropies and stabilizer nullity for generic many-body wavefunctions of $N$ qubits. The method combines the fast Walsh-Hadamard transform with an exact partition of Pauli operators. It achieves an exponential speedup over direct approaches, reducing the average cost per sampled Pauli string from $O(2^N)$ to $O(N)$. Building on this framework, we further develop a Monte-Carlo estimator for stabilizer Rényi entropies together with a Clifford-based variance-reduction scheme that suppresses sampling fluctuations. We benchmark the accuracy and efficiency on ensembles of random magic states, and apply the method to random Clifford circuits with doped $T$ gates, comparing different doping architectures. Our approach applies to arbitrary quantum states and provides quantitative access to magic resources both encoded in highly entangled states and generated by long-time nonequilibrium dynamics."}
{"id": "2601.00405", "categories": ["quant-ph", "hep-ph", "hep-th", "nucl-ex", "nucl-th"], "pdf": "https://arxiv.org/pdf/2601.00405", "abs": "https://arxiv.org/abs/2601.00405", "authors": ["Dmitri E. Kharzeev"], "title": "The Maximal Entanglement Limit in Statistical and High Energy Physics", "comment": "70 pages, 11 figures; Lectures at the 65th Jubilee Cracow School of Theoretical Physics, Zakopane, Tatra mountains, Poland, June 14-21, 2025", "summary": "These lectures advocate the idea that quantum entanglement provides a unifying foundation for both statistical physics and high-energy interactions. I argue that, at sufficiently long times or high energies, most quantum systems approach a Maximal Entanglement Limit (MEL) in which phases of quantum states become unobservable, reduced density matrices acquire a thermal form, and probabilistic descriptions emerge without invoking ergodicity or classical randomness. Within this framework, the emergence of probabilistic parton model, thermalization in the break-up of confining strings and in high-energy collisions, and the universal small $x$ behavior of structure functions arise as direct consequences of entanglement and geometry of high-dimensional Hilbert space."}
{"id": "2601.00425", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00425", "abs": "https://arxiv.org/abs/2601.00425", "authors": ["Salman Sajad Wani", "Mughees Ahmed Khan", "Abrar Ahmed Naqash", "Saif Al-Kuwari"], "title": "Chip scale superconducting quantum gravimeter based on a SQUID transmon mechanical resonator", "comment": "16figs", "summary": "Precise gravitational measurements are vital for geophysics and inertial navigation, but current platforms struggle to combine absolute accuracy with high-bandwidth tracking. We address this challenge with a chip-scale superconducting gravimeter that couples a flux-tunable transmon qubit to a high-$Q$ mechanical resonator. We embed the mechanical element inside the qubit's SQUID loop. This allows us to exploit the Josephson potential's nonlinearity, creating a motion-dependent inductance that maps gravitational displacement onto the qubit's geometric phase. Using a stroboscopic measurement protocol, we suppress mechanical decoherence at revival times. This yields a predicted sensitivity of $10^2\\,\\mathrm{nGal}/\\sqrt{\\mathrm{Hz}}$, approaching the performance of atomic sensors but with kilohertz-rate sampling. With electrical {in situ} tunability and SI traceability via microwave spectroscopy, this architecture offers a practical route to high-speed, quantum-limited on-chip gravimetry."}
{"id": "2601.00431", "categories": ["quant-ph", "physics.chem-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00431", "abs": "https://arxiv.org/abs/2601.00431", "authors": ["Seogjoo J. Jang"], "title": "Multistep quantum master equation theory for response functions in four wave mixing electronic spectroscopy of multichromophoric macromolecules", "comment": "14 pages, 0 figure", "summary": "This work provides an alternative derivation of third order response functions in four wave mixing spectroscopy of multichromophoric macromolecular systems considering only single exciton states. For the case of harmonic oscillator bath linearly and diagonally coupled to exciton states, closed form expressions showing all the explicit time dependences are derived. These expressions can provide more solid physical basis for understanding 2-dimensional electronic spectroscopy signals. For more general cases of system-bath coupling, the quantum master equation (QME) approach is employed for the derivation of multistep time evolution equations for Green function-like operators. Solution of these equations is feasible at the level of 2nd order non-Markovian QME, and the new approach can account for inter-exciton coupling, dephasing, relaxation, and non-Markovian effects in a consistent manner."}
{"id": "2601.00483", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00483", "abs": "https://arxiv.org/abs/2601.00483", "authors": ["Long Ma", "Larissa Inácio", "Dai-Nam Le", "Lilia M. Woods", "Mathias Boström"], "title": "Prediction of a measurable sign change in the Casimir force using a magnetic fluid", "comment": "9 pages, 4 figures. Submitted", "summary": "We demonstrate quantum levitation controlled by Casimir forces acting between a polystyrene surface and a Teflon-coated metallic substrate immersed in a mixture of Toluene and magnetite particles. This system experiences repulsion-attraction transitions in the Casimir interaction for distances where the effect is measurable. This Casimir trapping can be controlled by clever choices of metallic and ferrofluid materials, which are directly linked to the emergence of the trapping effect. Thermal and quantum contributions are investigated in detail, showing how the optical and magnetic properties of the ferrofluid and other materials affect the magnitude of the trapping and its distance range of observability."}
{"id": "2601.00484", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00484", "abs": "https://arxiv.org/abs/2601.00484", "authors": ["Ali Al-Bayaty", "Marek Perkowski"], "title": "A Geometrical Design Tool for Building Cost-Effective Layout-Aware n-Bit Quantum Gates Using the Bloch Sphere Approach", "comment": "11 pages, 8 figures, 5 tables", "summary": "The conventional design technique of any n-bit quantum gate is mainly achieved using unitary matrices multiplication, where n >= 2 and 1 <= m <= n-1 for m target qubits and n-m control qubits. These matrices represent quantum rotations by an n-bit quantum gate. For a quantum designer, such a conventional technique requires extensive computational time and effort, which may generate an n-bit quantum gate with a too high quantum cost. The Bloch sphere is only utilized as a visualization tool to verify the conventional design correctness for quantum rotations by a quantum gate. In contrast, this paper introduces a new concept of using the Bloch sphere as a \"geometrical design tool\" to build cost-effective n-bit quantum gates with lower quantum costs. This concept is termed the \"Bloch sphere approach (BSA)\". In BSA, a cost-effective n-bit quantum gate is built without using any unitary matrices multiplication. Instead, the quantum rotations for such a gate are visually selected using the geometrical planar intersections of the Bloch sphere. The BSA can efficiently map m targets among n-m controls for an n-bit quantum gate, to satisfy the limited layout connectivity for the physical neighboring qubits of a quantum computer. Experimentally, n-bit quantum gates built using the BSA always have lower quantum costs than those for such gates built using the conventional quantum design techniques."}
{"id": "2601.00487", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00487", "abs": "https://arxiv.org/abs/2601.00487", "authors": ["Wenxuan Xie", "John C Schotland"], "title": "Non-Hermitian Band Topology and Edge States in Atomic Lattices", "comment": null, "summary": "We investigate the band structure and topological phases of one- and two-dimensional bipartite atomic lattices mediated by long-range dissipative radiative coupling. By deriving an effective non-Hermitian Hamiltonian for the single-excitation sector, we demonstrate that the low-energy dynamics of the system are governed by a Dirac equation with a complex Fermi velocity. We analyze the associated topological invariants for both the SSH and honeycomb models, utilizing synthetic gauge fields to break time-reversal symmetry in the latter. Finally, we explicitly verify the non-Hermitian bulk-edge correspondence by deriving analytical solutions for edge states localized at domain boundaries."}
{"id": "2601.00489", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00489", "abs": "https://arxiv.org/abs/2601.00489", "authors": ["Modi Ke", "Dai-Nam Le", "Lilia M. Woods"], "title": "Casimir interactions and drift currents", "comment": "10 pages, 5 figures. Submitted", "summary": "We investigate the fluctuation-induced Casimir interactions between two parallel graphene sheets carrying steady-state drift currents. The graphene properties are modeled based on the shifted Fermi disk model to capture the non-equilibrium optical response of the system. We find that the drift current introduces a repulsive correction to the perpendicular to the layers Casimir interaction, thereby reducing the overall attractive force. Although the correction is repulsive, it does not overcome the underlying attraction between the layers. It also generates a lateral force that opposes the carrier flow direction. Both contributions are studied in terms of distance and drift velocity functionalities showing pathways for Casimir force control."}
{"id": "2601.00511", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00511", "abs": "https://arxiv.org/abs/2601.00511", "authors": ["Anton Kapustin", "Daniil Radamovich"], "title": "Chaos and thermalization in Clifford-Floquet dynamics", "comment": "20 pages, 6 figures", "summary": "We study the ergodic properties of a unitary Floquet dynamics arising from the repeated application of a translationally-invariant Clifford Quantum Cellular Automata to an infinite system of qubits in d dimensions. One expects that if the QCA does not exhibit any periodicity, a generic initial state of qubits will thermalize, that is, approach the infinite-temperature state. We show that this is true for many classes of states, both pure and mixed. In particular, this is true for all initial states that are short-range entangled and close to the equilibrium state. We also point out a subtle distinction between weak and strong thermalization."}
{"id": "2601.00622", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00622", "abs": "https://arxiv.org/abs/2601.00622", "authors": ["Thi Phuong Anh Nguyen", "Le Phuong Hoang", "Xuan Binh Cao"], "title": "Photonic Reservoir Engineering via 2D $Λ$-Type Atomic Arrays in Waveguide QED", "comment": null, "summary": "Electromagnetically induced transparency (EIT) in $Λ$-type atomic systems underpins quantum technologies such as high-fidelity memory and nonlinear optics, but conventional setups face intrinsic limitations. Standard geometries of one-dimensional atomic chains coupled to waveguides allow only a single bright superradiant channel, while subradiant modes remain weakly accessible, limiting control over collective radiative behavior and dark-state pathways. This leads to unwanted inelastic processes, degrading memory fidelity and reducing nonlinear photon generation efficiency. Here, we propose two two-dimensional (2D) atomic lattice geometries coupled to a photonic crystal waveguide, namely Zigzag and Orthogonal structures. In the Zigzag model, engineered collective super- and subradiant modes produce a flattened EIT window, broadening the transmission bandwidth and suppressing unwanted scattering to enhance quantum memory fidelity. In the Orthogonal model, four-wave mixing (FWM) intensity is amplified by up to six orders of magnitude relative to a conventional one-dimensional $Λ$-type EIT chain with identical $Γ_{1D}$, $Ω_c$, and probe intensity, with localized idler photons forming well-defined spectral modes. These results demonstrate a versatile route to engineer structured photonic reservoirs for on-demand photon generation, high-fidelity quantum storage, and enhanced nonlinear optical processes."}
{"id": "2601.00651", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00651", "abs": "https://arxiv.org/abs/2601.00651", "authors": ["Nicola Bortolotti", "Kristian Piscicchia", "Matthias Laubenstein", "Simone Manti", "Antonino Marcianò", "Federico Nola", "Catalina Curceanu"], "title": "Experimental exclusion of a generalized Károlyházy gravity-induced decoherence model", "comment": "10 pages, 4 figures", "summary": "We report new experimental constraints on the generalized version of the gravity-induced decoherence model originally proposed by Károlyházy. Using data collected by the VIP Collaboration at the INFN Gran Sasso National Laboratory with a high-purity germanium detector, we derive an improved lower bound on the spatial correlation length $R_K$ characterizing metric fluctuations in the model. We obtain a bound $R_K > 4.64$ m (95\\% C.L.), which exceeds by more than an order of magnitude the previous experimental limit. When combined with the theoretical upper bound $R_K <1.98$ m derived from macroscopic localization requirements, our result excludes the generalized Károlyházy model. The same conclusion applies to an associated non-Markovian formulation of the Continuous Spontaneous Localization (CSL) model. Our findings significantly tighten experimental constraints on gravity-related decoherence scenarios and demonstrate the sensitivity of underground low-background experiments to foundational modifications of quantum mechanics."}
{"id": "2601.00676", "categories": ["quant-ph", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.00676", "abs": "https://arxiv.org/abs/2601.00676", "authors": ["Ivaldevingles Rodrigues De Souza Junior", "Andrea Trombettoni", "Carla Braitenberg"], "title": "Ultracold Quantum Gravimeters: An Introduction for Geophysicists", "comment": null, "summary": "This paper aims at providing an accessible introduction to ultracold quantum gravimeters tailored for geophysicists. We do not focus here on geophysical applications, as these are already well known to geophysicists, but rather provide a pedagogical exposition of the quantum-mechanical concepts needed to understand the operation of quantum gravimeters. We present a review of gravimeters based on two- and three-level atomic systems, focusing on the fundamental mechanisms of atomic interferometry. The functioning of Mach-Zehnder interferometers is discussed through the action of $π/2$ and $π$ pulses, showing how the resulting phase shift encodes gravitational acceleration. The effect of noise is briefly discussed."}
{"id": "2601.00708", "categories": ["quant-ph", "physics.bio-ph", "physics.chem-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00708", "abs": "https://arxiv.org/abs/2601.00708", "authors": ["Seogjoo J. Jang"], "title": "Effects of Donor-Acceptor Quantum Coherence and Non-Markovian Bath on the Distance Dependence of Resonance Energy Transfer", "comment": "11 pages, 5 figures", "summary": "Accurate information on the distance dependence of resonance energy transfer (RET) is crucial for its utilization as a spectroscopic ruler \\re{of} nanometer scale distances. In this regard, understanding the effects of donor-acceptor quantum coherence and non-Markovian bath, which become significant at short distances, has significant implications. The present work investigates this issue theoretically by comparing results from a theory of coherent RET (CRET) with a nonequilibrium version of Förster's RET (FRET) theory, both accounting for non-Markovian bath effects. Even for a model where the donor-acceptor electronic coupling is of transition dipole interaction form, it is shown that the RET rate in general deviates from the inverse sixth power distance dependence as opposed to the prediction of the original FRET. It is shown that the donor-acceptor quantum coherence makes the \\re{distance} dependence steeper than the sixth power although detailed manner of enhancement is sensitive to specific values of parameters. On the other hand, the non-Markovian bath effects make the \\re{distance} dependence more moderate than the sixth power for both CRET and nonueqilibrium FRET because finite time scale of the bath causes the rate to be smaller than the prediction of original FRET. While these effects are \\re{demonstrated clearly} in the population dynamics at sub-picosecond time scales, their contributions to the conventional RET efficiency are relatively minor. This indicates that the actual detection of such effects through conventional RET efficiency measurement requires either high precision or utilization of a donor with fast spontaneous decay rate of excitation."}
{"id": "2601.00711", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.00711", "abs": "https://arxiv.org/abs/2601.00711", "authors": ["Ali Abbassi", "Yann Dujardin", "Eric Gourdin", "Philippe Lacomme", "Caroline Prodhon"], "title": "Assessing Quantum Annealing to Solve the Minimum Vertex Multicut", "comment": "Published in Codit 2025", "summary": "Cybersecurity in telecommunication networks often leads to hard combinatorial optimization problems that are challenging to solve with classical methods. This work investigates the practical feasibility of using quantum annealing to address the Restricted Vertex Minimum Multicut Problem. The problem is formulated as a Quadratic Unconstrained Binary Optimization model and implemented on D-Wave s quantum annealer. Rather than focusing on solution quality alone, we analyze key aspects of the quantum workflow including minor embedding techniques, chain length, topology constraints, chain strength selection, unembedding procedures, and postprocessing. Our results show that quantum annealing faces substantial hardware-level constraints limitations in embedding and scalability, especially for large instances, while hybrid quantum-classical solvers provide improved feasibility. This study offers a realistic assessment of the D-Wave system s current capabilities and identifies crucial parameters that govern the success of quantum optimization in cybersecurity-related network problems."}
{"id": "2601.00720", "categories": ["quant-ph", "cs.DM"], "pdf": "https://arxiv.org/pdf/2601.00720", "abs": "https://arxiv.org/abs/2601.00720", "authors": ["Ali Abbassi", "Yann Dujardin", "Eric Gourdin", "Philippe Lacomme", "Caroline Prodhon"], "title": "Quantum Approaches to the Minimum Edge Multiway Cut Problem", "comment": "Work published in QUEST IS 2025", "summary": "We investigate the minimum edge multiway cut problem, a fundamental task in evaluating the resilience of telecommunication networks. This study benchmarks the problem across three quantum computing paradigms: quantum annealing on a D-Wave quantum processing unit, photonic variational quantum circuits simulated on Quandela s Perceval platform, and IBM s gate-based Quantum Approximate Optimization Algorithm (QAOA). We assess the comparative feasibility of these approaches for early-stage quantum optimization, highlighting trade-offs in circuit constraints, encoding overhead, and scalability. Our findings suggest that quantum annealing currently offers the most scalable performance for this class of problems, while photonic and gate-based approaches remain limited by hardware and simulation depth. These results provide actionable insights for designing quantum workflows targeting combinatorial optimization in telecom security and resilience analysis."}
{"id": "2601.00735", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00735", "abs": "https://arxiv.org/abs/2601.00735", "authors": ["Alberto Acevedo", "Antonio Falcó"], "title": "Geometric Complexity of Quantum Channels via Unitary Dilations", "comment": null, "summary": "Nielsen's geometric approach to quantum circuit complexity provides a Riemannian framework for quantifying the cost of implementing unitary (closed--system) dynamics. For open dynamics, however, the reduced evolution is described by quantum channels and admits many inequivalent Stinespring realizations, so any meaningful complexity notion must specify which microscopic resources are counted as accessible and which transformations are regarded as gauge. We introduce and analyze a geometric complexity functional for families of quantum channels based on unitary dilations. We distinguish an implementation-dependent complexity, defined relative to explicit dilation data, from an intrinsic channel complexity obtained by minimizing over a physically motivated class of admissible dilations (e.g. bounded environment dimension, energy or norm constraints, and penalty structures). The functional has a subtractive form: it compares the geometric cost of the total unitary realization with a canonical surrogate term that removes purely environmental contributions. We justify this subtraction from concise postulates, including closed-system consistency, environment-only neutrality, and invariance under dilation gauge transformations that leave the channel unchanged. This leads to a companion quantity, noise complexity, quantifying the loss of geometric complexity relative to a prescribed ideal closed evolution. We establish a coherence-based lower bound for unitary geometric complexity, derive structural properties such as linear time scaling under time-homogeneous dilations, and obtain dissipator--controlled bounds in the Markovian (GKSL/Lindblad) regime under a standard dilation construction. Finally, we illustrate the framework on canonical benchmark noise models, including dephasing, amplitude damping, and depolarizing (Pauli) channels."}
{"id": "2601.00745", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00745", "abs": "https://arxiv.org/abs/2601.00745", "authors": ["Demerson N. Gonçalves", "Tharso D. Fernandes", "Pedro H. G. Lugao", "João T. Dias"], "title": "Training-Free Certified Bounds for Quantum Regression: A Scalable Framework", "comment": "16 pages, 3 tables", "summary": "We present a training-free, certified error bound for quantum regression derived directly from Pauli expectation values. Generalizing the heuristic of minimum accuracy from classification to regression, we evaluate axis-aligned predictors within the Pauli feature space. We formally prove that the optimal axis-aligned predictor constitutes a rigorous upper bound on the minimum training Mean Squared Error (MSE) attainable by any linear or kernel-based regressor defined on the same quantum feature map. Since computing this exact bound requires an intractable scan of the full Pauli basis, we introduce a Monte Carlo framework to efficiently estimate it using a tractable subset of measurement axes. We further provide non-asymptotic statistical guarantees to certify performance within a practical measurement budget. This method enables rapid comparison of quantum feature maps and early diagnosis of expressivity, allowing for the informed selection of architectures before deploying higher-complexity models."}
{"id": "2601.00761", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00761", "abs": "https://arxiv.org/abs/2601.00761", "authors": ["Zhenyu Xiao", "Shinsei Ryu"], "title": "Exponentially Accelerated Sampling of Pauli Strings for Nonstabilizerness", "comment": null, "summary": "Quantum magic, quantified by nonstabilizerness, measures departures from stabilizer structure and underlies potential quantum speedups. We introduce an efficient classical algorithm that exactly computes stabilizer Rényi entropies and stabilizer nullity for generic many-body wavefunctions of $N$ qubits. The method combines the fast Walsh-Hadamard transform with an exact partition of Pauli operators. It achieves an exponential speedup over direct approaches, reducing the average cost per sampled Pauli string from $O(2^N)$ to $O(N)$. Building on this framework, we further develop a Monte-Carlo estimator for stabilizer Rényi entropies together with a Clifford-based variance-reduction scheme that suppresses sampling fluctuations. We benchmark the accuracy and efficiency on ensembles of random magic states, and apply the method to random Clifford circuits with doped $T$ gates, comparing different doping architectures. Our approach applies to arbitrary quantum states and provides quantitative access to magic resources both encoded in highly entangled states and generated by long-time nonequilibrium dynamics."}
{"id": "2601.00772", "categories": ["quant-ph", "math.LO"], "pdf": "https://arxiv.org/pdf/2601.00772", "abs": "https://arxiv.org/abs/2601.00772", "authors": ["Dietmar Dorninger", "Helmut Länger"], "title": "On orthoposets of numerical events in quantum logic", "comment": null, "summary": "Let S be a set of states of a physical system and p(s) the probability of the occurrence of an event when the system is in state s in S. Such a function p from S to [0,1] is known as a numerical event or more accurately an S-probability. A set P of numerical events including the constant functions 0 and 1 and 1-p with every p in P becomes a poset when ordered by the order of real functions and can serve as a general setting for quantum logics. We call such a poset P a general set of events (GSE). The thoroughly investigated algebras of S-probabilities (including Hilbert logics), concrete logics and Boolean algebras can all be represented within this setting. In this paper we study various classes of GSEs, in particular those that are orthoposets and their interrelations and connections to known logics. Moreover, we characterize GSEs as posets by means of states and discuss the situation for GSEs to be lattices."}
{"id": "2601.00056", "categories": ["cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00056", "abs": "https://arxiv.org/abs/2601.00056", "authors": ["Nikolay V. Gnezdilov", "Andrei I. Pavlov"], "title": "Thermalization in a closed quantum system from randomized dynamics", "comment": "8 pages, 6 figures", "summary": "The emergence of statistical mechanics from quantum dynamics is a central problem in quantum many-body physics. Deriving observables aligned with the prediction of the canonical ensemble for a quantum system relies on the presence of a bath provided either as an external environment or as a larger part of a closed system. We demonstrate that thermal (canonical) observables for a whole closed quantum system of finite size can arise in the absence of a bath. These thermal observables stem from classical averaging over randomized unitary evolutions for a few-body system. The temperature in the canonical ensemble appears as a global constraint on the total energy of the system, determined by the choice of the initial state. From averaging randomized evolutions, we derive spin-spin correlation functions for a finite spin chain and show that they exhibit a temperature-dependent finite correlation length, in agreement with the prediction of the canonical ensemble. This establishes a method for computing thermal observables in a closed, finite-size system from real-time propagation without a bath. An implementation of this thermalization approach on a quantum computer can be utilized for thermal state preparation."}
{"id": "2601.00220", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00220", "abs": "https://arxiv.org/abs/2601.00220", "authors": ["Bibek Saha", "Sthitadhi Roy"], "title": "Anderson localisation in spatially structured random graphs", "comment": "18 pages, 12 figures", "summary": "We study Anderson localisation on high-dimensional graphs with spatial structure induced by long-ranged but distance-dependent hopping. To this end, we introduce a class of models that interpolate between the short-range Anderson model on a random regular graph and fully connected models with statistically uniform hopping, by embedding a random regular graph into a complete graph and allowing hopping amplitudes to decay exponentially with graph distance. The competition between the exponentially growing number of neighbours with graph distance and the exponentially decaying hopping amplitude positions our models effectively as power-law hopping generalisation of the Anderson model on random regular graphs. Using a combination of numerical exact diagonalisation and analytical renormalised perturbation theory, we establish the resulting localisation phase diagram emerging from the interplay of the lengthscale associated to the hopping range and the onsite disorder strength. We find that increasing the hopping range shifts the localisation transition to stronger disorder, and that beyond a critical range the localised phase ceases to exist even at arbitrarily strong disorder. Our results indicate a direct Anderson transition between delocalised and localised phases, with no evidence for an intervening multifractal phase, for both deterministic and random hopping models. A scaling analysis based on inverse participation ratios reveals behaviour consistent with a Kosterlitz-Thouless-like transition with two-parameter scaling, in line with Anderson transitions on high-dimensional graphs. We also observe distinct critical behaviour in average and typical correlation functions, reflecting the different scaling properties of generalised inverse participation ratios."}
{"id": "2601.00294", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00294", "abs": "https://arxiv.org/abs/2601.00294", "authors": ["Bo-Ting Chen", "Yu-Ping Wang", "Biao Lian"], "title": "Bridging Commutant and Polynomial Methods for Hilbert Space Fragmentation", "comment": "17 pages", "summary": "A quantum model exhibits Hilbert space fragmentation (HSF) if its Hilbert space decomposes into exponentially many dynamically disconnected subspaces, known as Krylov subspaces. A model may however have different HSFs depending on the method for identifying them. Here we establish a connection between two vastly distinct methods recently proposed for identifying HSF: the commutant algebra (CA) method and integer characteristic polynomial factorization (ICPF) method. For a Hamiltonian consisting of operators admitting rational number matrix representations, we prove a theorem that, if its center of commutant algebra have all eigenvalues being rational, the HSF from the ICPF method must be equal to or finer than that from the CA method. We show that this condition is satisfied by most known models exhibiting HSF, for which we demonstrate the validity of our theorem. We further discuss representative models for which ICPF and CA methods yield different HSFs. Our results may facilitate the exploration of a unified definition of HSF."}
