{"id": "2601.03920", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.03920", "abs": "https://arxiv.org/abs/2601.03920", "authors": ["Claudio Durastanti", "Radomyra Shevchenko"], "title": "Adaptive thresholding for wavelet-based nonparametric heteroskedastic variance estimation on the sphere", "comment": "53 pages", "summary": "This paper investigates the nonparametric estimation of a heteroskedastic variance function on the sphere in a regression framework, assuming the variance belongs to a Besov regularity class. A needlet-based estimator is proposed, combining multiresolution analysis with hard thresholding. The method exploits the spatial and spectral localization of needlets to adapt to unknown smoothness and is shown to attain minimax-optimal convergence rates over Besov spaces."}
{"id": "2601.03299", "categories": ["stat.ME", "math.ST", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.03299", "abs": "https://arxiv.org/abs/2601.03299", "authors": ["Richik Chakraborty"], "title": "Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation", "comment": null, "summary": "Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p<0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor."}
{"id": "2601.04049", "categories": ["q-fin.CP", "math.NA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04049", "abs": "https://arxiv.org/abs/2601.04049", "authors": ["Julien Hok", "Álvaro Leitao"], "title": "Quantum computing for multidimensional option pricing: End-to-end pipeline", "comment": null, "summary": "This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives."}
{"id": "2601.03281", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03281", "abs": "https://arxiv.org/abs/2601.03281", "authors": ["Mohamed Amine Ferrag", "Abderrahmane Lakas", "Merouane Debbah"], "title": "$α^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks", "comment": "20 pages", "summary": "Large Language Models (LLMs) are increasingly used as high level controllers for autonomous Unmanned Aerial Vehicle (UAV) missions. However, existing evaluations rarely assess whether such agents remain safe, protocol compliant, and effective under realistic next generation networking constraints. This paper introduces $α^3$-Bench, a benchmark for evaluating LLM driven UAV autonomy as a multi turn conversational reasoning and control problem operating under dynamic 6G conditions. Each mission is formulated as a language mediated control loop between an LLM based UAV agent and a human operator, where decisions must satisfy strict schema validity, mission policies, speaker alternation, and safety constraints while adapting to fluctuating network slices, latency, jitter, packet loss, throughput, and edge load variations.\n  To reflect modern agentic workflows, $α^3$-Bench integrates a dual action layer supporting both tool calls and agent to agent coordination, enabling evaluation of tool use consistency and multi agent interactions. We construct a large scale corpus of 113k conversational UAV episodes grounded in UAVBench scenarios and evaluate 17 state of the art LLMs using a fixed subset of 50 episodes per scenario under deterministic decoding. We propose a composite $α^3$ metric that unifies six pillars: Task Outcome, Safety Policy, Tool Consistency, Interaction Quality, Network Robustness, and Communication Cost, with efficiency normalized scores per second and per thousand tokens. Results show that while several models achieve high mission success and safety compliance, robustness and efficiency vary significantly under degraded 6G conditions, highlighting the need for network aware and resource efficient LLM based UAV agents. The dataset is publicly available on GitHub : https://github.com/maferrag/AlphaBench"}
{"id": "2601.03976", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03976", "abs": "https://arxiv.org/abs/2601.03976", "authors": ["Gorka Nieto", "Idoia de la Iglesia", "Cristina Perfecto", "Unai Lopez-Novoa"], "title": "On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process", "comment": "Submitted to IEEE Transactions on Cognitive Communications and Networking", "summary": "Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption."}
{"id": "2601.03393", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.03393", "abs": "https://arxiv.org/abs/2601.03393", "authors": ["Edward Groot", "Hannah Christensen", "Xia Sun", "Kathryn Newman", "Wahiba Lfarh", "Romain Roehrig", "Lisa Bengtsson", "Julia Simonson"], "title": "How different are deterministic physics suites when coupled to fixed model dynamics and why?", "comment": "Data of this study are available via Zenodo: DOI 10.5281/zenodo.18163757 Input files from DYAMOND is available via a DKRZ server http://tinyurl.com/MUMIPdata", "summary": "It is often difficult to attribute uncertainty and errors in atmospheric models to designated model components. This is because sub-grid parameterised processes interact strongly with the large-scale transport represented by the explicit model dynamics. We carry out experiments with prescribed large-scale dynamics and different sub-grid physics suites. This dataset has been constructed for the Model Uncertainty Model Intercomparison Project (MUMIP), in which each suite forecasts sub-grid tendencies at a 22km grid. The common dynamics is derived from a convection-permitting benchmark: an ICON DYAMOND experiment (2.5km grid).\n  We compare four different physics suites for atmospheric models in an Indian Ocean experiment. We analyse their joint PDFs of precipitation and associated physics tendencies for a full month. Precipitation is selected because it is a dominant uncertainty in the models that redistributes large amounts of heat. We find that all physics suites produce very similar precipitation amounts, with very high correlations between models, which exceed 0.95 at the native grid. However, the convection-permitting benchmark is more dissimilar from each of the physics suites, with correlations of $\\approx$0.80.\n  Similarly, we show that the vertically averaged physics tendencies in the free-troposphere are highly similar between the four physics suites, yet different if reconstructed for the benchmark. The water vapour sink is very closely linked with precipitation in the four physics suites. This suggests that the coarse-grid models are overconfident.\n  We hypothese is that variation in unresolved convective structures can lead to variation in the dynamics, following a given amount of latent heating at fine grids, but not in our physics suites.\n  The abstract length limit of ArXiv requires you to proceed in the PDF."}
{"id": "2601.03399", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2601.03399", "abs": "https://arxiv.org/abs/2601.03399", "authors": ["V. V. Ryazanov"], "title": "Multifractality, percolation threshold and critical point of a nuclear reactor", "comment": "24 pages, 8 figures", "summary": "A multifractal model is used to analyze neutron evolution within a reactor. For chain reactions, various characteristics of multifractal neutron behavior have been determined. These include the dimension of the multifractal carrier, information and correlation dimensions, the entropy of the fractal set, maximum and minimum dimension values, and the multifractal spectrum function. The geometric features of a multifractal allow for the description of a stochastic system consisting of hierarchically subordinate statistical ensembles, which are characterized by Cayley trees. A stationary distribution over hierarchical levels is established, which follows the Tsallis power law. The text also points out some potential applications of fractal patterns in nuclear reactor theory. The chance of percolation, which is when we see a state in the Bethe lattice where there's at least one continuous path through neighboring conducting nodes all the way across, is similar to the likelihood of a self-sustaining fission chain reaction happening. When this probability hits a critical point, we get a (conditionally) infinite cluster of neutrons forming. The percolation probability, influenced by how long the reactor has been running and its size, is linked to the reactor's criticality. We take a look at how the neutron multiplication factor behaves over time. We especially focus on the early stages of a self-sustaining nuclear fission chain reaction. We also highlight the ways to identify the boundaries of the critical region."}
{"id": "2601.03552", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03552", "abs": "https://arxiv.org/abs/2601.03552", "authors": ["Lujia Bo", "Mingxuan Chen", "Youduo Chen", "Xiaofan Gui", "Jiang Bian", "Chunyan Wang", "Yi Liu"], "title": "From Risk Perception to Behavior Large Language Models-Based Simulation of Pandemic Prevention Behaviors", "comment": null, "summary": "Individual prevention behaviors are a primary line of defense during the early stages of novel infectious disease outbreaks, yet their adoption is heterogeneous and difficult to forecast-especially when empirical data are scarce and epidemic-policy contexts evolve rapidly. To address this gap, we develop an LLM-based prevention-behavior simulation framework that couples (i) a static module for behavior-intensity prediction under a specified external context and (ii) a dynamic module that updates residents' perceived risk over time and propagates these updates into behavior evolution. The model is implemented via structured prompt engineering in a first-person perspective and is evaluated against two rounds of survey data from Beijing residents (R1: December 2020; R2: August 2021) under progressively realistic data-availability settings: zero-shot, few-shot, and cross-context transfer. Using Kolmogorov-Smirnov tests to compare simulated and observed behavior distributions (p > 0.001 as the validity criterion), the framework demonstrates robust performance and improves with limited reference examples; reported predictive accuracy increases from 72.7% (zero-shot) to 81.8% (few-shot), and remains high at 77.8% under transfer to novel contexts. We further apply the framework to simulate behavior changes during China's December 2022 policy relaxation and to stress-test behavioral responses across 120 systematically varied epidemic conditions (R0, CFR, and control-measure tiers). Results indicate broad behavioral loosening under relaxation but a distinctive counter-trend increase in drain-related disinfection, highlighting how low-cost, low-friction behaviors may persist or intensify even when external constraints recede-raising a potential environmental tradeoff."}
{"id": "2601.03260", "categories": ["cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03260", "abs": "https://arxiv.org/abs/2601.03260", "authors": ["Chenyang Shao", "Yong Li", "Fengli Xu"], "title": "SciNetBench: A Relation-Aware Benchmark for Scientific Literature Retrieval Agents", "comment": null, "summary": "The rapid development of AI agent has spurred the development of advanced research tools, such as Deep Research. Achieving this require a nuanced understanding of the relations within scientific literature, surpasses the scope of keyword-based or embedding-based retrieval. Existing retrieval agents mainly focus on the content-level similarities and are unable to decode critical relational dynamics, such as identifying corroborating or conflicting studies or tracing technological lineages, all of which are essential for a comprehensive literature review. Consequently, this fundamental limitation often results in a fragmented knowledge structure, misleading sentiment interpretation, and inadequate modeling of collective scientific progress. To investigate relation-aware retrieval more deeply, we propose SciNetBench, the first Scientific Network Relation-aware Benchmark for literature retrieval agents. Constructed from a corpus of over 18 million AI papers, our benchmark systematically evaluates three levels of relations: ego-centric retrieval of papers with novel knowledge structures, pair-wise identification of scholarly relationships, and path-wise reconstruction of scientific evolutionary trajectories. Through extensive evaluation of three categories of retrieval agents, we find that their accuracy on relation-aware retrieval tasks often falls below 20%, revealing a core shortcoming of current retrieval paradigms. Notably, further experiments on the literature review tasks demonstrate that providing agents with relational ground truth leads to a substantial 23.4% performance improvement in the review quality, validating the critical importance of relation-aware retrieval. We publicly release our benchmark at https://anonymous.4open.science/r/SciNetBench/ to support future research on advanced retrieval systems."}
{"id": "2601.03278", "categories": ["math.OC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03278", "abs": "https://arxiv.org/abs/2601.03278", "authors": ["Pablo Thomassin", "Guillaume Guerard", "Sonia Djebali", "Vincent Marc Lambert"], "title": "A Quantum Model for Constrained Markowitz Modern Portfolio Using Slack Variables to Process Mixed-Binary Optimization under QAOA", "comment": null, "summary": "Effectively encoding inequality constraints is a primary obstacle in applying quantum algorithms to financial optimization. A quantum model for Markowitz portfolio optimization is presented that resolves this by embedding slack variables directly into the problem Hamiltonian. The method maps each slack variable to a dedicated ancilla qubit, transforming the problem into a Quadratic Unconstrained Binary Optimization (QUBO) formulation suitable for the Quantum Approximate Optimization Algorithm (QAOA). This process internalizes the constraints within the quantum state, altering the problem's energy landscape to facilitate optimization. The model is empirically validated through simulation, showing it consistently finds the optimal portfolio where a standard penalty-based QAOA fails. This work demonstrates that modifying the Hamiltonian architecture via a slack-ancilla scheme provides a robust and effective pathway for solving constrained optimization problems on quantum computers. A fundamental quantum limit on the simultaneous precision of portfolio risk and return is also posited."}
{"id": "2601.03355", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.03355", "abs": "https://arxiv.org/abs/2601.03355", "authors": ["Valerio Pagni", "Friederike Ihssen", "Nicolò Defenu"], "title": "Critical aging and relaxation dynamics in long-range systems", "comment": "19 pages, 5 figures", "summary": "We study the dynamical scaling of long-range $\\mathrm{O}(N)$ models after a sudden quench to the critical temperature, using the functional renormalization group approach. We characterize both short-time aging and long-time relaxation as a function of the symmetry index $N$, the interaction range decay exponent $σ$ and the dimension $d$. Our results substantially improve on perturbative predictions, as demonstrated by benchmarks against Monte Carlo simulations and the large-$N$ limit. Finally, we demonstrate that long-range systems increase the performance of critical heat engines with respect to a local active medium."}
{"id": "2601.03902", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.03902", "abs": "https://arxiv.org/abs/2601.03902", "authors": ["Sonu George Alex", "Oleksandr Romanyuk", "Ivan Zorilo", "Alexander Andreev", "Frank Huber", "Thomas Gouder", "Petr Malinsky", "Alexander B. Shick", "Evgenia A. Tereshina-Chitrova"], "title": "Electronic Structure of UGe$_{2\\pm x}$ Thin Films from Photoelectron Spectroscopy", "comment": null, "summary": "Uranium digermanide UGe$_2$, the first ferromagnetic superconductor, represents a key composition in the U-Ge system dominated by U-5$f$ states. To examine the impact of controlled stoichiometric deviations on the electronic structure, UGe$_{2\\pm x}$ thin films ($0 \\le x \\le 1$) were prepared by triode sputtering and studied on pristine surfaces by X-ray (XPS) and Ultraviolet (UPS) photoelectron spectroscopy. XPS and UPS reveal a robust metallic valence band with a dominant U-5$f$ contribution at the Fermi level and a broad incoherent feature at higher binding energies, without qualitative changes in spectral line shape across the composition range. The experimental spectrum of UGe$_2$ thin films is well reproduced by DFT+U(ED) valence-band calculations combining density functional theory with exact diagonalization of the multiconfigurational U-5$f$ shell. These results demonstrate that the overall U-Ge electronic framework of UGe$_2$ thin films remains resilient to moderate stoichiometric deviations, providing a reliable electronic baseline for future studies of interface- and heterostructure-driven phenomena in uranium-based systems."}
{"id": "2601.03281", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03281", "abs": "https://arxiv.org/abs/2601.03281", "authors": ["Mohamed Amine Ferrag", "Abderrahmane Lakas", "Merouane Debbah"], "title": "$α^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks", "comment": "20 pages", "summary": "Large Language Models (LLMs) are increasingly used as high level controllers for autonomous Unmanned Aerial Vehicle (UAV) missions. However, existing evaluations rarely assess whether such agents remain safe, protocol compliant, and effective under realistic next generation networking constraints. This paper introduces $α^3$-Bench, a benchmark for evaluating LLM driven UAV autonomy as a multi turn conversational reasoning and control problem operating under dynamic 6G conditions. Each mission is formulated as a language mediated control loop between an LLM based UAV agent and a human operator, where decisions must satisfy strict schema validity, mission policies, speaker alternation, and safety constraints while adapting to fluctuating network slices, latency, jitter, packet loss, throughput, and edge load variations.\n  To reflect modern agentic workflows, $α^3$-Bench integrates a dual action layer supporting both tool calls and agent to agent coordination, enabling evaluation of tool use consistency and multi agent interactions. We construct a large scale corpus of 113k conversational UAV episodes grounded in UAVBench scenarios and evaluate 17 state of the art LLMs using a fixed subset of 50 episodes per scenario under deterministic decoding. We propose a composite $α^3$ metric that unifies six pillars: Task Outcome, Safety Policy, Tool Consistency, Interaction Quality, Network Robustness, and Communication Cost, with efficiency normalized scores per second and per thousand tokens. Results show that while several models achieve high mission success and safety compliance, robustness and efficiency vary significantly under degraded 6G conditions, highlighting the need for network aware and resource efficient LLM based UAV agents. The dataset is publicly available on GitHub : https://github.com/maferrag/AlphaBench"}
{"id": "2601.01960", "categories": ["quant-ph", "hep-th", "math-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2601.01960", "abs": "https://arxiv.org/abs/2601.01960", "authors": ["Alexander D. Popov"], "title": "Discrete symmetries in classical and quantum oscillators", "comment": "12 pages", "summary": "We consider the nature of the wave function using the example of a harmonic oscillator. We show that the eigenfunctions $ψ_n{=}z^n$ of the quantum Hamiltonian in the complex Bargmann-Fock-Segal representation with $z\\in\\mathbb C$ are the coordinates of a classical oscillator with energy $E_n=\\hbarωn$, $n=0,1,2,...\\,$. They are defined on conical spaces ${\\mathbb C}/{\\mathbb Z}_n$ with cone angles $2π/n$, which are embedded as subspaces in the phase space $\\mathbb C$ of the classical oscillator. Here ${\\mathbb Z}_n$ is the finite cyclic group of rotations of the space $\\mathbb C$ by an angle $2π/n$. The superposition $ψ=\\sum_n c_nψ_n$ of the eigenfunctions $ψ_n$ arises only with incomplete knowledge of the initial data for solving the Schrödinger equation, when the conditions of invariance with respect to the discrete groups ${\\mathbb Z}_n$ are not imposed and the general solution takes into account all possible initial data parametrized by the numbers $n\\in\\mathbb N$."}
{"id": "2601.03299", "categories": ["stat.ME", "math.ST", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.03299", "abs": "https://arxiv.org/abs/2601.03299", "authors": ["Richik Chakraborty"], "title": "Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation", "comment": null, "summary": "Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p<0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor."}
{"id": "2601.03843", "categories": ["hep-lat", "cond-mat.str-el", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03843", "abs": "https://arxiv.org/abs/2601.03843", "authors": ["Tomoya Hayata", "Yoshimasa Hidaka", "Hiromasa Watanabe"], "title": "Phases of the $q$-deformed $\\mathrm{SU}(N)$ Yang-Mills theory at large $N$", "comment": "7+4 pages, 5+2 figures, 2+1 tables; analysis code will be made publicly available on GitHub after publication", "summary": "We investigate the $(2+1)$-dimensional $q$-deformed $\\mathrm{SU}(N)_k$ Yang-Mills theory in the lattice Hamiltonian formalism, which is characterized by three parameters: the number of colors $N$, the coupling constant $g$, and the level $k$. By treating these as tunable parameters, we explore how key properties of the theory, such as confinement and topological order, emerge in different regimes. Employing a variational mean-field analysis that interpolates between the strong- and weak-coupling regimes, we determine the large-$N$ phase structure in terms of the 't Hooft coupling $λ_\\mathrm{tH}=g^2N$ and the ratio $k/N$. We find that the topologically ordered phase remains robust at large $N$ under appropriate scalings of these parameters. This result indicates that the continuum limit of large-$N$ gauge theory may be more intricate than naively expected, and motivates studies beyond the mean-field theory, both to achieve a further understanding of confinement in gauge theories and to guide quantum simulations of large-$N$ gauge theories."}
{"id": "2601.03668", "categories": ["math.NA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03668", "abs": "https://arxiv.org/abs/2601.03668", "authors": ["Kapil Chawla", "Youngjoon Hong", "Jae Yong Lee", "Sanghyun Lee"], "title": "Discontinuous Galerkin finite element operator network for solving non-smooth PDEs", "comment": "24 pages, 11 figures", "summary": "We introduce Discontinuous Galerkin Finite Element Operator Network (DG--FEONet), a data-free operator learning framework that combines the strengths of the discontinuous Galerkin (DG) method with neural networks to solve parametric partial differential equations (PDEs) with discontinuous coefficients and non-smooth solutions. Unlike traditional operator learning models such as DeepONet and Fourier Neural Operator, which require large paired datasets and often struggle near sharp features, our approach minimizes the residual of a DG-based weak formulation using the Symmetric Interior Penalty Galerkin (SIPG) scheme. DG-FEONet predicts element-wise solution coefficients via a neural network, enabling data-free training without the need for precomputed input-output pairs. We provide theoretical justification through convergence analysis and validate the model's performance on a series of one- and two-dimensional PDE problems, demonstrating accurate recovery of discontinuities, strong generalization across parameter space, and reliable convergence rates. Our results highlight the potential of combining local discretization schemes with machine learning to achieve robust, singularity-aware operator approximation in challenging PDE settings."}
{"id": "2601.03310", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03310", "abs": "https://arxiv.org/abs/2601.03310", "authors": ["Vincenzo Chilla"], "title": "Duality and measurement: the Copenhagen reconciliation", "comment": "25 pages", "summary": "Duality, not monism, constitutes the hermeneutic lens that characterizes the original Copenhagen interpretation of Quantum Mechanics. Therefore, evoking the principles of correspondence and complementarity, in this work we re assert a dual-aspect reading of quantum theory, structured through a multi-perspective schema encompassing its ontological, analytical, epistemological, causal, and information dimensions. We then show how this schema dissolves the so-called measurement problem, along with the associated knowledge-information and macro-micro dichotomies, issues historically raised within later monistic or universalist philosophical settings that ultimately depart from the traditional Copenhagen spirit."}
{"id": "2601.03282", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03282", "abs": "https://arxiv.org/abs/2601.03282", "authors": ["Boshuai Zhao", "Adam Abdin", "Jakob Puchinger"], "title": "Battery-time-space fragment-based formulation for the Electric Autonomous Dial-a-Ride Problem", "comment": null, "summary": "The Electric Autonomous Dial-A-Ride Problem (E-ADARP) optimizes routing and scheduling for electric autonomous vehicles to transport customers from origins to destinations. It features a combined objective that minimizes travel cost and excess user ride time, and allows partial recharging. Motivated by practical scenarios where time and battery data are available with limited precision, we introduce a discrete variant of the problem, termed D-E-ADARP, in which all time and battery parameters are discretized. This enables the development of our alternative solution approach: the discrete battery-time-space fragment-based formulation (BTSFF). In this framework, a fragment represents a subpath with an associated cost that accounts for both travel cost and excess user ride time. The BTSFF network integrates spatial, temporal, and battery dimensions, with the latter two discretized into finite indices. Computational results show that BTSFF solves D-E-ADARP significantly more efficiently than existing methods applied to the original E-ADARP. In addition, BTSFF efficiently provides high-quality lower bounds for E-ADARP and accelerates solving its battery swap variants. For E-ADARP, a relaxed network is constructed by rounding down travel times and battery consumptions, enabling a valid lower bound. For battery swap variants, BTSFF integrates lazy constraints via callbacks to correct time discretization errors, guaranteeing optimal solutions. Experiments show BTSFF outperforms benchmark methods in efficiency."}
{"id": "2601.03343", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.03343", "abs": "https://arxiv.org/abs/2601.03343", "authors": ["Tom Peham", "Erik Weilandt", "Robert Wille"], "title": "Optimizing Fault-tolerant Cat State Preparation", "comment": "17 pages, 8 figures", "summary": "Cat states are an important resource for fault-tolerant quantum computing, where they serve as building blocks for a variety of fault-tolerant primitives. Consequently, the ability to prepare high-quality cat states at large fault distances is essential. While optimizations for low fault distances or small numbers of qubits exist, higher fault distances can be achieved via generalized constructions with potentially suboptimal circuit sizes. In this work, we propose a cat state preparation scheme based on preparing two cat states with low-depth circuits, followed by a transversal CNOT and measurement of one of the states. This scheme prepares $w$-qubit cat states fault-tolerantly up to fault distances of $9$ using $\\lceil\\log_2 w\\rceil+1$ depth and at most $3w-2$ CNOTs and $2w$ qubits. We discuss that the combinatorially challenging aspect of this construction is the precise wiring of the transversal CNOT and propose three methods for finding these: two based on Satisfiability Modulo Theory solving and one heuristic search based on a local repair strategy. Numerical evaluations show that our circuits achieve a high fault-distance while requiring fewer resources as generalized constructions."}
{"id": "2601.03771", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.03771", "abs": "https://arxiv.org/abs/2601.03771", "authors": ["Hans van Haren"], "title": "Tidal motions in the deep Mediterranean", "comment": "17 pages, 6 figures", "summary": "The Mediterranean Sea is known for its limited tidal motions. For example, surface barotropic tidal elevations have an amplitude of 0.1 m in the Northwestern Mediterranean. Nevertheless, these small tides are noticeable in temperature records at the 2500-m deep seafloor, but only under near-homogeneous conditions when buoyancy frequency N < f, the inertial frequency. After transfer of pressure to temperature units via the local adiabatic lapse rate, the observed internal-wave temperature signals may thus be corrected for 1.5x10-5-degrC amplitude semidiurnal barotropic tides. The remaining baroclinic tides are embedded in the broad and featureless inertio-gravity wave band, with some energy enhancement near its boundaries, also under tenfold-larger energetic stratified water conditions."}
{"id": "2601.04155", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2601.04155", "abs": "https://arxiv.org/abs/2601.04155", "authors": ["Dafne Prado Bandeira", "Marco Tarzia"], "title": "Anderson Localization on Husimi Trees and its implications for Many-Body localization", "comment": null, "summary": "Motivated by the analogy between many-body localization (MBL) and single-particle Anderson localization on hierarchical graphs, we study localization on the Husimi tree, a generalization of the Bethe lattice with a finite density of local loops of arbitrary but finite length. The exact solution of the model provides a transparent and quantitative framework to systematically inspect the effect of loops on localization. Our analysis indicates that local loops enhance resonant processes, thereby reducing the critical disorder with increasing their number and size. At the same time, loops promote local hybridization, leading to an increase in the spatial extent of localized eigenstates. These effects reconcile key discrepancies between MBL phenomenology and its single-particle Anderson analog. These results show that local loops are a crucial structural ingredient for realistic single-particle analogies to many-body Hilbert spaces."}
{"id": "2601.03563", "categories": ["physics.soc-ph", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.03563", "abs": "https://arxiv.org/abs/2601.03563", "authors": ["Tung D. Nguyen", "Mason A. Porter"], "title": "A disease-spread model on hypergraphs with distinct droplet and aerosol transmission modes", "comment": "23 pages, 9 figures", "summary": "We examine the spread of an infectious disease, such as one that is caused by a respiratory virus, with two distinct modes of transmission. To do this, we consider a susceptible--infected--susceptible (SIS) disease on a hypergraph, which allows us to incorporate the effects of both dyadic (i.e., pairwise) and polyadic (i.e., group) interactions on disease propagation. This disease can spread either via large droplets through direct social contacts, which we associate with edges (i.e., hyperedges of size 2), or via infected aerosols in the environment through hyperedges of size at least 3 (i.e., polyadic interactions). We derive mean-field approximations of our model for two types of hypergraphs, and we obtain threshold conditions that characterize whether the disease dies out or becomes endemic. Additionally, we numerically simulate our model and a mean-field approximation of it to examine the impact of various factors, such as hyperedge size (when the size is uniform), hyperedge-size distribution (when the sizes are nonuniform), and hyperedge-recovery rates (when the sizes are nonuniform) on the disease dynamics."}
{"id": "2601.03859", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.03859", "abs": "https://arxiv.org/abs/2601.03859", "authors": ["Stanisław Stępień", "Michalina Janik", "Mateusz Nurek", "Akrati Saxena", "Radosław Michalski"], "title": "Fairness in Opinion Dynamics", "comment": null, "summary": "Ways in which people's opinions change are, without a doubt, subject to a rich tapestry of differing influences. Factors that affect how one arrives at an opinion reflect how they have been shaped by their environment throughout their lives, education, material status, what belief systems are they subscribed to, and what socio-economic minorities are they a part of. This already complex system is further expanded by the ever-changing nature of one's social network. It is therefore no surprise that many models have a tendency to perform best for the majority of the population and discriminating those people who are members of various marginalized groups . This bias and the study of how to counter it are subject to a rapidly developing field of Fairness in Social Network Analysis (SNA). The focus of this work is to look into how a state-of-the-art model discriminates certain minority groups and whether it is possible to reliably predict for whom it will perform worse. Moreover, is such prediction possible based solely on one's demographic or topological features? To this end, the NetSense dataset, together with a state-of-the-art CoDiNG model for opinion prediction have been employed. Our work explores how three classifier models (Demography-Based, Topology-Based, and Hybrid) perform when assessing for whom this algorithm will provide inaccurate predictions. Finally, through a comprehensive analysis of these experimental results, we identify four key patterns of algorithmic bias. Our findings suggest that no single paradigm provides the best results and that there is a real need for context-aware strategies in fairness-oriented social network analysis. We conclude that a multi-faceted approach, incorporating both individual attributes and network structures, is essential for reducing algorithmic bias and promoting inclusive decision-making."}
{"id": "2601.03935", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.03935", "abs": "https://arxiv.org/abs/2601.03935", "authors": ["Jianan Zeng", "Qi Li", "Yanbing Zhang", "Wei Su", "Lei Wu"], "title": "Accelerated simulation of multiscale gas-radiation coupling flows via a general synthetic iterative scheme", "comment": null, "summary": "Gas-radiation coupling critically influences hypersonic reentry flows, where extreme temperatures induce pronounced non-equilibrium gas and radiative heat transport. Accurate and efficient simulation of radiative gas dynamics is therefore indispensable for reliable design of thermal protection systems for atmospheric entry vehicles. In this study, a Boltzmann-type kinetic model for radiative gas flows is solved across a broad spectrum of flow and radiation transport regimes using the general synthetic iterative scheme (GSIS). The approach integrates an unstructured finite-volume discrete velocity method with a set of macroscopic synthetic equations. Within this framework, the kinetic model provides high-order closures for the constitutive relations in the synthetic equations. Simultaneously, the macroscopic synthetic equations drive the evolution of the mesoscopic kinetic system, significantly accelerating steady-state convergence in near-continuum regimes, as substantiated by linear Fourier stability analysis. Crucially, the algorithm is proven to be asymptotic-preserving, correctly recovering the continuum and optically thick limits, represented by the radiative Navier-Stokes-Fourier equations governing distinct translational, rotational, vibrational, and radiative temperatures, on coarse meshes independent of the mean free path. Numerical simulations of challenging benchmarks, including three-dimensional hypersonic flow over an Apollo reentry capsule, demonstrate that GSIS achieves orders-of-magnitude speedup over conventional iterative schemes in multiscale simulations of radiative gas flows while accurately capturing non-equilibrium effects and radiative heat transfer in hypersonic environments."}
{"id": "2601.03397", "categories": ["cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03397", "abs": "https://arxiv.org/abs/2601.03397", "authors": ["Hei Shing Cheung", "Qicheng Long", "Zhiyue Lin"], "title": "PIVONet: A Physically-Informed Variational Neuro ODE Model for Efficient Advection-Diffusion Fluid Simulation", "comment": "13 pages, 14 figures", "summary": "We present PIVONet (Physically-Informed Variational ODE Neural Network), a unified framework that integrates Neural Ordinary Differential Equations (Neuro-ODEs) with Continuous Normalizing Flows (CNFs) for stochastic fluid simulation and visualization. First, we demonstrate that a physically informed model, parameterized by CNF parameters θ, can be trained offline to yield an efficient surrogate simulator for a specific fluid system, eliminating the need to simulate the full dynamics explicitly. Second, by introducing a variational model with parameters φ that captures latent stochasticity in observed fluid trajectories, we model the network output as a variational distribution and optimize a pathwise Evidence Lower Bound (ELBO), enabling stochastic ODE integration that captures turbulence and random fluctuations in fluid motion (advection-diffusion behaviors)."}
{"id": "2601.03318", "categories": ["math.OC", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.03318", "abs": "https://arxiv.org/abs/2601.03318", "authors": ["Higor V. M. Ferreira", "Camila A. Tavares", "Nelson H. T. Lemes", "José Claudinei Ferreira", "José P. C. dos Santos"], "title": "An overview of the fractional-order gradient descent method and its applications", "comment": "26 pages, 2 tables, 8 figures", "summary": "Recent studies have shown that fractional calculus is an effective alternative mathematical tool in various scientific fields. However, some investigations indicate that results established in differential and integral calculus do not necessarily hold true in fractional calculus. In this work we will compare various methods presented in the literature to improve the Gradient Descent Method, in terms of convergence of the method, convergence to the extreme point, and convergence rate. In general, these methods that generalize the gradient descent algorithm by replacing the gradient with a fractional-order operator are inefficient in achieving convergence to the extremum point of the objective function. To avoid these difficulties, we proposed to choose the Fractional Continuous Time algorithm to generalize the gradient method. In this approach, the convergence of the method to the extreme point of the function is guaranteed by introducing the fractional order in the time derivative, rather than in of the gradient. In this case, the issue of finding the extreme point is resolved, while the issue of stability at the equilibrium point remains.\n  Fractional Continuous Time method converges to extreme point of cost function when fractional-order is between 0 and 1. The simulations shown in this work suggests that a similar result can be found when $1 \\leq α\\leq 2$. { This paper highlights the main advantages and disadvantages of generalizations of the gradient method using fractional derivatives, aiming to optimize convergence in complex problems. Some chemical problems, with n=11 and 24 optimization parameters, are employed as means of evaluating the efficacy of the propose algorithms. In general, previous studies are restricted to mathematical questions and simple illustrative examples."}
{"id": "2601.03439", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.03439", "abs": "https://arxiv.org/abs/2601.03439", "authors": ["Vijay Singh", "Shraddha Singh"], "title": "The Zubarev Double Time Greens function-A Vintage Many Body Technique", "comment": null, "summary": "These lecture notes present a comprehensive and powerful many-body technique pioneered in 1960 by D. N. Zubarev. The technique, known as the Zubarev Double Time Greens Function method, was used extensively by leading solid state physicists such as John Hubbard and Laura Roth in the 1960s. We present the technique and apply it to the non-interacting electron and boson gas. We next consider the (many-body) Hubbard model and show how it yields the Stoner criterion for ferromagnetism. It is easily extendable to superconductivity and related problems. Our treatment is pedagogical and understandable to those with just an elementary understanding of second quantization."}
{"id": "2601.04097", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.04097", "abs": "https://arxiv.org/abs/2601.04097", "authors": ["J. Sourd", "B. Vignolle", "E. Gaudin", "S. Burdin", "S. Tencé"], "title": "Superconductivity, Kondo physics and magnetic order: Tuning the groundstate in the La$_{1-x}$Ce$_x$FeSiH solid solution through the interplay between $3d$ and $4f$ correlated electrons", "comment": null, "summary": "We report a study of the La$_{1-x}$Ce$_x$FeSiH solid solution ($0 \\leq x \\leq 1$), a family of intermetallic hydrides of ZrCuSiAs-type structure, with space group $P4/nmm$. For low cerium concentrations $x \\leq 0.20$, we observe the presence of superconductivity, which originates from the correlated $3d$ electrons of iron. The superconducting regime is progressively suppressed by the cerium substitution. For moderate cerium concentration $0.07 \\leq x \\leq 0.50$, we observe evidence of the single-ion Kondo effect and no magnetic phase transition down to 2 K. For $0.07 \\leq x \\leq 0.20$, the single-ion Kondo effect coexists with a superconducting ground state at low temperatures. From $x > 0.50$, we observe signatures of Kondo coherence and a heavy Fermi liquid regime at low temperature. Finally, at high cerium concentration $x \\geq 0.85$, we observe signatures of magnetic ordering at low temperatures. We discuss our results by introducing temperature scales related to superconductivity, the Kondo effect, and magnetic order, which permits building a rich phase diagram temperature versus cerium content $x$. This shows that using the cerium concentration $x$ as a unique control parameter, we can explore the Kondo entanglement between correlated $3d$ and $4f$ electrons, which suggests an unusual change between the superconducting state related to the $3d$ electrons and the Kondo coherent state involving both $3d$ and $4f$ electrons."}
{"id": "2601.03282", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03282", "abs": "https://arxiv.org/abs/2601.03282", "authors": ["Boshuai Zhao", "Adam Abdin", "Jakob Puchinger"], "title": "Battery-time-space fragment-based formulation for the Electric Autonomous Dial-a-Ride Problem", "comment": null, "summary": "The Electric Autonomous Dial-A-Ride Problem (E-ADARP) optimizes routing and scheduling for electric autonomous vehicles to transport customers from origins to destinations. It features a combined objective that minimizes travel cost and excess user ride time, and allows partial recharging. Motivated by practical scenarios where time and battery data are available with limited precision, we introduce a discrete variant of the problem, termed D-E-ADARP, in which all time and battery parameters are discretized. This enables the development of our alternative solution approach: the discrete battery-time-space fragment-based formulation (BTSFF). In this framework, a fragment represents a subpath with an associated cost that accounts for both travel cost and excess user ride time. The BTSFF network integrates spatial, temporal, and battery dimensions, with the latter two discretized into finite indices. Computational results show that BTSFF solves D-E-ADARP significantly more efficiently than existing methods applied to the original E-ADARP. In addition, BTSFF efficiently provides high-quality lower bounds for E-ADARP and accelerates solving its battery swap variants. For E-ADARP, a relaxed network is constructed by rounding down travel times and battery consumptions, enabling a valid lower bound. For battery swap variants, BTSFF integrates lazy constraints via callbacks to correct time discretization errors, guaranteeing optimal solutions. Experiments show BTSFF outperforms benchmark methods in efficiency."}
{"id": "2601.03567", "categories": ["quant-ph", "gr-qc", "hep-th", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2601.03567", "abs": "https://arxiv.org/abs/2601.03567", "authors": ["Indrajit Sen", "Matthew Leifer"], "title": "Local Scale Invariance in Quantum Theory: A Non-Hermitian Pilot-Wave Formulation", "comment": "20 pages, 3 figures", "summary": "We show that Weyl's abandoned idea of local scale invariance has a natural realization at the quantum level in pilot-wave (deBroglie-Bohm) theory. We obtain the Weyl covariant derivative by complexifying the electromagnetic gauge coupling parameter. The resultant non-hermiticity has a natural interpretation in terms of local scale invariance of the quantum state in pilot-wave theory. The conserved current density is modified from $|ψ|^2$ to the local scale invariant, trajectory-dependent ratio $|ψ|^2/ \\mathbf{1}^2[\\mathcal{C}]$, where $\\mathbf 1[\\mathcal C]$ is a scale factor that depends on the pilot-wave trajectory $\\mathcal C$ in configuration space. Our approach is general, and we implement it for the Schrödinger, Pauli, and Dirac equations coupled to an external electromagnetic field. We also implement it in quantum field theory for the case of a quantized axion field interacting with a quantized electromagnetic field. We discuss the equilibrium probability density and show that the corresponding trajectories are unique."}
{"id": "2601.03377", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03377", "abs": "https://arxiv.org/abs/2601.03377", "authors": ["Edoardo Efrem Gervasoni", "Liesbet De Bus", "Stijn Vansteelandt", "Oliver Dukes"], "title": "On estimands in target trial emulation", "comment": "38 pages, 11 figures", "summary": "The target trial framework enables causal inference from longitudinal observational data by emulating randomized trials initiated at multiple time points. Precision is often improved by pooling information across trials, with standard models typically assuming - among other things - a time-constant treatment effect. However, this obscures interpretation when the true treatment effect varies, which we argue to be likely as a result of relying on noncollapsible estimands. To address these challenges, this paper introduces a model-free strategy for target trial analysis, centered around the choice of the estimand, rather than model specification. This ensures that treatment effects remain clearly interpretable for well-defined populations even under model misspecification. We propose estimands suitable for different study designs, and develop accompanying G-computation and inverse probability weighted estimators. Applications on simulations and real data on antimicrobial de-escalation in an intensive care unit setting demonstrate the greater clarity and reliability of the proposed methodology over traditional techniques."}
{"id": "2601.04147", "categories": ["hep-lat", "hep-ph", "hep-th", "nucl-th"], "pdf": "https://arxiv.org/pdf/2601.04147", "abs": "https://arxiv.org/abs/2601.04147", "authors": ["Stephen R. Sharpe"], "title": "Three-particle scattering amplitudes from lattice QCD", "comment": "Plenary talk at Lattice 2025. 26 pages, 7 figures", "summary": "I review recent progress in calculating scattering amplitudes and resonance properties involving three particles using results from lattice QCD. The necessary input is the finite-volume spectrum, and the outputs -- via solutions of integral equations -- are scattering amplitudes that can be continued into the complex plane to search for resonance poles. I describe the outlook for future extensions and applications of this work."}
{"id": "2601.03885", "categories": ["math.NA", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.03885", "abs": "https://arxiv.org/abs/2601.03885", "authors": ["Siddhartha E. Guzman", "Egor Tiunov", "Leandro Aolita"], "title": "Local Interpolation via Low-Rank Tensor Trains", "comment": "22 pages, 16 figures", "summary": "Tensor Train (TT) decompositions provide a powerful framework to compress grid-structured data, such as sampled function values, on regular Cartesian grids. Such high compression, in turn, enables efficient high-dimensional computations. Exact TT representations are only available for simple analytic functions. Furthermore, global polynomial or Fourier expansions typically yield TT-ranks that grow proportionally with the number of basis terms. State-of-the-art methods are often prohibitively expensive or fail to recover the underlying low-rank structure. We propose a low-rank TT interpolation framework that, given a TT describing a discrete (scalar-, vector-, or tensor-valued) function on a coarse regular grid with $n$ cores, constructs a finer-scale version of the same function represented by a TT with $n+m$ cores, where the last $m$ cores maintain constant rank. Our method guarantees a $\\ell^{2}$-norm error bound independent of the total number of cores, achieves exponential compression at fixed accuracy, and admits logarithmic complexity with respect of the number of grid points. We validate its performance through numerical experiments, including 1D, 2D, and 3D applications such as: 2D and 3D airfoil mask embeddings, image super-resolution, and synthetic noise fields such as 3D synthetic turbulence. In particular, we generate fractal noise fields directly in TT format with logarithmic complexity and memory. This work opens a path to scalable TT-native solvers with complex geometries and multiscale generative models, with implications from scientific simulation to imaging and real-time graphics."}
{"id": "2601.03343", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.03343", "abs": "https://arxiv.org/abs/2601.03343", "authors": ["Tom Peham", "Erik Weilandt", "Robert Wille"], "title": "Optimizing Fault-tolerant Cat State Preparation", "comment": "17 pages, 8 figures", "summary": "Cat states are an important resource for fault-tolerant quantum computing, where they serve as building blocks for a variety of fault-tolerant primitives. Consequently, the ability to prepare high-quality cat states at large fault distances is essential. While optimizations for low fault distances or small numbers of qubits exist, higher fault distances can be achieved via generalized constructions with potentially suboptimal circuit sizes. In this work, we propose a cat state preparation scheme based on preparing two cat states with low-depth circuits, followed by a transversal CNOT and measurement of one of the states. This scheme prepares $w$-qubit cat states fault-tolerantly up to fault distances of $9$ using $\\lceil\\log_2 w\\rceil+1$ depth and at most $3w-2$ CNOTs and $2w$ qubits. We discuss that the combinatorially challenging aspect of this construction is the precise wiring of the transversal CNOT and propose three methods for finding these: two based on Satisfiability Modulo Theory solving and one heuristic search based on a local repair strategy. Numerical evaluations show that our circuits achieve a high fault-distance while requiring fewer resources as generalized constructions."}
{"id": "2601.03386", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03386", "abs": "https://arxiv.org/abs/2601.03386", "authors": ["Zongyang Lv", "Yanmei Jia", "Yongqing Liu", "Alan F. Lynch", "Qing Zhao", "Yuhu Wu"], "title": "Modeling and Control for UAV with Off-center Slung Load", "comment": null, "summary": "Unmanned aerial vehicle (UAV) with slung load system is a classic air transportation system. In practical applications, the suspension point of the slung load does not always align with the center of mass (CoM) of the UAV due to mission requirements or mechanical interference. This offset creates coupling in the system's nonlinear dynamics which leads to a complicated motion control problem. In existing research, modeling of the system are performed about the UAV's CoM. In this work we use the point of suspension instead. Based on the new model, a cascade control strategy is developed. In the middle-loop controller, the acceleration of the suspension point is used to regulate the swing angle of the slung load without the need for considering the coupling between the slung load and the UAV. Using the off-center reference frame, an inner-loop controller is designed to track the UAV's attitude without the need of simplification on the coupling effects. We prove local exponential stability of the closed-loop using Lyapunov approach. Finally, simulations and experiments are conducted to validate the proposed control system."}
{"id": "2601.03870", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.03870", "abs": "https://arxiv.org/abs/2601.03870", "authors": ["Hans van Haren"], "title": "Turbulence demonstrates height variations in closely spaced deep-sea mooring lines", "comment": "30 pages, 13 figures", "summary": "It may be important to precisely know heights of moored oceanographic instrumentation. For example, moorings can be closely spaced or accidentally be located on small rocks or in small gullies. Height variations O(1 m) will yield registration of different values when conditions such as small-scale density stratification vary strongly. Such little height variations may prove difficult to measure in the deep sea, requiring high-accuracy pressure sensors preferably on all instruments in a mooring-array. In this paper, an alternative method for relative height determination is presented using high-resolution temperature sensors moored on multiple densely-spaced lines in the deep Western Mediterranean. While it was anticipated that height variations between lines could be detected under near-homogeneous conditions via adiabatic lapse rate O(0.0001degrC m-1) by the 0.00003degrC-noise-level sensors, such was prevented by the impossibility of properly correcting for short-term bias due to electronic drift. Instead, a satisfactory height determination was found during a period of relatively strong stratification and large turbulence activity. By band-pass filtering data of the highest-resolved turbulent motions across the strongest temperature gradient, significant height variations were detectable to within +/-0.2 m."}
{"id": "2601.04030", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.04030", "abs": "https://arxiv.org/abs/2601.04030", "authors": ["János Török", "Takashi Shimada", "Fumiko Ogushi", "Kata Tunyogi", "János Kertész", "Kimmo Kaski"], "title": "Convergence criteria for self-consistent measures in bipartite networks", "comment": "8 pages, 5 figures", "summary": "Many quantities that characterize network elements are defined in an explicit form and calculated directly from the network structure; examples of include several centrality measures like degree, closeness, or betweenness. However, there are also implicitly defined quantitative measures, which are usually calculated iteratively, in a self-consistent manner, like PageRank or countries' fitness / products' complexity relations. The iteration algorithms involve calculations over the entire network; therefore, their convergence properties depend on the structure of the network. Here, we focus on investigating self-consistently defined quantities in bipartite networks of two sets of nodes where the quantities in one set are determined by the quantities in the other set and vice versa. We derive an explicit convergence criterion for iterations of these quantities and describe two different approaches to improve the convergence properties. In the first one, we identify \"problematic nodes\" that can be removed or merged while in the second one, we introduce a regularization scheme and show how to estimate the regularization parameter."}
{"id": "2601.04134", "categories": ["cs.SI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.04134", "abs": "https://arxiv.org/abs/2601.04134", "authors": ["Eaman Jahani", "Blas Kolic", "Manuel Tonneau", "Hause Lin", "Daniel Barkoczi", "Edwin Ikhuoria", "Victor Orozco", "Samuel Fraiberger"], "title": "Celebrity messages reduce online hate and limit its spread", "comment": "16 pages, 5 figures", "summary": "Online hate spreads rapidly, yet little is known about whether preventive and scalable strategies can curb it. We conducted the largest randomized controlled trial of hate speech prevention to date: a 20-week messaging campaign on X in Nigeria targeting ethnic hate. 73,136 users who had previously engaged with hate speech were randomly assigned to receive prosocial video messages from Nigerian celebrities. The campaign reduced hate content by 2.5% to 5.5% during treatment, with about 75% of the reduction persisting over the following four months. Reaching a larger share of a user's audience reduced amplification of that user's hate posts among both treated and untreated users, cutting hate reposts by over 50% for the most exposed accounts. Scalable messaging can limit online hate without removing content."}
{"id": "2601.04099", "categories": ["physics.comp-ph", "astro-ph.IM", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2601.04099", "abs": "https://arxiv.org/abs/2601.04099", "authors": ["Samuel W. Jones", "Colin P. McNally", "Meritt Reynolds"], "title": "A constrained-transport embedded boundary method for compressible resistive magnetohydrodynamics", "comment": "31 pages, 15 figures", "summary": "Motivated by the increased interest in pulsed-power magneto-inertial fusion devices in recent years, we present a method for implementing an arbitrarily shaped embedded boundary on a Cartesian mesh while solving the equations of compressible resistive magnetohydrodynamics. The method is built around a finite volume formulation of the equations in which a Riemann solver is used to compute fluxes on the faces between grid cells, and a face-centered constrained transport formulation of the induction equation. The small time step problem associated with the cut cells is avoided by always computing fluxes on the faces and edges of the Cartesian mesh. We extend the method to model a moving interface between two materials with different properties using a ghost-fluid approach, and show some preliminary results including shock-wave-driven and magnetically-driven dynamical compressions of magnetohydrostatic equilibria. We present a thorough verification of the method and show that it converges at second order in the absence of discontinuities, and at first order with a discontinuity in material properties."}
{"id": "2601.03283", "categories": ["physics.soc-ph", "cs.CE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.03283", "abs": "https://arxiv.org/abs/2601.03283", "authors": ["Mina S. Khalaf"], "title": "Physics-Based Decline Curve Analysis and Machine Learning for Temperature Forecasting in Enhanced Geothermal Systems: Utah FORGE", "comment": null, "summary": "Reliable temperature forecasting in Enhanced Geothermal Systems (EGS) is essential, yet petroleum-based decline curves and many machine-learning surrogates do not enforce geothermal heat transfer, while thermo-hydro-mechanical (THM) simulation remains computationally expensive. This study proposes a physics-consistent framework that advances both decline-curve analysis and surrogate modeling. The classical Arps decline family is generalized for geothermal use by introducing an equilibrium-temperature term motivated by Newton-type cooling, ensuring finite late-time temperature limits while reducing exactly to the conventional Arps forms when the equilibrium term is set to zero. The extended decline curves are validated against Utah FORGE downhole temperature measurements and then used to construct learning surrogates on a controlled THM dataset spanning fracture count, well spacing, fracture spacing, host-rock thermal conductivity, and circulation rate. An equation-informed neural network embeds the modified decline equations as differentiable internal computational layers to produce full 0-60 month temperature trajectories from design and operational inputs. A probabilistic Gaussian Process Regression surrogate is also developed for direct multi-horizon forecasting with calibrated uncertainty, while a direct XGBoost regression baseline provides a purely data-driven reference. Across the simulation dataset, the extended decline models reproduce temperature trajectories with near-perfect fidelity (median RMSE = 0.071 °C), and the equation-informed network achieves typical hold-out errors of MAE = 3.06 °C and RMSE = 4.49 °C. The Gaussian Process surrogate delivers the strongest predictive accuracy across 3-60 month horizons (RMSE = 3.39 °C; MAE = 2.34 °C) with well-calibrated uncertainty, whereas the XGBoost baseline exhibits higher errors."}
{"id": "2601.03333", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.03333", "abs": "https://arxiv.org/abs/2601.03333", "authors": ["Mrinal Kanti Roychowdhury"], "title": "Optimal Quantization of Finite Uniform Data on the Sphere", "comment": null, "summary": "This paper develops a systematic and geometric theory of optimal quantization on the unit sphere $\\mathbb S^2$, focusing on finite uniform probability distributions supported on the spherical surface - rather than on lower-dimensional geodesic subsets such as circles or arcs. We first establish the existence of optimal sets of $n$-means and characterize them through centroidal spherical Voronoi tessellations. Three fundamental structural results are obtained. First, a cluster - purity theorem shows that when the support consists of well-separated components, each optimal Voronoi region remains confined to a single component. Second, a ring - allocation (discrete water - filling) theorem provides an explicit rule describing how optimal representatives are distributed across multiple latitudinal rings, together with closed-form distortion formulas. Third, a Lipschitz - type stability theorem quantifies the robustness of optimal configurations under small geodesic perturbations of the support. In addition, a spherical analogue of Lloyd's algorithm is presented, in which intrinsic (Karcher) means replace Euclidean centroids for iterative refinement. These results collectively provide a unified and transparent framework for understanding the geometric and algorithmic structure of optimal quantization on $\\mathbb S^2$."}
{"id": "2601.03591", "categories": ["cond-mat.stat-mech", "cond-mat.soft", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2601.03591", "abs": "https://arxiv.org/abs/2601.03591", "authors": ["Subhajit Paul", "Debasish Chaudhuri"], "title": "Interplay of activity and non-reciprocity in tracer dynamics: From non-equilibrium fluctuation-dissipation to giant diffusion", "comment": "14 pages, 2 figures", "summary": "Non-reciprocal interactions play a key role in shaping transport in active and passive systems, giving rise to striking nonequilibrium behavior. Here, we study the dynamics of a tracer -- active or passive -- embedded in a bath of active or passive particles, coupled through non-reciprocal interactions. Starting from the microscopic stochastic dynamics of the full system, we derive an overdamped generalized Langevin equation for the tracer, incorporating a non-Markovian memory kernel that captures bath-mediated correlations. This framework enables us to compute the tracer's velocity and displacement response, derive a generalized nonequilibrium fluctuation-dissipation relation that quantifies deviations from equilibrium behavior, and determine the mean-squared displacement (MSD). We find that while the MSD becomes asymptotically diffusive, the effective diffusivity depends non-monotonically on the degree of non-reciprocity and diverges at an intermediate value. This regime of giant diffusivity provides a generic mechanism for enhanced transport in active soft matter and has direct implications for biological systems exhibiting chase-and-run or predator-prey interactions. Our analytical predictions are supported by numerical simulations of active Brownian particles, highlighting experimentally accessible signatures of non-reciprocal interactions in soft matter."}
{"id": "2601.04104", "categories": ["cond-mat.str-el", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.04104", "abs": "https://arxiv.org/abs/2601.04104", "authors": ["Yunhao Fan", "Gia-Wei Chern"], "title": "Equivariant Neural Networks for Force-Field Models of Lattice Systems", "comment": "13 pages, 6 figures", "summary": "Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, symmetry-aware descriptors, whose construction is often system-specific and can hinder generality and transferability across different lattice Hamiltonians. Here we introduce a symmetry-preserving framework based on equivariant neural networks (ENNs) that provides a general, data-driven mapping from local configurations of dynamical variables to the associated on-site forces in a lattice Hamiltonian. In contrast to ENN architectures developed for molecular systems -- where continuous Euclidean symmetries dominate -- our approach aims to embed the discrete point-group and internal symmetries intrinsic to lattice models directly into the neural-network representation of the force field. As a proof of principle, we construct an ENN-based force-field model for the adiabatic dynamics of the Holstein Hamiltonian on a square lattice, a canonical system for electron-lattice physics. The resulting ML-enabled large-scale dynamical simulations faithfully capture mesoscale evolution of the symmetry-breaking phase, illustrating the utility of lattice-equivariant architectures for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems."}
{"id": "2601.03386", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03386", "abs": "https://arxiv.org/abs/2601.03386", "authors": ["Zongyang Lv", "Yanmei Jia", "Yongqing Liu", "Alan F. Lynch", "Qing Zhao", "Yuhu Wu"], "title": "Modeling and Control for UAV with Off-center Slung Load", "comment": null, "summary": "Unmanned aerial vehicle (UAV) with slung load system is a classic air transportation system. In practical applications, the suspension point of the slung load does not always align with the center of mass (CoM) of the UAV due to mission requirements or mechanical interference. This offset creates coupling in the system's nonlinear dynamics which leads to a complicated motion control problem. In existing research, modeling of the system are performed about the UAV's CoM. In this work we use the point of suspension instead. Based on the new model, a cascade control strategy is developed. In the middle-loop controller, the acceleration of the suspension point is used to regulate the swing angle of the slung load without the need for considering the coupling between the slung load and the UAV. Using the off-center reference frame, an inner-loop controller is designed to track the UAV's attitude without the need of simplification on the coupling effects. We prove local exponential stability of the closed-loop using Lyapunov approach. Finally, simulations and experiments are conducted to validate the proposed control system."}
{"id": "2601.03453", "categories": ["stat.ME", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03453", "abs": "https://arxiv.org/abs/2601.03453", "authors": ["Ioannis Ivrissimtzis", "Shauna Concannon", "Matthew Houliston", "Graham Roberts"], "title": "Measures of classification bias derived from sample size analysis", "comment": "9 pages, 3 figures", "summary": "We propose the use of a simple intuitive principle for measuring algorithmic classification bias: the significance of the differences in a classifier's error rates across the various demographics is inversely commensurate with the sample size required to statistically detect them. That is, if large sample sizes are required to statistically establish biased behavior, the algorithm is less biased, and vice versa. In a simple setting, we assume two distinct demographics, and non-parametric estimates of the error rates on them, e1 and e2, respectively. We use a well-known approximate formula for the sample size of the chi-squared test, and verify some basic desirable properties of the proposed measure. Next, we compare the proposed measure with two other commonly used statistics, the difference e2-e1 and the ratio e2/e1 of the error rates. We establish that the proposed measure is essentially different in that it can rank algorithms for bias differently, and we discuss some of its advantages over the other two measures. Finally, we briefly discuss how some of the desirable properties of the proposed measure emanate from fundamental characteristics of the method, rather than the approximate sample size formula we used, and thus, are expected to hold in more complex settings with more than two demographics."}
{"id": "2601.04092", "categories": ["quant-ph", "hep-lat", "hep-ph", "nucl-th"], "pdf": "https://arxiv.org/pdf/2601.04092", "abs": "https://arxiv.org/abs/2601.04092", "authors": ["Peng Guo", "Paul LeVan", "Frank X. Lee", "Yong Zhao"], "title": "Extracting scattering phase shift in quantum mechanics on quantum computers", "comment": "17 pages, 30 figures", "summary": "We investigate the feasibility of extracting infinite volume scattering phase shift on quantum computers in a simple one-dimensional quantum mechanical model, using the formalism established in Ref.~\\cite{Guo:2023ecc} that relates the integrated correlation functions (ICF) for a trapped system to the infinite volume scattering phase shifts through a weighted integral. The system is first discretized in a finite box with periodic boundary conditions, and the formalism in real time is verified by employing a contact interaction potential with exact solutions. Quantum circuits are then designed and constructed to implement the formalism on current quantum computing architectures. To overcome the fast oscillatory behavior of the integrated correlation functions in real-time simulation, different methods of post-data analysis are proposed and discussed. Test results on IBM hardware show that good agreement can be achieved with two qubits, but complete failure ensues with three qubits due to two-qubit gate operation errors and thermal relaxation errors."}
{"id": "2601.03931", "categories": ["math.NA", "math.OC", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.03931", "abs": "https://arxiv.org/abs/2601.03931", "authors": ["Yukuan Hu", "Laura Grazioli"], "title": "Constrained dynamics for searching saddle points on general Riemannian manifolds", "comment": "35 pages, 6 figures, 2 tables. All comments are welcome", "summary": "Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory."}
{"id": "2601.03365", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03365", "abs": "https://arxiv.org/abs/2601.03365", "authors": ["Boubakeur Khantoul", "Ahmed Tedjani"], "title": "Time-Dependent Dunkl-Pauli Oscillator in the Presence of the Aharonov-Bohm Effect", "comment": null, "summary": "We present an exact, time-dependent solution for a two-dimensional Pauli oscillator deformed by Dunkl operators in the presence of an Aharonov--Bohm (AB) flux. By replacing conventional momenta with Dunkl momenta and allowing arbitrary time dependence in both, mass and frequency, we derive a deformed Pauli Hamiltonian that encodes reflection symmetries and topological gauge phases. Employing the Lewis-Riesenfeld invariant method, we derive exact expressions for the eigenvalues and spinor eigenfunctions of the system. Crucially, the AB flux imposes symmetry constraints on the Dunkl parameters of the form $ν_1 = \\mp ν_2 $, linking the reflection symmetry ($ε= \\pm 1 $) to the quantization of angular momentum. These constraints modify the energy spectrum and wavefunctions of the angular operator and the invariant operator. Our framework reveals novel spectral characteristics arising from the interplay between topology and Dunkl symmetry, with potential implications for quantum simulation in engineered systems such as cold atoms and quantum dots."}
{"id": "2601.03442", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03442", "abs": "https://arxiv.org/abs/2601.03442", "authors": ["Zuang Wang", "Yongqiang Wang"], "title": "Provable Acceleration of Distributed Optimization with Local Updates", "comment": null, "summary": "In conventional distributed optimization, each agent performs a single local update between two communication rounds with its neighbors to synchronize solutions. Inspired by the success of using multiple local updates in federated learning, incorporating local updates into distributed optimization has recently attracted increasing attention. However, unlike federated learning, where multiple local updates can accelerate learning by improving gradient estimation under mini-batch settings, it remains unclear whether similar benefits hold in distributed optimization when gradients are exact. Moreover, existing theoretical results typically require reducing the step size when multiple local updates are employed, which can entirely offset any potential benefit of these additional local updates and obscure their true impact on convergence. In this paper, we focus on the classic DIGing algorithm and leverage the tight performance bounds provided by Performance Estimation Problems (PEP) to show that incorporating local updates can indeed accelerate distributed optimization. To the best of our knowledge, this is the first rigorous demonstration of such acceleration for a broad class of objective functions. Our analysis further reveals that, under an appropriate step size, performing only two local updates is sufficient to achieve the maximal possible improvement, and that additional local updates provide no further gains. Because more updates increase computational cost, these findings offer practical guidance for efficient implementation. Extensive experiments on both synthetic and real-world datasets corroborate the theoretical findings."}
{"id": "2601.03913", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.03913", "abs": "https://arxiv.org/abs/2601.03913", "authors": ["Hans van Haren"], "title": "Stratified-turbulence observations in the deep Mediterranean", "comment": "37 pages, 10 figures", "summary": "A nearly half-cubic hectometer of deep Mediterranean-Sea waters is yearlong sampled with about 3000 high-resolution temperature sensors to study different sources of turbulent waterflows, which are vital for life. Although temperature differences are never larger than 0.01degrC, daily, weekly, and seasonal variations are observed. About half the time, relatively warm stratified waters are moved from 100's of meters higher levels to near the seafloor. These internal-wave and sub-mesoscale eddy-induced motions are half an order of magnitude more turbulent than those induced via general geothermal heating from below, and about one order of magnitude more turbulent than those from open-ocean processes. A rough estimate shows that eddy-induced stratified turbulence is likely more important for deep-sea life than rare, not observed, deep dense-water formation at the abyssal-plain mooring site. With a delay of about a week, the stratified turbulence tracks atmospheric disturbances, which are found 35% more energetic in winter than in summer. From comparison of turbulence-calculation methods, of band-pass filtering with vertical-displacement reordering, for data over one-four days, a generalization is proposed for the filter cut-offs under weakly stratified and near-homogeneous conditions in the deep Mediterranean."}
{"id": "2601.04074", "categories": ["physics.soc-ph", "math.AP", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.04074", "abs": "https://arxiv.org/abs/2601.04074", "authors": ["Hanna Bartel", "Martin Burger", "Marie-Therese Wolfram"], "title": "Multi-Dimensional Opinion Formation", "comment": null, "summary": "In this paper we propose and investigate a multi-dimensional opinion dynamics model where people are characterised by both opinions and importance weights across these opinions. Opinion changes occur through binary interactions, with a novel coupling mechanism: the change in one topic depends on the weighted similarity across the full opinion vector. We state the kinetic equation for this process and derive its mean-field partial differential equation to describe the overall dynamics. Analytical computations and numerical simulations confirm that this model generates complex stationary states, and we demonstrate that the final opinion structures are critically determined by the peoples' opinion weights."}
{"id": "2601.03283", "categories": ["physics.soc-ph", "cs.CE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.03283", "abs": "https://arxiv.org/abs/2601.03283", "authors": ["Mina S. Khalaf"], "title": "Physics-Based Decline Curve Analysis and Machine Learning for Temperature Forecasting in Enhanced Geothermal Systems: Utah FORGE", "comment": null, "summary": "Reliable temperature forecasting in Enhanced Geothermal Systems (EGS) is essential, yet petroleum-based decline curves and many machine-learning surrogates do not enforce geothermal heat transfer, while thermo-hydro-mechanical (THM) simulation remains computationally expensive. This study proposes a physics-consistent framework that advances both decline-curve analysis and surrogate modeling. The classical Arps decline family is generalized for geothermal use by introducing an equilibrium-temperature term motivated by Newton-type cooling, ensuring finite late-time temperature limits while reducing exactly to the conventional Arps forms when the equilibrium term is set to zero. The extended decline curves are validated against Utah FORGE downhole temperature measurements and then used to construct learning surrogates on a controlled THM dataset spanning fracture count, well spacing, fracture spacing, host-rock thermal conductivity, and circulation rate. An equation-informed neural network embeds the modified decline equations as differentiable internal computational layers to produce full 0-60 month temperature trajectories from design and operational inputs. A probabilistic Gaussian Process Regression surrogate is also developed for direct multi-horizon forecasting with calibrated uncertainty, while a direct XGBoost regression baseline provides a purely data-driven reference. Across the simulation dataset, the extended decline models reproduce temperature trajectories with near-perfect fidelity (median RMSE = 0.071 °C), and the equation-informed network achieves typical hold-out errors of MAE = 3.06 °C and RMSE = 4.49 °C. The Gaussian Process surrogate delivers the strongest predictive accuracy across 3-60 month horizons (RMSE = 3.39 °C; MAE = 2.34 °C) with well-calibrated uncertainty, whereas the XGBoost baseline exhibits higher errors."}
{"id": "2601.03566", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03566", "abs": "https://arxiv.org/abs/2601.03566", "authors": ["Yanan Bo", "Yongqiang Wang"], "title": "Provably Convergent Decentralized Optimization over Directed Graphs under Generalized Smoothness", "comment": null, "summary": "Decentralized optimization has become a fundamental tool for large-scale learning systems; however, most existing methods rely on the classical Lipschitz smoothness assumption, which is often violated in problems with rapidly varying gradients. Motivated by this limitation, we study decentralized optimization under the generalized $(L_0, L_1)$-smoothness framework, in which the Hessian norm is allowed to grow linearly with the gradient norm, thereby accommodating rapidly varying gradients beyond classical Lipschitz smoothness. We integrate gradient-tracking techniques with gradient clipping and carefully design the clipping threshold to ensure accurate convergence over directed communication graphs under generalized smoothness. In contrast to existing distributed optimization results under generalized smoothness that require a bounded gradient dissimilarity assumption, our results remain valid even when the gradient dissimilarity is unbounded, making the proposed framework more applicable to realistic heterogeneous data environments. We validate our approach via numerical experiments on standard benchmark datasets, including LIBSVM and CIFAR-10, using regularized logistic regression and convolutional neural networks, demonstrating superior stability and faster convergence over existing methods."}
{"id": "2601.04039", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.04039", "abs": "https://arxiv.org/abs/2601.04039", "authors": ["Jie Gu"], "title": "Counterexamples to the conjectured ordering between the waiting-time bound and the thermodynamic uncertainty bound on entropy production", "comment": null, "summary": "Two widely used model-free lower bounds on the steady-state entropy production rate of a continuous-time Markov jump process are the thermodynamic uncertainty relation (TUR) bound $σ_\\text{TUR}$, derived from the mean and variance of a current, and the waiting-time distribution (WTD) bound $σ_\\mathcal{L}$, derived from the irreversibility of partially observed transition sequences together with their waiting times. It has been conjectured that $σ_{\\mathcal L}$ is always at least as tight as $σ_{\\mathrm{TUR}}$ when both are constructed from the same partially observed link. Here we provide four-state counterexamples in a nonequilibrium steady state where $σ_{\\mathcal L}<σ_{\\mathrm{TUR}}$. This result shows that no universal ordering exists between these two inference bounds under partial observation."}
{"id": "2601.03461", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.03461", "abs": "https://arxiv.org/abs/2601.03461", "authors": ["Harold Erbin", "Pierre-Louis Burdeau", "Corentin Bertrand", "Thomas Ayral", "Grégoire Misguich"], "title": "Many-body Quantum Score: a scalable benchmark for digital and analog quantum processors and first test on a commercial neutral atom device", "comment": "31 pages", "summary": "We propose the Many-body Quantum Score (MBQS), a practical and scalable application-level benchmark protocol designed to evaluate the capabilities of quantum processing units (QPUs)--both gate-based and analog--for simulating many-body quantum dynamics. MBQS quantifies performance by identifying the maximum number of qubits with which a QPU can reliably reproduce correlation functions of the transverse-field Ising model following a specific quantum quench. This paper presents the MBQS protocol and highlights its design principles, supported by analytical insights, classical simulations, and experimental data. It also displays results obtained with Ruby, an analog QPU based on Rydberg atoms developed by the Pasqal company. These findings demonstrate MBQS's potential as a robust and informative tool for benchmarking near-term quantum devices for many-body physics."}
{"id": "2601.03442", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03442", "abs": "https://arxiv.org/abs/2601.03442", "authors": ["Zuang Wang", "Yongqiang Wang"], "title": "Provable Acceleration of Distributed Optimization with Local Updates", "comment": null, "summary": "In conventional distributed optimization, each agent performs a single local update between two communication rounds with its neighbors to synchronize solutions. Inspired by the success of using multiple local updates in federated learning, incorporating local updates into distributed optimization has recently attracted increasing attention. However, unlike federated learning, where multiple local updates can accelerate learning by improving gradient estimation under mini-batch settings, it remains unclear whether similar benefits hold in distributed optimization when gradients are exact. Moreover, existing theoretical results typically require reducing the step size when multiple local updates are employed, which can entirely offset any potential benefit of these additional local updates and obscure their true impact on convergence. In this paper, we focus on the classic DIGing algorithm and leverage the tight performance bounds provided by Performance Estimation Problems (PEP) to show that incorporating local updates can indeed accelerate distributed optimization. To the best of our knowledge, this is the first rigorous demonstration of such acceleration for a broad class of objective functions. Our analysis further reveals that, under an appropriate step size, performing only two local updates is sufficient to achieve the maximal possible improvement, and that additional local updates provide no further gains. Because more updates increase computational cost, these findings offer practical guidance for efficient implementation. Extensive experiments on both synthetic and real-world datasets corroborate the theoretical findings."}
{"id": "2601.03480", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03480", "abs": "https://arxiv.org/abs/2601.03480", "authors": ["Apu Chandra Das", "Sakib Salam", "Aninda Roy", "Rakhi Chowdhury", "Antar Chandra Das", "Ashim Chandra Das"], "title": "Improving operating characteristics of clinical trials by augmenting control arm using propensity score-weighted borrowing-by-parts power prior", "comment": "25 pages, 1 figure, 7 tables", "summary": "Borrowing external data can improve estimation efficiency but may introduce bias when populations differ in covariate distributions or outcome variability. A proper balance needs to be maintained between the two datasets to justify the borrowing. We propose a propensity score weighting borrowing-by-parts power prior (PSW-BPP) that integrates causal covariate adjustment through propensity score weighting with a flexible Bayesian borrowing approach to address these challenges in a unified framework. The proposed approach first applies propensity score weighting to align the covariate distribution of the external data with that of the current study, thereby targeting a common estimand and reducing confounding due to population heterogeneity. The weighted external likelihood is then incorporated into a Bayesian model through a borrowing-by-parts power prior, which allows distinct power parameters for the mean and variance components of the likelihood, enabling differential and calibrated information borrowing. Additionally, we adopt the idea of the minimal plausibility index (mPI) to calculate the power parameters. This separate borrowing provides greater robustness to prior-data conflict compared with traditional power prior methods that impose a single borrowing parameter. We study the operating characteristics of PSW-BPP through extensive simulation and a real data example. Simulation studies demonstrate that PSW-BPP yields more efficient and stable estimation than no borrowing and fixed borrowing, particularly under moderate covariate imbalance and outcome heterogeneity. The proposed framework offers a principled and extensible methodological contribution for Bayesian inference with external data in observational and hybrid study designs."}
{"id": "2601.03967", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.03967", "abs": "https://arxiv.org/abs/2601.03967", "authors": ["M. H. Gfrerer", "P. Gangl"], "title": "On the importance of smoothness, interface resolution and numerical sensitivities in shape and topological sensitivity analysis", "comment": null, "summary": "In this paper we investigate the influence of the discretization of PDE constraints on shape and topological derivatives. To this end, we study a tracking-type functional and a two-material Poisson problem in one spatial dimension. We consider the discretization by a standard method and an enriched method. In the standard method we use splines of degree $p$ such that we can control the smoothness of the basis functions easily, but do not take any interface location into consideration. This includes for p=1 the usual hat basis functions. In the enriched method we additionally capture the interface locations in the ansatz space by enrichment functions. For both discretization methods shape and topological sensitivity analysis is performed. It turns out that the regularity of the shape derivative depends on the regularity of the basis functions. Furthermore, for point-wise convergence of the shape derivative the interface has to be considered in the ansatz space. For the topological derivative we show that only the enriched method converges."}
{"id": "2601.03383", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03383", "abs": "https://arxiv.org/abs/2601.03383", "authors": ["Mei Yu", "Walter T. Strunz", "Stefan Nimmrichter"], "title": "Non-Markovian dynamics of the giant atom beyond the rotating-wave approximation", "comment": "12 pages, 5 figure", "summary": "Superconducting qubits coupled to meandering transmission lines or surface acoustic waves may realize giant artificial atoms, whose spatially separated coupling points give rise to long-lived non-Markovian dynamics. Previous studies were limited to the zero-temperature, weak-coupling regime, where the rotating-wave approximation applies and only single-phonon processes contribute. Here we go beyond these limits using the hierarchical equations of motion (HEOM). We show that HEOM accurately captures the exact dynamics at zero temperature and weak coupling, whereas perturbative Redfield theory fails due to long bath memory times. The non-Markovian effects persist at finite temperatures. In the strong-coupling regime, they are further enhanced, and we observe bound-state formation at zero temperature with only two coupling points. These results establish giant atoms as a powerful platform for exploring non-Markovian open quantum dynamics and their applications in quantum information and thermodynamics."}
{"id": "2601.03445", "categories": ["eess.SY", "cs.FL", "math.LO", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.03445", "abs": "https://arxiv.org/abs/2601.03445", "authors": ["Negar Monir", "Sadegh Soudjani"], "title": "Policy Synthesis for Interval MDPs via Polyhedral Lyapunov Functions", "comment": null, "summary": "Decision-making under uncertainty is central to many safety-critical applications, where decisions must be guided by probabilistic modeling formalisms. This paper introduces a novel approach to policy synthesis in multi-objective interval Markov decision processes using polyhedral Lyapunov functions. Unlike previous Lyapunov-based methods that mainly rely on quadratic functions, our method utilizes polyhedral functions to enhance accuracy in managing uncertainties within value iteration of dynamic programming. We reformulate the value iteration algorithm as a switched affine system with interval uncertainties and apply control-theoretic stability principles to synthesize policies that guide the system toward a desired target set. By constructing an invariant set of attraction, we ensure that the synthesized policies provide convergence guarantees while minimizing the impact of transition uncertainty in the underlying model. Our methodology removes the need for computationally intensive Pareto curve computations by directly determining a policy that brings objectives within a specified range of their target values. We validate our approach through numerical case studies, including a recycling robot and an electric vehicle battery, demonstrating its effectiveness in achieving policy synthesis under uncertainty."}
{"id": "2601.04013", "categories": ["physics.ao-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.04013", "abs": "https://arxiv.org/abs/2601.04013", "authors": ["Steffen Maaß", "Sergey Danilov"], "title": "Effects of Horizontal Discretization on Triangular and Hexagonal Grids on Linear Baroclinic and Symmetric Instabilities", "comment": "36 pages, 6 figures", "summary": "As global ocean general circulation models are run at eddy-permitting resolutions, reproducing accurate growth rates of baroclinic instabilities is a major concern when choosing a discretization of the equations of motion. From this viewpoint, we analyze discretizations on triangular and hexagonal grids with different types of variable staggering used in several ocean circulation models. By extending the linear baroclinic instability analysis in the Eady configuration to discretizations on more complex grids, several numerical subtleties are revealed. In comparison to discretizations on quadrilateral grids, the analyzed discretizations are less robust against unstable spurious modes, partly created by the mesh geometry. Some of the subtleties arise because spurious modes on staggered triangular and hexagonal grids do not adhere to Galilean invariance. As a consequence, their growth rates demonstrate a dependence on the alignment between the background flow and the grid, as well as the strength of a uniform background flow. The interactions with spurious modes become more significant on the axis of symmetric instabilities where the physical and spurious branches of instability are more difficult to separate in wavenumber space. Our analysis shows that in most cases moderate biharmonic viscosity and diffusion suppress spurious branches. However, one needs to carefully calibrate the viscosity and diffusivity parameters for each of the considered discretizations in order to achieve this."}
{"id": "2601.03931", "categories": ["math.NA", "math.OC", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.03931", "abs": "https://arxiv.org/abs/2601.03931", "authors": ["Yukuan Hu", "Laura Grazioli"], "title": "Constrained dynamics for searching saddle points on general Riemannian manifolds", "comment": "35 pages, 6 figures, 2 tables. All comments are welcome", "summary": "Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory."}
{"id": "2601.03747", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.03747", "abs": "https://arxiv.org/abs/2601.03747", "authors": ["Julia Ackermann", "Thomas Kruse", "Petr Petrov", "Alexandre Popier"], "title": "Matrix Riccati BSDEs with singular terminal condition and stochastic LQ control with linear terminal constraint", "comment": null, "summary": "We analyze a class of multidimensional linear-quadratic stochastic control problems with random coefficients, motivated by multi-asset optimal trade execution. The problems feature non-diffusive controlled state dynamics and a terminal constraint that restricts the terminal state to a prescribed random linear subspace. We derive the associated Riccati backward stochastic differential equation (BSDE) and identify a suitable formalization of its singular terminal condition. Via a penalization approach, we establish existence of a minimal supersolution of the Riccati BSDE and use it to characterize both the value function and the optimal control. We analyze the asymptotic behavior of the supersolution near terminal time and discuss special cases where closed-form solutions can be obtained."}
{"id": "2601.04102", "categories": ["cond-mat.stat-mech", "math.GT", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.04102", "abs": "https://arxiv.org/abs/2601.04102", "authors": ["Jason Cantarella", "Tetsuo Deguchi", "Henrik Schumacher", "Clayton Shonkwiler", "Erica Uehara"], "title": "Random knotting in very long off-lattice self-avoiding polygons", "comment": "12 pages, 6 figures, 2 tables", "summary": "We present experimental results on knotting in off-lattice self-avoiding polygons in the bead-chain model. Using Clisby's tree data structure and the scale-free pivot algorithm, for each $k$ between $10$ and $27$ we generated $2^{43-k}$ polygons of size $n=2^k$. Using a new knot diagram simplification and invariant-free knot classification code, we were able to determine the precise knot type of each polygon. The results show that the number of prime summands of knot type $K$ in a random $n$-gon is very well described by a Poisson distribution. We estimate the characteristic length of knotting as $656500 \\pm 2500$. We use the count of summands for large $n$ to measure knotting rates and amplitude ratios of knot probabilities more accurately than previous experiments. Our calculations agree quite well with previous on-lattice computations, and support both knot localization and the knot entropy conjecture."}
{"id": "2601.03843", "categories": ["hep-lat", "cond-mat.str-el", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03843", "abs": "https://arxiv.org/abs/2601.03843", "authors": ["Tomoya Hayata", "Yoshimasa Hidaka", "Hiromasa Watanabe"], "title": "Phases of the $q$-deformed $\\mathrm{SU}(N)$ Yang-Mills theory at large $N$", "comment": "7+4 pages, 5+2 figures, 2+1 tables; analysis code will be made publicly available on GitHub after publication", "summary": "We investigate the $(2+1)$-dimensional $q$-deformed $\\mathrm{SU}(N)_k$ Yang-Mills theory in the lattice Hamiltonian formalism, which is characterized by three parameters: the number of colors $N$, the coupling constant $g$, and the level $k$. By treating these as tunable parameters, we explore how key properties of the theory, such as confinement and topological order, emerge in different regimes. Employing a variational mean-field analysis that interpolates between the strong- and weak-coupling regimes, we determine the large-$N$ phase structure in terms of the 't Hooft coupling $λ_\\mathrm{tH}=g^2N$ and the ratio $k/N$. We find that the topologically ordered phase remains robust at large $N$ under appropriate scalings of these parameters. This result indicates that the continuum limit of large-$N$ gauge theory may be more intricate than naively expected, and motivates studies beyond the mean-field theory, both to achieve a further understanding of confinement in gauge theories and to guide quantum simulations of large-$N$ gauge theories."}
{"id": "2601.03445", "categories": ["eess.SY", "cs.FL", "math.LO", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.03445", "abs": "https://arxiv.org/abs/2601.03445", "authors": ["Negar Monir", "Sadegh Soudjani"], "title": "Policy Synthesis for Interval MDPs via Polyhedral Lyapunov Functions", "comment": null, "summary": "Decision-making under uncertainty is central to many safety-critical applications, where decisions must be guided by probabilistic modeling formalisms. This paper introduces a novel approach to policy synthesis in multi-objective interval Markov decision processes using polyhedral Lyapunov functions. Unlike previous Lyapunov-based methods that mainly rely on quadratic functions, our method utilizes polyhedral functions to enhance accuracy in managing uncertainties within value iteration of dynamic programming. We reformulate the value iteration algorithm as a switched affine system with interval uncertainties and apply control-theoretic stability principles to synthesize policies that guide the system toward a desired target set. By constructing an invariant set of attraction, we ensure that the synthesized policies provide convergence guarantees while minimizing the impact of transition uncertainty in the underlying model. Our methodology removes the need for computationally intensive Pareto curve computations by directly determining a policy that brings objectives within a specified range of their target values. We validate our approach through numerical case studies, including a recycling robot and an electric vehicle battery, demonstrating its effectiveness in achieving policy synthesis under uncertainty."}
{"id": "2601.03497", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03497", "abs": "https://arxiv.org/abs/2601.03497", "authors": ["Shuo Wang", "Joseph Feldman", "Jerome P. Reiter"], "title": "Differentially Private Bayesian Inference for Gaussian Copula Correlations", "comment": null, "summary": "Gaussian copulas are widely used to estimate multivariate distributions and relationships. We present algorithms for estimating Gaussian copula correlations that ensure differential privacy. We first convert data values into sets of two-way tables of counts above and below marginal medians. We then add noise to these counts to satisfy differential privacy. We utilize the one-to-one correspondence between the true counts and the copula correlation to estimate a posterior distribution of the copula correlation given the noisy counts, marginalizing over the distribution of the underlying true counts using a composite likelihood. We also present an alternative, maximum likelihood approach for point estimation. Using simulation studies, we compare these methods to extant methods in the literature for computing differentially private copula correlations."}
{"id": "2601.03971", "categories": ["math.NA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03971", "abs": "https://arxiv.org/abs/2601.03971", "authors": ["Josie König", "Han Cheng Lie"], "title": "Posterior error bounds for prior-driven balancing in linear Gaussian inverse problems", "comment": null, "summary": "In large-scale Bayesian inverse problems, it is often necessary to apply approximate forward models to reduce the cost of forward model evaluations, while controlling approximation quality. In the context of Bayesian inverse problems with linear forward models, Gaussian priors, and Gaussian noise, we use perturbation theory for inverses to bound the error in the approximate posterior mean and posterior covariance resulting from a linear approximate forward model. We then focus on the smoothing problem of inferring the initial condition of linear time-invariant dynamical systems, using finitely many partial state observations. For such problems, and for a specific model order reduction method based on balanced truncation, we show that the impulse response of a certain prior-driven system is closely related to the prior-preconditioned Hessian of the inverse problem. This reveals a novel connection between systems theory and inverse problems. We exploit this connection to prove the first a priori error bounds for system-theoretic model order reduction methods applied to smoothing problems. The bounds control the approximation error of the posterior mean and covariance in terms of the truncated Hankel singular values of the underlying system."}
{"id": "2601.03395", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03395", "abs": "https://arxiv.org/abs/2601.03395", "authors": ["Paul M. Alsing", "Richard J. Birrittella", "Peter L. Kaulfuss"], "title": "Multiphoton Interference with a symmetric SU(N) beam splitter and the generalization of the extended Hong-Ou-Mandel effect", "comment": "31 pages including 2 source code, and10 figures", "summary": "We examine multiphoton interference with a symmetric $SU(N)$ beam splitter $S_N$, an extension of features of the $SU(2)$ 50/50 beam splitter extended Hong-Ou-Mandel (eHOM) effect, whereby one obtains a zero amplitude (probability) for the output coincidence state (defined by equal number of photons $n/N$ in each output port), when a total number $n$ of photons impinges on the $N$-port device. These are transitions of the form $|n_1,n_2,\\ldots,n_N\\rangle\\overset{S_N}{\\to}|n/N\\rangle^{\\otimes N}$, where $n=\\sum_{i=1}^N n_i$, which generalize the Hong-Ou-Mandel (HOM) effect $|1,1\\rangle \\overset{S_2}{\\to}|1,1\\rangle $, the eHOM effect $|n_1,n_2\\rangle \\overset{S_2}{\\to}|\\tfrac{n_1+n_2}{2},\\tfrac{n_1+n_2}{2}\\rangle $, and the generalized HOM effect (gHOM) $|1\\rangle^{\\otimes N}\\overset{S_N}{\\to}|1\\rangle^{\\otimes N}$, which have previously been studied in the literature. The emphasis of this work is on illuminating how the overall destructive interference occurs in separate groups of destructive interferences of sub-amplitudes of the total zero amplitude. We develop symmetry properties for the generalized eHOM effect (geHOM) $|n_1,n_2,\\ldots,n_N\\rangle\\overset{S_N}{\\to}|n/N\\rangle^{\\otimes N}$ involving a zero amplitude governed by Perm($Λ$)=0, for an appropriately constructed matrix $Λ(S_N)$ built from the matrix elements of $S_N$. We develop an analytical constraint equation for Perm$(Λ)$ for arbitrary $N$ that allows us to determine when it is zero. We generalize the SU(2) beam splitter feature of central nodal line (CNL), which has a zero diagonal along the output probability distribution when one of the input states is of odd parity (containing only odd number of photons), to the general case of $N = 2 * N'$ where $N'\\in odd$."}
{"id": "2601.03452", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03452", "abs": "https://arxiv.org/abs/2601.03452", "authors": ["Vincent P. Paglioni", "Graeme Troxell", "Aaron Brown", "Steve Conrad", "Mazdak Arabi"], "title": "Developing a Quantitative Resiliency Approach", "comment": "18 pages", "summary": "Resiliency has garnered attention in the management of critical infrastructure as a metric of system performance, but there are significant roadblocks to its implementation in a realistic decision-making framework. Contrasted to risk and reliability, which have robust quantification approaches and undergird many regulatory approaches to system safety (e.g., \"risk-informed decision-making\"), resiliency is a diffuse, qualitatively-understood characteristic, often treated differently or distinctly. However, in the emerging context of highly-complex, highly-interdependent critical systems, the idea of reliability (as the probability of non-failure) may not be an appropriate metric of system health. As a result, focus is shifting towards resiliency-centered approaches that value the response to failure as much as the avoidance of failure. Supporting this approach requires a robustly-defined, quantitative understanding of resiliency. In this paper, we explore the foundations of reliability and resiliency engineering, and propose an approach to resiliency-informed decision-making bolstered by a quantitative understanding of resiliency."}
{"id": "2601.03877", "categories": ["nlin.AO", "physics.soc-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.03877", "abs": "https://arxiv.org/abs/2601.03877", "authors": ["Maxime Lucas", "Corentin Bisot", "Giovanni Petri", "Stéphane Declerck", "Timoteo Carletti"], "title": "Minimal branching and fusion morphogenesis approaches biological multi-objective optimality", "comment": null, "summary": "Many biological networks grow by elongation of filaments that can branch and fuse -- typical examples include fungal mycelium or slime mold. These networks must simultaneously perform multiple tasks such as transport, exploration, and robustness under finite resources. Yet, how such multi-task architectures emerge from local growth processes remains poorly understood. Here, we introduce a minimal model of spatial network morphogenesis based solely on stochastic branching, fusion, and stopping, during elongation. Despite the absence of global optimization or feedback, the model generates a broad morphospace from tree-like, to loopy, as well as hybrid architectures. By quantifying multiple functional objectives, we show that (i) these synthetic structures occupy similar regions of performance space than evolved empirical fungal networks, and (ii) that their Pareto front of optimal trade-offs lies close to that of these same fungal networks. Our results show that biological architectures approaching multi-objective optimality can arise from simple local growth rules, and identify branching and fusion as fundamental ingredients shaping the architecture of living transport networks."}
{"id": "2601.04104", "categories": ["cond-mat.str-el", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.04104", "abs": "https://arxiv.org/abs/2601.04104", "authors": ["Yunhao Fan", "Gia-Wei Chern"], "title": "Equivariant Neural Networks for Force-Field Models of Lattice Systems", "comment": "13 pages, 6 figures", "summary": "Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, symmetry-aware descriptors, whose construction is often system-specific and can hinder generality and transferability across different lattice Hamiltonians. Here we introduce a symmetry-preserving framework based on equivariant neural networks (ENNs) that provides a general, data-driven mapping from local configurations of dynamical variables to the associated on-site forces in a lattice Hamiltonian. In contrast to ENN architectures developed for molecular systems -- where continuous Euclidean symmetries dominate -- our approach aims to embed the discrete point-group and internal symmetries intrinsic to lattice models directly into the neural-network representation of the force field. As a proof of principle, we construct an ENN-based force-field model for the adiabatic dynamics of the Holstein Hamiltonian on a square lattice, a canonical system for electron-lattice physics. The resulting ML-enabled large-scale dynamical simulations faithfully capture mesoscale evolution of the symmetry-breaking phase, illustrating the utility of lattice-equivariant architectures for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems."}
{"id": "2601.03754", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.03754", "abs": "https://arxiv.org/abs/2601.03754", "authors": ["Roland Schwan", "Daniel Kuhn", "Colin N. Jones"], "title": "GPU-Accelerated Cholesky Factorization of Block Tridiagonal Matrices", "comment": null, "summary": "This paper presents a GPU-accelerated framework for solving block tridiagonal linear systems that arise naturally in numerous real-time applications across engineering and scientific computing. Through a multi-stage permutation strategy based on nested dissection, we reduce the computational complexity from $\\mathcal{O}(Nn^3)$ for sequential Cholesky factorization to $\\mathcal{O}(\\log_2(N)n^3)$ when sufficient parallel resources are available, where $n$ is the block size and $N$ is the number of blocks. The algorithm is implemented using NVIDIA's Warp library and CUDA to exploit parallelism at multiple levels within the factorization algorithm. Our implementation achieves speedups exceeding 100x compared to the sparse solver QDLDL, 25x compared to a highly optimized CPU implementation using BLASFEO, and more than 2x compared to NVIDIA's CUDSS library. The logarithmic scaling with horizon length makes this approach particularly attractive for long-horizon problems in real-time applications. Comprehensive numerical experiments on NVIDIA GPUs demonstrate the practical effectiveness across different problem sizes and precisions. The framework provides a foundation for GPU-accelerated optimization solvers in robotics, autonomous systems, and other domains requiring repeated solution of structured linear systems. The implementation is open-source and available at https://github.com/PREDICT-EPFL/socu."}
{"id": "2601.04116", "categories": ["cond-mat.stat-mech", "nlin.CG"], "pdf": "https://arxiv.org/pdf/2601.04116", "abs": "https://arxiv.org/abs/2601.04116", "authors": ["Johannes Schmidt", "Žiga Krajnik", "Vladislav Popkov"], "title": "Universality in driven systems with a multiply-degenerate umbilic point", "comment": "15 + 4 pages", "summary": "We investigate a driven particle system, a multilane asymmetric exclusion process, where the particle number in every lane is conserved, and stationary state is fully uncorrelated. The phase space has, starting from three lanes and more, an umbilic manifold where characteristic velocities of all the modes but one coincide, thus allowing us to study a weakly hyperbolic system with arbitrarily large degeneracy. We then study space-time fluctuations in the steady state, at the umbilic manifold, which are expected to exhibit universal scaling features. We formulate an effective mode-coupling theory (MCT) for the multilane model within the umbilic subspace and test its predictions. Unlike in the bidirectional two-lane model with an umbilic point studied earlier, here we find a robust $z=3/2$ dynamical exponent for the umbilic mode. The umbilic scaling function, obtained from Monte-Carlo simulations, for the simplest 3-lane scenario, appears to have an universal shape for a range of interaction parameters. Remarkably, the shape and dynamic exponent of the non-degenerate mode can be analytically predicted on the base of effective MCT, up to non-universal scaling factor. Our findings suggest the existence of novel universality classes with dynamical exponent $3/2$, appearing in long-lived hydrodynamic modes with equal characteristic velocities."}
{"id": "2601.03452", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03452", "abs": "https://arxiv.org/abs/2601.03452", "authors": ["Vincent P. Paglioni", "Graeme Troxell", "Aaron Brown", "Steve Conrad", "Mazdak Arabi"], "title": "Developing a Quantitative Resiliency Approach", "comment": "18 pages", "summary": "Resiliency has garnered attention in the management of critical infrastructure as a metric of system performance, but there are significant roadblocks to its implementation in a realistic decision-making framework. Contrasted to risk and reliability, which have robust quantification approaches and undergird many regulatory approaches to system safety (e.g., \"risk-informed decision-making\"), resiliency is a diffuse, qualitatively-understood characteristic, often treated differently or distinctly. However, in the emerging context of highly-complex, highly-interdependent critical systems, the idea of reliability (as the probability of non-failure) may not be an appropriate metric of system health. As a result, focus is shifting towards resiliency-centered approaches that value the response to failure as much as the avoidance of failure. Supporting this approach requires a robustly-defined, quantitative understanding of resiliency. In this paper, we explore the foundations of reliability and resiliency engineering, and propose an approach to resiliency-informed decision-making bolstered by a quantitative understanding of resiliency."}
{"id": "2601.03532", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.03532", "abs": "https://arxiv.org/abs/2601.03532", "authors": ["Andrew Gerard Roberts", "Michael Dietze", "Jonathan Huggins"], "title": "Propagating Surrogate Uncertainty in Bayesian Inverse Problems", "comment": null, "summary": "Standard Bayesian inference schemes are infeasible for inverse problems with computationally expensive forward models. A common solution is to replace the model with a cheaper surrogate. To avoid overconfident conclusions, it is essential to acknowledge the surrogate approximation by propagating its uncertainty. At present, a variety of distinct uncertainty propagation methods have been suggested, with little understanding of how they vary. To fill this gap, we propose a mixture distribution termed the expected posterior (EP) as a general baseline for uncertainty-aware posterior approximation, justified by decision theoretic and modular Bayesian inference arguments. We then investigate the expected unnormalized posterior (EUP), a popular heuristic alternative, analyzing when it may deviate from the EP baseline. Our results show that this heuristic can break down when the surrogate uncertainty is highly non-uniform over the design space, as can be the case when the log-likelihood is emulated by a Gaussian process. Finally, we present the random kernel preconditioned Crank-Nicolson (RKpCN) algorithm, an approximate Markov chain Monte Carlo scheme that provides practical EP approximation in the challenging setting involving infinite-dimensional Gaussian process surrogates."}
{"id": "2601.04022", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.04022", "abs": "https://arxiv.org/abs/2601.04022", "authors": ["Ming-Jun Lai"], "title": "A Bivariate Spline Construction of Orthonormal Polynomials over Polygonal Domains and Its Applications to Quadrature", "comment": "36 pages, 110 figures", "summary": "We present computational methods for constructing orthogonal/orthonormal polynomials over arbitrary polygonal domains in $\\mathbb{R}^2$\n  using bivariate spline functions. Leveraging a mature MATLAB implementation which generates spline spaces of any degree, any smoothness over any triangulation, we have exact polynomial representation over the polygonal domain of interest. Two algorithms are developed: one constructs orthonormal polynomials of degree $d>0$\n  over a polygonal domain, and the other constructs orthonormal polynomials of degree $d+1$ in the orthogonal complement of $\\mathbb{P}_d$. Numerical examples for degrees $d=1--5$ illustrate the structure and zero curves of these polynomials, providing evidence against the existence of Gauss quadrature on centrally symmetric domains. In addition, we introduce polynomial reduction strategies based on odd- and even-degree orthogonal polynomials, reducing the integration to the integration of its residual quadratic or linear polynomials. These reductions motivate new quadrature schemes, which we further extend through polynomial interpolation to obtain efficient, high-precision quadrature rules for various polygonal domains."}
{"id": "2601.03407", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03407", "abs": "https://arxiv.org/abs/2601.03407", "authors": ["Roman Ovsiannikov", "Kurt Jacobs", "Andrii G. Sotnikov", "Matthew E. Trusheim", "Denys I. Bondar"], "title": "Hybrid non-degenerate parametric amplifier for a microwave cavity mode and an NV ensemble", "comment": "12 pages, 5 figures", "summary": "We introduce an implementation of a non-degenerate parametric amplifier in which the signal and idler modes, respectively, a microwave mode and an ensemble of spins (e.g., nitrogen-vacancy centers in diamond), are operated in their linear regime. This paramp, which amplifies signals in both parts at room and cryogenic temperatures, can be used to generate both the two-mode and single-mode squeezing of either system. It requires merely modulating the frequency of the spin ensemble at the sum of the cavity and spin frequencies (providing the classical pump) with the two systems sufficiently detuned. This effect is remarkable given that modulating a spin ensemble by itself produces neither amplification nor squeezing, unlike modulating an oscillator, and that an off-resonant perturbative analysis would suggest that modulating the spin ensemble merely parametrically drives the cavity mode. With typical cavity parameters including a cavity quality factor~$Q=10^4$, and a 1 GHz modulation amplitude, the microwave signal can be amplified by approximately $18~\\mbox{dB}$ in $1.7~\\mbox{$μ$s}$, with a resonant bandwidth of about $0.5~\\mbox{MHz}$. At $10~\\mbox{mK}$ with the same modulation amplitude and a cavity and spin $Q=5\\times 10^4$ it generates approximately $5~\\mbox{dB}$ of squeezing. We also examine the experimental requirements for implementation."}
{"id": "2601.03476", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.03476", "abs": "https://arxiv.org/abs/2601.03476", "authors": ["Rishav Sen", "Yunuo Zhang", "Fangqi Liu", "Jose Paolo Talusan", "Ava Pettet", "Yoshinori Suzue", "Ayan Mukhopadhyay", "Abhishek Dubey"], "title": "Online Decision-Making Under Uncertainty for Vehicle-to-Building Systems", "comment": "17 pages, 2 figures, 10 tables. Published in the Proceedings of the 16th ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS '25), May 06--09, 2025, Irvine, CA, USA", "summary": "Vehicle-to-building (V2B) systems integrate physical infrastructures, such as smart buildings and electric vehicles (EVs) connected to chargers at the building, with digital control mechanisms to manage energy use. By utilizing EVs as flexible energy reservoirs, buildings can dynamically charge and discharge them to optimize energy use and cut costs under time-variable pricing and demand charge policies. This setup leads to the V2B optimization problem, where buildings coordinate EV charging and discharging to minimize total electricity costs while meeting users' charging requirements. However, the V2B optimization problem is challenging because of: (1) fluctuating electricity pricing, which includes both energy charges ($/kWh) and demand charges ($/kW); (2) long planning horizons (typically over 30 days); (3) heterogeneous chargers with varying charging rates, controllability, and directionality (i.e., unidirectional or bidirectional); and (4) user-specific battery levels at departure to ensure user requirements are met. In contrast to existing approaches that often model this setting as a single-shot combinatorial optimization problem, we highlight critical limitations in prior work and instead model the V2B optimization problem as a Markov decision process (MDP), i.e., a stochastic control process. Solving the resulting MDP is challenging due to the large state and action spaces. To address the challenges of the large state space, we leverage online search, and we counter the action space by using domain-specific heuristics to prune unpromising actions. We validate our approach in collaboration with Nissan Advanced Technology Center - Silicon Valley. Using data from their EV testbed, we show that the proposed framework significantly outperforms state-of-the-art methods."}
{"id": "2601.04172", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.04172", "abs": "https://arxiv.org/abs/2601.04172", "authors": ["Ryan T. Grimm", "Joel D. Eaves"], "title": "Stochastic Path Compression for Spectral Tensor Networks on Cyclic Graphs", "comment": "7 pages, 3 figures", "summary": "We develop a new approach to compress cyclic tensor networks called stochastic path compression (SPC) that uses an iterative importance sampling procedure to target edges with large bond-dimensions. Closed random walks in SPC form compression pathways that spatially localize large bond-dimensions in the tensor network. Analogous to the phase separation of two immiscible liquids, SPC separates the graph of bond-dimensions into spatially distinct high and low density regions. When combined with our integral decimation algorithm, SPC facilitates the accurate compression of cyclic tensor networks with continuous degrees of freedom. To benchmark and illustrate the methods, we compute the absolute thermodynamics of $q$-state clock models on two-dimensional square lattices and an XY model on a Watts-Strogatz graph, which is a small-world network with random connectivity between spins."}
{"id": "2601.03759", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.03759", "abs": "https://arxiv.org/abs/2601.03759", "authors": ["Jean B Lasserre"], "title": "Connecting Max-entropy With Computational Geometry, LP And SDP", "comment": null, "summary": "We consider the well-known max-(relative) entropy problem $Θ$(y) = infQ$\\ll$P DKL(Q P ) with Kullback-Leibler divergence on a domain $Ω$ $\\subset$ R d , and with ''moment'' constraints h dQ = y, y $\\in$ R m . We show that when m $\\le$ d, $Θ$ is the Cram{é}r transform of a function v that solves a simply related computational geometry problem. Also, and remarkably, to the canonical LP: min x$\\ge$0 {c T x\\,: A x = y}, with A $\\in$ R mxd , one may associate a max-entropy problem with a suitably chosen reference measure P on R d + and linear mapping h(x) = Ax, such that its associated perspective function $ε$ $Θ$(y/$ε$) is the optimal value of the log-barrier formulation (with parameter $ε$) of the dual LP (and so it converges to the LP optimal value as $ε$ $\\rightarrow$ 0). An analogous result also holds for the canonical SDP: min X 0 { C, X\\,: A(X) = y }."}
{"id": "2601.04172", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.04172", "abs": "https://arxiv.org/abs/2601.04172", "authors": ["Ryan T. Grimm", "Joel D. Eaves"], "title": "Stochastic Path Compression for Spectral Tensor Networks on Cyclic Graphs", "comment": "7 pages, 3 figures", "summary": "We develop a new approach to compress cyclic tensor networks called stochastic path compression (SPC) that uses an iterative importance sampling procedure to target edges with large bond-dimensions. Closed random walks in SPC form compression pathways that spatially localize large bond-dimensions in the tensor network. Analogous to the phase separation of two immiscible liquids, SPC separates the graph of bond-dimensions into spatially distinct high and low density regions. When combined with our integral decimation algorithm, SPC facilitates the accurate compression of cyclic tensor networks with continuous degrees of freedom. To benchmark and illustrate the methods, we compute the absolute thermodynamics of $q$-state clock models on two-dimensional square lattices and an XY model on a Watts-Strogatz graph, which is a small-world network with random connectivity between spins."}
{"id": "2601.03476", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.03476", "abs": "https://arxiv.org/abs/2601.03476", "authors": ["Rishav Sen", "Yunuo Zhang", "Fangqi Liu", "Jose Paolo Talusan", "Ava Pettet", "Yoshinori Suzue", "Ayan Mukhopadhyay", "Abhishek Dubey"], "title": "Online Decision-Making Under Uncertainty for Vehicle-to-Building Systems", "comment": "17 pages, 2 figures, 10 tables. Published in the Proceedings of the 16th ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS '25), May 06--09, 2025, Irvine, CA, USA", "summary": "Vehicle-to-building (V2B) systems integrate physical infrastructures, such as smart buildings and electric vehicles (EVs) connected to chargers at the building, with digital control mechanisms to manage energy use. By utilizing EVs as flexible energy reservoirs, buildings can dynamically charge and discharge them to optimize energy use and cut costs under time-variable pricing and demand charge policies. This setup leads to the V2B optimization problem, where buildings coordinate EV charging and discharging to minimize total electricity costs while meeting users' charging requirements. However, the V2B optimization problem is challenging because of: (1) fluctuating electricity pricing, which includes both energy charges ($/kWh) and demand charges ($/kW); (2) long planning horizons (typically over 30 days); (3) heterogeneous chargers with varying charging rates, controllability, and directionality (i.e., unidirectional or bidirectional); and (4) user-specific battery levels at departure to ensure user requirements are met. In contrast to existing approaches that often model this setting as a single-shot combinatorial optimization problem, we highlight critical limitations in prior work and instead model the V2B optimization problem as a Markov decision process (MDP), i.e., a stochastic control process. Solving the resulting MDP is challenging due to the large state and action spaces. To address the challenges of the large state space, we leverage online search, and we counter the action space by using domain-specific heuristics to prune unpromising actions. We validate our approach in collaboration with Nissan Advanced Technology Center - Silicon Valley. Using data from their EV testbed, we show that the proposed framework significantly outperforms state-of-the-art methods."}
{"id": "2601.03647", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03647", "abs": "https://arxiv.org/abs/2601.03647", "authors": ["Koki Momoki", "Takuma Yoshida"], "title": "Small area estimation of dependent extreme value indices", "comment": "32 pages, 9 figures", "summary": "In extreme value analysis, tail behavior of a heavy-tailed data distribution is modeled by a Pareto-type distribution in which the so-called extreme value index (EVI) controls the tail behavior. For heavy-tailed data obtained from multiple population subgroups, or areas, this study efficiently predicts the EVIs of all areas using information among areas. For this purpose, we propose a mixed effects model, which is a useful approach in small area estimation. In this model, we represent differences among areas in the EVIs by latent variables called random effects. Using correlated random effects across areas, we incorporate the relations among areas into the model. The obtained model achieves simultaneous prediction of EVIs of all areas. Herein, we describe parameter estimation and random effect prediction in the model, and clarify theoretical properties of the estimator. Additionally, numerical experiments are presented to demonstrate the effectiveness of the proposed method. As an application of our model, we provide a risk assessment of heavy rainfall in Japan."}
{"id": "2601.04075", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.04075", "abs": "https://arxiv.org/abs/2601.04075", "authors": ["Julia Muñoz-Echániz", "Christoph Reisinger"], "title": "A higher order sparse grid combination technique", "comment": null, "summary": "We show that a generalised sparse grid combination technique which combines multi-variate extrapolation of finite difference solutions with the standard combination formula lifts a second order accurate scheme on regular meshes to a fourth order combined sparse grid solution. In the analysis, working in a general dimension, we characterise all terms in a multivariate error expansion of the scheme as solutions of a sequence of semi-discrete problems. This is first carried out formally under suitable assumptions on the truncation error of the scheme, stability and regularity of solutions. We then verify the assumptions on the example of the Poisson problem with smooth data, and illustrate the practical convergence in up to seven dimensions."}
{"id": "2601.03426", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03426", "abs": "https://arxiv.org/abs/2601.03426", "authors": ["Ryohei Weil", "Dmytro Bondarenko", "Arnab Adhikary", "Robert Raussendorf"], "title": "Testing measurement-based computational phases of quantum matter on a quantum processor", "comment": "22 pages, 12 figures", "summary": "Many symmetry protected or symmetry enriched phases of quantum matter have the property that every ground state in a given such phase endows measurement based quantum computation with the same computational power. Such phases are called computational phases of quantum matter. Here, we experimentally verify four theoretical predictions for them on an IBM superconducting quantum device. We comprehensively investigate how symmetric imperfections of the resource states translate into logical decoherence, and how this decoherence is mitigated. In particular, the central experiment probes the scaling law from which the uniformity of computational power follows. We also analyze the correlated regime, where local measurements give rise to logical operations collectively. We test the prediction that densest packing of a measurement-based algorithms remains the most efficient, in spite of the correlations. Our experiments corroborate the operational stability of measurement based quantum computation in quantum phases of matter with symmetry."}
{"id": "2601.03486", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03486", "abs": "https://arxiv.org/abs/2601.03486", "authors": ["Zeyu Dong", "Yuke Tian", "Yu Sun"], "title": "Adaptive Model-Based Reinforcement Learning for Orbit Feedback Control in NSLS-II Storage Ring", "comment": "Accepted by the 20th International Conference on Accelerator and Large Experimental Physics Control Systems (ICALEPCS 2025)", "summary": "The National Synchrotron Light Source II (NSLS-II) uses highly stable electron beam to produce high-quality X-ray beams with high brightness and low-emittance synchrotron radiation. The traditional algorithm to stabilize the beam applies singular value decomposition (SVD) on the orbit response matrix to remove noise and extract actions. Supervised learning has been studied on NSLS-II storage ring stabilization and other accelerator facilities recently. Several problems, for example, machine status drifting, environment noise, and non-linear accelerator dynamics, remain unresolved in the SVD-based and supervised learning algorithms. To address these problems, we propose an adaptive training framework based on model-based reinforcement learning. This framework consists of two types of optimizations: trajectory optimization attempts to minimize the expected total reward in a differentiable environment, and online model optimization learns non-linear machine dynamics through the agent-environment interaction. Through online training, this framework tracks the internal status drifting in the electron beam ring. Simulation and real in-facility experiments on NSLS-II reveal that our method stabilizes the beam position and minimizes the alignment error, defined as the root mean square (RMS) error between adjusted beam positions and the reference position, down to ~1$μ$m."}
{"id": "2601.03946", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03946", "abs": "https://arxiv.org/abs/2601.03946", "authors": ["Valentine Olanubi", "Phineas Agar", "Brendan Ames"], "title": "Provably Finding a Hidden Dense Submatrix among Many Planted Dense Submatrices via Convex Programming", "comment": null, "summary": "We consider the densest submatrix problem, which seeks the submatrix of fixed size of a given binary matrix that contains the most nonzero entries. This problem is a natural generalization of fundamental problems in combinatorial optimization, e.g., the densest subgraph, maximum clique, and maximum edge biclique problems, and has wide application the study of complex networks. Much recent research has focused on the development of sufficient conditions for exact solution of the densest submatrix problem via convex relaxation. The vast majority of these sufficient conditions establish identification of the densest submatrix within a graph containing exactly one large dense submatrix hidden by noise. The assumptions of these underlying models are not observed in real-world networks, where the data may correspond to a matrix containing many dense submatrices of varying sizes.\n  We extend and generalize these results to the more realistic setting where the input matrix may contain \\emph{many} large dense subgraphs. Specifically, we establish sufficient conditions under which we can expect to solve the densest submatrix problem in polynomial time for random input matrices sampled from a generalization of the stochastic block model. Moreover, we also provide sufficient conditions for perfect recovery under a deterministic adversarial. Numerical experiments involving randomly generated problem instances and real-world collaboration and communication networks are used empirically to verify the theoretical phase-transitions to perfect recovery given by these sufficient conditions."}
{"id": "2601.03651", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.03651", "abs": "https://arxiv.org/abs/2601.03651", "authors": ["Zhouhao Guo", "Jiaju Zhang"], "title": "Double interval entanglement in quasiparticle excited states", "comment": "14 pages, 6 figures", "summary": "We investigate double-interval entanglement measures, specifically reflected entropy, mutual information, and logarithmic negativity, in quasiparticle excited states for classical, bosonic, and fermionic systems. We develop an algorithm that efficiently calculates these measures from density matrices expressed in a non-orthonormal basis, enabling straightforward numerical implementation. We find a universal additivity property that emerges at large momentum differences, where the entanglement measures for states with distinct quasiparticle sets equal the sum of their individual contributions. The classical limit arises as a special case of this additivity, with both bosonic and fermionic results converging to classical behavior when all momentum differences are large."}
{"id": "2601.03486", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03486", "abs": "https://arxiv.org/abs/2601.03486", "authors": ["Zeyu Dong", "Yuke Tian", "Yu Sun"], "title": "Adaptive Model-Based Reinforcement Learning for Orbit Feedback Control in NSLS-II Storage Ring", "comment": "Accepted by the 20th International Conference on Accelerator and Large Experimental Physics Control Systems (ICALEPCS 2025)", "summary": "The National Synchrotron Light Source II (NSLS-II) uses highly stable electron beam to produce high-quality X-ray beams with high brightness and low-emittance synchrotron radiation. The traditional algorithm to stabilize the beam applies singular value decomposition (SVD) on the orbit response matrix to remove noise and extract actions. Supervised learning has been studied on NSLS-II storage ring stabilization and other accelerator facilities recently. Several problems, for example, machine status drifting, environment noise, and non-linear accelerator dynamics, remain unresolved in the SVD-based and supervised learning algorithms. To address these problems, we propose an adaptive training framework based on model-based reinforcement learning. This framework consists of two types of optimizations: trajectory optimization attempts to minimize the expected total reward in a differentiable environment, and online model optimization learns non-linear machine dynamics through the agent-environment interaction. Through online training, this framework tracks the internal status drifting in the electron beam ring. Simulation and real in-facility experiments on NSLS-II reveal that our method stabilizes the beam position and minimizes the alignment error, defined as the root mean square (RMS) error between adjusted beam positions and the reference position, down to ~1$μ$m."}
{"id": "2601.03674", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03674", "abs": "https://arxiv.org/abs/2601.03674", "authors": ["Yuanying Chen", "Tongyu Li", "Yang Bai", "Zhenhua Lin"], "title": "Multi-transport Distributional Regression", "comment": null, "summary": "We study distribution-on-distribution regression problems in which a response distribution depends on multiple distributional predictors. Such settings arise naturally in applications where the outcome distribution is driven by several heterogeneous distributional sources, yet remain challenging due to the nonlinear geometry of the Wasserstein space. We propose an intrinsic regression framework that aggregates predictor-specific transported distributions through a weighted Fréchet mean in the Wasserstein space. The resulting model admits multiple distributional predictors, assigns interpretable weights quantifying their relative contributions, and defines a flexible regression operator that is invariant to auxiliary construction choices, such as the selection of a reference distribution. From a theoretical perspective, we establish identifiability of the induced regression operator and derive asymptotic guarantees for its estimation under a predictive Wasserstein semi-norm, which directly characterizes convergence of the composite prediction map. Extensive simulation studies and a real data application demonstrate the improved predictive performance and interpretability of the proposed approach compared with existing Wasserstein regression methods."}
{"id": "2601.04112", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.04112", "abs": "https://arxiv.org/abs/2601.04112", "authors": ["Ben S. Southworth", "Hussam Al Daas", "Golo A> Wimmer", "Ed Threlfall"], "title": "Algebraic Multigrid with Overlapping Schwarz Smoothers and Local Spectral Coarse Grids for Least Squares Problems", "comment": null, "summary": "This paper develops a new algebraic multigrid (AMG) method for sparse least-squares systems of the form $A=G^TG$ motivated by challenging applications in scientific computing where classical AMG methods fail. First we review and relate the use of local spectral problems in distinct fields of literature on AMG, domain decomposition (DD), and multiscale finite elements. We then propose a new approach blending aggregation-based coarsening, overlapping Schwarz smoothers, and locally constructed spectral coarse spaces. By exploiting the factorized structure of $A$, we construct an inexpensive symmetric positive semidefinite splitting that yields local generalized eigenproblems whose solutions define sparse, nonoverlapping coarse basis functions. This enables a fully algebraic and naturally recursive multilevel hierarchy that can either coarsen slowly to achieve AMG-like operator complexities, or coarsen aggressively-with correspondingly larger local spectral problems-to ensure robustness on problems that cannot be solved by existing AMG methods. The method requires no geometric information, avoids global eigenvalue solves, and maintains efficient parallelizable setup through localized operations. Numerical experiments demonstrate that the proposed least-squares AMG-DD method achieves convergence rates independent of anisotropy on rotated diffusion problems and remains scalable with problem size, while for small amounts of anisotropy we obtain convergence and operator complexities comparable with classical AMG methods. Most notably, for extremely anisotropic heat conduction operators arising in magnetic confinement fusion, where AMG and smoothed aggregation fail to reduce the residual even marginally, our method provides robust and efficient convergence across many orders of magnitude in anisotropy strength."}
{"id": "2601.03461", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.03461", "abs": "https://arxiv.org/abs/2601.03461", "authors": ["Harold Erbin", "Pierre-Louis Burdeau", "Corentin Bertrand", "Thomas Ayral", "Grégoire Misguich"], "title": "Many-body Quantum Score: a scalable benchmark for digital and analog quantum processors and first test on a commercial neutral atom device", "comment": "31 pages", "summary": "We propose the Many-body Quantum Score (MBQS), a practical and scalable application-level benchmark protocol designed to evaluate the capabilities of quantum processing units (QPUs)--both gate-based and analog--for simulating many-body quantum dynamics. MBQS quantifies performance by identifying the maximum number of qubits with which a QPU can reliably reproduce correlation functions of the transverse-field Ising model following a specific quantum quench. This paper presents the MBQS protocol and highlights its design principles, supported by analytical insights, classical simulations, and experimental data. It also displays results obtained with Ruby, an analog QPU based on Rydberg atoms developed by the Pasqal company. These findings demonstrate MBQS's potential as a robust and informative tool for benchmarking near-term quantum devices for many-body physics."}
{"id": "2601.03495", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03495", "abs": "https://arxiv.org/abs/2601.03495", "authors": ["Osasumwen Cedric Ogiesoba-Eguakun", "Suman Rath"], "title": "Cyberattack Detection in Virtualized Microgrids Using LightGBM and Knowledge-Distilled Classifiers", "comment": "12 pages", "summary": "Modern microgrids depend on distributed sensing and communication interfaces, making them increasingly vulnerable to cyber physical disturbances that threaten operational continuity and equipment safety. In this work, a complete virtual microgrid was designed and implemented in MATLAB/Simulink, integrating heterogeneous renewable sources and secondary controller layers. A structured cyberattack framework was developed using MGLib to inject adversarial signals directly into the secondary control pathways. Multiple attack classes were emulated, including ramp, sinusoidal, additive, coordinated stealth, and denial of service behaviors. The virtual environment was used to generate labeled datasets under both normal and attack conditions. The datasets trained Light Gradient Boosting Machine (LightGBM) models to perform two functions: detecting the presence of an intrusion (binary) and distinguishing among attack types (multiclass). The multiclass model attained 99.72% accuracy and a 99.62% F1 score, while the binary model attained 94.8% accuracy and a 94.3% F1 score. A knowledge-distillation step reduced the size of the multiclass model, allowing faster predictions with only a small drop in performance. Real-time tests showed a processing delay of about 54 to 67 ms per 1000 samples, demonstrating suitability for CPU-based edge deployment in microgrid controllers. The results confirm that lightweight machine learning based intrusion detection methods can provide fast, accurate, and efficient cyberattack detection without relying on complex deep learning models. Key contributions include: (1) development of a complete MATLAB-based virtual microgrid, (2) structured attack injection at the control layer, (3) creation of multiclass labeled datasets, and (4) design of low-cost AI models suitable for practical microgrid cybersecurity."}
{"id": "2601.04003", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.04003", "abs": "https://arxiv.org/abs/2601.04003", "authors": ["P. Gangl", "M. Winkler"], "title": "Continuation methods for higher-order topology optimization", "comment": null, "summary": "We aim to solve a topology optimization problem where the distribution of material in the design domain is represented by a density function. To obtain candidates for local minima, we want to solve the first order optimality system via Newton's method. This requires the initial guess to be sufficiently close to the a priori unknown solution. Introducing a stepsize rule often allows for less restrictions on the initial guess while still preserving convergence. In topology optimization one typically encounters nonconvex problems where this approach might fail. We therefore opt for a homotopy (continuation) approach which is based on solving a sequence of parametrized problems to approach the solution of the original problem. In the density based framework the values of the design variable are constrained by 0 from below and 1 from above. Coupling the homotopy method with a barrier strategy enforces these constraints to be satisified. The numerical results for a PDE-constrained compliance minimization problem demonstrate that this combined approach maintains feasibility of the density function and converges to a (candidate for a) locally optimal design without a priori knowledge of the solution."}
{"id": "2601.03495", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03495", "abs": "https://arxiv.org/abs/2601.03495", "authors": ["Osasumwen Cedric Ogiesoba-Eguakun", "Suman Rath"], "title": "Cyberattack Detection in Virtualized Microgrids Using LightGBM and Knowledge-Distilled Classifiers", "comment": "12 pages", "summary": "Modern microgrids depend on distributed sensing and communication interfaces, making them increasingly vulnerable to cyber physical disturbances that threaten operational continuity and equipment safety. In this work, a complete virtual microgrid was designed and implemented in MATLAB/Simulink, integrating heterogeneous renewable sources and secondary controller layers. A structured cyberattack framework was developed using MGLib to inject adversarial signals directly into the secondary control pathways. Multiple attack classes were emulated, including ramp, sinusoidal, additive, coordinated stealth, and denial of service behaviors. The virtual environment was used to generate labeled datasets under both normal and attack conditions. The datasets trained Light Gradient Boosting Machine (LightGBM) models to perform two functions: detecting the presence of an intrusion (binary) and distinguishing among attack types (multiclass). The multiclass model attained 99.72% accuracy and a 99.62% F1 score, while the binary model attained 94.8% accuracy and a 94.3% F1 score. A knowledge-distillation step reduced the size of the multiclass model, allowing faster predictions with only a small drop in performance. Real-time tests showed a processing delay of about 54 to 67 ms per 1000 samples, demonstrating suitability for CPU-based edge deployment in microgrid controllers. The results confirm that lightweight machine learning based intrusion detection methods can provide fast, accurate, and efficient cyberattack detection without relying on complex deep learning models. Key contributions include: (1) development of a complete MATLAB-based virtual microgrid, (2) structured attack injection at the control layer, (3) creation of multiclass labeled datasets, and (4) design of low-cost AI models suitable for practical microgrid cybersecurity."}
{"id": "2601.03675", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03675", "abs": "https://arxiv.org/abs/2601.03675", "authors": ["Fangyong Zheng", "Pengfei Li", "Tao Yu"], "title": "Maximum smoothed likelihood method for the combination of multiple diagnostic tests, with application to the ROC estimation", "comment": null, "summary": "In medical diagnostics, leveraging multiple biomarkers can significantly improve classification accuracy compared to using a single biomarker. While existing methods based on exponential tilting or density ratio models have shown promise, their assumptions may be overly restrictive in practice. In this paper, we adopt a flexible semiparametric model that relates the density ratio of diseased to healthy subjects through an unknown monotone transformation of a linear combination of biomarkers. To enhance estimation efficiency, we propose a smoothed likelihood framework that exploits the smoothness in the underlying densities and transformation function. Building on the maximum smoothed likelihood methodology, we construct estimators for the model parameters and the associated probability density functions. We develop an effective computational algorithm for implementation, derive asymptotic properties of the proposed estimators, and establish procedures for estimating the receiver operating characteristic (ROC) curve and the area under the curve (AUC). Through simulation studies and a real-data application, we demonstrate that the proposed method yields more accurate and efficient estimates than existing approaches."}
{"id": "2601.04119", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.04119", "abs": "https://arxiv.org/abs/2601.04119", "authors": ["Martin Ehler", "Karlheinz Gröchenig"], "title": "Quantitative Constraints for Stable Sampling on the Sphere", "comment": null, "summary": "We derive quantitative volume constraints for sampling measures $μ_t$ on the unit sphere $\\mathbb{S}^d$ that satisfy Marcinkiewicz-Zygmund inequalities of order $t$. Using precise localization estimates for Jacobi polynomials, we obtain explicit upper and lower bounds on the $μ_t$-mass of geodesic balls at the natural scale $t^{-1}$. Whereas constants are typically left implicit in the literature, we place special emphasis on fully explicit constants, and the results are genuinely quantitative. Moreover, these bounds yield quantitative constraints for the $s$-dimensional Hausdorff volume of Marcinkiewicz-Zygmund sampling sets and, in particular, optimal lower bounds for the length of Marcinkiewicz-Zygmund curves."}
{"id": "2601.03494", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2601.03494", "abs": "https://arxiv.org/abs/2601.03494", "authors": ["Kaiyuan Cao", "Haodong Wang", "Xiang-Ping Jiang", "Shu chen", "Jian Wang"], "title": "Tailoring Dynamical Quantum Phase Transitions via Double-Mode Squeezing Manipulation", "comment": "7 pages, 4 figures", "summary": "We propose a protocol to tailor dynamical quantum phase transitions (DQPTs) by double-mode squeezing onto the initial state in the XY chain. The effect of squeezing depends critically on the system's symmetry and parameters. When the squeezing operator breaks particle-hole symmetry (PHS), DQPTs become highly tunable, allowing one to either induce transitions within a single phase or suppress them. Remarkably, when PHS is preserved and the squeezing strength reaches $r=π/4$, a universal class of DQPTs emerges, independent of the quench path. This universality is characterized by two key features: (i) the collapse of all Fisher zeros onto the real-time axis, and (ii) the saturation of intermode entanglement to its maximum in each $(k,-k)$ modes. Moreover, the critical momenta governing the DQPTs coincide exactly with the modes attaining the maximal entanglement. At this universal point, the dynamical phase vanishes, leading to a purely geometric evolution marked by $π$-jumps in the Pancharatnam geometric phase. Our work establishes initial-state squeezing as a versatile tool for tailoring far-from-equilibrium criticality and reveals a direct link between entanglement saturation and universal nonanalytic dynamics."}
{"id": "2601.03536", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03536", "abs": "https://arxiv.org/abs/2601.03536", "authors": ["Apoorva Khairnar", "Yogesh Phalak", "Jun Wang", "Ziyang Zhou", "Benjamin Jantzen", "Suyi Li", "Noel Naughton"], "title": "Spider web-inspired sensing and computation with fiber network physical reservoirs", "comment": "13 pages, 6 figures", "summary": "Physical reservoir computing leverages the intrinsic dynamics of mechanical systems to perform computation through their natural responses to input signals. Here, we study a compliant fiber network inspired by orb-weaving spider webs and investigate how its mechanical design and operating conditions shape its computational capability. Using Cosserat rod-based simulations, we identify how network topology, geometry, actuation, and axial tension impact the nonlinear computation and memory capacity of the network. We further evaluate several readout reduction strategies to assess how computational performance varies with the number and placement of measured outputs. We then experimentally validate these results using a physical fiber-network prototype. Overall, results provide insights and guidance on design, actuation, and sensing choices to enable fiber networks for mechano-intelligent computation. They demonstrate the ability of structured compliant fibers networks to serve as physical reservoirs capable of nonlinear transformation and input-history retention."}
{"id": "2601.04120", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04120", "abs": "https://arxiv.org/abs/2601.04120", "authors": ["Yongcun Song", "Shangzhi Zeng", "Jin Zhang", "Lvgang Zhang"], "title": "A Single-Loop Bilevel Deep Learning Method for Optimal Control of Obstacle Problems", "comment": null, "summary": "Optimal control of obstacle problems arises in a wide range of applications and is computationally challenging due to its nonsmoothness, nonlinearity, and bilevel structure. Classical numerical approaches rely on mesh-based discretization and typically require solving a sequence of costly subproblems. In this work, we propose a single-loop bilevel deep learning method, which is mesh-free, scalable to high-dimensional and complex domains, and avoids repeated solution of discretized subproblems. The method employs constraint-embedding neural networks to approximate the state and control and preserves the bilevel structure. To train the neural networks efficiently, we propose a Single-Loop Stochastic First-Order Bilevel Algorithm (S2-FOBA), which eliminates nested optimization and does not rely on restrictive lower-level uniqueness assumptions. We analyze the convergence behavior of S2-FOBA under mild assumptions. Numerical experiments on benchmark examples, including distributed and obstacle control problems with regular and irregular obstacles on complex domains, demonstrate that the proposed method achieves satisfactory accuracy while reducing computational cost compared to classical numerical methods."}
{"id": "2601.03536", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03536", "abs": "https://arxiv.org/abs/2601.03536", "authors": ["Apoorva Khairnar", "Yogesh Phalak", "Jun Wang", "Ziyang Zhou", "Benjamin Jantzen", "Suyi Li", "Noel Naughton"], "title": "Spider web-inspired sensing and computation with fiber network physical reservoirs", "comment": "13 pages, 6 figures", "summary": "Physical reservoir computing leverages the intrinsic dynamics of mechanical systems to perform computation through their natural responses to input signals. Here, we study a compliant fiber network inspired by orb-weaving spider webs and investigate how its mechanical design and operating conditions shape its computational capability. Using Cosserat rod-based simulations, we identify how network topology, geometry, actuation, and axial tension impact the nonlinear computation and memory capacity of the network. We further evaluate several readout reduction strategies to assess how computational performance varies with the number and placement of measured outputs. We then experimentally validate these results using a physical fiber-network prototype. Overall, results provide insights and guidance on design, actuation, and sensing choices to enable fiber networks for mechano-intelligent computation. They demonstrate the ability of structured compliant fibers networks to serve as physical reservoirs capable of nonlinear transformation and input-history retention."}
{"id": "2601.04132", "categories": ["math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.04132", "abs": "https://arxiv.org/abs/2601.04132", "authors": ["Matieyendou Lamboni", "Sergei Kucherenko"], "title": "Active subspace methods and derivative-based Shapley effects for functions with non-independent variables", "comment": null, "summary": "Lower-dimensional subspaces that impact estimates of uncertainty are often described by Linear combinations of input variables, leading to active variables. This paper extends the derivative-based active subspace methods and derivative-based Shapley effects to cope with functions with non-independent variables, and it introduces sensitivity-based active subspaces. While derivative-based subspace methods focus on directions along which the function exhibits significant variation, sensitivity-based subspace methods seek a reduced set of active variables that enables a reduction in the function's variance. We propose both theoretical results using the recent development of gradients of functions with non-independent variables and practical settings by making use of optimal computations of gradients, which admit dimension-free upper-bounds of the biases and the parametric rate of convergence. Simulations show that the relative performance of derivative-based and sensitivity-based active subspaces methods varies across different functions."}
{"id": "2601.03567", "categories": ["quant-ph", "gr-qc", "hep-th", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2601.03567", "abs": "https://arxiv.org/abs/2601.03567", "authors": ["Indrajit Sen", "Matthew Leifer"], "title": "Local Scale Invariance in Quantum Theory: A Non-Hermitian Pilot-Wave Formulation", "comment": "20 pages, 3 figures", "summary": "We show that Weyl's abandoned idea of local scale invariance has a natural realization at the quantum level in pilot-wave (deBroglie-Bohm) theory. We obtain the Weyl covariant derivative by complexifying the electromagnetic gauge coupling parameter. The resultant non-hermiticity has a natural interpretation in terms of local scale invariance of the quantum state in pilot-wave theory. The conserved current density is modified from $|ψ|^2$ to the local scale invariant, trajectory-dependent ratio $|ψ|^2/ \\mathbf{1}^2[\\mathcal{C}]$, where $\\mathbf 1[\\mathcal C]$ is a scale factor that depends on the pilot-wave trajectory $\\mathcal C$ in configuration space. Our approach is general, and we implement it for the Schrödinger, Pauli, and Dirac equations coupled to an external electromagnetic field. We also implement it in quantum field theory for the case of a quantized axion field interacting with a quantized electromagnetic field. We discuss the equilibrium probability density and show that the corresponding trajectories are unique."}
{"id": "2601.03638", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03638", "abs": "https://arxiv.org/abs/2601.03638", "authors": ["Jaeyeon Park", "Jiyu Lee", "Junyeol Maeng", "Shenghui Cui"], "title": "DSP-Based Sub-Switching-Period Current-Limiting Control for Grid-Tied Inverter under Grid Faults", "comment": "8 pages", "summary": "This paper presents a sub-switching period current-limiting control for a grid-tied inverter to prevent transient overcurrents during grid faults and enable seamless fault ride-through (FRT). Sudden grid-voltage disturbances, such as voltage sags or phase jumps, can induce large transient currents within a switching period, particularly at low switching frequencies. Upon disturbance detection, the proposed method immediately modifies the pulse-width modulation carrier, enabling continuous regulation of the inverter output current within a time much shorter than a switching period without interrupting current flow. The proposed method can be implemented on commonly used digital signal processors without requiring specialized analog or digital circuits or high-speed computing devices. Experimental results from a 2-level, 3-phase inverter switching at 3.6 kHz validate the effectiveness of the proposed method under symmetric and asymmetric voltage sags and phase jumps."}
{"id": "2601.03445", "categories": ["eess.SY", "cs.FL", "math.LO", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.03445", "abs": "https://arxiv.org/abs/2601.03445", "authors": ["Negar Monir", "Sadegh Soudjani"], "title": "Policy Synthesis for Interval MDPs via Polyhedral Lyapunov Functions", "comment": null, "summary": "Decision-making under uncertainty is central to many safety-critical applications, where decisions must be guided by probabilistic modeling formalisms. This paper introduces a novel approach to policy synthesis in multi-objective interval Markov decision processes using polyhedral Lyapunov functions. Unlike previous Lyapunov-based methods that mainly rely on quadratic functions, our method utilizes polyhedral functions to enhance accuracy in managing uncertainties within value iteration of dynamic programming. We reformulate the value iteration algorithm as a switched affine system with interval uncertainties and apply control-theoretic stability principles to synthesize policies that guide the system toward a desired target set. By constructing an invariant set of attraction, we ensure that the synthesized policies provide convergence guarantees while minimizing the impact of transition uncertainty in the underlying model. Our methodology removes the need for computationally intensive Pareto curve computations by directly determining a policy that brings objectives within a specified range of their target values. We validate our approach through numerical case studies, including a recycling robot and an electric vehicle battery, demonstrating its effectiveness in achieving policy synthesis under uncertainty."}
{"id": "2601.03638", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03638", "abs": "https://arxiv.org/abs/2601.03638", "authors": ["Jaeyeon Park", "Jiyu Lee", "Junyeol Maeng", "Shenghui Cui"], "title": "DSP-Based Sub-Switching-Period Current-Limiting Control for Grid-Tied Inverter under Grid Faults", "comment": "8 pages", "summary": "This paper presents a sub-switching period current-limiting control for a grid-tied inverter to prevent transient overcurrents during grid faults and enable seamless fault ride-through (FRT). Sudden grid-voltage disturbances, such as voltage sags or phase jumps, can induce large transient currents within a switching period, particularly at low switching frequencies. Upon disturbance detection, the proposed method immediately modifies the pulse-width modulation carrier, enabling continuous regulation of the inverter output current within a time much shorter than a switching period without interrupting current flow. The proposed method can be implemented on commonly used digital signal processors without requiring specialized analog or digital circuits or high-speed computing devices. Experimental results from a 2-level, 3-phase inverter switching at 3.6 kHz validate the effectiveness of the proposed method under symmetric and asymmetric voltage sags and phase jumps."}
{"id": "2601.03777", "categories": ["stat.ME", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03777", "abs": "https://arxiv.org/abs/2601.03777", "authors": ["Md Nafees Fuad Rafi", "Zhaomiao Guo"], "title": "Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems", "comment": null, "summary": "While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks."}
{"id": "2601.04148", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.04148", "abs": "https://arxiv.org/abs/2601.04148", "authors": ["Dhivya Prabhu K", "Sanjeev Singh", "Antony Vijesh"], "title": "Efficient third-order iterative algorithms for computing zeros of special functions", "comment": null, "summary": "This manuscript presents a novel and reliable third-order iterative procedure for computing the zeros of solutions to second-order ordinary differential equations. By approximating the solution of the related Riccati differential equation using the trapezoidal rule, this study has derived the proposed third-order method. This work establishes sufficient conditions to ensure the theoretical non-local convergence of the proposed method. This study provides suitable initial guesses for the proposed third-order iterative procedure to compute all zeros in a given interval of the solutions to second-order ordinary differential equations. The orthogonal polynomials like Legendre and Hermite, as well as the special functions like Bessel, Coulomb wave, confluent hypergeometric, and cylinder functions, satisfy the proposed conditions for convergence. Numerical simulations demonstrate the effectiveness of the proposed theory. This work also presents a comparative analysis with recent studies."}
{"id": "2601.03616", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03616", "abs": "https://arxiv.org/abs/2601.03616", "authors": ["Shi Jin", "Chuwen Ma", "Enrique Zuazua"], "title": "Transmutation based Quantum Simulation for Non-unitary Dynamics", "comment": null, "summary": "We present a quantum algorithm for simulating dissipative diffusion dynamics generated by positive semidefinite operators of the form $A=L^\\dagger L$, a structure that arises naturally in standard discretizations of elliptic operators. Our main tool is the Kannai transform, which represents the diffusion semigroup $e^{-TA}$ as a Gaussian-weighted superposition of unitary wave propagators. This representation leads to a linear-combination-of-unitaries implementation with a Gaussian tail and yields query complexity $\\tilde{\\mathcal{O}}(\\sqrt{\\|A\\| T \\log(1/\\varepsilon)})$, up to standard dependence on state-preparation and output norms, improving the scaling in $\\|A\\|, T$ and $\\varepsilon$ compared with generic Hamiltonian-simulation-based methods. We instantiate the method for the heat equation and biharmonic diffusion under non-periodic physical boundary conditions, and we further use it as a subroutine for constant-coefficient linear parabolic surrogates arising in entropy-penalization schemes for viscous Hamilton--Jacobi equations. In the long-time regime, the same framework yields a structured quantum linear solver for $A\\mathbf{x}=\\mathbf{b}$ with $A=L^\\dagger L$, achieving $\\tilde{\\mathcal{O}}(κ^{3/2}\\log^2(1/\\varepsilon))$ queries and improving the condition-number dependence over standard quantum linear-system algorithms in this factorized setting."}
{"id": "2601.03679", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03679", "abs": "https://arxiv.org/abs/2601.03679", "authors": ["Simon Halvdansson", "Lucas Ferreira Bernardino", "Brage Rugstad Knudsen"], "title": "Accounting for Optimal Control in the Sizing of Isolated Hybrid Renewable Energy Systems Using Imitation Learning", "comment": "11 pages, 9 figures", "summary": "Decarbonization of isolated or off-grid energy systems through phase-in of large shares of intermittent solar or wind generation requires co-installation of energy storage or continued use of existing fossil dispatchable power sources to balance supply and demand. The effective CO2 emission reduction depends on the relative capacity of the energy storage and renewable sources, the stochasticity of the renewable generation, and the optimal control or dispatch of the isolated energy system. While the operations of the energy storage and dispatchable sources may impact the optimal sizing of the system, it is challenging to account for the effect of finite horizon, optimal control at the stage of system sizing. Here, we present a flexible and computationally efficient sizing framework for energy storage and renewable capacity in isolated energy systems, accounting for uncertainty in the renewable generation and the optimal feedback control. To this end, we implement an imitation learning approach to stochastic neural model predictive control (MPC) which allows us to relate the battery storage and wind peak capacities to the emissions reduction and investment costs while accounting for finite horizon, optimal control. Through this approach, decision makers can evaluate the effective emission reduction and costs of different storage and wind capacities at any price point while accounting for uncertainty in the renewable generation with limited foresight. We evaluate the proposed sizing framework on a case study of an offshore energy system with a gas turbine, a wind farm and a battery energy storage system (BESS). In this case, we find a nonlinear, nontrivial relationship between the investment costs and reduction in gas usage relative to the wind and BESS capacities, emphasizing the complexity and importance of accounting for optimal control in the design of isolated energy systems."}
{"id": "2601.03906", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.03906", "abs": "https://arxiv.org/abs/2601.03906", "authors": ["Jad Wehbeh", "Eric C. Kerrigan"], "title": "Exact Continuous Reformulations of Logic Constraints in Nonlinear Optimization and Optimal Control Problems", "comment": "8 pages, 11 figures, submitted for publication to Automatica", "summary": "Many nonlinear optimal control and optimization problems involve constraints that combine continuous dynamics with discrete logic conditions. Standard approaches typically rely on mixed-integer programming, which introduces scalability challenges and requires specialized solvers. This paper presents an exact reformulation of broad classes of logical constraints as binary-variable-free expressions whose differentiability properties coincide with those of the underlying predicates, enabling their direct integration into nonlinear programming models. Our approach rewrites arbitrary logical propositions into conjunctive normal form, converts them into equivalent max--min constraints, and applies a smoothing procedure that preserves the exact feasible set. The method is evaluated on two benchmark problems, a quadrotor trajectory optimization with obstacle avoidance and a hybrid two-tank system with temporal logic constraints, and is shown to obtain optimal solutions more consistently and efficiently than existing binary variable elimination techniques."}
{"id": "2601.03679", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03679", "abs": "https://arxiv.org/abs/2601.03679", "authors": ["Simon Halvdansson", "Lucas Ferreira Bernardino", "Brage Rugstad Knudsen"], "title": "Accounting for Optimal Control in the Sizing of Isolated Hybrid Renewable Energy Systems Using Imitation Learning", "comment": "11 pages, 9 figures", "summary": "Decarbonization of isolated or off-grid energy systems through phase-in of large shares of intermittent solar or wind generation requires co-installation of energy storage or continued use of existing fossil dispatchable power sources to balance supply and demand. The effective CO2 emission reduction depends on the relative capacity of the energy storage and renewable sources, the stochasticity of the renewable generation, and the optimal control or dispatch of the isolated energy system. While the operations of the energy storage and dispatchable sources may impact the optimal sizing of the system, it is challenging to account for the effect of finite horizon, optimal control at the stage of system sizing. Here, we present a flexible and computationally efficient sizing framework for energy storage and renewable capacity in isolated energy systems, accounting for uncertainty in the renewable generation and the optimal feedback control. To this end, we implement an imitation learning approach to stochastic neural model predictive control (MPC) which allows us to relate the battery storage and wind peak capacities to the emissions reduction and investment costs while accounting for finite horizon, optimal control. Through this approach, decision makers can evaluate the effective emission reduction and costs of different storage and wind capacities at any price point while accounting for uncertainty in the renewable generation with limited foresight. We evaluate the proposed sizing framework on a case study of an offshore energy system with a gas turbine, a wind farm and a battery energy storage system (BESS). In this case, we find a nonlinear, nontrivial relationship between the investment costs and reduction in gas usage relative to the wind and BESS capacities, emphasizing the complexity and importance of accounting for optimal control in the design of isolated energy systems."}
{"id": "2601.03815", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03815", "abs": "https://arxiv.org/abs/2601.03815", "authors": ["Shizhe Hong", "Weiming Li", "Guangming Pan"], "title": "High-Dimensional Precision Matrix Quadratic Forms: Estimation Framework for $p > n$", "comment": null, "summary": "We propose a novel estimation framework for quadratic functionals of precision matrices in high-dimensional settings, particularly in regimes where the feature dimension $p$ exceeds the sample size $n$. Traditional moment-based estimators with bias correction remain consistent when $p<n$ (i.e., $p/n \\to c <1$). However, they break down entirely once $p>n$, highlighting a fundamental distinction between the two regimes due to rank deficiency and high-dimensional complexity. Our approach resolves these issues by combining a spectral-moment representation with constrained optimization, resulting in consistent estimation under mild moment conditions.\n  The proposed framework provides a unified approach for inference on a broad class of high-dimensional statistical measures. We illustrate its utility through two representative examples: the optimal Sharpe ratio in portfolio optimization and the multiple correlation coefficient in regression analysis. Simulation studies demonstrate that the proposed estimator effectively overcomes the fundamental $p>n$ barrier where conventional methods fail."}
{"id": "2601.04167", "categories": ["math.NA", "gr-qc"], "pdf": "https://arxiv.org/pdf/2601.04167", "abs": "https://arxiv.org/abs/2601.04167", "authors": ["Anıl Zenginoğlu"], "title": "From Penrose to Melrose: Computing Scattering Amplitudes at Infinity for Unbounded Media", "comment": "36 pages, 11 figures", "summary": "We develop a method to compute scattering amplitudes for the Helmholtz equation in variable, unbounded media with possibly long-range asymptotics. Combining Penrose's conformal compactification and Melrose's geometric scattering theory, we formulate the time-harmonic scattering problem on a compactified manifold with boundary and construct a two-step solver for scattering amplitudes at infinity. The construction is asymptotic: it treats a neighborhood of infinity, and is meant to couple to interior solvers via domain decomposition. The method provides far-field data without relying on explicit solutions or Green's function representation. Scattering in variable media is treated in a unified framework where both the incident and scattered fields solve the same background Helmholtz operator. Numerical experiments for constant, short-range, and long-range media with single-mode and Gaussian beam incidence demonstrate spectral convergence of the computed scattering amplitudes in all cases."}
{"id": "2601.03623", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.03623", "abs": "https://arxiv.org/abs/2601.03623", "authors": ["Mohammad Rowshan"], "title": "Strip-Symmetric Quantum Codes for Biased Noise: Z-Decoupling in Stabilizer and Floquet Codes", "comment": null, "summary": "Bias-tailored codes such as the XZZX surface code and the domain wall color code achieve high dephasing-biased thresholds because, in the infinite-bias limit, their $Z$ syndromes decouple into one-dimensional repetition-like chains; the $X^3Z^3$ Floquet code shows an analogous strip-wise structure for detector events in spacetime. We capture this common mechanism by defining strip-symmetric biased codes, a class of static stabilizer and dynamical (Floquet) codes for which, under pure dephasing and perfect measurements, each elementary $Z$ fault is confined to a strip and the Z-detector--fault incidence matrix is block diagonal. For such codes the Z-detector hypergraph decomposes into independent strip components and maximum-likelihood $Z$ decoding factorizes across strips, yielding complexity savings for matching-based decoders. We characterize strip symmetry via per-strip stabilizer products, viewed as a $\\mathbb{Z}_2$ 1-form symmetry, place XZZX, the domain wall color code, and $X^3Z^3$ in this framework, and introduce synthetic strip-symmetric detector models and domain-wise Clifford constructions that serve as design tools for new bias-tailored Floquet codes."}
{"id": "2601.03716", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03716", "abs": "https://arxiv.org/abs/2601.03716", "authors": ["Lauritz Zendel", "Chiara Springer", "Frank Dammel", "Peter Stephan"], "title": "Derivation of the Thermal Conductivity in a Latent Thermal Energy Storage Unit for Use in Simplified System Models", "comment": null, "summary": "Latent Thermal Energy Storages (LTES) can store thermal energy in a narrow temperature range. Therefore, they are favorable for integration into Rankine-based Carnot Batteries. For the design of such systems, simulations based on accurate models are desirable. However, physical phenomena such as natural convection in LTES units cannot be modeled directly in transient system models. Simplified models are required. Therefore, the objective of this work is to derive simplified LTES unit models for use in system models. In transient simulations the state of charge of the LTES influences its temperature profile. The temperature profile depends on the geometry of the LTES unit. Therefore, the geometry must be considered to model the transient behavior of an LTES unit. The LTES unit under investigation has a shell and tube heat exchanger structure. The phase change material (PCM) is located between the hexagonal fins and in the space between the finned tubes. Aluminum fins are used. They have a high thermal conductivity and thus compensate for the low thermal conductivity of the sodium nitrate used as PCM. The interaction between fins and PCM is complex. Therefore, a numerical approach can be used to gain insight into the behavior of the LTES unit. To transfer the results of a complex model to a simplified model where fins and PCM are not considered individually, the effective thermal conductivity of a single finned tube can be used to approximate the performance of the LTES unit. In this study, a model of a section with a single finned tube is developed using the COMSOL software. The effective thermal conductivity of the system is determined by varying the effective thermal conductivity in a simplified model and comparing the results with reference cases based on a complex modeling approach. The results can serve as model input for simplified system models of Carnot Batteries, among others."}
{"id": "2601.03931", "categories": ["math.NA", "math.OC", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.03931", "abs": "https://arxiv.org/abs/2601.03931", "authors": ["Yukuan Hu", "Laura Grazioli"], "title": "Constrained dynamics for searching saddle points on general Riemannian manifolds", "comment": "35 pages, 6 figures, 2 tables. All comments are welcome", "summary": "Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory."}
{"id": "2601.03716", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03716", "abs": "https://arxiv.org/abs/2601.03716", "authors": ["Lauritz Zendel", "Chiara Springer", "Frank Dammel", "Peter Stephan"], "title": "Derivation of the Thermal Conductivity in a Latent Thermal Energy Storage Unit for Use in Simplified System Models", "comment": null, "summary": "Latent Thermal Energy Storages (LTES) can store thermal energy in a narrow temperature range. Therefore, they are favorable for integration into Rankine-based Carnot Batteries. For the design of such systems, simulations based on accurate models are desirable. However, physical phenomena such as natural convection in LTES units cannot be modeled directly in transient system models. Simplified models are required. Therefore, the objective of this work is to derive simplified LTES unit models for use in system models. In transient simulations the state of charge of the LTES influences its temperature profile. The temperature profile depends on the geometry of the LTES unit. Therefore, the geometry must be considered to model the transient behavior of an LTES unit. The LTES unit under investigation has a shell and tube heat exchanger structure. The phase change material (PCM) is located between the hexagonal fins and in the space between the finned tubes. Aluminum fins are used. They have a high thermal conductivity and thus compensate for the low thermal conductivity of the sodium nitrate used as PCM. The interaction between fins and PCM is complex. Therefore, a numerical approach can be used to gain insight into the behavior of the LTES unit. To transfer the results of a complex model to a simplified model where fins and PCM are not considered individually, the effective thermal conductivity of a single finned tube can be used to approximate the performance of the LTES unit. In this study, a model of a section with a single finned tube is developed using the COMSOL software. The effective thermal conductivity of the system is determined by varying the effective thermal conductivity in a simplified model and comparing the results with reference cases based on a complex modeling approach. The results can serve as model input for simplified system models of Carnot Batteries, among others."}
{"id": "2601.03909", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03909", "abs": "https://arxiv.org/abs/2601.03909", "authors": ["Clara Bertinelli Salucci"], "title": "Asymptotic distribution of the likelihood ratio test statistic with inequality-constrained nuisance parameters", "comment": "23 pages, 8 figures", "summary": "The asymptotic distribution of the likelihood-ratio statistic for testing parameters on the boundary is well known to be a chi-squared mixture. The mixture weights have been shown to correspond to the intrinsic volumes of an associated tangent cone, unifying a wide range of previously isolated special cases. While the weights are fully understood for an arbitrary number of parameters of interest on the boundary, much less is known when nuisance parameters are also constrained to the boundary, a situation that frequently arises in applications. We provide the first general characterization of the asymptotic distribution of the likelihood-ratio test statistic when both the number of parameters of interest and the number of nuisance parameters on the boundary are arbitrary. We analyze how the cone geometry changes when moving from a problem with K parameters of interest on the boundary to one with K-m parameters of interest and m nuisances. In the orthogonal case we show that the resulting change in the chi-bar weights admits a closed-form difference pattern that redistributes probability mass across adjacent degrees of freedom, and that this pattern remains the dominant component of the weight shift under arbitrary covariance structures when the nuisance vector is one-dimensional. For a generic number of nuisance parameters, we introduce a new rank-based aggregation of intrinsic volumes that yields an accurate approximation of the mixture weights. Comprehensive simulations support the theory and demonstrate the accuracy of the proposed approximation."}
{"id": "2601.03856", "categories": ["physics.geo-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.03856", "abs": "https://arxiv.org/abs/2601.03856", "authors": ["Alejandro González", "Cléa Denamiel", "Jorge Macías"], "title": "Assessing Meteo-HySEA Performance for Adriatic Meteotsunami Events", "comment": "39 pages, 19 figures", "summary": "Meteotsunamis are atmospherically driven sea-level oscillations that can trigger hazardous coastal flooding, particularly in resonant bays. This study assesses the GPU-based Meteo-HySEA model for meteotsunami simulation in the Adriatic Sea, benchmarking its performance against the CPU-based AdriSC-ADCIRC system. Three documented events (2014, 2017, 2020) were simulated using WRF downscaling of ERA reanalyses and validated with tide-gauge and microbarograph observations. Both models are limited by the underestimation of mesoscale pressure disturbances in the atmospheric forcing. Meteo-HySEA generally reproduces the timing and spatial variability of sea-level oscillations and often yields larger amplitudes than ADCIRC, but it tends to overestimate dominant wave periods, particularly in enclosed basins. Differences in oscillation persistence underscore the need for further validation against high-resolution tide-gauge data to assess whether Meteo-HySEA captures harbor seiches more realistically or ADCIRC better represents physical energy dissipation. Crucially, GPU acceleration provides order-of-magnitude gains in computational efficiency, enabling rapid high-resolution, multi-grid simulations including inundation, and thus offering strong potential for operational early warning."}
{"id": "2601.03651", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.03651", "abs": "https://arxiv.org/abs/2601.03651", "authors": ["Zhouhao Guo", "Jiaju Zhang"], "title": "Double interval entanglement in quasiparticle excited states", "comment": "14 pages, 6 figures", "summary": "We investigate double-interval entanglement measures, specifically reflected entropy, mutual information, and logarithmic negativity, in quasiparticle excited states for classical, bosonic, and fermionic systems. We develop an algorithm that efficiently calculates these measures from density matrices expressed in a non-orthonormal basis, enabling straightforward numerical implementation. We find a universal additivity property that emerges at large momentum differences, where the entanglement measures for states with distinct quasiparticle sets equal the sum of their individual contributions. The classical limit arises as a special case of this additivity, with both bosonic and fermionic results converging to classical behavior when all momentum differences are large."}
{"id": "2601.03767", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03767", "abs": "https://arxiv.org/abs/2601.03767", "authors": ["Shibo Han", "Bonan Hou", "Chong Jin Ong"], "title": "Output Consensus on Periodic References for Constrained Multi-agent Systems Under a Switching Network", "comment": null, "summary": "This work addresses the output consensus problem of constrained heterogeneous multi-agent systems under a switching network with potential communication delay, where outputs are periodic and characterized by a linear exosystem. Since periodic references have more complex dynamics, it is more challenging to track periodic references and achieve consensus on them. In this paper, a model predictive control method incorporating an artificial reference and a modified cost is proposed to track periodic references, which maintains recursive feasibility even when reference switches. Moreover, consensus protocols are proposed to achieve consensus on periodic references in different scenarios, in which global information such as the set of globally admissible references and the global time index are not involved. Theoretical analysis proves that constrained output consensus is asymptotically achieved with the proposed algorithm as the references of each agent converge and agents track their references while maintaining constraint satisfaction. Finally, numerical examples are provided to verify the effectiveness of the proposed algorithm."}
{"id": "2601.03967", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.03967", "abs": "https://arxiv.org/abs/2601.03967", "authors": ["M. H. Gfrerer", "P. Gangl"], "title": "On the importance of smoothness, interface resolution and numerical sensitivities in shape and topological sensitivity analysis", "comment": null, "summary": "In this paper we investigate the influence of the discretization of PDE constraints on shape and topological derivatives. To this end, we study a tracking-type functional and a two-material Poisson problem in one spatial dimension. We consider the discretization by a standard method and an enriched method. In the standard method we use splines of degree $p$ such that we can control the smoothness of the basis functions easily, but do not take any interface location into consideration. This includes for p=1 the usual hat basis functions. In the enriched method we additionally capture the interface locations in the ansatz space by enrichment functions. For both discretization methods shape and topological sensitivity analysis is performed. It turns out that the regularity of the shape derivative depends on the regularity of the basis functions. Furthermore, for point-wise convergence of the shape derivative the interface has to be considered in the ansatz space. For the topological derivative we show that only the enriched method converges."}
{"id": "2601.03767", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03767", "abs": "https://arxiv.org/abs/2601.03767", "authors": ["Shibo Han", "Bonan Hou", "Chong Jin Ong"], "title": "Output Consensus on Periodic References for Constrained Multi-agent Systems Under a Switching Network", "comment": null, "summary": "This work addresses the output consensus problem of constrained heterogeneous multi-agent systems under a switching network with potential communication delay, where outputs are periodic and characterized by a linear exosystem. Since periodic references have more complex dynamics, it is more challenging to track periodic references and achieve consensus on them. In this paper, a model predictive control method incorporating an artificial reference and a modified cost is proposed to track periodic references, which maintains recursive feasibility even when reference switches. Moreover, consensus protocols are proposed to achieve consensus on periodic references in different scenarios, in which global information such as the set of globally admissible references and the global time index are not involved. Theoretical analysis proves that constrained output consensus is asymptotically achieved with the proposed algorithm as the references of each agent converge and agents track their references while maintaining constraint satisfaction. Finally, numerical examples are provided to verify the effectiveness of the proposed algorithm."}
{"id": "2601.03957", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03957", "abs": "https://arxiv.org/abs/2601.03957", "authors": ["Paul Guillot", "Antoine Godichon-Baggioni", "Stéphane Robin", "Laure Sansonnet"], "title": "Online robust covariance matrix estimation and outlier detection", "comment": null, "summary": "Robust estimation of the covariance matrix and detection of outliers remain major challenges in statistical data analysis, particularly when the proportion of contaminated observations increases with the size of the dataset. Outliers can severely bias parameter estimates and induce a masking effect, whereby some outliers conceal the presence of other outliers, further complicating their detection. Although many approaches have been proposed for covariance estimation and outlier detection, to our knowledge, none of these methods have been implemented in an online setting. In this paper, we focus on online covariance matrix estimation and outlier detection. Specifically, we propose a new method for simultaneously and online estimating the geometric median and variance, which allows us to calculate the Mahalanobis distance for each incoming data point before deciding whether it should be considered an outlier. To mitigate the masking effect, robust estimation techniques for the mean and variance are required. Our approach uses the geometric median for robust estimation of the location and the median covariance matrix for robust estimation of the dispersion parameters. The new online methods proposed for parameter estimation and outlier detection allow real-time identification of outliers as data are observed sequentially. The performance of our methods is demonstrated on simulated datasets."}
{"id": "2601.04049", "categories": ["q-fin.CP", "math.NA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04049", "abs": "https://arxiv.org/abs/2601.04049", "authors": ["Julien Hok", "Álvaro Leitao"], "title": "Quantum computing for multidimensional option pricing: End-to-end pipeline", "comment": null, "summary": "This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives."}
{"id": "2601.03734", "categories": ["quant-ph", "cs.CC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.03734", "abs": "https://arxiv.org/abs/2601.03734", "authors": ["Yupan Liu"], "title": "Computational hardness of estimating quantum entropies via binary entropy bounds", "comment": "39 pages, 3 tables. To appear in STACS 2026", "summary": "We investigate the computational hardness of estimating the quantum $α$-Rényi entropy ${\\rm S}^{\\tt R}_α(ρ) = \\frac{\\ln {\\rm Tr}(ρ^α)}{1-α}$ and the quantum $q$-Tsallis entropy ${\\rm S}^{\\tt T}_q(ρ) = \\frac{1-{\\rm Tr}(ρ^q)}{q-1}$, both converging to the von Neumann entropy as the order approaches $1$. The promise problems Quantum $α$-Rényi Entropy Approximation (RényiQEA$_α$) and Quantum $q$-Tsallis Entropy Approximation (TsallisQEA$_q$) ask whether $ {\\rm S}^ {\\tt R}_α(ρ)$ or ${\\rm S}^{\\tt T}_q(ρ)$, respectively, is at least $τ_{\\tt Y}$ or at most $τ_{\\tt N}$, where $τ_{\\tt Y} - τ_{\\tt N}$ is typically a positive constant. Previous hardness results cover only the von Neumann entropy (order $1$) and some cases of the quantum $q$-Tsallis entropy, while existing approaches do not readily extend to other orders.\n  We establish that for all positive real orders, the rank-$2$ variants Rank2RényiQEA$_α$ and Rank2TsallisQEA$_q$ are ${\\sf BQP}$-hard. Combined with prior (rank-dependent) quantum query algorithms in Wang, Guan, Liu, Zhang, and Ying (TIT 2024), Wang, Zhang, and Li (TIT 2024), and Liu and Wang (SODA 2025), our results imply:\n  - For all real orders $α> 0$ and $0 < q \\leq 1$, LowRankRényiQEA$_α$ and LowRankTsallisQEA$_q$ are ${\\sf BQP}$-complete, where both are restricted versions of RényiQEA$_α$ and TsallisQEA$_q$ with $ρ$ of polynomial rank.\n  - For all real order $q>1$, TsallisQEA$_q$ is ${\\sf BQP}$-complete.\n  Our hardness results stem from reductions based on new inequalities relating the $α$-Rényi or $q$-Tsallis binary entropies of different orders, where the reductions differ substantially from previous approaches, and the inequalities are also of independent interest."}
{"id": "2601.03819", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03819", "abs": "https://arxiv.org/abs/2601.03819", "authors": ["Woraphrut Kornmaneesang", "Tsu-Chin Tsao", "Niloufar Esfandi", "Shyh-Leh Chen"], "title": "Unified and Efficient Analysis of Machining Chatter and Surface Location Error", "comment": "13 pages, 9 figures, and 1 table", "summary": "Although machining chatter can be suppressed by the choice of stable cutting parameters through means of stability lobe diagram (SLD), surface roughness still remains due to the forced vibration, which limits surface quality, especially in the surface finish. Better cutting parameters can be achieved considering surface location error (SLE) together with SLD. This paper proposes an innovative modeling framework of the machining dynamic system that enables efficient computation of the chatter stability and SLE. The framework mainly embodies two techniques, namely semi-discretization method (SDM) and lifting method. The machining dynamics system is mathematically expressed as an angle-varying delay differential equation (DDE). The SDM approximates the angle-varying and delayed terms to ordinary terms using zero-phase interpolations and governs the discrete angle-varying dynamics system. Then, the system is merged over the tooth passing angle using the lifted approach to establish an explicit dynamic system in the compact state-space form. Based on the compact state-space model, the chatter stability and SLE prediction are easily and efficiently conducted. Simulation results show the improved efficiency of the proposed method over other well-known methods."}
{"id": "2601.03819", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03819", "abs": "https://arxiv.org/abs/2601.03819", "authors": ["Woraphrut Kornmaneesang", "Tsu-Chin Tsao", "Niloufar Esfandi", "Shyh-Leh Chen"], "title": "Unified and Efficient Analysis of Machining Chatter and Surface Location Error", "comment": "13 pages, 9 figures, and 1 table", "summary": "Although machining chatter can be suppressed by the choice of stable cutting parameters through means of stability lobe diagram (SLD), surface roughness still remains due to the forced vibration, which limits surface quality, especially in the surface finish. Better cutting parameters can be achieved considering surface location error (SLE) together with SLD. This paper proposes an innovative modeling framework of the machining dynamic system that enables efficient computation of the chatter stability and SLE. The framework mainly embodies two techniques, namely semi-discretization method (SDM) and lifting method. The machining dynamics system is mathematically expressed as an angle-varying delay differential equation (DDE). The SDM approximates the angle-varying and delayed terms to ordinary terms using zero-phase interpolations and governs the discrete angle-varying dynamics system. Then, the system is merged over the tooth passing angle using the lifted approach to establish an explicit dynamic system in the compact state-space form. Based on the compact state-space model, the chatter stability and SLE prediction are easily and efficiently conducted. Simulation results show the improved efficiency of the proposed method over other well-known methods."}
{"id": "2601.04066", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.04066", "abs": "https://arxiv.org/abs/2601.04066", "authors": ["Tomeu López-Nieto-Veitch", "Rossella De Sabbata", "Ryung Kim", "Sven Ove Samuelsen", "Nathalie C. Støer", "Vivian Viallon"], "title": "On the estimation of inclusion probabilities for weighted analyses of nested case control studies", "comment": null, "summary": "Nested case-control (NCC) studies are a widely adopted design in epidemiology to investigate exposure-disease relationships. This paper examines weighted analyses in NCC studies, focusing on two prominent weighting methods: Kaplan-Meier (KM) weights and Generalized Additive Model (GAM) weights. We consider three target estimands: log-hazard ratios, conditional survival, and associations between exposures. While KM- and GAM-weights are generally robust, we identify specific scenarios where they can lead to biased estimates. We demonstrate that KM-weights can lead to biased estimates when a proportion of the originating cohort is effectively ineligible for NCC selection, particularly with small case proportions or numerous matching factors. Instead, GAM-weights can yield biased results if interactions between matching factors influence disease risk and are not adequately incorporated into weight calculation. Using Directed Acyclic Graphs (DAGs), we develop a framework to systematically determine which variables should be included in weight calculations. We show that the optimal set of variables depends on the target estimand and the causal relationships between matching factors, exposures, and disease risk. We illustrate our findings with both synthetic and real data from the European Prospective Investigation into Cancer and nutrition (EPIC) study. Additionally, we extend the application of GAM-weights to \"untypical\" NCC studies, where only a subset of cases are included. Our work provides crucial insights for conducting accurate and robust weighted analyses in NCC studies."}
{"id": "2601.03817", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03817", "abs": "https://arxiv.org/abs/2601.03817", "authors": ["Nandana T Raveendranath", "Travis J. Baker", "Emanuele Polino", "Marwan Haddara", "Lynden K. Shalm", "Varun B. Verma", "Geoff J. Pryde", "Sergei Slussarenko", "Howard M. Wiseman", "Nora Tischler"], "title": "Detection-loophole-free nonlocality in the simplest scenario", "comment": null, "summary": "Loophole-free quantum nonlocality often demands experiments with high complexity (defined by all parties' settings and outcomes) and multiple efficient detectors. Here, we identify the fundamental efficiency and complexity thresholds for quantum steering using two-qubit entangled states. Remarkably, it requires only one photon detector on the untrusted side, with efficiency $ε> 1/X$, where $X \\geq 2$ is the number of settings on that side. This threshold applies to all pure entangled states, in contrast to analogous Bell-nonlocality tests, which require almost unentangled states to be loss-tolerant. We confirm these predictions in a minimal-complexity ($X = 2$ for the untrusted party and a single three-outcome measurement for the trusted party), detection-loophole-free photonic experiment with $ε= (51.6 \\pm 0.4)\\% $."}
{"id": "2601.03867", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03867", "abs": "https://arxiv.org/abs/2601.03867", "authors": ["Asitha Lakruwan Kulasekera"], "title": "A Systems-Engineered ESP32 DAQ Architecture and FAIR Data Workflow for Small-Scale Wind Turbine Performance Measurement in Tropical Environments", "comment": null, "summary": "Small-scale wind turbine research in resource-constrained academic settings frequently produces unreliable or unpublishable datasets due to ad-hoc instrumentation, inadequate time synchronization, storage failures, and weak data governance. This paper presents a systematic data acquisition (DAQ) methodology and ESP32-based reference implementation design for field characterization of small wind turbines (100~W--5~kW), emphasizing tropical/coastal deployment constraints typical of Low- and Middle-Income Countries (LMIC). We integrate (i)~a student-adapted V-model with requirements traceability, (ii)~hardware selection strategies for high-humidity and salt-spray environments, (iii)~an embedded firmware architecture featuring interrupt-driven rotor speed measurement, state-machine fault handling, and NTP-based time synchronization, (iv)~a local-first hybrid storage design combining SD-card persistence with optional MQTT cloud telemetry, and (v)~a data-management workflow adapting CRISP-DM and FAIR principles with explicit quality dimensions and publication templates. A detailed helical vertical-axis wind turbine (VAWT) design scenario for coastal Sri Lanka illustrates the complete methodology, targeting $>90\\%$ data completeness over six-month campaigns. The methodology is accompanied by open-source firmware, hardware templates, and data-publication workflow artifacts released via GitHub and Zenodo."}
{"id": "2601.03867", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03867", "abs": "https://arxiv.org/abs/2601.03867", "authors": ["Asitha Lakruwan Kulasekera"], "title": "A Systems-Engineered ESP32 DAQ Architecture and FAIR Data Workflow for Small-Scale Wind Turbine Performance Measurement in Tropical Environments", "comment": null, "summary": "Small-scale wind turbine research in resource-constrained academic settings frequently produces unreliable or unpublishable datasets due to ad-hoc instrumentation, inadequate time synchronization, storage failures, and weak data governance. This paper presents a systematic data acquisition (DAQ) methodology and ESP32-based reference implementation design for field characterization of small wind turbines (100~W--5~kW), emphasizing tropical/coastal deployment constraints typical of Low- and Middle-Income Countries (LMIC). We integrate (i)~a student-adapted V-model with requirements traceability, (ii)~hardware selection strategies for high-humidity and salt-spray environments, (iii)~an embedded firmware architecture featuring interrupt-driven rotor speed measurement, state-machine fault handling, and NTP-based time synchronization, (iv)~a local-first hybrid storage design combining SD-card persistence with optional MQTT cloud telemetry, and (v)~a data-management workflow adapting CRISP-DM and FAIR principles with explicit quality dimensions and publication templates. A detailed helical vertical-axis wind turbine (VAWT) design scenario for coastal Sri Lanka illustrates the complete methodology, targeting $>90\\%$ data completeness over six-month campaigns. The methodology is accompanied by open-source firmware, hardware templates, and data-publication workflow artifacts released via GitHub and Zenodo."}
{"id": "2601.04192", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.04192", "abs": "https://arxiv.org/abs/2601.04192", "authors": ["Edoardo Ratti", "Federico L. Perlino", "Stefania Galimberti", "Maria G. Valsecchi"], "title": "Prediction Intervals for Interim Events in Randomized Clinical Trials with Time-to-Event Endpoints", "comment": "35 pages, 18 figures", "summary": "Time-to-event endpoints are central to evaluate treatment efficacy across many disease areas. Many trial protocols include interim analyses within group-sequential designs that control type I error via spending functions or boundary methods. The corresponding operating characteristics depend on the number of looks and the information accrued. Planning interim analyses with time-to-event endpoints is challenging because statistical information depends on the number of observed events. Ensuring adequate follow-up to accrue the required events is therefore critical, making interim prediction of information at scheduled looks and at the final analysis essential. While several methods have been developed to predict the calendar time required to reach a target number of events, to the best of our knowledge there is no established framework that addresses the prediction of the number of events at a future date with corresponding prediction intervals. Starting from an prediction interval approach originally developed in reliability engineering for the number of future component failures, we reformulated and extended it to the context of interim monitoring in clinical trials. This adaptation yields a general framework for event-count prediction intervals in the clinical setting, taking the patient as the unit of analysis and accommodating a range of parametric survival models, patient-level covariates, stagged entry and possible dependence between entry dates and lost to follow-up. Prediction intervals are obtained in a frequentist framework from a bootstrap estimator of the conditional distribution of future events. The performance of the proposed approach is investigated via simulation studies and illustrated by analyzing a real-world phase III trial in childhood acute lymphoblastic leukaemia."}
{"id": "2601.03821", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2601.03821", "abs": "https://arxiv.org/abs/2601.03821", "authors": ["Xiaowei Tong", "Xingze Qiu", "Xiang Zhan", "Quan Lin", "Kunkun Wang", "Franco Nori", "Peng Xue"], "title": "Topological Sensing in the Dynamics of Quantum Walks with Defects", "comment": "13 pages, 9 figures", "summary": "Topological quantum sensing leverages unique topological features to suppress noise and improve the precision of parameter estimation, emerging as a promising tool in both fundamental research and practical application. In this Letter, we propose a sensing protocol that exploits the dynamics of topological quantum walks incorporating localized defects. Unlike conventional schemes that rely on topological protection to suppress disorder and defects, our protocol harnesses the evolution time as a resource to enable precise estimation of the defect parameter. By utilizing topologically nontrivial properties of the quantum walks, the sensing precision can approach the Heisenberg limit. We further demonstrate the performance and robustness of the protocol through Bayesian estimation. Our results show that this approach maintains high precision over a broad range of parameters and exhibits strong robustness against disorder, offering a practical pathway for topologically enhanced quantum metrology."}
{"id": "2601.03893", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03893", "abs": "https://arxiv.org/abs/2601.03893", "authors": ["Markus Walker", "Marcel Reith-Braun", "Tai Hoang", "Gerhard Neumann", "Uwe D. Hanebeck"], "title": "Smooth Sampling-Based Model Predictive Control Using Deterministic Samples", "comment": null, "summary": "Sampling-based model predictive control (MPC) is effective for nonlinear systems but often produces non-smooth control inputs due to random sampling. To address this issue, we extend the model predictive path integral (MPPI) framework with deterministic sampling and improvements from cross-entropy method (CEM)--MPC, such as iterative optimization, proposing deterministic sampling MPPI (dsMPPI). This combination leverages the exponential weighting of MPPI alongside the efficiency of deterministic samples. Experiments demonstrate that dsMPPI achieves smoother trajectories compared to state-of-the-art methods."}
{"id": "2601.03893", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03893", "abs": "https://arxiv.org/abs/2601.03893", "authors": ["Markus Walker", "Marcel Reith-Braun", "Tai Hoang", "Gerhard Neumann", "Uwe D. Hanebeck"], "title": "Smooth Sampling-Based Model Predictive Control Using Deterministic Samples", "comment": null, "summary": "Sampling-based model predictive control (MPC) is effective for nonlinear systems but often produces non-smooth control inputs due to random sampling. To address this issue, we extend the model predictive path integral (MPPI) framework with deterministic sampling and improvements from cross-entropy method (CEM)--MPC, such as iterative optimization, proposing deterministic sampling MPPI (dsMPPI). This combination leverages the exponential weighting of MPPI alongside the efficiency of deterministic samples. Experiments demonstrate that dsMPPI achieves smoother trajectories compared to state-of-the-art methods."}
{"id": "2601.03829", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03829", "abs": "https://arxiv.org/abs/2601.03829", "authors": ["Gabriele Staffieri", "Giovanni Scala", "Cosmo Lupo"], "title": "Finite-size security of QKD: comparison of three proof techniques", "comment": "8 pages, 4 figures", "summary": "We compare three proof techniques for composable finite-size security of quantum key distribution under collective attacks, with emphasis on how the resulting secret-key rates behave at practically relevant block lengths. As a benchmark, we consider the BB84 protocol and evaluate finite-size key-rate estimates obtained from entropic uncertainty relations (EUR), from the asymptotic equipartition property (AEP), and from a direct finite-block analysis based on the conditional min-entropy, which we refer to as the finite-size min-entropy (FME) approach. For BB84 we show that the EUR-based bound provides the most favorable performance across the considered parameter range, while the AEP bound is asymptotically tight but can become overly pessimistic at moderate and small block sizes, where it may fail to certify a positive key. The FME approach remains effective in this small-block regime, yielding nonzero rates in situations where the AEP estimate vanishes, although it is not asymptotically optimal for BB84. These results motivate the use of FME-type analyses for continuous-variable protocols in settings where tight EUR-based bounds are unavailable, notably for coherent-state schemes where current finite-size analyses typically rely on AEP-style corrections."}
{"id": "2601.03906", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.03906", "abs": "https://arxiv.org/abs/2601.03906", "authors": ["Jad Wehbeh", "Eric C. Kerrigan"], "title": "Exact Continuous Reformulations of Logic Constraints in Nonlinear Optimization and Optimal Control Problems", "comment": "8 pages, 11 figures, submitted for publication to Automatica", "summary": "Many nonlinear optimal control and optimization problems involve constraints that combine continuous dynamics with discrete logic conditions. Standard approaches typically rely on mixed-integer programming, which introduces scalability challenges and requires specialized solvers. This paper presents an exact reformulation of broad classes of logical constraints as binary-variable-free expressions whose differentiability properties coincide with those of the underlying predicates, enabling their direct integration into nonlinear programming models. Our approach rewrites arbitrary logical propositions into conjunctive normal form, converts them into equivalent max--min constraints, and applies a smoothing procedure that preserves the exact feasible set. The method is evaluated on two benchmark problems, a quadrotor trajectory optimization with obstacle avoidance and a hybrid two-tank system with temporal logic constraints, and is shown to obtain optimal solutions more consistently and efficiently than existing binary variable elimination techniques."}
{"id": "2601.03906", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.03906", "abs": "https://arxiv.org/abs/2601.03906", "authors": ["Jad Wehbeh", "Eric C. Kerrigan"], "title": "Exact Continuous Reformulations of Logic Constraints in Nonlinear Optimization and Optimal Control Problems", "comment": "8 pages, 11 figures, submitted for publication to Automatica", "summary": "Many nonlinear optimal control and optimization problems involve constraints that combine continuous dynamics with discrete logic conditions. Standard approaches typically rely on mixed-integer programming, which introduces scalability challenges and requires specialized solvers. This paper presents an exact reformulation of broad classes of logical constraints as binary-variable-free expressions whose differentiability properties coincide with those of the underlying predicates, enabling their direct integration into nonlinear programming models. Our approach rewrites arbitrary logical propositions into conjunctive normal form, converts them into equivalent max--min constraints, and applies a smoothing procedure that preserves the exact feasible set. The method is evaluated on two benchmark problems, a quadrotor trajectory optimization with obstacle avoidance and a hybrid two-tank system with temporal logic constraints, and is shown to obtain optimal solutions more consistently and efficiently than existing binary variable elimination techniques."}
{"id": "2601.03832", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03832", "abs": "https://arxiv.org/abs/2601.03832", "authors": ["Mei Ian Sam", "Tzu-Ling Kuo", "Tai-Yue Li"], "title": "Iterative Matrix Product State Simulation for Scalable Grover's Algorithm", "comment": null, "summary": "Grover's algorithm is a cornerstone of quantum search algorithm, offering quadratic speedup for unstructured problems. However, limited qubit counts and noise in today's noisy intermediate-scale quantum (NISQ) devices hinder large-scale hardware validation, making efficient classical simulation essential for algorithm development and hardware assessment. We present an iterative Grover simulation framework based on matrix product states (MPS) to efficiently simulate large-scale Grover's algorithm. Within the NVIDIA CUDA-Q environment, we compare iterative and common (non-iterative) Grover's circuits across statevector and MPS backends. On the MPS backend at 29 qubits, the iterative Grover's circuit runs about 15x faster than the common (non-iterative) Grover's circuit, and about 3-4x faster than the statevector backend. In sampling experiments, Grover's circuits demonstrate strong low-shot stability: as the qubit number increases beyond 13, a single-shot measurement still closely mirrors the results from 4,096 shots, indicating reliable estimates with minimal sampling and significant potential to cut measurement costs. Overall, an iterative MPS design delivers speed and scalability for Grover's circuit simulation, enabling practical large-scale implementations."}
{"id": "2601.04136", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.04136", "abs": "https://arxiv.org/abs/2601.04136", "authors": ["Alessandro Lo Schiavo", "Luigi Costanzo", "Massimo Vitelli"], "title": "A Load Impedance Emulation Active Interface for Piezoelectric Vibration Energy Harvesters", "comment": null, "summary": "A single stage active AC/DC interface able to emulate the optimal load impedance of a Resonant Piezoelectric Vibration Energy Harvester (RPVEH) is proposed. As theoretically shown, unlike an electronic interface that emulates an optimal load generator, an interface that emulates an optimal load impedance does not require adaptation to the acceleration of input vibrations. This allows the use of a very simple control, avoiding the implementation of Maximum Power Point Tracking (MPPT) algorithms that require lossy microcontrollers. Thus, the proposed interface is equipped with a simple analog controller allowing the RPVEH to work in its Maximum Power Point (MPP) in both steady-state and variable conditions of vibrations, without recurring to multivariable perturbative approaches, as it happens for the most of single stage AC/DC interfaces proposed in the literature. The absence of perturbative techniques allows a significant improvement of both stationary and dynamic performances. Experimental tests of a prototype of the proposed interface confirm the theoretical findings and the predicted behavior."}
{"id": "2601.04136", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.04136", "abs": "https://arxiv.org/abs/2601.04136", "authors": ["Alessandro Lo Schiavo", "Luigi Costanzo", "Massimo Vitelli"], "title": "A Load Impedance Emulation Active Interface for Piezoelectric Vibration Energy Harvesters", "comment": null, "summary": "A single stage active AC/DC interface able to emulate the optimal load impedance of a Resonant Piezoelectric Vibration Energy Harvester (RPVEH) is proposed. As theoretically shown, unlike an electronic interface that emulates an optimal load generator, an interface that emulates an optimal load impedance does not require adaptation to the acceleration of input vibrations. This allows the use of a very simple control, avoiding the implementation of Maximum Power Point Tracking (MPPT) algorithms that require lossy microcontrollers. Thus, the proposed interface is equipped with a simple analog controller allowing the RPVEH to work in its Maximum Power Point (MPP) in both steady-state and variable conditions of vibrations, without recurring to multivariable perturbative approaches, as it happens for the most of single stage AC/DC interfaces proposed in the literature. The absence of perturbative techniques allows a significant improvement of both stationary and dynamic performances. Experimental tests of a prototype of the proposed interface confirm the theoretical findings and the predicted behavior."}
{"id": "2601.03855", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03855", "abs": "https://arxiv.org/abs/2601.03855", "authors": ["Chong-Wei Wang", "Mei Ian Sam", "Tzu-Ling Kuo", "Nan-Yow Chen", "Tai-Yue Li"], "title": "MPM-QIR: Measurement-Probability Matching for Quantum Image Representation and Compression via Variational Quantum Circuit", "comment": null, "summary": "We present MPM-QIR, a variational-quantum-circuit (VQC) framework for classical image compression and representation whose core objective is to achieve equal or better reconstruction quality at a lower Parameter Compression Ratio (PCR). The method aligns a generative VQC's measurement-probability distribution with normalized pixel intensities and learns positional information implicitly via an ordered mapping to the flattened pixel array, thus eliminating explicit coordinate qubits and tying compression efficiency directly to circuit (ansatz) complexity. A bidirectional convolutional architecture induces long-range entanglement at shallow depth, capturing global image correlations with fewer parameters. Under a unified protocol, the approach attains PSNR $\\geq$ 30 dB with lower PCR across benchmarks: MNIST 31.80 dB / SSIM 0.81 at PCR 0.69, Fashion-MNIST 31.30 dB / 0.91 at PCR 0.83, and CIFAR-10 31.56 dB / 0.97 at PCR 0.84. Overall, this compression-first design improves parameter efficiency, validates VQCs as direct and effective generative models for classical image compression, and is amenable to two-stage pipelines with classical codecs and to extensions beyond 2D imagery."}
{"id": "2601.04190", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04190", "abs": "https://arxiv.org/abs/2601.04190", "authors": ["Juan F. Gutierrez", "Nhung Nguyen", "Jesus M. Quintero", "Andres Gomez"], "title": "Solar Panel-based Visible Light Communication for Batteryless Systems", "comment": "This is an open-access, author-archived version of a manuscript published in ApplePies 2025 Conference", "summary": "This paper presents a batteryless wireless communication node for the Internet of Things, powered entirely by ambient light and capable of receiving data through visible light communication. A solar panel serves dual functions as an energy harvester and an optical antenna, capturing modulated signals from LED light sources. A lightweight analog front-end filters and digitizes the signals for an 8-bit low-power processor, which manages the system's operational states based on stored energy levels. The main processor is selectively activated to minimize energy consumption. Data reception is synchronized with the harvester's open-circuit phase, reducing interference and improving signal quality. The prototype reliably decodes 32-bit VLC frames at 800\\,Herz, consuming less than 2.8\\,mJ, and maintains sleep-mode power below 30\\,uW."}
{"id": "2601.04190", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04190", "abs": "https://arxiv.org/abs/2601.04190", "authors": ["Juan F. Gutierrez", "Nhung Nguyen", "Jesus M. Quintero", "Andres Gomez"], "title": "Solar Panel-based Visible Light Communication for Batteryless Systems", "comment": "This is an open-access, author-archived version of a manuscript published in ApplePies 2025 Conference", "summary": "This paper presents a batteryless wireless communication node for the Internet of Things, powered entirely by ambient light and capable of receiving data through visible light communication. A solar panel serves dual functions as an energy harvester and an optical antenna, capturing modulated signals from LED light sources. A lightweight analog front-end filters and digitizes the signals for an 8-bit low-power processor, which manages the system's operational states based on stored energy levels. The main processor is selectively activated to minimize energy consumption. Data reception is synchronized with the harvester's open-circuit phase, reducing interference and improving signal quality. The prototype reliably decodes 32-bit VLC frames at 800\\,Herz, consuming less than 2.8\\,mJ, and maintains sleep-mode power below 30\\,uW."}
{"id": "2601.03922", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.supr-con", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.03922", "abs": "https://arxiv.org/abs/2601.03922", "authors": ["Shiro Kawabata"], "title": "Integration and Resource Estimation of Cryoelectronics for Superconducting Fault-Tolerant Quantum Computers", "comment": "8 pages, 3 figures", "summary": "Scaling superconducting quantum computers to the fault-tolerant regime calls for a commensurate scaling of the classical control and readout stack. Today's systems largely rely on room-temperature, rack-based instrumentation connected to dilution-refrigerator cryostats through many coaxial cables. Looking ahead, superconducting fault-tolerant quantum computers (FTQCs) will likely adopt a heterogeneous quantum-classical architecture that places selected electronics at cryogenic stages -- for example, cryo-CMOS at 4~K and superconducting digital logic at 4~K and/or mK stages -- to curb wiring and thermal-load overheads. This review distills key requirements, surveys representative room-temperature and cryogenic approaches, and provides a transparent first-order accounting framework for cryoelectronics. Using an RSA-2048-scale benchmark as a concrete reference point, we illustrate how scaling targets motivate constraints on multiplexing and stage-wise cryogenic power, and discuss implications for functional partitioning across room-temperature electronics, cryo-CMOS, and superconducting logic."}
{"id": "2601.03777", "categories": ["stat.ME", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03777", "abs": "https://arxiv.org/abs/2601.03777", "authors": ["Md Nafees Fuad Rafi", "Zhaomiao Guo"], "title": "Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems", "comment": null, "summary": "While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks."}
{"id": "2601.03777", "categories": ["stat.ME", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03777", "abs": "https://arxiv.org/abs/2601.03777", "authors": ["Md Nafees Fuad Rafi", "Zhaomiao Guo"], "title": "Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems", "comment": null, "summary": "While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks."}
{"id": "2601.03975", "categories": ["quant-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.03975", "abs": "https://arxiv.org/abs/2601.03975", "authors": ["Himanshu Kumar", "Rahul Gupta", "Saikat Ghosh", "Himadri Shekhar Dhar", "Kasturi Saha"], "title": "Cavity-Driven Multispectral Gain for High-Sensitivity NV Center Magnetometers", "comment": "The first two authors contributed equally to this work. 6 pages, 4 figures", "summary": "We report a cavity-enabled solid-state magnetometer based on an NV ensemble coupled with a dielectric cavity, achieving 12 pT/$\\sqrt{\\rm{Hz}}$ sensitivity and a nearly threefold gain from multispectral features. The features originate from cavity-induced splitting of the NV hyperfine levels and leverages robust quantum coherence in the doubly dressed states of the system to achieve high sensitivity. We project simulated near-term sensitivities approaching 100 fT/$\\sqrt{\\rm{Hz}}$, close to the Johnson-Nyquist limit. Our results establish frequency multiplexing as a new operational paradigm, offering a robust and scalable quantum resource for metrology under ambient conditions."}
{"id": "2601.03971", "categories": ["math.NA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03971", "abs": "https://arxiv.org/abs/2601.03971", "authors": ["Josie König", "Han Cheng Lie"], "title": "Posterior error bounds for prior-driven balancing in linear Gaussian inverse problems", "comment": null, "summary": "In large-scale Bayesian inverse problems, it is often necessary to apply approximate forward models to reduce the cost of forward model evaluations, while controlling approximation quality. In the context of Bayesian inverse problems with linear forward models, Gaussian priors, and Gaussian noise, we use perturbation theory for inverses to bound the error in the approximate posterior mean and posterior covariance resulting from a linear approximate forward model. We then focus on the smoothing problem of inferring the initial condition of linear time-invariant dynamical systems, using finitely many partial state observations. For such problems, and for a specific model order reduction method based on balanced truncation, we show that the impulse response of a certain prior-driven system is closely related to the prior-preconditioned Hessian of the inverse problem. This reveals a novel connection between systems theory and inverse problems. We exploit this connection to prove the first a priori error bounds for system-theoretic model order reduction methods applied to smoothing problems. The bounds control the approximation error of the posterior mean and covariance in terms of the truncated Hankel singular values of the underlying system."}
{"id": "2601.03971", "categories": ["math.NA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03971", "abs": "https://arxiv.org/abs/2601.03971", "authors": ["Josie König", "Han Cheng Lie"], "title": "Posterior error bounds for prior-driven balancing in linear Gaussian inverse problems", "comment": null, "summary": "In large-scale Bayesian inverse problems, it is often necessary to apply approximate forward models to reduce the cost of forward model evaluations, while controlling approximation quality. In the context of Bayesian inverse problems with linear forward models, Gaussian priors, and Gaussian noise, we use perturbation theory for inverses to bound the error in the approximate posterior mean and posterior covariance resulting from a linear approximate forward model. We then focus on the smoothing problem of inferring the initial condition of linear time-invariant dynamical systems, using finitely many partial state observations. For such problems, and for a specific model order reduction method based on balanced truncation, we show that the impulse response of a certain prior-driven system is closely related to the prior-preconditioned Hessian of the inverse problem. This reveals a novel connection between systems theory and inverse problems. We exploit this connection to prove the first a priori error bounds for system-theoretic model order reduction methods applied to smoothing problems. The bounds control the approximation error of the posterior mean and covariance in terms of the truncated Hankel singular values of the underlying system."}
{"id": "2601.04020", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04020", "abs": "https://arxiv.org/abs/2601.04020", "authors": ["Adrian Skasberg Aasen", "Martin Gärttner"], "title": "Limitations for adaptive quantum state tomography in the presence of detector noise", "comment": "10 pages and 6 figures + appendix 9 pages and 6 figures", "summary": "Assumption-free reconstruction of quantum states from measurements is essential for benchmarking and certifying quantum devices, but it remains difficult due to the extensive measurement statistics and experimental resources it demands. An approach to alleviating these demands is provided by adaptive measurement strategies, which can yield up to a quadratic improvement in reconstruction accuracy for pure states by dynamically optimizing measurement settings during data acquisition. A key open question is whether these asymptotic advantages remain in realistic experiments, where readout is inevitably noisy. In this work, we analyze the impact of readout noise on adaptive quantum state tomography with readout-error mitigation, focusing on the challenging regime of reconstructing pure states using mixed-state estimators. Using analytical arguments based on Fisher information optimization and extensive numerical simulations using Bayesian inference, we show that any nonzero readout noise eliminates the asymptotic quadratic scaling advantage of adaptive strategies. We numerically investigate the behavior for finite measurement statistics for single- and two-qubit systems with exact readout-error mitigation and find a gradual transition from ideal to sub-optimal scaling. We furthermore investigate realistic scenarios where detector tomography is performed with a limited number of state copies for calibration, showing that insufficient detector characterization leads to estimator bias and limited reconstruction accuracy. Although our result imposes an upper bound on the reconstruction accuracy that can be achieved with adaptive strategies, we nevertheless observe numerically a constant-factor gain in reconstruction accuracy, which becomes larger as the readout noise decreases. This indicates potential practical benefits in using adaptive measurement strategies in well-calibrated experiments."}
{"id": "2601.03976", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03976", "abs": "https://arxiv.org/abs/2601.03976", "authors": ["Gorka Nieto", "Idoia de la Iglesia", "Cristina Perfecto", "Unai Lopez-Novoa"], "title": "On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process", "comment": "Submitted to IEEE Transactions on Cognitive Communications and Networking", "summary": "Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption."}
{"id": "2601.03976", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03976", "abs": "https://arxiv.org/abs/2601.03976", "authors": ["Gorka Nieto", "Idoia de la Iglesia", "Cristina Perfecto", "Unai Lopez-Novoa"], "title": "On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process", "comment": "Submitted to IEEE Transactions on Cognitive Communications and Networking", "summary": "Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption."}
{"id": "2601.04031", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04031", "abs": "https://arxiv.org/abs/2601.04031", "authors": ["Yuen San Lo", "Adam H. Brzosko", "Peter R. Smith", "Robert I. Woodward", "Davide G. Marangon", "James F. Dynes", "Sergio Juárez", "Taofiq K. Paraïso", "R. Mark Stevenson", "Andrew J. Shields"], "title": "Phase-Randomized Laser Pulse Generation at 10 GHz for Quantum Photonic Applications", "comment": "16 pages, 5 figures. The following article has been submitted to APL Photonics. Copyright 2026 Authors. This article is distributed under a Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC) License. https://creativecommons.org/licenses/by-nc/4.0/", "summary": "Gain-switching laser diodes is a well-established technique for generating optical pulses with random phases, where the quantum randomness arises naturally from spontaneous emission. However, the maximum switching rate is limited by phase diffusion: at high repetition rates, residual photons in the cavity seed subsequent pulses, leading to phase correlations, which degrade randomness. We present a method to overcome this limitation by employing an external source of spontaneous emission in conjunction with the laser. Our results show that this approach effectively removes interpulse phase correlations and restores phase randomization at repetition rates as high as 10 GHz. This technique opens new opportunities for high-rate quantum key distribution and quantum random number generation."}
{"id": "2601.04082", "categories": ["quant-ph", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.04082", "abs": "https://arxiv.org/abs/2601.04082", "authors": ["Simon J. K. Lang", "Ignaz Eisele", "Alwin Maiwald", "Emir Music", "Luis Schwarzenbach", "Carla Morán-Guizán", "Johannes Weber", "Daniela Zahn", "Thomas Mayer", "Rui N. Pereira", "Christoph Kutter"], "title": "Surface Optimization of Aluminum Resonators for Robust Quantum Device Fabrication", "comment": "7 pages, 9 figures", "summary": "Aluminum remains the central material for superconducting qubits, and considerable effort has been devoted to optimizing its deposition and patterning for quantum devices. However, while post-processing of Nb- and Ta-based resonators has been widely explored, primarily focusing on oxide removal using buffered oxide etch (BOE), post-treatment strategies for Al resonators remain underdeveloped. This challenge becomes particularly relevant for industry-scale fabrication with multichip bonding, where delays between sample preparation and cooldown require surface treatments that preserve low dielectric loss during extended exposure to ambient conditions. In this work, we investigate surface modification approaches for Al resonators subjected to a 24-hour delay prior to cryogenic measurement. Passivation using self-limiting oxygen and fluorine chemistries was evaluated utilizing different plasma processes. Remote oxygen plasma treatment reduced dielectric losses, in contrast to direct plasma, likely due to additional ashing of residual resist despite the formation of a thicker oxide layer on both Si and Al surfaces. A fluorine-based plasma process was developed that passivated the Al surface with fluorine for subsequent BOE treatment. However, increasing fluorine incorporation in the aluminum oxide correlated with higher loss, identifying fluorine as an unsuitable passivation material for Al resonators. Finally, selective oxide removal using HF vapor and phosphoric acid was assessed for surface preparation. HF vapor selectively etched SiO2 while preserving Al2O3, whereas phosphoric acid exhibited the opposite selectivity. Sequential application of both etches yielded dielectric losses as low as $δ_\\mathrm{LP} = 5.2 \\times 10^{-7}$ ($Q\\mathrm{i} \\approx 1.9\\,\\mathrm{M}$) in the single photon regime, demonstrating a promising pathway for robust Al-based resonator fabrication."}
{"id": "2601.04092", "categories": ["quant-ph", "hep-lat", "hep-ph", "nucl-th"], "pdf": "https://arxiv.org/pdf/2601.04092", "abs": "https://arxiv.org/abs/2601.04092", "authors": ["Peng Guo", "Paul LeVan", "Frank X. Lee", "Yong Zhao"], "title": "Extracting scattering phase shift in quantum mechanics on quantum computers", "comment": "17 pages, 30 figures", "summary": "We investigate the feasibility of extracting infinite volume scattering phase shift on quantum computers in a simple one-dimensional quantum mechanical model, using the formalism established in Ref.~\\cite{Guo:2023ecc} that relates the integrated correlation functions (ICF) for a trapped system to the infinite volume scattering phase shifts through a weighted integral. The system is first discretized in a finite box with periodic boundary conditions, and the formalism in real time is verified by employing a contact interaction potential with exact solutions. Quantum circuits are then designed and constructed to implement the formalism on current quantum computing architectures. To overcome the fast oscillatory behavior of the integrated correlation functions in real-time simulation, different methods of post-data analysis are proposed and discussed. Test results on IBM hardware show that good agreement can be achieved with two qubits, but complete failure ensues with three qubits due to two-qubit gate operation errors and thermal relaxation errors."}
{"id": "2601.04139", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.04139", "abs": "https://arxiv.org/abs/2601.04139", "authors": ["Cristofero Oglialoro", "Gerard J. Machado", "Felix Farsch", "Daniel F. Urrego", "Alejandra A. Padilla", "Raj B. Patel", "Ian A. Walmsley", "Markus Gräfe", "Juan P. Torres", "Enno Giese"], "title": "Below-shot-noise capacity in phase estimation using nonlinear interferometers", "comment": "14 pages, 5 figures", "summary": "Over the past decade, several schemes for imaging and sensing based on nonlinear interferometers have been proposed and demonstrated experimentally. These interferometers exhibit two main advantages. First, they enable probing a sample at a chosen wavelength while detecting light at a different wavelength with high efficiency (bicolor quantum imaging and sensing with undetected light). Second, they can show quantum-enhanced sensitivities below the shot-noise limit, potentially reaching Heisenberg-limited precision in parameter estimation. Here, we compare three quantum-imaging configurations using only easily accessible intensity-based measurements for phase estimation: a Yurke-type SU(1,1) interferometer, a Mandel-type induced-coherence interferometer, and a hybrid scheme that continuously interpolates between them. While an ideal Yurke interferometer can exhibit Heisenberg scaling, this advantage is known to be fragile under realistic detection constraints and in the presence of loss. We demonstrate that differential intensity detection in the Mandel interferometer provides the highest and most robust phase sensitivity among the considered schemes, reaching but not surpassing the shot-noise limit, even in the presence of loss. Intensity measurements in a Yurke-type configuration can achieve genuine sub-shot-noise sensitivity under balanced losses and moderate gain; however, their performance degrades in realistic high-gain regimes. Consequently, in this regime, the Mandel configuration with differential detection outperforms the Yurke-type setup and constitutes the most robust approach for phase estimation."}
{"id": "2601.04180", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.04180", "abs": "https://arxiv.org/abs/2601.04180", "authors": ["Aadil Oufkir", "Filippo Girardi"], "title": "Improved Lower Bounds for Learning Quantum Channels in Diamond Distance", "comment": "21 pages, 2 figures", "summary": "We prove that learning an unknown quantum channel with input dimension $d_A$, output dimension $d_B$, and Choi rank $r$ to diamond distance $\\varepsilon$ requires $ Ω\\!\\left( \\frac{d_A d_B r}{\\varepsilon \\log(d_B r / \\varepsilon)} \\right)$ queries. This improves the best previous $Ω(d_A d_B r)$ bound by introducing explicit $\\varepsilon$-dependence, with a scaling in $\\varepsilon$ that is near-optimal when $d_A=rd_B$ but not tight in general. The proof constructs an ensemble of channels that are well-separated in diamond norm yet admit Stinespring isometries that are close in operator norm."}
{"id": "2601.03843", "categories": ["hep-lat", "cond-mat.str-el", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.03843", "abs": "https://arxiv.org/abs/2601.03843", "authors": ["Tomoya Hayata", "Yoshimasa Hidaka", "Hiromasa Watanabe"], "title": "Phases of the $q$-deformed $\\mathrm{SU}(N)$ Yang-Mills theory at large $N$", "comment": "7+4 pages, 5+2 figures, 2+1 tables; analysis code will be made publicly available on GitHub after publication", "summary": "We investigate the $(2+1)$-dimensional $q$-deformed $\\mathrm{SU}(N)_k$ Yang-Mills theory in the lattice Hamiltonian formalism, which is characterized by three parameters: the number of colors $N$, the coupling constant $g$, and the level $k$. By treating these as tunable parameters, we explore how key properties of the theory, such as confinement and topological order, emerge in different regimes. Employing a variational mean-field analysis that interpolates between the strong- and weak-coupling regimes, we determine the large-$N$ phase structure in terms of the 't Hooft coupling $λ_\\mathrm{tH}=g^2N$ and the ratio $k/N$. We find that the topologically ordered phase remains robust at large $N$ under appropriate scalings of these parameters. This result indicates that the continuum limit of large-$N$ gauge theory may be more intricate than naively expected, and motivates studies beyond the mean-field theory, both to achieve a further understanding of confinement in gauge theories and to guide quantum simulations of large-$N$ gauge theories."}
{"id": "2601.04049", "categories": ["q-fin.CP", "math.NA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04049", "abs": "https://arxiv.org/abs/2601.04049", "authors": ["Julien Hok", "Álvaro Leitao"], "title": "Quantum computing for multidimensional option pricing: End-to-end pipeline", "comment": null, "summary": "This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives."}
