{"id": "2512.13810", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13810", "abs": "https://arxiv.org/abs/2512.13810", "authors": ["Darin Jeff", "Eytan Modiano"], "title": "Delay Optimization in a Simple Offloading System: Extended Version", "comment": null, "summary": "We consider a computation offloading system where jobs are processed sequentially at a local server followed by a higher-capacity cloud server. The system offers two service modes, differing in how the processing is split between the servers. Our goal is to design an optimal policy for assigning jobs to service modes and partitioning server resources in order to minimize delay. We begin by characterizing the system's stability region and establishing design principles for service modes that maximize throughput. For any given job assignment strategy, we derive the optimal resource partitioning and present a closed-form expression for the resulting delay. Moreover, we establish that the delay-optimal assignment policy exhibits a distinct breakaway structure: at low system loads, it is optimal to route all jobs through a single service mode, whereas beyond a critical load threshold, jobs must be assigned across both modes. We conclude by validating these theoretical insights through numerical evaluation."}
{"id": "2512.13836", "categories": ["eess.SY", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13836", "abs": "https://arxiv.org/abs/2512.13836", "authors": ["Ricardo Tapia", "Iman Soltani"], "title": "A Convex Obstacle Avoidance Formulation", "comment": "18 pages, 17 figures", "summary": "Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear."}
{"id": "2512.13868", "categories": ["eess.SY", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13868", "abs": "https://arxiv.org/abs/2512.13868", "authors": ["Tianyu Zhou", "Zihao Liang", "Zehui Lu", "Shaoshuai Mou"], "title": "Safe Online Control-Informed Learning", "comment": null, "summary": "This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems."}
{"id": "2512.13871", "categories": ["eess.SY", "cs.HC", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13871", "abs": "https://arxiv.org/abs/2512.13871", "authors": ["Shaun Sweeney", "Robert Shorten", "Mark O'Malley"], "title": "A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory", "comment": "PhD thesis", "summary": "This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.\n  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.\n  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.\n  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.\n  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants."}
{"id": "2512.13810", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13810", "abs": "https://arxiv.org/abs/2512.13810", "authors": ["Darin Jeff", "Eytan Modiano"], "title": "Delay Optimization in a Simple Offloading System: Extended Version", "comment": null, "summary": "We consider a computation offloading system where jobs are processed sequentially at a local server followed by a higher-capacity cloud server. The system offers two service modes, differing in how the processing is split between the servers. Our goal is to design an optimal policy for assigning jobs to service modes and partitioning server resources in order to minimize delay. We begin by characterizing the system's stability region and establishing design principles for service modes that maximize throughput. For any given job assignment strategy, we derive the optimal resource partitioning and present a closed-form expression for the resulting delay. Moreover, we establish that the delay-optimal assignment policy exhibits a distinct breakaway structure: at low system loads, it is optimal to route all jobs through a single service mode, whereas beyond a critical load threshold, jobs must be assigned across both modes. We conclude by validating these theoretical insights through numerical evaluation."}
{"id": "2512.13740", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.13740", "abs": "https://arxiv.org/abs/2512.13740", "authors": ["Álvaro Fernández Corral", "Yahya Saleh"], "title": "Enhancing polynomial approximation of continuous functions by composition with homeomorphisms", "comment": null, "summary": "We enhance the approximation capabilities of algebraic polynomials by composing them with homeomorphisms. This composition yields families of functions that remain dense in the space of continuous functions, while enabling more accurate approximations. For univariate continuous functions exhibiting a finite number of local extrema, we prove that there exist a polynomial of finite degree and a homeomorphism whose composition approximates the target function to arbitrary accuracy. The construction is especially relevant for multivariate approximation problems, where standard numerical methods often suffer from the curse of dimensionality. To support our theoretical results, we investigate both regression tasks and the construction of molecular potential-energy surfaces, parametrizing the underlying homeomorphism using invertible neural networks. The numerical experiments show strong agreement with our theoretical analysis."}
{"id": "2512.13915", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.13915", "abs": "https://arxiv.org/abs/2512.13915", "authors": ["Victor Livernoche", "Andreea Musulan", "Zachary Yang", "Jean-François Godbout", "Reihaneh Rabbany"], "title": "Deepfakes in the 2025 Canadian Election: Prevalence, Partisanship, and Platform Dynamics", "comment": "4 pages, 6 figures", "summary": "Concerns about AI-generated political content are growing, yet there is limited empirical evidence on how deepfakes actually appear and circulate across social platforms during major events in democratic countries. In this study, we present one of the first in-depth analyses of how these realistic synthetic media shape the political landscape online, focusing specifically on the 2025 Canadian federal election. By analyzing 187,778 posts from X, Bluesky, and Reddit with a high-accuracy detection framework trained on a diverse set of modern generative models, we find that 5.86% of election-related images were deepfakes. Right-leaning accounts shared them more frequently, with 8.66% of their posted images flagged compared to 4.42% for left-leaning users, often with defamatory or conspiratorial intent. Yet, most detected deepfakes were benign or non-political, and harmful ones drew little attention, accounting for only 0.12% of all views on X. Overall, deepfakes were present in the election conversation, but their reach was modest, and realistic fabricated images, although less common, drew higher engagement, highlighting growing concerns about their potential misuse."}
{"id": "2512.13928", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2512.13928", "abs": "https://arxiv.org/abs/2512.13928", "authors": ["Jakub Ślęzak"], "title": "Codifference as a measure of dispersion and dependence for mixture models", "comment": "23 pages, 4 figures", "summary": "Codifference is a commonly used measure of dependence for stable vectors and processes for which covariance is infinite. However, we argue that it can also be used for other heavy-tail distributions and it provides useful information for other non-Gaussian distributions as well, no matter the tails. Motivated by this, we analyse codifference using as little assumptions as possible about the studied model. It leads us to propose its natural domain and three natural variants of it. Using the wide class of variable scale mixture distributions we argue that the codifference can be interpreted as the measure of bulk properties which ignores the tails much more than the covariance. It can also detect forms of non-linear memory which covariance cannot. Finally, we show the asymptotic distribution of its estimator."}
{"id": "2512.14370", "categories": ["nlin.AO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2512.14370", "abs": "https://arxiv.org/abs/2512.14370", "authors": ["Florian Voss", "Uwe Thiele"], "title": "From bipedal to chaotic motion of chemically fueled partially wetting liquid drops", "comment": null, "summary": "We employ a thermodynamically consistent out-of-equilibrium continuum model to study the motion patterns of partially wetting liquid drops covered by autocatalytically reacting surfactants. When ambient chemostats feed a chemomechanical feedback loop involving a nonlinear reaction network, surface stresses caused by the Marangoni effect and the ensuing hydrodynamic motion, drops show a variety of increasingly complex biomimetic motility modes including shuttling, bipedal, rotational, intermittently chaotic and chaotic motion. We determine the corresponding nonequilibrium phase diagram and show that the complexity of the motion arises from competing length scales."}
{"id": "2512.13965", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.13965", "abs": "https://arxiv.org/abs/2512.13965", "authors": ["Joseph A. Farmer", "Aidan Murray", "Johannes Krotz", "Ryan G. McClarren"], "title": "Generative Monte Carlo Sampling for Constant-Cost Particle Transport", "comment": "10 pages, 4 figures", "summary": "We present Generative Monte Carlo (GMC), a novel paradigm for particle transport simulation that integrates generative artificial intelligence directly into the stochastic solution of the linear Boltzmann equation. By reformulating the cell-transmission problem as a conditional generation task, we train neural networks using conditional flow matching to sample particle exit states, including position, direction, and path length, without simulating scattering histories. The method employs optical coordinate scaling, enabling a single trained model to generalize across any material. We validate GMC on two canonical benchmarks, namely a heterogeneous lattice problem characteristic of nuclear reactor cores and a linearized hohlraum geometry representative of high-energy density radiative transfer. Results demonstrate that GMC preserves the statistical fidelity of standard Monte Carlo, exhibiting the expected $1/\\sqrt{N}$ convergence rate while maintaining accurate scalar flux profiles. While standard Monte Carlo computational cost scales linearly with optical thickness in the diffusive limit, GMC achieves constant $O(1)$ cost per cell transmission, yielding order-of-magnitude speedups in optically thick regimes. This framework strategically aligns particle transport with modern computing architectures optimized for neural network inference, positioning transport codes to leverage ongoing advances in AI hardware and algorithms."}
{"id": "2512.13841", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13841", "abs": "https://arxiv.org/abs/2512.13841", "authors": ["Eduardo Gutiérrez-Peña", "Carlos Octavio Pérez-Mendoza", "Alan Riva Palacio", "Arno Siri-Jégousse"], "title": "Parameter Estimation for Partially Observed Stable Continuous-State Branching Processes", "comment": null, "summary": "In this article, we present a novel inference framework for estimating the parameters of Continuous-State Branching Processes (CSBPs). We do so by leveraging their subordinator representation. Our method reformulates the estimation problem by shifting the stochastic dynamics to the associated subordinator, enabling a parametric estimation procedure without requiring additional assumptions. This reformulation allows for efficient numerical recovery of the likelihood function via Laplace transform inversion, even in models where closed-form transition densities are unavailable. In addition to offering a flexible approach to parameter estimation, we propose a dynamic simulation framework that generates discrete-time trajectories of CSBPs using the same subordinator-based structure."}
{"id": "2512.13940", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13940", "abs": "https://arxiv.org/abs/2512.13940", "authors": ["Ibon Gracia", "Morteza Lahijanian"], "title": "Data-Driven Control via Conditional Mean Embeddings: Formal Guarantees via Uncertain MDP Abstraction", "comment": null, "summary": "Controlling stochastic systems with unknown dynamics and under complex specifications is specially challenging in safety-critical settings, where performance guarantees are essential. We propose a data-driven policy synthesis framework that yields formal performance guarantees for such systems using conditional mean embeddings (CMEs) and uncertain Markov decision processes (UMDPs). From trajectory data, we learn the system's transition kernel as a CME, then construct a finite-state UMDP abstraction whose transition uncertainties capture learning and discretization errors. Next, we generate a policy with formal performance bounds through robust dynamic programming. We demonstrate and empirically validate our method through a temperature regulation benchmark."}
{"id": "2512.13836", "categories": ["eess.SY", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13836", "abs": "https://arxiv.org/abs/2512.13836", "authors": ["Ricardo Tapia", "Iman Soltani"], "title": "A Convex Obstacle Avoidance Formulation", "comment": "18 pages, 17 figures", "summary": "Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear."}
{"id": "2512.13963", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.13963", "abs": "https://arxiv.org/abs/2512.13963", "authors": ["Quincy Huhn", "Jean Ragusa", "Youngsoo Choi"], "title": "Offline Maximizing Minimally Invasive Proper Orthogonal Decomposition for Reduced Order Modeling of $S_n$ Radiation Transport", "comment": null, "summary": "Deterministic solutions to the Sn transport equation can be computationally expensive to calculate. Reduced Order Models (ROMs) provide an efficient means of approximating the Full Order Model (FOM) solution. We propose a novel approach for constructing ROMs of the Sn radiation transport equation, Offline Maximizing Minimally Invasive (OMMI) Proper Orthogonal Decomposition (POD). POD uses snapshot data to build a reduced basis, which is then used to project the FOM. Minimally Invasive POD leverages the sweep infrastructure within deterministic Sn transport solvers to construct the reduced linear system, even though the FOM linear system is never directly assembled. OMMI-POD extends Minimally Invasive POD by performing transport sweeps offline, thereby maximizing the potential speedup. It achieves this by generating a library of reduced systems from a training set, which is then interpolated in the online stage to provide a rapid approximate solution to the Sn transport equation. The model's performance is evaluated on a multigroup 2-D test problem, demonstrating low error and a 1600-fold speedup over the full order model."}
{"id": "2512.14564", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.14564", "abs": "https://arxiv.org/abs/2512.14564", "authors": ["Arthur Capozzi"], "title": "\"Talking past each other\": Issue ownership and microtargeting in Swiss online political ads", "comment": null, "summary": "Switzerland's unique system of direct democracy, characterized by frequent popular referenda, provides a critical context for studying the impact of online political advertising beyond standard electoral cycles. This paper presents a large-scale, data-driven analysis of 40k political ads published on Facebook and Instagram in Switzerland between 2021 and 2025. Despite a voting population of only 5.6 million, the ad campaigns were significant in scale, costing CHF 4.5 million and achieving 560 million impressions. This study shows that political ads are used not only for federal elections, but also to influence referenda, where greater exposure to ``pro-Yes'' advertising correlates significantly with approval outcomes. The analysis of microtargeting reveals distinct partisan strategies: centrist and right-wing parties predominantly target older men, whereas left-wing parties focus on young women. Furthermore, significant region-specific demographic variations are observed even within the same party, reflecting Switzerland's strong territorial divisions. Regarding content, a clear pattern of ``talking past each other'' is identified: in line with issue ownership theory, parties avoid direct debate on shared issues, preferring to promote exclusively owned topics. Finally, it is demonstrated that these strategies are so distinct that an ad's author can be predicted using a machine learning model trained exclusively on its audience and topic features. This study sheds light on how microtargeting and issue divergence on social platforms may fragment the public sphere and bypass traditional democratic deliberation."}
{"id": "2512.14062", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2512.14062", "abs": "https://arxiv.org/abs/2512.14062", "authors": ["Matjaž Omladič", "Martin Vuk", "Aljaž Zalar"], "title": "Extreme Mass Distributions For K-Increasing Quasi-Copulas", "comment": "15 pages, 1 figure", "summary": "The rating of quasi-copula problems in the dependence modeling community has recently risen in spite of the lack of probability interpretation of quasi-copulas. The trendsetting paper J.J. Arias-Garcia, R. Mesiar, and B. De Baets, The unwalked path between quasi-copulas and copulas: Stepping stones in higher dimensions, Internat. J. of Approx. Reasoning, 80 (2017) 89--99, proposes the k-increasing property for some k {\\le} d as a property of d-variate quasi-copulas that would shed some light on what is in-between. This hierarchy of classes extends the bivariate notion of supermodularity property. The same authors propose a number of open problems in the continuation of this paper (Fuzzy Sets and Systems 393 (2020), 1--28). Their Open problem 5 asks for the extreme values of the mass distributions associated with multivariate quasi-copulas and was recently solved by the authors of this paper (Fuzzy Sets and Systems 527 (2026) 109698). The main goal of the present paper is to solve the maximal-volume problem (in absolute value) within each of the previously mentioned subclasses. By formulating and solving suitably simplified primal and dual linear programs, we derive the exact maximal negative and positive masses together with the corresponding extremal boxes."}
{"id": "2512.14010", "categories": ["physics.comp-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14010", "abs": "https://arxiv.org/abs/2512.14010", "authors": ["Che-Chia Chang", "Te-Sheng Lin", "Ming-Chih Lai"], "title": "Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems", "comment": null, "summary": "The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability."}
{"id": "2512.13875", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13875", "abs": "https://arxiv.org/abs/2512.13875", "authors": ["Michael C. Stanley", "Peter W. Spaeth", "James E. Warner", "Matthew R. Webster"], "title": "Bond strength uncertainty quantification via confidence intervals for nondestructive evaluation of bonded composites", "comment": null, "summary": "As bonded composite materials are used more frequently for aerospace applications, it is necessary to certify that parts achieve desired levels of certain physical characteristics (e.g., strength) for safety and performance. Nondestructive evaluation (NDE) of adhesively bonded structures enables verification of bond physical characteristics, but uncertainty quantification (UQ) of NDE estimates is crucial for understanding risks, especially for NDE estimates like bond strength. To address the critical need for NDE UQ for adhesive bond strength estimates, we propose an optimization--based approach to computing finite--sample confidence intervals showing the range of bond strengths that could feasibly be produced by the observed data. A statistical inverse model approach is used to compute a confidence interval of specimen interfacial stiffness from swept--frequency ultrasonic phase observations and a method for propagating the interval to bond strength via a known interfacial stiffness regression is proposed. This approach requires innovating the optimization--based confidence interval to handle both a nonlinear forward model and unknown variance and developing a calibration approach to ensure that the final bond strength interval achieves at least the desired coverage level. Using model assumptions in line with current literature, we demonstrate our approach on simulated measurement data using a variety of low to high noise settings under two prototypical parameter settings. Relative to a baseline approach, we show that our method achieves better coverage and smaller intervals in high--noise settings and when a nuisance parameter is near the constraint boundary."}
{"id": "2512.14315", "categories": ["physics.hist-ph", "gr-qc", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14315", "abs": "https://arxiv.org/abs/2512.14315", "authors": ["Karen Crowther"], "title": "Another 100 Years of Quantum Interpretation?", "comment": "An edited version is to appear in: How to Understand Quantum Mechanics - 100 Years of Ongoing Interpretation, edited by Lars-Göran Johansson and Jan Faye", "summary": "Interpretation is not the only way to explain a theory's success, form and features, and nor is it the only way to solve problems we see with a theory. This can also be done by giving a reductive explanation of the theory, by reference to a newer, more accurate, and/or more fundamental theory. We are seeking a theory of quantum gravity, a more fundamental theory than both quantum mechanics and general relativity, yet, while this theory is supposed to explain general relativity, it's not typically been thought to be necessary, or able, to explain quantum mechanics -- a task instead assigned to interpretation. Here, I question why this is. I also present a new way of assessing the various interpretations of quantum mechanics, in terms of their heuristic and unificatory potential in helping us find a more fundamental theory."}
{"id": "2512.13812", "categories": ["hep-lat", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13812", "abs": "https://arxiv.org/abs/2512.13812", "authors": ["Itay Gomelski", "Jonathan Elyovich", "Ariel Kelman", "Erez Zohar", "Patrick Emonts"], "title": "Algorithmic aspects of gauged Gaussian fermionic projected entangled pair states", "comment": null, "summary": "Lattice gauge theories (LGTs) provide a powerful framework for studying non-perturbative phenomena in gauge theories. However, conventional approaches such as Monte Carlo (MC) simulations in imaginary time are limited, as they do not allow real time evolution and suffer from a sign problem in many important cases. Using Gauged Gaussian fermionic projected entangled pair states (GGFPEPS) as a variational ground state ansatz offers an alternative for studying LGTs through a sign-problem-free variational MC. As this method is extended to larger and more complex systems, understanding its numerical behavior becomes essential. While conventional action based MC has been extensively studied, the performance and characteristics of non-action-based MC within the GGFPEPS framework are far less explored. In this work, we investigate these algorithmic aspects, identifying an optimal update size for GGFPEPS-based MC simulations for $\\mathbb{Z}_2$ in $2+1$ dimensions. We show that gauge fixing generally slows convergence, and demonstrate that not exploiting the translation-invariance can, in some cases, improve the computational time scaling of error convergence. We expect that these improvements will allow advancing the simulation to larger and more complex systems."}
{"id": "2512.13899", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.13899", "abs": "https://arxiv.org/abs/2512.13899", "authors": ["Charlie Becker", "David John Gagne", "Julie Demuth", "John S. Schreck", "Jacob Radford", "Gabrielle Gantos", "Eliot Kim", "Dhamma Kimpara", "Sophia Reiner", "Justin Willson", "Christopher D. Wirz"], "title": "Winter Precipitation Type Diagnosis and Uncertainty Quantification with a Physically Consistent Machine Learning Method", "comment": "24 pages, 11 figures, submitted to AMS Weather and Forecasting", "summary": "Correctly forecasting the timing and location of changes in winter precipitation type could help decision makers mitigate the worst impacts of winter storms. Multiple precipitation type algorithms have been developed from both physical and statistical perspectives, but all of them struggle in certain scenarios, and most of them do not account for uncertainty with a single model. We developed an evidential neural network that can predict both the probability of each winter precipitation type as well as the epistemic uncertainty. We trained our model on quality controlled and curated observations from the crowd-sourced mPING dataset in conjunction with vertical profiles from the NOAA Rapid Refresh model analyses. Our static and interactive evaluation revealed that the data curation procedure resulted in meteorologically consistent forecasts and appropriately represents uncertainty in difficult regimes where predictability may be limited by the atmospheric representations of current NWP models. We compare our model to both the Rapid Refresh NWP model in addition to other thermodynamic area-based methods from June of 2020 through June of 2022 and from a High Resolution Rapid Refresh central plains case study from December 24-26, 2023."}
{"id": "2512.13722", "categories": ["quant-ph", "hep-ph"], "pdf": "https://arxiv.org/pdf/2512.13722", "abs": "https://arxiv.org/abs/2512.13722", "authors": ["Deepak Sah", "Manoranjan P. Singh"], "title": "Electron-positron pair creation induced by multi-pulse train of electric fields: effect of randomness in time-delay", "comment": null, "summary": "We investigate the creation of electron-positron pairs (EPPs) in a sequence of alternating-sign, time-dependent electric field pulse trains by solving the quantum Vlasov equations. Specifically, we focus on Sauter-like pulse trains with random time delays between successive pulses, drawn from a Gaussian distribution wherein the extent of fluctuations is controlled by the standard deviation $σ_T$ of the distribution. We find that increasing $σ_T$ leads to a dramatic transformation in the longitudinal momentum spectrum. The well-known fringe pattern, akin to that in the multi-slit interference, gets significantly modified. The averaged spectra exhibit a robust Gaussian-like envelope with residual oscillations, which are much more prominent in the central momentum region. Notably, we find that in certain cases, stochastic time delays lead to a pronounced enhancement in the central peak of the distribution function for pulse train containing $N$ pulses. For example, for $N=20$ pulses, $σ_T \\approx 31$ $[m^{-1}]$(about $17\\%$ of the mean time delay) yields nearly a tenfold increase in the central peak, which for $σ_T \\approx 50$ $[m^{-1}]$ (about $27\\%$ of the mean time delay), scales up to $10^3.$ This may open up new possibilities for optimizing multi-pulse field configurations and guide future experimental designs aimed at maximizing EPPs creation."}
{"id": "2512.13746", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13746", "abs": "https://arxiv.org/abs/2512.13746", "authors": ["Elham Kiyani", "Amit Makarand Deshpande", "Madhura Limaye", "Zhiwei Gao", "Sai Aditya Pradeep", "Srikanth Pilla", "Gang Li", "Zhen Li", "George Em Karniadakis"], "title": "Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network", "comment": "21 pages, 13 figures", "summary": "Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites."}
{"id": "2512.13786", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2512.13786", "abs": "https://arxiv.org/abs/2512.13786", "authors": ["Fabian Pichler", "Clemens Kuhlenkamp", "Michael Knap"], "title": "False Vacuum Decay in Flat-Band Ferromagnets: Role of Quantum Geometry and Chiral Edge States", "comment": "4 pages main text + 5 pages Appendix and References; 4 + 1 figures", "summary": "Dynamical control of quantum matter is a challenging, yet promising direction for probing strongly correlated states. Motivated by recent experiments in twisted MoTe$_2$ that demonstrated optical control of magnetization, we propose a protocol for probing magnetization dynamics in flat-band ferromagnets. We investigate the nucleation and dynamical growth of magnetic bubbles prepared on top of a false vaccum in both itinerant ferromagnets and spin-polarized Chern insulators. For ferromagnetic metals, we emphasize the crucial role of a non-trivial quantum geometry in the magnetization dynamics, which in turn also provides a probe for the quantum metric. Furthermore, for quantum Hall ferromagnets, we show how properties of chiral edge modes localized at domain-wall boundaries can be dynamically accessed. Our work demonstrates the potential for nonequilibrium protocols to control and probe strongly correlated phases, with particular relevance for twisted MoTe$_2$ and graphene-based flat-band ferromagnets."}
{"id": "2512.13863", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.13863", "abs": "https://arxiv.org/abs/2512.13863", "authors": ["Alex L. Wang"], "title": "Optimal Subgradient Methods for Lipschitz Convex Optimization with Error Bounds", "comment": null, "summary": "We study the iteration complexity of Lipschitz convex optimization problems satisfying a general error bound. We show that for this class of problems, subgradient descent with either Polyak stepsizes or decaying stepsizes achieves minimax optimal convergence guarantees for decreasing distance-to-optimality. The main contribution is a novel lower-bounding argument that produces hard functions simultaneously satisfying zero-chain conditions and global error bounds."}
{"id": "2512.13790", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.13790", "abs": "https://arxiv.org/abs/2512.13790", "authors": ["Yannick Stade", "Lukas Burgholzer", "Robert Wille"], "title": "Search Smarter, Not Harder: A Scalable, High-Quality Zoned Neutral Atom Compiler", "comment": "7 pages, 8 figures", "summary": "Zoned neutral atom architectures are emerging as a promising platform for large-scale quantum computing. Their growing scale, however, creates a critical need for efficient and automated compilation solutions. Yet, existing methods fail to scale to the thousands of qubits these devices promise. State-of-the-art compilers, in particular, suffer from immense memory requirements that limit them to small-scale problems. This work proposes a scalable compilation strategy that \"searches smarter, not harder\". We introduce Iterative Diving Search (IDS), a goal-directed search algorithm that avoids the memory issues of previous methods, and relaxed routing, an optimization to mitigate atom rearrangement overhead. Our evaluation confirms that this approach compiles circuits with thousands of qubits and, in addition, even reduces rearrangement overhead by 28.1% on average. The complete code is publicly available in open-source as part of the Munich Quantum Toolkit (MQT) at https://github.com/munich-quantum-toolkit/qmap."}
{"id": "2512.13761", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.13761", "abs": "https://arxiv.org/abs/2512.13761", "authors": ["V. S. Ryumshin"], "title": "Monte Carlo study of phase transitions in model orthonickelate", "comment": null, "summary": "The results of numerical simulation using a classical Monte Carlo method with a kinematic accounting of the bosons concentration for a pseudospin model of orthonickelates are presented. Type of the phase transitions of the model orthonickelates is investigated."}
{"id": "2512.14169", "categories": ["nlin.CD", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14169", "abs": "https://arxiv.org/abs/2512.14169", "authors": ["Parsa Kafashi", "Mozhgan Orujlu"], "title": "Quantum-Inspired Approach to Analyzing Complex System Dynamics", "comment": null, "summary": "We present a quantum information-inspired framework for analyzing complex systems through multivariate time series. In this approach the system's state is encoded into a density matrix, providing a compact representation of higher-order correlations and dependencies. This formulation enables precise quantification of the relative influence among time series, tracking of their response to external perturbations and also the definition of a recovery timescale without need for dimensional reduction. By leveraging tools such as fidelity from quantum information theory, our method naturally captures higher-order co-fluctuations beyond pairwise statistics, offering a holistic characterization of resilience and similarity in high-dimensional dynamics. We validate this approach on synthetic data generated by a 9-dimensional modified Lorenz-96 model and demonstrate its utility on real-world climate data, analyzing global temperature anomalies across nine regions, quantifying the dissimilarity of each 288-month time window up to July 2025 relative to the 1850-1874 baseline period."}
{"id": "2512.14128", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14128", "abs": "https://arxiv.org/abs/2512.14128", "authors": ["Xiaojie Tao", "Rajit Gadh"], "title": "Fast Frequency Response Potential of Data Centers through Workload Modulation and UPS Coordination", "comment": null, "summary": "The rapid growth of renewable energy sources has significantly reduced system inertia and increased the need for fast frequency response (FFR) in modern power systems. Data centers, as large and flexible electrical consumers, hold great potential to contribute to frequency stabilization due to their controllable IT workloads and on-site uninterruptible power supply (UPS) systems. This paper investigates the feasibility of leveraging data centers for providing fast frequency response through real-time workload modulation and UPS coordination. A dynamic model combining data center power consumption and grid frequency dynamics is developed, capturing the interactions between IT servers, cooling systems, and energy storage. Control strategies based on frequency deviation are implemented to adjust server power and discharge UPS batteries during frequency events. Case studies on a modified IEEE 39-bus system demonstrate that the proposed strategy can effectively reduce frequency nadir and shorten recovery time without compromising service quality. The results highlight the promising role of data centers as grid-supporting resources in future low-inertia systems."}
{"id": "2512.13868", "categories": ["eess.SY", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13868", "abs": "https://arxiv.org/abs/2512.13868", "authors": ["Tianyu Zhou", "Zihao Liang", "Zehui Lu", "Shaoshuai Mou"], "title": "Safe Online Control-Informed Learning", "comment": null, "summary": "This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems."}
{"id": "2512.13975", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.13975", "abs": "https://arxiv.org/abs/2512.13975", "authors": ["Marc Dambrine", "Helmut Harbrecht"], "title": "An inverse problem for the one-phase Stefan problem with varying melting temperature", "comment": null, "summary": "The present article is dedicated to the forward and backward solution of a transient one-phase Stefan problem. In the forward problem, we compute the evolution of the initial domain for a Stefan problem where the melting temperature varies over time. This occurs in practice, for example, when the pressure in the external space changes in time. In the corresponding backward problem, we then reconstruct the time-dependent melting temperature from the knowledge of the evolving geometry. We develop respective numerical algorithms using a moving mesh finite element method and provide numerical simulations."}
{"id": "2512.14337", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2512.14337", "abs": "https://arxiv.org/abs/2512.14337", "authors": ["T. Tony Cai", "Abhinav Chakraborty", "Lasse Vuursteen"], "title": "The Cost of Adaptation under Differential Privacy: Optimal Adaptive Federated Density Estimation", "comment": "Main article is 24 pages, 1 figure, 26 page supplement", "summary": "Privacy-preserving data analysis has become a central challenge in modern statistics. At the same time, a long-standing goal in statistics is the development of adaptive procedures -- methods that achieve near-optimal performance across diverse function classes without prior knowledge of underlying smoothness or complexity. While adaptation is often achievable at no extra cost in the classical non-private setting, this naturally raises a fundamental question: to what extent is adaptation still possible under privacy constraints?\n  We address this question in the context of density estimation under federated differential privacy (FDP), a framework that encompasses both central and local DP models. We establish sharp results that characterize the cost of adaptation under FDP for both global and pointwise estimation, revealing fundamental differences from the non-private case. We then propose an adaptive FDP estimator that achieves explicit performance guarantees by introducing a new noise mechanism, enabling one-shot adaptation via post-processing. This approach strictly improves upon existing adaptive DP methods. Finally, we develop new lower bound techniques that capture the limits of adaptive inference under privacy and may be of independent interest beyond this problem.\n  Our findings reveal a sharp contrast between private and non-private settings. For global estimation, where adaptation can be achieved for free in the classical non-private setting, we prove that under FDP an intrinsic adaptation cost is unavoidable. For pointwise estimation, where a logarithmic penalty is already known to arise in the non-private setting, we show that FDP introduces an additional logarithmic factor, thereby compounding the cost of adaptation. Taken together, these results provide the first rigorous characterization of the adaptive privacy-accuracy trade-off."}
{"id": "2512.13828", "categories": ["quant-ph", "physics.comp-ph", "physics.optics", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2512.13828", "abs": "https://arxiv.org/abs/2512.13828", "authors": ["Artur Czerwinski", "Jakub J. Borkowski", "Saeed Haddadi"], "title": "Optical Downlink Modeling for LEO and MEO Satellites under Atmospheric Turbulence with a Quantum State Tomography Use Case", "comment": null, "summary": "This paper presents a comprehensive analysis of the link budget for free-space optical systems involving Low Earth Orbit (LEO) and Medium Earth Orbit (MEO) satellites. We develop a detailed model of the satellite-to-ground channel that accounts for the primary physical processes affecting transmittance: atmospheric absorption and scattering, free-space diffraction, and turbulence-induced fluctuations. The study introduces a general method for computing transmittance along a slant path between a satellite and an optical ground station, incorporating zenith angle, slant range, and altitude-dependent attenuation. The proposed framework is intended to support the design and evaluation of space-based optical links and serves as a critical tool for defining technical specifications in satellite communication demonstrators and simulations. Numerical estimates are provided to illustrate the magnitude of losses under typical operational conditions, including the role of aperture averaging. In addition to the link budget analysis, we introduce a satellite-based quantum use case. We propose a scheme for quantum state tomography performed on states generated by an onboard photon source on an LEO or MEO satellite and transmitted to the optical ground station. This approach enables continuous verification of the quality of quantum resources that can be used to perform quantum protocols within quantum information networks."}
{"id": "2512.13939", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13939", "abs": "https://arxiv.org/abs/2512.13939", "authors": ["Alessandro Casa", "Thomas Brendan Murphy", "Michael Fop"], "title": "A latent variable model for identifying and characterizing food adulteration", "comment": null, "summary": "Recently, growing consumer awareness of food quality and sustainability has led to a rising demand for effective food authentication methods. Vibrational spectroscopy techniques have emerged as a promising tool for collecting large volumes of data to detect food adulteration. However, spectroscopic data pose significant challenges from a statistical viewpoint, highlighting the need for more sophisticated modeling strategies. To address these challenges, in this work we propose a latent variable model specifically tailored for food adulterant detection, while accommodating the features of spectral data. Our proposal offers greater granularity with respect to existing approaches, since it does not only identify adulterated samples but also estimates the level of adulteration, and detects the spectral regions most affected by the adulterant. Consequently, the methodology offers deeper insights, and could facilitate the development of portable and faster instruments for efficient data collection in food authenticity studies. The method is applied to both synthetic and real honey mid-infrared spectroscopy data, delivering precise estimates of the adulteration level and accurately identifying which portions of the spectra are most impacted by the adulterant."}
{"id": "2512.14153", "categories": ["hep-lat", "hep-th"], "pdf": "https://arxiv.org/pdf/2512.14153", "abs": "https://arxiv.org/abs/2512.14153", "authors": ["Michael Mandl", "Erhard Seiler", "Dénes Sexty"], "title": "Complex Langevin simulations with a kernel", "comment": "9 pages, 2 figures; Proceedings of the 42nd International Symposium on Lattice Field Theory (LATTICE2025), 2-8 November 2025, Mumbai, India", "summary": "We discuss recent developments regarding the use of kernels in complex Langevin simulations. In particular, we outline how a kernel can be used to solve the problem of wrong convergence in a simple toy model. Since conventional correctness criteria for complex Langevin results are only necessary but not sufficient, the correct convergence of complex Langevin simulations is not always straightforward to assess. Hence, we furthermore discuss a condition for correctness that we have recently derived, which is both necessary and sufficient. Finally, we outline a machine-learning approach for finding suitable kernels in lattice gauge theories and present preliminary results of its application to the heavy-dense limit of QCD."}
{"id": "2512.13987", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.13987", "abs": "https://arxiv.org/abs/2512.13987", "authors": ["Bryn Ward-Leikis", "Neelesh Rampal", "Yun Sing Koh", "Peter B. Gibson", "Hong-Yang Liu", "Vassili Kitsios", "Tristan Meyers", "Jeff Adie", "Yang Juntao", "Steven C. Sherwood"], "title": "An intercomparison of generative machine learning methods for downscaling precipitation at fine spatial scales", "comment": "26 pages, 7 figures", "summary": "Machine learning (ML) offers a computationally efficient approach for generating large ensembles of high-resolution climate projections, but deterministic ML methods often smooth fine-scale structures and underestimate extremes. While stochastic generative models show promise for predicting fine-scale weather and extremes, few studies have compared their performance under present-day and future climates. This study compares a previously developed conditional Generative Adversarial Network (cGAN) with an intensity constraint against different configurations of diffusion models for downscaling daily precipitation from a regional climate model (RCM) over Aotearoa New Zealand. Model skill is comprehensively assessed across spatial structure, distributional metrics, means, extremes, and their respective climate change signals. Both generative approaches outperform the deterministic baseline across most metrics and exhibit similar overall skill. Diffusion models better predict the fine-scale spatial structure of precipitation and the length of dry spells, but underestimate climate change signals for extreme precipitation compared to the ground truth RCMs. In contrast, cGANs achieve comparable skill for most metrics while better predicting the overall precipitation distribution and climate change responses for extremes at a fraction of the computational cost. These results demonstrate that while diffusion models can readily generate predictions with greater visual \"realism\", they do not necessarily better preserve climate change responses compared to cGANs with intensity constraints. At present, incorporating constraints into diffusion models remains challenging compared to cGANs, but may represent an opportunity to further improve skill for predicting climate change responses."}
{"id": "2512.13745", "categories": ["quant-ph", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13745", "abs": "https://arxiv.org/abs/2512.13745", "authors": ["Xiuying Zhang", "Qinsheng Zhu", "Xiaodong Xing"], "title": "A Spatio-Temporal Hybrid Quantum-Classical Graph Convolutional Neural Network Approach for Urban Taxi Destination Prediction", "comment": null, "summary": "We propose a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) algorithm by combining the strengths of quantum computing and classical deep learning to predict the taxi destination within urban road networks. Our algorithm consists of two branches: spatial processing and time evolution. Regarding the spatial processing, the classical module encodes the local topological features of the road network based on the GCN method, and the quantum module is designed to map graph features onto parameterized quantum circuits through a differentiable pooling layer. The time evolution is solved by integrating multi-source contextual information and capturing dynamic trip dependencies on the classical TCN theory. Finally, our experimental results demonstrate that the proposed algorithm outperforms the current methods in terms of prediction accuracy and stability, validating the unique advantages of the quantum-enhanced mechanism in capturing high-dimensional spatial dependencies."}
{"id": "2512.13845", "categories": ["cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.13845", "abs": "https://arxiv.org/abs/2512.13845", "authors": ["Lars T. Kyllingstad"], "title": "Co-simulation errors due to step size changes", "comment": "17 pages, 10 figures. Code to perform simulations and produce plots is available at https://doi.org/10.60609/6p8m-0713", "summary": "When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps."}
{"id": "2512.13793", "categories": ["cond-mat.str-el", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2512.13793", "abs": "https://arxiv.org/abs/2512.13793", "authors": ["Hui Yang", "Ya-Hui Zhang"], "title": "Magnetism and superconductivity in bilayer nickelate", "comment": "12 pages, 13 figures", "summary": "The discovery of high-temperature superconductivity in bilayer nickelate La$_{3}$Ni$_{2}$O$_{7}$ necessitates a minimal theoretical model that unifies the superconducting phase with the spin-density-wave (SDW) phase without external pressure or strain. We propose a model where half-filled $d_{z^{2}}$ local moments interact with itinerant $d_{x^{2}-y^{2}}$ electrons via strong Hund's coupling $J_H$, which reduces to a bilayer type-II t-J model in the large $J_H$ limit. Using iDMRG calculations on an $L_y=4, L_z=2$ cylinder, we demonstrate that the competition between double-exchange ferromagnetism and in-plane superexchange generates period-4 stripe-like SDW order-a feature absent in one-orbital t-J model with only $d_{x^2-y^2}$ orbital. Furthermore, increasing the interlayer exchange coupling suppresses magnetic order and stabilizes interlayer s-wave superconductivity. These results identify the type-II t-J model as a minimal framework for capturing the interplay of magnetism and superconductivity in bilayer nickelates."}
{"id": "2512.13920", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.13920", "abs": "https://arxiv.org/abs/2512.13920", "authors": ["Haoyuan Cai", "Sulaiman A. Alghunaim", "Ali H. Sayed"], "title": "DAMA: A Unified Accelerated Approach for Decentralized Nonconvex Minimax Optimization-Part I: Algorithm Development and Results", "comment": null, "summary": "In this work and its accompanying Part II [1], we develop an accelerated algorithmic framework, DAMA (Decentralized Accelerated Minimax Approach), for nonconvex Polyak-Lojasiewicz minimax optimization over decentralized multi-agent networks. Our approach integrates online and offline stochastic minimax algorithms with various decentralized learning strategies, yielding a versatile framework with broader flexibility than existing methods. Our unification is threefold: (i) we propose a unified decentralized learning strategy for minimax optimization that subsumes existing bias-correction techniques, such as gradient tracking, while introducing new variants that achieve tighter network-dependent bounds; (ii) we introduce a probabilistic gradient estimator, GRACE (Gradient Acceleration Estimator), which unifies momentum-based methods and loopless variance-reduction techniques for constructing accelerated gradients within DAMA, and is broadly applicable to general stochastic optimization problems; and (iii) we develop a unified analytical framework that establishes a general performance bound for DAMA, achieving state-of-the-art results with the best-known sample complexity. To the best of our knowledge, DAMA is the first framework to achieve a multi-level unification of decentralized learning strategies and accelerated gradient techniques. This work focuses on algorithm development and the main results, while Part II provides the theoretical analysis that substantiates these results and presents empirical validation across diverse network topologies using synthetic and real-world datasets."}
{"id": "2512.14541", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.14541", "abs": "https://arxiv.org/abs/2512.14541", "authors": ["Subrata Das", "Archisman Ghosh", "Swaroop Ghosh"], "title": "A Graph-Based Forensic Framework for Inferring Hardware Noise of Cloud Quantum Backend", "comment": "11 pages, 5 figures, conference", "summary": "Cloud quantum platforms give users access to many backends with different qubit technologies, coupling layouts, and noise levels. The execution of a circuit, however, depends on internal allocation and routing policies that are not observable to the user. A provider may redirect jobs to more error-prone regions to conserve resources, balance load or for other opaque reasons, causing degradation in fidelity while still presenting stale or averaged calibration data. This lack of transparency creates a security gap: users cannot verify whether their circuits were executed on the hardware for which they were charged. Forensic methods that infer backend behavior from user-visible artifacts are therefore becoming essential. In this work, we introduce a Graph Neural Network (GNN)-based forensic framework that predicts per-qubit and per-qubit link error rates of an unseen backend using only topology information and aggregated features extracted from transpiled circuits. We construct a dataset from several IBM 27-qubit devices, merge static calibration features with dynamic transpilation features and train separate GNN regressors for one- and two-qubit errors. At inference time, the model operates without access to calibration data from the target backend and reconstructs a complete error map from the features available to the user. Our results on the target backend show accurate recovery of backend error rate, with an average mismatch of approximately 22% for single-qubit errors and 18% for qubit-link errors. The model also exhibits strong ranking agreement, with the ordering induced by predicted error values closely matching that of the actual calibration errors, as reflected by high Spearman correlation. The framework consistently identifies weak links and high-noise qubits and remains robust under realistic temporal noise drift."}
{"id": "2512.13873", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.13873", "abs": "https://arxiv.org/abs/2512.13873", "authors": ["Nisarg Bhatt", "Purnendu Das", "Subroto Mukerjee", "Sriram Ramaswamy"], "title": "Conservation laws and chaos propagation in a non-reciprocal classical magnet", "comment": null, "summary": "We study a nonreciprocal generalization [EPL 60, 418 (2002)] of the classical Heisenberg spin chain, in which the exchange coupling is nonsymmetric, and show that it displays a ballistic spreading of chaos as measured by the decorrelator. We show that the interactions are reciprocal in terms of transformed variables, with conserved quantities that can be identified as magnetization and energy, with a Poisson-bracket algebra and Hamiltonian dynamics. For strictly antisymmetric couplings in the original model the conserved quantities diffuse, the decorrelator spreads symmetrically, and a simple hydrodynamic theory emerges. The general case in which the interaction has symmetric and antisymmetric parts presents complexities in the limit of large scales. Ballistic propagation of chaos survives the inclusion of interactions beyond nearest neighbours, but the conservation laws in general do not."}
{"id": "2512.13897", "categories": ["physics.geo-ph", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2512.13897", "abs": "https://arxiv.org/abs/2512.13897", "authors": ["Taylan Demir", "Atakan Koçyiğit"], "title": "Seismic wave propagation in viscoelastic media under Atangana-Baleanu fractional dynamics: Model formulation and numerical simulations", "comment": "10 pages, 4 figures", "summary": "We propose a one-dimensional viscoelastic seismic-wave model driven by the Atangana-BaleanuCaputo fractional derivative with a non-singular Mittag-Leffler kernel. A finite-difference discretization in space and an Adams-Bashforth-Moulton predictor-corrector scheme in time are used to compute solutions for several fractional orders. Simulations indicate that fractional memory alters both attenuation and dispersion, leading to non-exponential energy decay compared with the classical integer-order case."}
{"id": "2512.13825", "categories": ["cond-mat.dis-nn", "cond-mat.mes-hall", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13825", "abs": "https://arxiv.org/abs/2512.13825", "authors": ["Jacob Taylor", "Haining Pan", "Sankar Das Sarma"], "title": "Unreasonable effectiveness of unsupervised learning in identifying Majorana topology", "comment": "7 pages, 4 figures", "summary": "In unsupervised learning, the training data for deep learning does not come with any labels, thus forcing the algorithm to discover hidden patterns in the data for discerning useful information. This, in principle, could be a powerful tool in identifying topological order since topology does not always manifest in obvious physical ways (e.g., topological superconductivity) for its decisive confirmation. The problem, however, is that unsupervised learning is a difficult challenge, necessitating huge computing resources, which may not always work. In the current work, we combine unsupervised and supervised learning using an autoencoder to establish that unlabeled data in the Majorana splitting in realistic short disordered nanowires may enable not only a distinction between `topological' and `trivial', but also where their crossover happens in the relevant parameter space. This may be a useful tool in identifying topology in Majorana nanowires."}
{"id": "2512.14319", "categories": ["physics.soc-ph", "cond-mat.dis-nn", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14319", "abs": "https://arxiv.org/abs/2512.14319", "authors": ["Marc Barthelemy"], "title": "Not all Chess960 positions are equally complex", "comment": "11 pages, 7 figures", "summary": "We analyze strategic complexity across all 960 Chess960 (Fischer Random Chess) starting positions. Stockfish evaluations show a near-universal first-move advantage for White ($\\langle E \\rangle = +0.30 \\pm 0.14$ pawns), indicating that the advantage conferred by moving first is a robust structural feature of the game. To quantify decision difficulty, we introduce an information-based measure $S(n)$ describing the cumulative information required to identify optimal moves over the first $n$ plies. This measure decomposes into contributions from White and Black, $S_W$ and $S_B$, yielding a total opening complexity $S_{\\mathrm{tot}} = S_W + S_B$ and a decision asymmetry $A=S_B-S_W$. Across the ensemble, $S_{\\mathrm{tot}}$ varies by a factor of three, while $A$ spans from $-2.5$ to $+1.8$ bits, showing that some openings burden White and others Black. The mean $\\langle A \\rangle = -0.25$ bits indicates a slight tendency for White to face harder opening decisions. Standard chess (position \\#518, \\texttt{RNBQKBNR}) exhibits above-average asymmetry (91st percentile) but typical overall complexity (47th percentile). The most complex opening is \\#226 (\\texttt{BNRQKBNR}), whereas \\#198 (\\texttt{QNBRKBNR})is the most balanced, with both evaluation and asymmetry near zero. These results reveal a highly heterogeneous Chess960 landscape in which small rearrangements of the back-rank pieces can significantly alter strategic depth and competitive fairness. Remarkably, the classical starting position-despite centuries of cultural selection-lies far from the most balanced configuration."}
{"id": "2512.14136", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14136", "abs": "https://arxiv.org/abs/2512.14136", "authors": ["Xiaojie Tao", "Rajit Gadh"], "title": "Coordinated Fast Frequency Response from Electric Vehicles, Data Centers, and Battery Energy Storage Systems", "comment": null, "summary": "High renewable penetration has significantly reduced system inertia in modern power grids, increasing the need for fast frequency response (FFR) from distributed and non-traditional resources. While electric vehicles (EVs), data centers, and battery energy storage systems (BESS) have each demonstrated the capability to provide sub-second active power support, their combined frequency response potential has not been systematically evaluated. This paper proposes a coordinated control framework that aggregates these heterogeneous resources to provide fast, stable, and reliable FFR. Dynamic models for EV fleets, data center UPS and workload modulation, and BESS are developed, explicitly capturing their response times, power limits, and operational constraints. A hierarchical control architecture is introduced, where an upper-level coordinator dynamically allocates FFR among resources based on response speed and available capacity, and lower-level controllers implement the actual power response. Case studies based on the IEEE 39-bus test system demonstrate that the coordinated EV-DC-BESS framework improves frequency nadir by up to 0.2 Hz, reduces RoCoF, and accelerates frequency recovery compared with single-resource FFR. Results confirm that synergistic coordination significantly enhances grid stability, especially in low-inertia scenarios. This work highlights the value of multi-resource aggregation for future frequency regulation markets in renewable-dominated grids."}
{"id": "2512.13871", "categories": ["eess.SY", "cs.HC", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13871", "abs": "https://arxiv.org/abs/2512.13871", "authors": ["Shaun Sweeney", "Robert Shorten", "Mark O'Malley"], "title": "A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory", "comment": "PhD thesis", "summary": "This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.\n  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.\n  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.\n  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.\n  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants."}
{"id": "2512.13993", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13993", "abs": "https://arxiv.org/abs/2512.13993", "authors": ["Nicholas J. E. Richardson", "Noah Marusenko", "Michael P. Friedlander"], "title": "Multiple Scale Methods For Optimization Of Discretized Continuous Functions", "comment": "25 pages, 8 figures, supplemental materials is 28 pages", "summary": "A multiscale optimization framework for problems over a space of Lipschitz continuous functions is developed. The method solves a coarse-grid discretization followed by linear interpolation to warm-start project gradient descent on progressively finer grids. Greedy and lazy variants are analyzed and convergence guarantees are derived that show the multiscale approach achieves provably tighter error bounds at lower computational cost than single-scale optimization. The analysis extends to any base algorithm with iterate convergence at a fixed rate. Constraint modification techniques preserve feasibility across scales. Numerical experiments on probability density estimation problems, including geological data, demonstrate speedups of an order of magnitude or better."}
{"id": "2512.14473", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2512.14473", "abs": "https://arxiv.org/abs/2512.14473", "authors": ["Guillaume Lecué", "Zhifan Li", "Zong Shang"], "title": "Sharp convergence rates for Spectral methods via the feature space decomposition method", "comment": null, "summary": "In this paper, we apply the Feature Space Decomposition (FSD) method developed in [LS24, GLS25, ALSS26] to obtain, under fairly general conditions, matching upper and lower bounds for the population excess risk of spectral methods in linear regression under the squared loss, for every covariance and every signal. This result enables us, for a given linear regression problem, to define a partial order on the set of spectral methods according to their convergence rates, thereby characterizing which spectral algorithm is superior for that specific problem. Furthermore, this allows us to generalize the saturation effect proposed in inverse problems and to provide necessary and sufficient conditions for its occurrence. Our method also shows that, under broad conditions, any spectral algorithm lacks a feature learning property, and therefore cannot overcome the barrier of the information exponent in problems such as single-index learning."}
{"id": "2512.13944", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13944", "abs": "https://arxiv.org/abs/2512.13944", "authors": ["Souhardya Sengupta", "Kosuke Imai", "Georgia Papadogeorgou"], "title": "Low-rank Covariate Balancing Estimators under Interference", "comment": null, "summary": "A key methodological challenge in observational studies with interference between units is twofold: (1) each unit's outcome may depend on many others' treatments, and (2) treatment assignments may exhibit complex dependencies across units. We develop a general statistical framework for constructing robust causal effect estimators to address these challenges. We first show that, without restricting the patterns of interference, the standard inverse probability weighting (IPW) estimator is the only uniformly unbiased estimator when the propensity score is known. In contrast, no estimator has such a property if the propensity score is unknown. We then introduce a \\emph{low-rank structure} of potential outcomes as a broad class of structural assumptions about interference. This framework encompasses common assumptions such as anonymous, nearest-neighbor, and additive interference, while flexibly allowing for more complex study-specific interference assumptions. Under this low-rank assumption, we show how to construct an unbiased weighting estimator for a large class of causal estimands. The proposed weighting estimator does not require knowledge of true propensity scores and is therefore robust to unknown treatment assignment dependencies that often exist in observational studies. If the true propensity score is known, we can obtain an unbiased estimator that is more efficient than the IPW estimator by leveraging a low-rank structure. We establish the finite sample and asymptotic properties of the proposed weighting estimator, develop a data-driven procedure to select among candidate low-rank structures, and validate our approach through simulation and empirical studies."}
{"id": "2512.14267", "categories": ["physics.ao-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.14267", "abs": "https://arxiv.org/abs/2512.14267", "authors": ["Shruti Tandon", "Apoorva Singh", "B. N. Goswami", "R. I. Sujith"], "title": "Large-scale patterns of small-scale vorticity interactions foster moist convection during cyclogenesis", "comment": null, "summary": "The formation and intensification of a tropical cyclone is a complex phenomenon involving several feedback interactions between momentum and energetics of the storm, and across multiple spatio-temporal scales. Background vorticity interactions in the turbulent atmosphere play a crucial role in the formation of cyclones. How these vorticity interactions lead to convective organization and sustain a disastrous cyclonic vortex amidst a turbulent atmosphere remains elusive. Moreover, what processes distinguish depressions that develop into a cyclone from those that do not? Here, we investigate the role of small-scale vorticity interactions in the background flow in sustaining large-scale organization during the emergence of a cyclone. We construct time-varying complex networks where geographical locations are nodes and connections between nodes represent short-time vorticity correlations. Only those nodes are connected that are in spatial proximity corresponding to sub-meso length scales. Each network is constructed for 29 hours of data; consecutive networks are separated by three hours, thus revealing the evolution of local coherence in vorticity dynamics. We discover that small-scale vorticity interactions manifest as large-scale emergent patterns. Further, we establish that organized moist convection is significantly correlated to regions of locally coherent vorticity dynamics during the intensification of a depression that forms a cyclone; however, such correlations are not sustained during non-developing cases. Using modal analysis of time-evolving network connectivity, we show that these large-scale patterns are essentially large-scale modes of propagation of coherence in small-scale vorticity dynamics. We explain that such propagation is facilitated by moisture feedback at small-scales and self-organized patterns at large-scales."}
{"id": "2512.13774", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2512.13774", "abs": "https://arxiv.org/abs/2512.13774", "authors": ["Rahel Lea Baumgartner", "Pietro Pelliconi", "Soumik Bandyopadhyay", "Francesca Orsi", "Philipp Hauke", "Jean-Philippe Brantut", "Julian Sonner"], "title": "Quantum simulation using Trotterized disorder Hamiltonians in a single-mode optical cavity", "comment": "54 pages, 11 figures, plus appendices", "summary": "All-to-all interacting and disordered many-body systems are notoriously hard to simulate on quantum platforms, as interactions are commonly mediated by auxiliary degrees of freedom that lower the amount of disorder, introducing undesired correlations. In this work, we show how a Trotterization scheme can be effectively utilized to densify the disorder of the model. In particular, we study the statistical properties of the resulting model, as well as Trotterization errors in the simulation that affect the time evolution and dynamical observables. As a concrete example, we propose an implementation via a single-mode cavity QED platform of the complex Sachdev-Ye-Kitaev model. We analyze several features of the effective model, such as the distribution of the effective couplings, the number of interacting sites, state preparation, and the behavior of quantum chaos probes. We conclude this work with a detailed investigation of the robustness of our findings against dissipation, both analytically and numerically."}
{"id": "2512.14042", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.14042", "abs": "https://arxiv.org/abs/2512.14042", "authors": ["Ruize Gao", "Mei Yang", "Yu Wang", "Shaoze Cui"], "title": "Dynamic stacking ensemble learning with investor knowledge representations for stock market index prediction based on multi-source financial data", "comment": null, "summary": "The patterns of different financial data sources vary substantially, and accordingly, investors exhibit heterogeneous cognition behavior in information processing. To capture different patterns, we propose a novel approach called the two-stage dynamic stacking ensemble model based on investor knowledge representations, which aims to effectively extract and integrate the features from multi-source financial data. In the first stage, we identify different financial data property from global stock market indices, industrial indices, and financial news based on the perspective of investors. And then, we design appropriate neural network architectures tailored to these properties to generate effective feature representations. Based on learned feature representations, we design multiple meta-classifiers and dynamically select the optimal one for each time window, enabling the model to effectively capture and learn the distinct patterns that emerge across different temporal periods. To evaluate the performance of the proposed model, we apply it to predicting the daily movement of Shanghai Securities Composite index, SZSE Component index and Growth Enterprise index in Chinese stock market. The experimental results demonstrate the effectiveness of our model in improving the prediction performance. In terms of accuracy metric, our approach outperforms the best competing models by 1.42%, 7.94%, and 7.73% on the SSEC, SZEC, and GEI indices, respectively. In addition, we design a trading strategy based on the proposed model. The economic results show that compared to the competing trading strategies, our strategy delivers a superior performance in terms of the accumulated return and Sharpe ratio."}
{"id": "2512.14075", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14075", "abs": "https://arxiv.org/abs/2512.14075", "authors": ["Toshiya Hikihara", "Akira Furusaki"], "title": "Correlation functions at the topological quantum phase transition in the S=1 XXZ chain with single-ion anisotropy", "comment": "14 pages, 12 figures, 1 table", "summary": "We study the one-dimensional S=1 XXZ spin model with single-ion anisotropy. It is known that at the transition points between the Haldane and large-D phases, the model exhibits a quantum criticality described by the Gaussian theory, i.e., a conformal field theory with the central charge c=1. Using the bosonization approach, we investigate various correlation functions at the phase transition and derive their asymptotic forms. This allows us to clarify their peculiar behavior: the longitudinal (transverse) two-point spin correlation function has components that decay algebraically only in the uniform (staggered) sector. These theoretical predictions are verified by the numerical calculations using the density-matrix renormalization group method. The effect of weak bond alternation on the critical ground state at the phase transition is also discussed. It is shown that the bond alternation induces the missing power-law components in the correlation functions."}
{"id": "2512.13923", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.13923", "abs": "https://arxiv.org/abs/2512.13923", "authors": ["Haoyuan Cai", "Sulaiman A. Alghunaim", "Ali H. Sayed"], "title": "DAMA: A Unified Accelerated Approach for Decentralized Nonconvex Minimax Optimization-Part II: Convergence and Performance Analyses", "comment": null, "summary": "In Part I of this work [1], we developed an accelerated algorithmic framework, DAMA (Decentralized Accelerated Minimax Approach), for nonconvex Polyak-Lojasiewicz (PL) minimax optimization over decentralized multi-agent networks. To further enhance convergence in online and offline scenarios, Part I of this work [1] also proposed a novel accelerated gradient estimator, namely, GRACE (GRadient ACceleration Estimator), which unifies several momentum-based methods (e.g., STORM) and loopless variance-reduction techniques (e.g., PAGE, Loopless SARAH), thereby enabling accelerated gradient updates within DAMA. Part I reported a unified performance bound for DAMA and refined guarantees for specific algorithmic instances, demonstrating the superior performance of several new variants on sparsely connected networks. In this Part II, we focus on the convergence and performance bounds that substantiate the main results presented in Part I [1]. In particular, we establish a unified performance bound for DAMA using the transformed recursion derived in Part I and subsequently refine this bound for its various special cases."}
{"id": "2512.13883", "categories": ["cond-mat.stat-mech", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.13883", "abs": "https://arxiv.org/abs/2512.13883", "authors": ["Philipp Fleig"], "title": "Renormalization group for spectral collapse in random matrices with power-law variance profiles", "comment": "18 pages, 9 figures", "summary": "We propose a renormalization group (RG) approach to compare and collapse eigenvalue densities of random matrix models of complex systems across different system sizes. The approach is to fix a natural spectral scale by letting the model normalization run with size, turning raw spectra into comparable, collapsed density curves. We demonstrate this approach on generalizations of two classic random matrix ensembles--Wigner and Wishart--modified to have power-law variance profiles. We use random matrix theory methods to derive self-consistent fixed-point equations for the resolvent to compute their eigenvalue densities, we define an RG scheme based on matrix decimation, and compute the Beta function controlling the RG flow as a function of the variance profile power-law exponent. The running normalization leads to spectral collapse which we confirm in simulations and solutions of the fixed-point equations. We expect this RG approach to carry over to other ensembles, providing a method for data analysis of a broad range of complex systems."}
{"id": "2512.13985", "categories": ["physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2512.13985", "abs": "https://arxiv.org/abs/2512.13985", "authors": ["Altamirano-Muñiz Emilio Fernando"], "title": "A Generalized Formulation for Accurate and Robust Determination of Soil Shear Strength from Triaxial Tests", "comment": null, "summary": "This work presents an extended formulation of the Least Squares with Virtual Displacements (LSVD) method for estimating shear strength parameters from multiple soil samples under varying resistance conditions including cohesionless, frictional, and mixed types. LSVD is designed to identify a common tangent across n Mohr circles, even in the presence of measurement errors that render an exact solution infeasible. Beyond its original linear formulation, we introduce generalized LSVD variants like logarithmic, parabolic, polynomial, power law and generalized forms allowing the method to adapt to diverse failure envelope shapes observed in geotechnical materials. We benchmark these variants against established approaches such as the p-q method and CTPAC, analyzing performance under synthetic noise to simulate measurement uncertainty. This provides a comparative framework to assess each method's robustness, especially considering their differing selections of representative points on the Mohr circles. The results highlight LSVD's flexibility and reliability in modeling complex soil behavior and suggest its potential as a versatile tool for geomechanical analysis."}
{"id": "2512.14526", "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2512.14526", "abs": "https://arxiv.org/abs/2512.14526", "authors": ["Debendra Meher", "Nikhil V. S. Avula", "Sundaram Balasubramanian"], "title": "On the Boroxol Ring Fraction in Melt-Quenched B$_2$O$_3$ Glass", "comment": null, "summary": "An atomistic structural model for melt-quenched B$_2$O$_3$ glass has eluded the simulation community so far. The difficulty lies in the abundance of the six-membered boroxol rings - an intermediate-range order motif suggested through Raman and NMR spectroscopy - which is challenging to obtain in atomistic molecular dynamics simulations. Here, we report the development of a DFT-accurate machine-learned potential for B$_2$O$_3$ and employ quench rates as low as 10$^{9}$ K/s to obtain B$_2$O$_3$ glasses with more than 30% of boron atoms in boroxol rings. Also, we show that the pressure, and consequently the boroxol fraction, in the deep potential molecular dynamics (DPMD) simulations critically depends on the range of the geometry descriptor used in the embedding neural network, and at least a 9 $\\unicode{x212B}$ range is required. The boroxol ring fraction increases with decreasing quench rate. Finally, amorphous B$_2$O$_3$ configurations display a minimum in energy at a boroxol fraction of 75%, intriguingly close to the experimental estimate in B$_2$O$_3$ glass."}
{"id": "2512.14175", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14175", "abs": "https://arxiv.org/abs/2512.14175", "authors": ["Lauritz Rismark Fosso", "Christian Holden", "Sveinung Johan Ohrem"], "title": "KalMRACO: Unifying Kalman Filter and Model Reference Adaptive Control for Robust Control and Estimation of Uncertain Systems", "comment": "7 pages, 4 figures", "summary": "A common assumption when applying the Kalman filter is a priori knowledge of the system parameters. These parameters are not necessarily known, and this may limit real-world applications of the Kalman filter. The well-established Model Reference Adaptive Controller (MRAC) utilizes a known reference model and ensures that the input-output behavior of a potentially unknown system converges to that of the reference model. We present KalMRACO, a unification of the Kalman filter and MRAC leveraging the reference model of MRAC as the Kalman filter system model, thus eliminating, to a large degree, the need for knowledge of the underlying system parameters in the application of the Kalman filter. We also introduce the concept of blending estimated states and measurements in the feedback law to handle stability issues during the initial transient. KalMRACO is validated through simulations and lab trials on an underwater vehicle. Results show superior tracking of the reference model state, observer state convergence, and noise mitigation properties."}
{"id": "2512.13940", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13940", "abs": "https://arxiv.org/abs/2512.13940", "authors": ["Ibon Gracia", "Morteza Lahijanian"], "title": "Data-Driven Control via Conditional Mean Embeddings: Formal Guarantees via Uncertain MDP Abstraction", "comment": null, "summary": "Controlling stochastic systems with unknown dynamics and under complex specifications is specially challenging in safety-critical settings, where performance guarantees are essential. We propose a data-driven policy synthesis framework that yields formal performance guarantees for such systems using conditional mean embeddings (CMEs) and uncertain Markov decision processes (UMDPs). From trajectory data, we learn the system's transition kernel as a CME, then construct a finite-state UMDP abstraction whose transition uncertainties capture learning and discretization errors. Next, we generate a policy with formal performance bounds through robust dynamic programming. We demonstrate and empirically validate our method through a temperature regulation benchmark."}
{"id": "2512.14089", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14089", "abs": "https://arxiv.org/abs/2512.14089", "authors": ["Taylan Demir", "Atakan Koçyiğit"], "title": "Adaptive Wavelet-Galerkin Modelling of Heat Conduction in Heterogeneous Composite Materials", "comment": "12 pages, 3 figures", "summary": "We present an adaptive wavelet Galerkin method for transient heat conduction in heterogeneous composite materials. The approach combines multiresolution wavelet bases with an implicit time discretization to efficiently resolve sharp temperature gradients near material interfaces and boundary layers. Adaptive refinement is driven by wavelet coefficients, significantly reducing the number of degrees of freedom compared to uniform discretizations. Numerical examples demonstrate accurate resolution of layered, inclusion-based, and functionally graded composites with improved computational efficiency."}
{"id": "2512.14567", "categories": ["math.ST", "cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.14567", "abs": "https://arxiv.org/abs/2512.14567", "authors": ["Timothy L. H. Wee", "Cheng Mao"], "title": "Cluster expansion of the log-likelihood ratio: Optimal detection of planted matchings", "comment": "81 pages, 9 figures", "summary": "To understand how hidden information can be extracted from statistical networks, planted models in random graphs have been the focus of intensive study in recent years. In this work, we consider the detection of a planted matching, i.e., an independent edge set, hidden in an Erdős-Rényi random graph, which is formulated as a hypothesis testing problem. We identify the critical regime for this testing problem and prove that the log-likelihood ratio is asymptotically normal. Via analyses of computationally efficient edge or wedge count test statistics that attain the optimal limits of detection, our results also reveal the absence of a statistical-to-computational gap. Our main technical tool is the cluster expansion from statistical physics, which allows us to prove a precise, non-asymptotic characterization of the log-likelihood ratio. Our analyses rely on a careful reorganization and cancellation of terms that occur in the difference between monomer-dimer log partition functions on the complete and Erdős-Rényi graphs. This combinatorial and statistical physics approach represents a significant departure from the more established methods such as orthogonal decompositions, and positions the cluster expansion as a viable technique in the study of log-likelihood ratios for planted models in general."}
{"id": "2512.13962", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.13962", "abs": "https://arxiv.org/abs/2512.13962", "authors": ["Benjamin Christoffersen", "Keith Humphreys", "Alessandro Gasparini", "Birzhan Akynkozhayev", "Hedvig Kjellström", "Mark Clements"], "title": "Joint Models with Multiple Markers and Multiple Time-to-event Outcomes Using Variational Approximations", "comment": null, "summary": "Joint models are well suited to modelling linked data from laboratories and health registers. However, there are few examples of joint models that allow for (a) multiple markers, (b) multiple survival outcomes (including terminal events, competing events, and recurrent events), (c) delayed entry and (d) scalability. We propose a full likelihood approach for joint models based on a Gaussian variational approximation to satisfy criteria (a)-(d). We provide an open-source implementation for this approach, allowing for flexible sets of models for the longitudinal markers and survival outcomes. Through simulations, we find that the lower bound for the variational approximation is close to the full likelihood. We also find that our approach and implementation are fast and scalable. We provide an application with a joint model for longitudinal measurements of dense and fatty breast tissue and time to first breast cancer diagnosis. The use of variational approximations provides a promising approach for extending current joint models."}
{"id": "2512.14304", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.14304", "abs": "https://arxiv.org/abs/2512.14304", "authors": ["Andrew Dowdy", "Andrew Brown", "Todd Lane", "Mateusz Taszarek"], "title": "Climatological variability of a thunderstorm environment dataset in tropical and temperate regions", "comment": "31 pages, 17 figures, 4 tables", "summary": "Spatiotemporal variations in thunderstorm occurrence frequency are considered here using an environmental dataset derived from ERA5 reanalysis data. Interannual variability in the thunderstorm environments is examined for the period 1979-2023, with the standard deviation and coefficient of variation showing considerable spatial differences through the world. Atmospheric and oceanic modes of climate variability account for some of this interannual variability, particularly for the El Nino-Southern Oscillation through tropical and maritime regions, as well as to a lesser degree for the Indian Ocean Dipole, Arctic Oscillation and Antarctic Oscillation. Long-term trends can also contribute to interannual variability, with results showing increases are more common than decreases in the thunderstorm environments through the study region over the period 1979-2023. However, considerable uncertainties in those trends are noted as is also suggested from some additional analysis of global climate models, indicating that although more favorable thunderstorm environments might occur in a warming world, the estimated change over the period 1979-2023 is relatively small compared to the standard deviation in most locations. The study findings are intended to be complementary to other studies and contribute as part of a broader range of information available on thunderstorms and climate variability."}
{"id": "2512.13777", "categories": ["quant-ph", "cond-mat.str-el", "hep-th", "math-ph"], "pdf": "https://arxiv.org/pdf/2512.13777", "abs": "https://arxiv.org/abs/2512.13777", "authors": ["Alison Warman", "Sakura Schafer-Nameki"], "title": "Transversal Clifford-Hierarchy Gates via Non-Abelian Surface Codes", "comment": "18 pages", "summary": "We present a purely 2D transversal realization of phase gates at any level of the Clifford hierarchy, and beyond, using non-Abelian surface codes. Our construction encodes a logical qubit in the quantum double $D(G)$ of a non-Abelian group $G$ on a triangular spatial patch. The logical gate is implemented transversally by stacking on the spatial region a symmetry-protected topological (SPT) phase specified by a group 2-cocycle. The Bravyi--König theorem limits the unitary gates implementable by constant-depth quantum circuits on Pauli stabilizer codes in $D$ dimensions to the $D$-th level of the Clifford hierarchy. We bypass this, by constructing transversal unitary gates at arbitrary levels of the Clifford hierarchy purely in 2D, without sacrificing locality or fault tolerance, however at the cost of using the quantum double of a non-Abelian group $G$. Specifically, for $G = D_{4N}$, the dihedral group of order $8N$, we realize the phase gate $T^{1/N} = \\mathrm{diag}(1, e^{iπ/(4N)})$ in the logical $\\overline{Z}$ basis. For $8N = 2^n$, this gate lies at the $n$-th level of the Clifford hierarchy and, importantly, has a qubit-only realization: we show that it can be constructed in terms of Clifford-hierarchy stabilizers for a code with $n$ physical qubits on each edge of the lattice. We also discuss code-switching to the $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ and $\\mathbb{Z}_2$ toric codes, which can be utilized for the quantum error correction in this setup."}
{"id": "2512.14161", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.14161", "abs": "https://arxiv.org/abs/2512.14161", "authors": ["Keiichi Ishikawa", "Yuma Matsumoto", "Taro Yaoyama", "Sangwon Lee", "Tatsuya Itoi"], "title": "Transfer Learning-Based Surrogate Modeling for Nonlinear Time-History Response Analysis of High-Fidelity Structural Models", "comment": "20 pages, 21 figures", "summary": "In a performance based earthquake engineering (PBEE) framework, nonlinear time-history response analysis (NLTHA) for numerous ground motions are required to assess the seismic risk of buildings or civil engineering structures. However, such numerical simulations are computationally expensive, limiting the real-world practical application of the framework. To address this issue, previous studies have used machine learning to predict the structural responses to ground motions with low computational costs. These studies typically conduct NLTHAs for a few hundreds ground motions and use the results to train and validate surrogate models. However, most of the previous studies focused on computationally-inexpensive response analysis models such as single degree of freedom. Surrogate models of high-fidelity response analysis are required to enrich the quantity and diversity of information used for damage assessment in PBEE. Notably, the computational cost of creating training and validation datasets increases if the fidelity of response analysis model becomes higher. Therefore, methods that enable surrogate modeling of high-fidelity response analysis without a large number of training samples are needed. This study proposes a framework that uses transfer learning to construct the surrogate model of a high-fidelity response analysis model. This framework uses a surrogate model of low-fidelity response analysis as the pretrained model and transfers its knowledge to construct surrogate models for high-fidelity response analysis with substantially reduced computational cost. As a case study, surrogate models that predict responses of a 20-story steel moment frame were constructed with only 20 samples as the training dataset. The responses to the ground motions predicted by constructed surrogate model were consistent with a site-specific time-based hazard."}
{"id": "2512.14149", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14149", "abs": "https://arxiv.org/abs/2512.14149", "authors": ["Yuki Miyazaki", "Shiori Tanigawa", "Giacomo Marmorini", "Nobuo Furukawa", "Daisuke Yamamoto"], "title": "A sine-square deformation approach to quantum critical points in one-dimensional systems", "comment": "23 pages, 11 figures", "summary": "We propose a method to determine the quantum phase boundaries of one-dimensional systems using sine-square deformation (SSD). Based on the proposition, supported by several exactly solved cases though not proven in full generality, that ``if a one-dimensional system is gapless, then the expectation value of any local observable in the ground state of the Hamiltonian with SSD exhibits translational symmetry in the thermodynamic limit,\" we determine the quantum critical point as the location where a local observable becomes site-independent, identified through finite-size scaling analysis. As case studies, we consider two models: the antiferromagnetic Ising chain in mixed transverse and longitudinal magnetic fields with nearest-neighbor and long-range interactions. We calculate the ground state of these Hamiltonians with SSD using the density-matrix renormalization-group algorithm and evaluate the local transverse magnetization. For the nearest-neighbor model, we show that the quantum critical point can be accurately estimated by our procedure with systems of up to 84 sites, or even smaller, in good agreement with results from the literature. For the long-range model, we find that the phase boundary between the antiferromagnetic and paramagnetic phases is slightly shifted relative to the nearest-neighbor case, leading to a reduced region of antiferromagnetic order. Moreover, we propose an experimental procedure to implement the antiferromagnetic $J_1$-$J_2$ Ising couplings with SSD using Rydberg atom arrays in optical tweezers, which can be achieved within a very good approximation. Because multiple independent scaling conditions naturally emerge, our approach enables precise determination of quantum critical points and possibly even the extraction of additional critical phenomena, such as critical exponents, from relatively small system sizes."}
{"id": "2512.13964", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.13964", "abs": "https://arxiv.org/abs/2512.13964", "authors": ["Lillian Makhoul", "Emily Speakman"], "title": "Volume Formulae for the Convex Hull of the Graph of a Trilinear Monomial: A Complete Characterization for General Box Domains", "comment": null, "summary": "Solving difficult mixed-integer nonlinear programs via spatial branch-and-bound requires effective convex outer-approximations of nonconvex sets. In this framework, complex problem formulations are decomposed into simpler library functions, whose relaxations are then composed to build relaxations of the overall problem. The trilinear monomial serves as one such fundamental library function, appearing frequently as a building block across diverse applications. By definition, its convex hull provides the tightest possible relaxation and thus serves as a benchmark for evaluating alternatives. Mixed volume techniques have yielded a parameterized volume formula for the convex hull of the graph of a trilinear monomial; however, existing results only address the case where all six bounds of the box domain are nonnegative. This restriction represents a notable gap in the literature, as variables with mixed-sign domains arise naturally in practice. In this work, we close the gap by extending to the general case via an exhaustive case analysis. We demonstrate that removing the nonnegative domain assumption alters the underlying structure of the convex hull polytope, leading to six distinct volume formulae that together characterize all possible parameter configurations."}
{"id": "2512.13925", "categories": ["cond-mat.stat-mech", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2512.13925", "abs": "https://arxiv.org/abs/2512.13925", "authors": ["C. Reichhardt", "C. J. O. Reichhardt"], "title": "Hysteresis, Laning, and Negative Drag in Binary Systems with Opposite and Perpendicular Driving", "comment": "15 pages, 28 postscript figures", "summary": "We consider a binary system of particles with repulsive interactions that move in opposite or perpendicular directions to each other under an applied external drive. For opposite driving, at higher drives a phase-separated laned state forms that has strong hysteresis in the velocity-force curve and the fraction of topological defects as the drive is cycled up and down from zero. The amount of hysteresis depends on the drive value at which the drive changes from increasing to decreasing. For perpendicular driving, we find a jammed state that transitions into a disordered state or a tilted lane state, both of which also show strong hysteresis effects. Additionally, a negative drag effect can appear in which one species moves in the direction opposite to the other species due to a tilting of the lanes by the perpendicular drive. When a constant drive is applied along one direction while the drive in the perpendicular direction is increased, we observe a series of drops and jumps in the velocity as the system forms locked and tilted laned states. For weakly interacting particles, the jammed system can show co-tilted stripe-forming states."}
{"id": "2512.14076", "categories": ["physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2512.14076", "abs": "https://arxiv.org/abs/2512.14076", "authors": ["Xun Cai", "Xiucheng Yang", "Peter A. Raymond"], "title": "Seesaw of Saltwater and Inundation Drives Methane Emissions in Coastal Tidal Wetlands", "comment": null, "summary": "Wetlands are significant carbon sinks, yet methane emissions partially offset this function due to its high global warming potential. Coastal tidal wetlands, unlike non-tidal wetlands, are regulated by oceanic drivers like salinity gradients and tidal inundation, which strongly influence methane production and release but remain poorly represented in regional assessments. Here, we estimate methane emissions from U.S. East Coast tidal marshes, by integrating ocean model, remote sensing datasets, empirical relationships from metadata. Spatially, emissions reflect the combined effects of marsh extent and per-unit-area flux rates, with hotspots occurring under lower salinity, higher inundation, and lower latitudes. Temporally, temperature and salinity dominate decadal-scale interannual variability. Between 2001 to 2020, total methane emissions are estimated at 0.019 - 0.038 Tg yr-1, with local fluxes rate ranging from 0 to 20 g m-2 day-1. Following pronounced hydrological variability in the early 2000s, emissions have increased steadily since 2007 at approximately 802 t yr-1, driven by warming, freshening, and enhanced inundation. Projections under IPCC climate scenarios indicate that increasing inundation will amplify methane emissions with sea-level rise, until a threshold near 0.75 m SLR, beyond which saltwater intrusion increasingly suppresses further growth, highlighting the critical role of salinity-inundation interactions in coastal methane dynamics."}
{"id": "2512.13803", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2512.13803", "abs": "https://arxiv.org/abs/2512.13803", "authors": ["Alexander Altland", "Francisco Divi", "Tobias Micklitz", "Maedeh Rezaei"], "title": "Freeness Reined in by a Single Qubit", "comment": "5 pages, 5 figures. 4 pages of Supplemental Material", "summary": "Free probability provides a framework for describing correlations between non-commuting observables in complex quantum systems whose Hilbert-space states follow maximum-entropy distributions. We examine the robustness of this framework under a minimal deviation from freeness: the coupling of a single ancilla qubit to a Haar-distributed quantum circuit of dimension $D0 \\gg 1$. We find that, even in this setting, the correlation functions predicted by free probability theory receive corrections of order $O(1)$. These modifications persist at long times, when the dynamics of the coupled system is already ergodic. We trace their origin to non-uniformly distributed stationary quantum states, which we characterize analytically and confirm numerically."}
{"id": "2512.14344", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14344", "abs": "https://arxiv.org/abs/2512.14344", "authors": ["Eymen Ipek", "Mario Hirz"], "title": "A Data-Driven Approach for Electric Vehicle Powertrain Modeling", "comment": null, "summary": "Electrification in the automotive industry and increasing powertrain complexity demand accelerated, cost-effective development cycles. While data-driven models are recently investigated at component level, a gap exists in systematically integrating them into cohesive, system-level simulations for virtual validation. This paper addresses this gap by presenting a modular framework for developing powertrain simulations. By defining standardized interfaces for key components-the battery, inverter, and electric motor-our methodology enables independently developed models, whether data-driven, physics-based, or empirical, to be easily integrated. This approach facilitates scalable system-level modeling, aims to shorten development timelines and to meet the agile demands of the modern automotive industry."}
{"id": "2512.14128", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14128", "abs": "https://arxiv.org/abs/2512.14128", "authors": ["Xiaojie Tao", "Rajit Gadh"], "title": "Fast Frequency Response Potential of Data Centers through Workload Modulation and UPS Coordination", "comment": null, "summary": "The rapid growth of renewable energy sources has significantly reduced system inertia and increased the need for fast frequency response (FFR) in modern power systems. Data centers, as large and flexible electrical consumers, hold great potential to contribute to frequency stabilization due to their controllable IT workloads and on-site uninterruptible power supply (UPS) systems. This paper investigates the feasibility of leveraging data centers for providing fast frequency response through real-time workload modulation and UPS coordination. A dynamic model combining data center power consumption and grid frequency dynamics is developed, capturing the interactions between IT servers, cooling systems, and energy storage. Control strategies based on frequency deviation are implemented to adjust server power and discharge UPS batteries during frequency events. Case studies on a modified IEEE 39-bus system demonstrate that the proposed strategy can effectively reduce frequency nadir and shorten recovery time without compromising service quality. The results highlight the promising role of data centers as grid-supporting resources in future low-inertia systems."}
{"id": "2512.14163", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14163", "abs": "https://arxiv.org/abs/2512.14163", "authors": ["Ole Løseth Elvetun", "Bjørn Fredrik Nielsen", "Niranjana Sudheer"], "title": "Weighted Group Lasso for a static EEG problem", "comment": null, "summary": "We investigate the weighted Group Lasso formulation for the static inverse electroencephalography (EEG) problem, aiming at reconstructing the unknown underlying neuronal sources from voltage measurements on the scalp. By modelling the three orthogonal dipole components at each location as a single coherent group, we demonstrate that depth bias and orientation bias can be effectively mitigated through the proposed regularization framework. On the theoretical front, we provide concise recovery guarantees for both single and multiple group sources. Our numerical experiments highlight that while theoretical bounds hold for a broad range of weight definitions, the practical reconstruction quality, for cases not covered by the theory, depends significantly on the specific weighting strategy employed. Specifically, employing a truncated Moore-Penrose pseudoinverse for the involved weighting matrix gives a small Dipole Localization Error (DLE). The proposed method offers a robust approach for inverse EEG problems, enabling improved spatial accuracy and a more physiologically realistic reconstruction of neural activity."}
{"id": "2512.14624", "categories": ["math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14624", "abs": "https://arxiv.org/abs/2512.14624", "authors": ["Rebecca M. Lewis", "Oliver Y. Feng", "Henry W. J. Reeve", "Min Xu", "Richard J. Samworth"], "title": "Learning the score under shape constraints", "comment": "70 pages, 5 figures", "summary": "Score estimation has recently emerged as a key modern statistical challenge, due to its pivotal role in generative modelling via diffusion models. Moreover, it is an essential ingredient in a new approach to linear regression via convex $M$-estimation, where the corresponding error densities are projected onto the log-concave class. Motivated by these applications, we study the minimax risk of score estimation with respect to squared $L^2(P_0)$-loss, where $P_0$ denotes an underlying log-concave distribution on $\\mathbb{R}$. Such distributions have decreasing score functions, but on its own, this shape constraint is insufficient to guarantee a finite minimax risk. We therefore define subclasses of log-concave densities that capture two fundamental aspects of the estimation problem. First, we establish the crucial impact of tail behaviour on score estimation by determining the minimax rate over a class of log-concave densities whose score function exhibits controlled growth relative to the quantile levels. Second, we explore the interplay between smoothness and log-concavity by considering the class of log-concave densities with a scale restriction and a $(β,L)$-Hölder assumption on the log-density for some $β\\in [1,2]$. We show that the minimax risk over this latter class is of order $L^{2/(2β+1)}n^{-β/(2β+1)}$ up to poly-logarithmic factors, where $n$ denotes the sample size. When $β< 2$, this rate is faster than could be obtained under either the shape constraint or the smoothness assumption alone. Our upper bounds are attained by a locally adaptive, multiscale estimator constructed from a uniform confidence band for the score function. This study highlights intriguing differences between the score estimation and density estimation problems over this shape-constrained class."}
{"id": "2512.13992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13992", "abs": "https://arxiv.org/abs/2512.13992", "authors": ["Jyotishka Datta", "Nick Polson", "Vadim Sokolov"], "title": "Bayesian Global-Local Regularization", "comment": null, "summary": "We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments."}
{"id": "2512.14656", "categories": ["physics.ao-ph", "cs.CV", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.14656", "abs": "https://arxiv.org/abs/2512.14656", "authors": ["Gabriele Accarino", "Viviana Acquaviva", "Sara Shamekh", "Duncan Watson-Parris", "David Lawrence"], "title": "WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields", "comment": null, "summary": "We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim."}
{"id": "2512.13790", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.13790", "abs": "https://arxiv.org/abs/2512.13790", "authors": ["Yannick Stade", "Lukas Burgholzer", "Robert Wille"], "title": "Search Smarter, Not Harder: A Scalable, High-Quality Zoned Neutral Atom Compiler", "comment": "7 pages, 8 figures", "summary": "Zoned neutral atom architectures are emerging as a promising platform for large-scale quantum computing. Their growing scale, however, creates a critical need for efficient and automated compilation solutions. Yet, existing methods fail to scale to the thousands of qubits these devices promise. State-of-the-art compilers, in particular, suffer from immense memory requirements that limit them to small-scale problems. This work proposes a scalable compilation strategy that \"searches smarter, not harder\". We introduce Iterative Diving Search (IDS), a goal-directed search algorithm that avoids the memory issues of previous methods, and relaxed routing, an optimization to mitigate atom rearrangement overhead. Our evaluation confirms that this approach compiles circuits with thousands of qubits and, in addition, even reduces rearrangement overhead by 28.1% on average. The complete code is publicly available in open-source as part of the Munich Quantum Toolkit (MQT) at https://github.com/munich-quantum-toolkit/qmap."}
{"id": "2512.14329", "categories": ["cs.CE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14329", "abs": "https://arxiv.org/abs/2512.14329", "authors": ["Yanning Dai", "Chenyu Tang", "Ruizhi Zhang", "Wenyu Yang", "Yilan Zhang", "Yuhui Wang", "Junliang Chen", "Xuhang Chen", "Ruimou Xie", "Yangyue Cao", "Qiaoying Li", "Jin Cao", "Tao Li", "Hubin Zhao", "Yu Pan", "Arokia Nathan", "Xin Gao", "Peter Smielewski", "Shuo Gao"], "title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data", "comment": "26 pages, 6 figures", "summary": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies."}
{"id": "2512.14173", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14173", "abs": "https://arxiv.org/abs/2512.14173", "authors": ["Pratyay Ghosh", "Frédéric Mila"], "title": "Simplex Crystal Ground State and Magnetization Plateaus in the Spin-$1/2$ Heisenberg Model on the Ruby Lattice", "comment": "11 pages, 7 figures", "summary": "We investigate the spin-$1/2$ Heisenberg antiferromagnet on the ruby lattice with uniform first- and second-neighbor interactions, which forms a two-dimensional network of corner-sharing tetrahedra. Using infinite projected entangled pair states (iPEPS), we study the ground state of the system to find that it assumes a gapped threefold-degenerate simplex crystal ground state, with strong singlets formed on pairs of neighboring triangles. We argue that the formation of the simplex singlet ground state at the isotropic point relates to the weak inter-triangle coupling limit where an effective spin-chirality Hamiltonian on the honeycomb lattice exhibits an extensively degenerate ground state manifold of singlet coverings at the mean-field level. Under an applied Zeeman field, the iPEPS simulations uncover magnetization plateaus at $m/m_s = 0, 1/3, 1/2,$ and $2/3$, separated by intermediate supersolid phases, all breaking the sixfold rotational symmetry of the lattice. Unlike the checkerboard lattice, these plateaus cannot be described by strongly localized magnons."}
{"id": "2512.14124", "categories": ["math.OC", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.14124", "abs": "https://arxiv.org/abs/2512.14124", "authors": ["Boris S. Mordukhovich", "Peipei Tang", "Chengjing Wang"], "title": "Complete Characterizations of Well-Posedness in Parametric Composite Optimization", "comment": null, "summary": "This paper provides complete characterization of well-posedness for Karush-Kuhn-Tucker (KKT) systems associated with general problems of perturbed composite optimization. Leveraging the property of parabolic regularity for composite models, we show that the second-order subderivative of the cost function reduces to the novel second-order variational function playing a crucial role in the subsequent analysis. This foundational result implies that the strong second-order sufficient condition (SSOSC) introduced in this work for the general class of composite optimization problems naturally extends the classical second-order sufficient condition in nonlinear programming. Then we obtain several equivalent characterizations of the second-order qualification condition (SOQC) and highlight its equivalence to the constraint nondegeneracy condition under the $\\mathcal{C}^{2}$-cone reducibility assumption. These insights lead us to multiple equivalent conditions for the major Lipschitz-like/Aubin property of KKT systems, including the SOQC combined with the new second-order subdifferential condition and the SOQC combined with tilt stability of local minimizers. Furthermore, under $\\mathcal{C}^{2}$-cone reducibility, we prove that the Lipschitz-like property of the reference KKT system is equivalent to its strong regularity. Finally, we demonstrate that the Lipschitz-like property is equivalent to the nonsingularity of the generalized Jacobian associated with the KKT system under a certain verifiable assumption. These results provide a unified and rigorous framework for analyzing stability and sensitivity of solutions to composite optimization problems, as well as for the design and justification of numerical algorithms."}
{"id": "2512.13933", "categories": ["cond-mat.stat-mech", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2512.13933", "abs": "https://arxiv.org/abs/2512.13933", "authors": ["Matthew P. Leighton", "Christopher W. Lynn"], "title": "Decomposing Non-Markovian History Dependence", "comment": "8 pages, 6 figures", "summary": "Non-Markovian stochastic processes are ubiquitous in biology. Nevertheless, we lack a general framework for quantifying historical dependencies. In this Letter, we propose an information-theoretic approach to decompose history dependence in systems with non-Markovian dynamics, quantifying the information encoded in dependencies of each order. In minimal models of non-Markovian dynamics, we show that this framework correctly captures the underlying historical dependencies, even when autocorrelations do not. In prolonged recordings of fly behavior, we find that the scaling of non-Markovian dependencies is invariant across timescales from fractions of a second to minutes. Despite this invariance, the overall amount of non-Markovian information is non-monotonic, suggesting a unique timescale on which historical dependencies are strongest."}
{"id": "2512.14182", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2512.14182", "abs": "https://arxiv.org/abs/2512.14182", "authors": ["Ling-Zhi Tang", "Xiao Li", "Z. D. Wang", "Dan-Wei Zhang"], "title": "Discrete time crystals enabled by Floquet strong Hilbert space fragmentation", "comment": "9 pages, 5 figures", "summary": "Discrete time crystals (DTCs) are non-equilibrium phases of matter that break the discrete time-translation symmetry and is characterized by a robust subharmonic response in periodically driven quantum systems. Here, we explore the DTC in a disorder-free, periodically kicked XXZ spin chain, which is stabilized by the Floquet strong Hilbert space fragmentation. We numerically show the period-doubling response of the conventional DTC order, and uncover a multiple-period response with beating dynamics due to the coherent interplay of multiple $π$-pairs in the Floquet spectrum of small-size systems. The lifetime of the DTC order exhibits independence of the driving frequency and a power-law dependence on the ZZ interaction strength. It also grows exponentially with the system size, as a hallmark of the strong fragmentation inherent to the Floquet model. We analytically reveal the approximate conservation of the magnetization and domain-wall number in the Floquet operator for the emergent strong fragmentation, which is consistent with numerical results of the dimensionality ratio of symmetry subspaces. The rigidity and phase regime of the DTC order are identified through finite-size scaling of the Floquet-spectrum-averaged mutual information, as well as via dynamical probes. Our work establishes the Floquet Hilbert space fragmentation as a disorder-free mechanism for sustaining nontrivial temporal orders in out-of-equilibrium quantum many-body systems."}
{"id": "2512.14349", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14349", "abs": "https://arxiv.org/abs/2512.14349", "authors": ["Federico Califano", "Camilla Rota", "Riccardo Zanella", "Antonio Franchi"], "title": "A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators", "comment": null, "summary": "We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $η=J(q)\\dot{q}$, where $q$ is a point on the configuration manifold, $η$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \\textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation."}
{"id": "2512.14136", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14136", "abs": "https://arxiv.org/abs/2512.14136", "authors": ["Xiaojie Tao", "Rajit Gadh"], "title": "Coordinated Fast Frequency Response from Electric Vehicles, Data Centers, and Battery Energy Storage Systems", "comment": null, "summary": "High renewable penetration has significantly reduced system inertia in modern power grids, increasing the need for fast frequency response (FFR) from distributed and non-traditional resources. While electric vehicles (EVs), data centers, and battery energy storage systems (BESS) have each demonstrated the capability to provide sub-second active power support, their combined frequency response potential has not been systematically evaluated. This paper proposes a coordinated control framework that aggregates these heterogeneous resources to provide fast, stable, and reliable FFR. Dynamic models for EV fleets, data center UPS and workload modulation, and BESS are developed, explicitly capturing their response times, power limits, and operational constraints. A hierarchical control architecture is introduced, where an upper-level coordinator dynamically allocates FFR among resources based on response speed and available capacity, and lower-level controllers implement the actual power response. Case studies based on the IEEE 39-bus test system demonstrate that the coordinated EV-DC-BESS framework improves frequency nadir by up to 0.2 Hz, reduces RoCoF, and accelerates frequency recovery compared with single-resource FFR. Results confirm that synergistic coordination significantly enhances grid stability, especially in low-inertia scenarios. This work highlights the value of multi-resource aggregation for future frequency regulation markets in renewable-dominated grids."}
{"id": "2512.14193", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14193", "abs": "https://arxiv.org/abs/2512.14193", "authors": ["Yasuhiro Matsumoto", "Kei Matsushima"], "title": "Efficient LU factorization exploiting direct-indirect Burton-Miller equation for Helmholtz transmission problems", "comment": null, "summary": "This paper proposes a direct-indirect mixed Burton-Miller boundary integral equation for solving Helmholtz scattering problems with transmissive scatterers. The proposed formulation has three unknowns, one more than the number of unknowns for the ordinary formulation. However, we can construct efficient numerical solvers based on LU factorization by exploiting the sparse alignment of the boundary integral operators of the proposed formulation. Numerical examples demonstrate that the direct solver based on the proposed formulation is approximately 40% faster than the ordinary formulation when the LU-factorization-based solver is used. In addition, the proposed formulation is applied to a fast direct solver employing LU factorization in its algorithm. In the application to the fast direct solver, the proxy method with a weak admissibility low-rank approximation is developed. The speedup achieved using the proposed formulation is also shown to be effective in finding nonlinear eigenvalues, which are related to the uniqueness of the solution, in boundary value problems. Furthermore, the well-posedness of the proposed boundary integral equation is established for scatterers with boundaries of class $C^2$, using the mapping property of boundary integral operators in Hölder space."}
{"id": "2512.13992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13992", "abs": "https://arxiv.org/abs/2512.13992", "authors": ["Jyotishka Datta", "Nick Polson", "Vadim Sokolov"], "title": "Bayesian Global-Local Regularization", "comment": null, "summary": "We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments."}
{"id": "2512.14131", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14131", "abs": "https://arxiv.org/abs/2512.14131", "authors": ["Prasanjit Dubey", "Xiaoming Huo"], "title": "Most Powerful Test with Exact Family-Wise Error Rate Control: Necessary Conditions and a Path to Fast Computing", "comment": null, "summary": "Identifying the most powerful test in multiple hypothesis testing under strong family-wise error rate (FWER) control is a fundamental problem in statistical methodology. State-of-the-art approaches formulate this as a constrained optimisation problem, for which a dual problem with strong duality has been established in a general sense. However, a constructive method for solving the dual problem is lacking, leaving a significant computational gap. This paper fills this gap by deriving novel, necessary optimality conditions for the dual optimisation. We show that these conditions motivate an efficient coordinate-wise algorithm for computing the optimal dual solution, which, in turn, provides the most powerful test for the primal problem. We prove the linear convergence of our algorithm, i.e., the computational complexity of our proposed algorithm is proportional to the logarithm of the reciprocal of the target error. To the best of our knowledge, this is the first time such a fast and computationally efficient algorithm has been proposed for finding the most powerful test with family-wise error rate control. The method's superior power is demonstrated through simulation studies, and its practical utility is shown by identifying new, significant findings in both clinical and financial data applications."}
{"id": "2512.13803", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2512.13803", "abs": "https://arxiv.org/abs/2512.13803", "authors": ["Alexander Altland", "Francisco Divi", "Tobias Micklitz", "Maedeh Rezaei"], "title": "Freeness Reined in by a Single Qubit", "comment": "5 pages, 5 figures. 4 pages of Supplemental Material", "summary": "Free probability provides a framework for describing correlations between non-commuting observables in complex quantum systems whose Hilbert-space states follow maximum-entropy distributions. We examine the robustness of this framework under a minimal deviation from freeness: the coupling of a single ancilla qubit to a Haar-distributed quantum circuit of dimension $D0 \\gg 1$. We find that, even in this setting, the correlation functions predicted by free probability theory receive corrections of order $O(1)$. These modifications persist at long times, when the dynamics of the coupled system is already ergodic. We trace their origin to non-uniformly distributed stationary quantum states, which we characterize analytically and confirm numerically."}
{"id": "2512.14496", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.14496", "abs": "https://arxiv.org/abs/2512.14496", "authors": ["Lazlo Bleker", "Mustafa Cem Güneş", "Pierluigi D'Acunto"], "title": "BridgeNet: A Dataset of Graph-based Bridge Structural Models for Machine Learning Applications", "comment": "GNI Symposium on Artifical Intelligence for the Built World 2026", "summary": "Machine learning (ML) is increasingly used in structural engineering and design, yet its broader adoption is hampered by the lack of openly accessible datasets of structural systems. We introduce BridgeNet, a publicly available graph-based dataset of 20,000 form-found bridge structures aimed at enabling Graph ML and multi-modal learning in the context of conceptual structural design. Each datapoint consists of (i) a pin-jointed equilibrium wireframe model generated with the Combinatorial Equilibrium Modeling (CEM) form-finding method, (ii) a volumetric 3D mesh obtained through force-informed materialization, and (iii) rendered images from two canonical camera angles. The resulting dataset is modality-rich and application-agnostic, supporting tasks such as CEM-specific edge classification and parameter inference, surrogate modeling of form-finding, cross-modal reconstruction between graphs, meshes and images, and generative structural design. BridgeNet addresses a key bottleneck in data-driven applications for structural engineering and design by providing a dataset that facilitates the development of new ML-based approaches for equilibrium bridge structures."}
{"id": "2512.14216", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14216", "abs": "https://arxiv.org/abs/2512.14216", "authors": ["Takeshi Matsumura", "Takumi Hasegawa", "Ryuma Nakajima", "Kenshin Kurauchi", "Satoshi Tsutsui", "Daisuke Ishikawa", "Alfred Q. R. Baron", "Hiroshi Tanida"], "title": "Acoustic phonon softening and lattice instability driven by on-site $f$-$d$ hybridization in CeCoSi", "comment": "7 pages, 4 figures, with Supplemental Material, Accepted for publication in Phys. Rev. B", "summary": "Soft phonon modes in tetragonal CeCoSi, which undergoes a structural transition at $T_0=12$ K followed by antiferromagnetic order at $T_{\\text{N}}=9.5$ K, have been investigated using high-resolution inelastic x-ray scattering. Pronounced softening was detected in the transverse acoustic modes corresponding to the $(yz+zx)$-type monoclinic distortion, consistent with the experimentally determined triclinic structure. Remarkably, the softening persists up to the zone boundary along (0, 0, $q$), indicating a short correlation length of the lattice instability. This instability, characterized by a Curie-type strain susceptibility, is interpreted as a consequence of the on-site $4f$-$5d$ hybridization, which is intrinsic to this crystal structure due to the lack of inversion symmetry at the two Ce sites."}
{"id": "2512.14226", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.14226", "abs": "https://arxiv.org/abs/2512.14226", "authors": ["Yixin Tan", "Fang Feng", "Shengfeng Zhu"], "title": "Shape design with phase field methods for structural hemivariational inequalities in contact problems", "comment": null, "summary": "We develop mathematical models for shape design and topology optimization in structural contact problems involving friction between elastic and rigid bodies. The governing mechanical constraint is a nonlinear, non-smooth, and non-convex hemivariational inequality, which provides a more general and realistic description of frictional contact forces than standard variational inequalities, but is also more challenging due to its non-convexity. For energy-type shape functionals, the Eulerian derivative of the hemivariational inequality is derived through rigorous shape sensitivity analysis. The rationality of a regularization approach is justified by asymptotic analysis, and this method is further applied to handle the non-smoothness of general shape functionals in the sensitivity framework. Based on these theoretical results, a numerical boundary variational method is proposed for shape optimization. For topology optimization, three phase-field algorithms are developed: a gradient-flow phase-field method, a phase-field method with second-order regularization of the cost functional, and a phase-field method coupled with topological derivatives. To the best of our knowledge, these approaches are new for shape design in hemivariational inequalities. Various numerical experiments confirm the accuracy and effectiveness of the proposed shape and topology optimization algorithms."}
{"id": "2512.13936", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.13936", "abs": "https://arxiv.org/abs/2512.13936", "authors": ["Matthew P. Leighton", "Christopher W. Lynn"], "title": "Tractable Model for Tunable Non-Markovian Dynamics", "comment": "16 pages, 3 figures", "summary": "Non-Markovian dynamics are ubiquitous across physics, biology, and engineering. Yet our understanding of non-Markovian processes significantly lags that of simpler Markovian processes, due largely to a lack of tractable models. In this article, we present a minimal model of non-Markovian dynamics in which the current state copies past states with arbitrary history dependence. We show that many properties of this process can be studied analytically, providing insight into the relationships between history dependence, autocorrelations, and information-theoretic metrics like entropy and dynamical information. Strikingly, we find that autocorrelations can fail, even qualitatively, to capture the underlying dependencies. Ultimately, this model serves as a tractable sandbox for exploring non-Markovian dynamics."}
{"id": "2512.14319", "categories": ["physics.soc-ph", "cond-mat.dis-nn", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14319", "abs": "https://arxiv.org/abs/2512.14319", "authors": ["Marc Barthelemy"], "title": "Not all Chess960 positions are equally complex", "comment": "11 pages, 7 figures", "summary": "We analyze strategic complexity across all 960 Chess960 (Fischer Random Chess) starting positions. Stockfish evaluations show a near-universal first-move advantage for White ($\\langle E \\rangle = +0.30 \\pm 0.14$ pawns), indicating that the advantage conferred by moving first is a robust structural feature of the game. To quantify decision difficulty, we introduce an information-based measure $S(n)$ describing the cumulative information required to identify optimal moves over the first $n$ plies. This measure decomposes into contributions from White and Black, $S_W$ and $S_B$, yielding a total opening complexity $S_{\\mathrm{tot}} = S_W + S_B$ and a decision asymmetry $A=S_B-S_W$. Across the ensemble, $S_{\\mathrm{tot}}$ varies by a factor of three, while $A$ spans from $-2.5$ to $+1.8$ bits, showing that some openings burden White and others Black. The mean $\\langle A \\rangle = -0.25$ bits indicates a slight tendency for White to face harder opening decisions. Standard chess (position \\#518, \\texttt{RNBQKBNR}) exhibits above-average asymmetry (91st percentile) but typical overall complexity (47th percentile). The most complex opening is \\#226 (\\texttt{BNRQKBNR}), whereas \\#198 (\\texttt{QNBRKBNR})is the most balanced, with both evaluation and asymmetry near zero. These results reveal a highly heterogeneous Chess960 landscape in which small rearrangements of the back-rank pieces can significantly alter strategic depth and competitive fairness. Remarkably, the classical starting position-despite centuries of cultural selection-lies far from the most balanced configuration."}
{"id": "2512.14412", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14412", "abs": "https://arxiv.org/abs/2512.14412", "authors": ["Gil Serrano", "Pedro Lourenço", "Bruno J. Guerreiro", "Rita Cunha"], "title": "Equivariant Filter Cascade for Relative Attitude, Target's Angular Velocity, and Gyroscope Bias Estimation", "comment": "This work has been submitted to the 2026 European Control Conference", "summary": "Rendezvous and docking between a chaser spacecraft and an uncooperative target, such as an inoperative satellite, require synchronization between the chaser spacecraft and the target. In these scenarios, the chaser must estimate the relative attitude and angular velocity of the target using onboard sensors, in the presence of gyroscope bias. In this work, we propose a cascade of Equivariant Filters (EqF) to address this problem. The first stage of the cascade estimates the chaser's attitude and the bias, using measurements from a star tracker, while the second stage of the cascade estimates the relative attitude and the target's angular velocity, using observations of two known, non-collinear vectors fixed in the target frame. The stability of the EqF cascade is theoretically analyzed and simulation results demonstrate the filter cascade's performance."}
{"id": "2512.14175", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14175", "abs": "https://arxiv.org/abs/2512.14175", "authors": ["Lauritz Rismark Fosso", "Christian Holden", "Sveinung Johan Ohrem"], "title": "KalMRACO: Unifying Kalman Filter and Model Reference Adaptive Control for Robust Control and Estimation of Uncertain Systems", "comment": "7 pages, 4 figures", "summary": "A common assumption when applying the Kalman filter is a priori knowledge of the system parameters. These parameters are not necessarily known, and this may limit real-world applications of the Kalman filter. The well-established Model Reference Adaptive Controller (MRAC) utilizes a known reference model and ensures that the input-output behavior of a potentially unknown system converges to that of the reference model. We present KalMRACO, a unification of the Kalman filter and MRAC leveraging the reference model of MRAC as the Kalman filter system model, thus eliminating, to a large degree, the need for knowledge of the underlying system parameters in the application of the Kalman filter. We also introduce the concept of blending estimated states and measurements in the feedback law to handle stability issues during the initial transient. KalMRACO is validated through simulations and lab trials on an underwater vehicle. Results show superior tracking of the reference model state, observer state convergence, and noise mitigation properties."}
{"id": "2512.14219", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14219", "abs": "https://arxiv.org/abs/2512.14219", "authors": ["Weifeng Qiu"], "title": "Analysis of a finite element method for second order uniformly elliptic PDEs in non-divergence form", "comment": null, "summary": "We propose one finite element method for both second order linear uniformly elliptic PDE in non-divergence form and the elliptic Hamilton-Jacobi-Bellman (HJB) equation. For the linear elliptic PDE in non-divergence form, we consider two scenarios of the matrix coefficient matrix $A$. One is $A$ is uniformly continuous. The other is $A$ is discontinuous but $γA$ is dominated by $I_{d}$ where $γ$ is a positive weight function.\n  We prove that optimal convergence in discrete $W^{2,p}$-norm of the numerical approximation to the strong solution for $1<p\\leq 2$ on convex polyhedra in $\\mathbb{R}^{d}$ ($d=2,3$). If the domain is a two dimensional non-convex polygon, $p$ is valid in a neighbourhood of $\\frac{4}{3}$. We also prove the well-posedness of strong solution in $W^{2,p}(Ω)$ for both linear elliptic PDE in non-divergence form and the HJB equation for $1< p \\leq 2$ on convex polyhedra in $\\mathbb{R}^{d}$ ($d=2,3$) and for $p$ in an open interval starting from $1$ and including $\\frac{4}{3}$ on two dimensional non-convex polygon. Furthermore, we relax the assumptions on the continuity of coefficients of the HJB equation, which have been widely used in literature."}
{"id": "2512.14353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.14353", "abs": "https://arxiv.org/abs/2512.14353", "authors": ["Ritabrata Dutta", "Yuehao Xu", "Sherman Khoo", "Francesca Basini", "Andreas Futschik"], "title": "Signature-Informed Selection Detection: A Novel Method for Multi-Locus Temporal Population Genetic Model with Recombination", "comment": null, "summary": "In population genetics, there is often interest in inferring selection coefficients. This task becomes more challenging if multiple linked selected loci are considered simultaneously. For such a situation, we propose a novel generalized Bayesian framework where we compute a scoring rule posterior for the selection coefficients in multi-locus temporal population genetics models. As we consider trajectories of allele frequencies over time as our data, we choose to use a signature kernel scoring rule - a kernel scoring rule defined for high-dimensional time-series data using iterated path integrals of a path (called signatures). We can compute an unbiased estimate of the signature kernel score using model simulations. This enables us to sample asymptotically from the signature kernel scoring rule posterior of the selection coefficients using pseudo-marginal MCMC-type algorithms. Through a simulation study, we were able to show the inferential efficacy of our method compared to existing benchmark methods for two and three selected locus scenarios under the standard Wright-Fisher model with recombination and selection. We also consider a negative frequency-dependent selection model for one and two locus scenarios, and also joint inference of selection coefficients and initial haplotype frequencies under the standard Wright-Fisher model. Finally, we illustrate the application of our inferential method for two real-life dataset. More specifically, we consider a data set on Yeast, as well as data from an Evolve and Resequence (E\\&R) experiment on {\\em Drosophila simulans}."}
{"id": "2512.13809", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2512.13809", "abs": "https://arxiv.org/abs/2512.13809", "authors": ["Kabir Khanna", "Romain Vasseur"], "title": "Universal Statistics of Measurement-Induced Entanglement in Tomonaga-Luttinger liquids", "comment": null, "summary": "We study the statistics of measurement-induced entanglement (MIE) after partial measurement on a class of one-dimensional quantum critical states described by Tomonaga-Luttinger liquids at low energies. Using a replica trick to average over measurement outcomes in the charge basis and tools from conformal field theory (CFT), we derive closed-form expressions for the cumulants of MIE. We show that exact Born-averaging over microscopic measurement outcomes becomes equivalent at low energy to averaging over conformal boundary conditions weighted by their corresponding partition functions. Our results yield distinctive critical behavior across all cumulants in the regime where the unmeasured parts of the system are maximally separated. We also obtain the full distribution of the post-measurement entanglement entropy, finding that it is generically bimodal and exhibits fat-tails. We corroborate our analytical predictions by numerical calculations and find good agreement between them."}
{"id": "2512.14314", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.14314", "abs": "https://arxiv.org/abs/2512.14314", "authors": ["M. ElMassalami", "S. Favre", "M. B. Silva Neto"], "title": "Pressure-induced hole delocalization in the strongly correlated quasicubic charge-transfer perovskite $LaBa_2Fe_3O_{8+δ}$d", "comment": "9 pages, 5 figures", "summary": "Analysis of the thermal and baric evolution of resistance in $LaBa_2Fe_3O_{8+δ}$ enabled the construction of its pressure-temperature (P-T) phase diagram, which prominently displays a critical boundary, $P^{MIT}_c(T)$, marking the transition from localized to hole-type extended states. The relatively low critical pressures [$P^{MIT}_c(T) \\approx 3$-8 GPa] suggest that, as $P \\rightarrow P_c$ in this narrow-gap, strongly correlated charge-transfer system, both the hybridization strength and the charge-transfer character are progressively enhanced - ultimately leading to the emergence of metallicity. Emphasizing the electronic nature of this transition, pressure-dependent structural analyses at room temperature reveal no associated structural phase transition at $P^{MIT}_c(T)$; the system retains a (weakly tetragonally distorted) quasicubic perovskite structure with Murnaghan-type compressibility up to 30\\,GPa. The emergence of hole delocalization and metallic conduction, coupled with suppressed antiferromagnetism, suggests proximity to quantum criticality."}
{"id": "2512.14246", "categories": ["math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14246", "abs": "https://arxiv.org/abs/2512.14246", "authors": ["Evgenii Chzhen", "Mohamed Hebiri", "Gayane Taturyan"], "title": "Randomized multi-class classification under system constraints: a unified approach via post-processing", "comment": null, "summary": "We study the problem of multi-class classification under system-level constraints expressible as linear functionals over randomized classifiers. We propose a post-processing approach that adjusts a given base classifier to satisfy general constraints without retraining. Our method formulates the problem as a linearly constrained stochastic program over randomized classifiers, and leverages entropic regularization and dual optimization techniques to construct a feasible solution. We provide finite-sample guarantees for the risk and constraint satisfaction for the final output of our algorithm under minimal assumptions. The framework accommodates a broad class of constraints, including fairness, abstention, and churn requirements."}
{"id": "2512.14345", "categories": ["cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2512.14345", "abs": "https://arxiv.org/abs/2512.14345", "authors": ["Baruch Meerson", "Ohad Vilk"], "title": "Age-structured hydrodynamics of ensembles of anomalously diffusing particles with renewal resetting", "comment": "10 pages, 6 figures", "summary": "We develop an age-structured hydrodynamic (HD) theory which describes the collective behavior of $N\\gg 1$ anomalously diffusing particles under stochastic renewal resetting. The theory treats the age of a particle -- the time since its last reset -- as an explicit dynamical variable and allows for resetting rules which introduce global inter-particle correlations. The anomalous diffusion is modeled by the scaled Brownian motion (sBm): a Gaussian process with independent increments, characterized by a power-law time dependence of the diffusion coefficient, $D(t)\\sim t^{2H-1}$, where $H>0$. We apply this theory to three different resetting protocols: independent resetting to the origin (model~A), resetting to the origin of the particle farthest from it (model~B), and a scaled-diffusion extension of the ``Brownian bees\" model of Berestycki et al, Ann. Probab. \\textbf{50}, 2133 (2022). In all these models non-equilibrium steady states are reached at long times, and we determine the steady-state densities. For model A the (normalized to unity) steady-state density coincides with the steady-state probability density of a single particle undergoing sBM with resetting to the origin. For model B, and for the scaled Brownian bees, the HD steady-state densities are markedly different: in particular, they have compact supports for all $H>0$. The age-structured HD formalism can be extended to other anomalous diffusion processes with renewal resetting protocols which introduce global inter-particle correlations."}
{"id": "2512.14450", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14450", "abs": "https://arxiv.org/abs/2512.14450", "authors": ["Riccardo Busetto", "Elia Cereda", "Marco Forgione", "Gabriele Maroni", "Dario Piga", "Daniele Palossi"], "title": "Nonlinear System Identification Nano-drone Benchmark", "comment": null, "summary": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics."}
{"id": "2512.14344", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14344", "abs": "https://arxiv.org/abs/2512.14344", "authors": ["Eymen Ipek", "Mario Hirz"], "title": "A Data-Driven Approach for Electric Vehicle Powertrain Modeling", "comment": null, "summary": "Electrification in the automotive industry and increasing powertrain complexity demand accelerated, cost-effective development cycles. While data-driven models are recently investigated at component level, a gap exists in systematically integrating them into cohesive, system-level simulations for virtual validation. This paper addresses this gap by presenting a modular framework for developing powertrain simulations. By defining standardized interfaces for key components-the battery, inverter, and electric motor-our methodology enables independently developed models, whether data-driven, physics-based, or empirical, to be easily integrated. This approach facilitates scalable system-level modeling, aims to shorten development timelines and to meet the agile demands of the modern automotive industry."}
{"id": "2512.14231", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14231", "abs": "https://arxiv.org/abs/2512.14231", "authors": ["Kevin Dijkstra", "Deepesh Toshniwal"], "title": "Structure-preserving Variational Multiscale Stabilization of the Incompressible Navier-Stokes Equations", "comment": null, "summary": "This paper introduces a Variational Multiscale Stabilization (VMS) formulation of the incompressible Navier--Stokes equations that utilizes the Finite Element Exterior Calculus (FEEC) framework. The FEEC framework preserves the geometric and topological structure of continuous spaces and PDEs in the discrete spaces and model, and helps build stable and convergent discretizations. For the Navier-Stokes equations, this structure is encoded in the de Rham complex. In this work, we consider the vorticity-velocity-pressure formulation discretized within the FEEC framework. We model the effect of the unresolved scales on the finite-dimensional solution by introducing appropriate fine-scale governing equations, which we also discretize using the FEEC approach. This preserves the structure of the continuous problem in both the coarse- and fine-scale solutions; for instance, both the coarse- and fine-scale velocities are pointwise incompressible. We demonstrate that the resulting formulation is residual-based, energetically stable, and optimally convergent. Moreover, our fine-scale model provides an efficient computational approach: by decoupling fine-scale problems across elements, they can be solved in parallel. In fact, the fine-scale equations can be eliminated during matrix assembly, leading to a VMS formulation in which the problem size is governed solely by the coarse-scale discretization. Finally, the proposed formulation applies to both the lowest regularity discretizations of the de Rham complex and high-regularity isogeometric discretizations. We validate our theoretical results through numerical experiments, simulating both steady-, unsteady-, viscous-, and inviscid-flow problems. These tests show that the stabilized solutions are qualitatively better than the unstabilized ones, converge at optimal rates, and, as the mesh is refined, the stabilization is asymptotically turned off."}
{"id": "2512.14378", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14378", "abs": "https://arxiv.org/abs/2512.14378", "authors": ["E. Androulakis", "K. Chatterjee", "H. Evangelaras"], "title": "On the E(s^2)-optimality of two-level supersaturated designs constructed using Wu's method of partially aliased interactions on certain two-level orthogonal arrays", "comment": null, "summary": "Wu [10] proposed a method for constructing two-level supersaturated designs by using a Hadamard design with n runs and n-1 columns as a staring design and by supplementing it with two-column interactions, as long as they are partially aliased. Bulutoglu and Cheng [2] proved that this method results in E(s^2)-optimal supersaturated designs when certain interaction columns are selected. In this paper, we extend these results and prove E(s^2)-optimality for supersaturated designs that are constructed using Wu's method when the starting design is any orthogonal array with n runs and n-1, n-2 or n-3 columns, as long as its main effects and two-column interactions are partially aliased with two-column interactions."}
{"id": "2512.13817", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13817", "abs": "https://arxiv.org/abs/2512.13817", "authors": ["Hiroki Nakabayashi", "Hayato Kinkawa", "Takano Taira", "Naomichi Hatano"], "title": "Fractional decay in the spontaneous emission of a two-level system", "comment": "6+15 pages, 3+1 figures", "summary": "We find that when the environment of a two-level system has an energy spectrum with a lower bound but without an upper one, the survival probability of the spontaneous emission of the two-level system scales with the spatial dimension $D$ and the exponent $n$ of the energy dispersion $|\\vec{k}|^n$ of the environment in the form $1-αt^{2-D/n}$ in the short-time and in the form $αt^{D/n-2}$ in the long-time regime. The former fractional scaling of the survival probability leads to a quantum Zeno effect with a different scaling of the Zeno time."}
{"id": "2512.14396", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14396", "abs": "https://arxiv.org/abs/2512.14396", "authors": ["Marvin Leusch", "Alessandro Toschi", "Andreas Hausoel", "Giorgio Sangiovanni", "Georg Rohringer"], "title": "Impact of nonlocal spatial correlations for different lattice geometries", "comment": "12 pages, 11 figures", "summary": "We analyze the impact of the lattice geometry on the thermodynamic transition to magnetically ordered phases in strongly interacting electron systems for various Bravais lattices in three and four dimensions, including both local and nonlocal correlation effects. In a first step we use the dynamical mean field theory (DMFT), which takes into account purely local correlations, to calculate the magnetic susceptibilities of the Hubbard model on three (3d-sc) and four dimensional (4d-sc) simple cubic/hypercubic, as well as on three dimensional body- (bcc) and face-centered (fcc) cubic lattices, and determine the transition temperature to the corresponding magnetically-ordered state. In a second step, we exploit the dynamical vertex approximation (D$Γ$A), a diagrammatic extension of DMFT, to include the effect of nonlocal correlations which are particularly important in the vicinity of the corresponding phase transition. For the bipartite 3d-sc, 4d-sc and bcc lattices nonlocal fluctuations lead to a substantial reduction of the DMFT transition temperature consistent to the overall tendency of mean-field approaches to overestimate the stability of ordered phases. As expected, the magnitude of the difference between the DMFT, being exact in the limit of large connectivity/dimensions, and D$Γ$A transition temperatures decreases with increasing coordination number. On a more practical perspective, these results also provide a reasonable guidance to evaluate the expected overestimation of the DMFT ordering temperature for different material geometries. For the fcc lattice, on the other hand, the ordered phase observed in DMFT vanishes completely within D$Γ$A which is consistent with the existence of strong geometric frustration in this lattice."}
{"id": "2512.14387", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.14387", "abs": "https://arxiv.org/abs/2512.14387", "authors": ["Fabio DiFonzo", "Michael Holst", "Morteza Kimiaei", "Vyacheslav Kungurtsev", "Songqiang Qiu"], "title": "Towards Real Time Control of Water Engineering with Nonlinear Hyperbolic Partial Differential Equations", "comment": null, "summary": "This paper examines aspirational requirements for software addressing mixed-integer optimization problems constrained by the nonlinear Shallow Water partial differential equations (PDEs), motivated by applications such as river-flow management in hydropower cascades. Realistic deployment of such software would require the simultaneous treatment of nonlinear and potentially non-smooth PDE dynamics, limited theoretical guarantees on the existence and regularity of control-to-state mappings under varying boundary conditions, and computational performance compatible with operational decision-making. In addition, practical settings motivate consideration of uncertainty arising from forecasts of demand, inflows, and environmental conditions. At present, the theoretical foundations, numerical optimization methods, and large-scale scientific computing tools required to address these challenges in a unified and tractable manner remain the subject of ongoing research across the associated research communities. Rather than proposing a complete solution, this work uses the problem as a case study to identify and organize the mathematical, algorithmic, and computational components that would be necessary for its realization. The resulting framework highlights open challenges and intermediate research directions, and may inform both more circumscribed related problems and the design of future large-scale collaborative efforts aimed at addressing such objectives."}
{"id": "2512.14487", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.14487", "abs": "https://arxiv.org/abs/2512.14487", "authors": ["Yu. Kozitsky"], "title": "50 years of Yukhnovskii's critical point theory: its place in the constant flow of theoretical physics", "comment": "11 pages, 4 figures", "summary": "Half a century ago, Ihor Yukhnovskii elaborated a method of studying the critical point of the three-dimensional Ising model based on a layer-by-layer integration in the space of collective variables. His method was an alternative to that based on the $\\varepsilon$-expansion for which K. G. Wilson was awarded the Nobel Prize in Physics in 1982. However, Yukhnovskii's technique, which yielded similar results, provided even deeper insight into the nature of this phenomenon. At that time, we, professor's students, saw only this aspect of his theory. Later, I realized that the mentioned Yukhnovskii's work naturally fits into a more general context of the turbulent development of quantum field theory and statistical physics in the last quarter of the twentieth century. The aim of the present article is to look at the main aspects and the impact of Yukhnovskii's theory from this perspective."}
{"id": "2512.14451", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14451", "abs": "https://arxiv.org/abs/2512.14451", "authors": ["Gil Serrano", "Marcelo Jacinto", "Bruno J. Guerreiro", "Rita Cunha"], "title": "Equivariant Observer for Bearing Estimation with Linear and Angular Velocity Inputs", "comment": "This work has been submitted to the 2026 European Control Conference", "summary": "This work addresses the problem of designing an equivariant observer for a first order dynamical system on the unit-sphere. Building upon the established case of unit bearing vector dynamics with angular velocity inputs, we introduce an additional linear velocity input projected onto the unit-sphere tangent space. This extended formulation is particularly useful in image-based visual servoing scenarios where stable bearing estimates are required and the relative velocity between the vehicle and target features must be accounted for. Leveraging lifted kinematics to the Special Orthogonal group, we design an observer for the bearing vector and prove its almost global asymptotic stability. Additionally, we demonstrate how the equivariant observer can be expressed in the original state manifold. Numerical simulation results validate the effectiveness of the proposed algorithm."}
{"id": "2512.14349", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14349", "abs": "https://arxiv.org/abs/2512.14349", "authors": ["Federico Califano", "Camilla Rota", "Riccardo Zanella", "Antonio Franchi"], "title": "A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators", "comment": null, "summary": "We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $η=J(q)\\dot{q}$, where $q$ is a point on the configuration manifold, $η$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \\textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation."}
{"id": "2512.14258", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14258", "abs": "https://arxiv.org/abs/2512.14258", "authors": ["Marcin Baranek", "Paweł Przybyłowicz"], "title": "SPINNs -- Deep learning framework for approximation of stochastic differential equations", "comment": null, "summary": "In this paper, we introduce the SPINNs (stochastic physics-informed neural networks) in a systematic manner. This provides a mathematical framework for approximating the solution of stochastic differential equations (SDEs) driven by Levy noise using artificial neural networks."}
{"id": "2512.14399", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14399", "abs": "https://arxiv.org/abs/2512.14399", "authors": ["Dániel Pfeifer", "Edith Alice Kovács"], "title": "Trunc-Opt vine building algorithms", "comment": null, "summary": "Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kovács and Szántai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods."}
{"id": "2512.13828", "categories": ["quant-ph", "physics.comp-ph", "physics.optics", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2512.13828", "abs": "https://arxiv.org/abs/2512.13828", "authors": ["Artur Czerwinski", "Jakub J. Borkowski", "Saeed Haddadi"], "title": "Optical Downlink Modeling for LEO and MEO Satellites under Atmospheric Turbulence with a Quantum State Tomography Use Case", "comment": null, "summary": "This paper presents a comprehensive analysis of the link budget for free-space optical systems involving Low Earth Orbit (LEO) and Medium Earth Orbit (MEO) satellites. We develop a detailed model of the satellite-to-ground channel that accounts for the primary physical processes affecting transmittance: atmospheric absorption and scattering, free-space diffraction, and turbulence-induced fluctuations. The study introduces a general method for computing transmittance along a slant path between a satellite and an optical ground station, incorporating zenith angle, slant range, and altitude-dependent attenuation. The proposed framework is intended to support the design and evaluation of space-based optical links and serves as a critical tool for defining technical specifications in satellite communication demonstrators and simulations. Numerical estimates are provided to illustrate the magnitude of losses under typical operational conditions, including the role of aperture averaging. In addition to the link budget analysis, we introduce a satellite-based quantum use case. We propose a scheme for quantum state tomography performed on states generated by an onboard photon source on an LEO or MEO satellite and transmitted to the optical ground station. This approach enables continuous verification of the quality of quantum resources that can be used to perform quantum protocols within quantum information networks."}
{"id": "2512.14414", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14414", "abs": "https://arxiv.org/abs/2512.14414", "authors": ["Hongyu Chen", "Yangfeng Fu", "Weiqiang Yu", "Rong Yu", "Z. Y. Xie"], "title": "A single-layer framework of variational tensor network states", "comment": null, "summary": "We propose a single-layer tensor network framework for the variational determination of ground states in two-dimensional quantum lattice models. By combining the nested tensor network method [Phys. Rev. B 96, 045128 (2017)] with the automatic differentiation technique, our approach can reduce the computational cost by three orders of magnitude in bond dimension, and therefore enables highly efficient variational ground-state calculations. We demonstrate the capability of this framework through two quantum spin models: the antiferromagnetic Heisenberg model on a square lattice and the frustrated Shastry-Sutherland model. Even without GPU acceleration or symmetry implimention, we have achieved the bond dimension of 9 and obtained accurate ground-state energy and consistent order parameters compared to prior studies. In particular, we confirm the existence of an intermediate empty-plaquette valence bond solid ground state in the Shastry-Sutherland model. We have further discussed the convergence of the algorithm and its potential improvements. Our work provides a promising route for large-scale tensor network calculations of two-dimensional quantum systems."}
{"id": "2512.14468", "categories": ["math.OC", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.14468", "abs": "https://arxiv.org/abs/2512.14468", "authors": ["Xinhua Shen", "Hongpeng Sun"], "title": "A preconditioned second-order convex splitting algorithm with extrapolation", "comment": null, "summary": "Nonconvex optimization problems are widespread in modern machine learning and data science. We introduce an extrapolation strategy into a class of preconditioned second-order convex splitting algorithms for nonconvex optimization problems. The proposed algorithms combine second-order backward differentiation formulas (BDF2) with an extrapolation method. Meanwhile, the implicit-explicit scheme simplifies the subproblem through a preconditioned process. As a result, our approach solves nonconvex problems efficiently without significant computational overhead. Theoretical analysis establishes global convergence of the algorithms using Kurdyka-Łojasiewicz properties. Numerical experiments include a benchmark problem, the least squares problem with SCAD regularization, and an image segmentation problem. These results demonstrate that our algorithms are highly efficient, as they achieve reduced solution times and competitive performance."}
{"id": "2512.13371", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.13371", "abs": "https://arxiv.org/abs/2512.13371", "authors": ["Lindsay Bassman Oftelie", "Michele Campisi"], "title": "Impact of Information on Quantum Heat Engines", "comment": "11 pages, 3 figures", "summary": "The emerging field of quantum thermodynamics is beginning to reveal the intriguing role that information can play in quantum thermal engines. Information enters as a resource when considering feedback-controlled thermal machines. While both a general theory of quantum feedback control as well as specific examples of quantum feedback-controlled engines have been presented, still lacking is a general framework for such machines. Here, we present a framework for a generic, two-stroke quantum heat engine interacting with $N$ thermal baths and Maxwell's demon. The demon performs projective measurements on the engine working substance, the outcome of which is recorded in a classical memory, embedded in its own thermal bath. To perform feedback control, the demon enacts unitary operations on the working substance, conditioned on the recorded outcome. By considering the compound machine-memory as a hybrid (classical-quantum) standard thermal machine interacting with $N+1$ thermal baths, our framework puts the working substance and memory on equal footing, thereby enabling a comprehensible resolution to Maxwell's paradox. We illustrate the application of our framework with a two-qubit engine. A remarkable observation is that more information does not necessarily result in better thermodynamic performance: sometimes knowing less is better."}
{"id": "2512.14510", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14510", "abs": "https://arxiv.org/abs/2512.14510", "authors": ["Aihui Liu", "Magnus Jansson"], "title": "Closed-Loop Consistent, Causal Data-Driven Predictive Control via SSARX", "comment": null, "summary": "We propose a fundamental-lemma-free data-driven predictive control (DDPC) scheme for synthesizing model predictive control (MPC)-like policies directly from input-output data. Unlike the well-known DeePC approach and other DDPC methods that rely on Willems' fundamental lemma, our method avoids stacked Hankel representations and the DeePC decision variable g. Instead, we develop a closed-loop consistent, causal DDPC scheme based on the multi-step predictor Subspace-ARX (SSARX). The method first (i) estimates predictor/observer Markov parameters via a high-order ARX model to decouple the noise, then (ii) learns a multi-step past-to-future map by regression, optionally with a reduced-rank constraint. The SSARX predictor is strictly causal, which allows it to be integrated naturally into an MPC formulation. Our experimental results show that SSARX performs competitively with other methods when applied to closed-loop data affected by measurement and process noise."}
{"id": "2512.14412", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14412", "abs": "https://arxiv.org/abs/2512.14412", "authors": ["Gil Serrano", "Pedro Lourenço", "Bruno J. Guerreiro", "Rita Cunha"], "title": "Equivariant Filter Cascade for Relative Attitude, Target's Angular Velocity, and Gyroscope Bias Estimation", "comment": "This work has been submitted to the 2026 European Control Conference", "summary": "Rendezvous and docking between a chaser spacecraft and an uncooperative target, such as an inoperative satellite, require synchronization between the chaser spacecraft and the target. In these scenarios, the chaser must estimate the relative attitude and angular velocity of the target using onboard sensors, in the presence of gyroscope bias. In this work, we propose a cascade of Equivariant Filters (EqF) to address this problem. The first stage of the cascade estimates the chaser's attitude and the bias, using measurements from a star tracker, while the second stage of the cascade estimates the relative attitude and the target's angular velocity, using observations of two known, non-collinear vectors fixed in the target frame. The stability of the EqF cascade is theoretically analyzed and simulation results demonstrate the filter cascade's performance."}
{"id": "2512.14286", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.14286", "abs": "https://arxiv.org/abs/2512.14286", "authors": ["Samuel Cruz Alegría", "Bindi Çapriqi", "Shega Likaj", "Ken Trotti", "Rolf Krause"], "title": "An Additively Preconditioned Trust Region Strategy for Machine Learning", "comment": "13 Pages", "summary": "Modern machine learning, especially the training of deep neural networks, depends on solving large-scale, highly nonconvex optimization problems, whose objective function exhibit a rough landscape. Motivated by the success of parallel preconditioners in the context of Krylov methods for large scale linear systems, we introduce a novel nonlinearly preconditioned Trust-Region method that makes use of an additive Schwarz correction at each minimization step, thereby accelerating convergence.\n  More precisely, we propose a variant of the Additively Preconditioned Trust-Region Strategy (APTS), which combines a right-preconditioned additive Schwarz framework with a classical Trust-Region algorithm. By decomposing the parameter space into sub-domains, APTS solves local non-linear sub-problems in parallel and assembles their corrections additively. The resulting method not only shows fast convergence; due to the underlying Trust-Region strategy, it furthermore largely obviates the need for hyperparameter tuning."}
{"id": "2512.14413", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14413", "abs": "https://arxiv.org/abs/2512.14413", "authors": ["Aymen Echarghaoui", "Robert Tibshirani"], "title": "Univariate-Guided Interaction Modeling", "comment": null, "summary": "We propose a procedure for sparse regression with pairwise interactions, by generalizing the Univariate Guided Sparse Regression (UniLasso) methodology. A central contribution is our introduction of a concept of univariate (or marginal) interactions. Using this concept, we propose two algorithms -- uniPairs and uniPairs-2stage -- , and evaluate their performance against established methods, including Glinternet and Sprinter. We show that our framework yields sparser models with more interpretable interactions. We also prove support recovery results for our proposal under suitable conditions."}
{"id": "2512.13835", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13835", "abs": "https://arxiv.org/abs/2512.13835", "authors": ["Hilario Espinós", "Omkar Dhungel", "Arne Wickenbrock", "Dmitry Budker", "Ricardo Puebla", "Erik Torrontegui"], "title": "Microwave-free vector magnetometry and crystal orientation determination with Nitrogen-Vacancy centers using Bayesian inference", "comment": "11 pages, 6 figures", "summary": "Nitrogen-vacancy (NV) centers in diamond provide a solid-state platform for quantum sensing. While optically detected magnetic resonance techniques offer high sensitivity, their reliance on microwaves introduces heating and stray electromagnetic fields that can perturb nearby samples. Optical approaches based on cross-relaxation between differently oriented NV centers remove this constraint but have so far required stringent alignment of the external field with crystallographic axes, restricting their practicality. Here we introduce a general framework for microwave-free vector magnetometry at near-zero field that leverages Bayesian inference to extract both the magnetic field vector and the NV orientation directly from photoluminescence maps. An analytical model of cross-relaxation resonances enables efficient inference under arbitrary field and orientation configurations, while naturally incorporating the discrete degeneracies of the NV symmetry. We experimentally demonstrate robust orientation determination and vector-field reconstruction, establishing a general route toward compact and alignment-free NV magnetometers for practical sensing applications."}
{"id": "2512.14558", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14558", "abs": "https://arxiv.org/abs/2512.14558", "authors": ["Mikhail M. Glazov", "Atac Imamoglu"], "title": "Long-range ferroelectric order in two dimensional excitonic insulators", "comment": "15 pages, 6 figures", "summary": "It is generally argued that Mermin-Wagner theorem excludes the possibility of long-range order in two dimensional bosonic systems at non-zero temperatures. In contrast, we show here that generic bilayer semiconductors could demonstrate true Bose-Einstein condensation of interlayer excitons. We show that the key requirements include (i) reduction of the interlayer band gap using an applied electric field so that excitons spontaneously appear in the ground state, (ii) band structure that allows for long-range electron-hole exchange interaction, and (iii) a finite magnetic field. Our results indicate that superfluidity and ferroelectric order can co-exist in two dimensional excitonic insulators."}
{"id": "2512.14507", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2512.14507", "abs": "https://arxiv.org/abs/2512.14507", "authors": ["Nathan Allaire", "Sébastien Le Digabel", "Dominique Orban"], "title": "An Inexact Modified Quasi-Newton Method for Nonsmooth Regularized Optimization", "comment": null, "summary": "We introduce iR2N, a modified proximal quasi-Newton method for minimizing the sum of a smooth function $f$ and a lower semi-continuous prox-bounded function $h$, allowing inexact evaluations of $f$, its gradient, and the associated proximal operators. Both $f$ and $h$ may be nonconvex. iR2N is particularly suited to settings where proximal operators are computed via iterative procedures that can be stopped early, or where the accuracy of $f$ and $\\nabla f$ can be controlled, leading to significant computational savings. At each iteration, the method approximately minimizes the sum of a quadratic model of $f$, a model of $h$, and an adaptive quadratic regularization term ensuring global convergence. Under standard accuracy assumptions, we prove global convergence in the sense that a first-order stationarity measure converges to zero, with worst-case evaluation complexity $O(ε^{-2})$. Numerical experiments with $\\ell_p$ norms, $\\ell_p$ total variation, and the indicator of the nonconvex pseudo $p$-norm ball illustrate the effectiveness and flexibility of the approach, and show how controlled inexactness can substantially reduce computational effort."}
{"id": "2512.13809", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2512.13809", "abs": "https://arxiv.org/abs/2512.13809", "authors": ["Kabir Khanna", "Romain Vasseur"], "title": "Universal Statistics of Measurement-Induced Entanglement in Tomonaga-Luttinger liquids", "comment": null, "summary": "We study the statistics of measurement-induced entanglement (MIE) after partial measurement on a class of one-dimensional quantum critical states described by Tomonaga-Luttinger liquids at low energies. Using a replica trick to average over measurement outcomes in the charge basis and tools from conformal field theory (CFT), we derive closed-form expressions for the cumulants of MIE. We show that exact Born-averaging over microscopic measurement outcomes becomes equivalent at low energy to averaging over conformal boundary conditions weighted by their corresponding partition functions. Our results yield distinctive critical behavior across all cumulants in the regime where the unmeasured parts of the system are maximally separated. We also obtain the full distribution of the post-measurement entanglement entropy, finding that it is generically bimodal and exhibits fat-tails. We corroborate our analytical predictions by numerical calculations and find good agreement between them."}
{"id": "2512.14535", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14535", "abs": "https://arxiv.org/abs/2512.14535", "authors": ["Thomas O. de Jong", "Mircea Lazar", "Siep Weiland", "Florian Dörfler"], "title": "Scalable Nonlinear DeePC: Bridging Direct and Indirect Methods and Basis Reduction", "comment": null, "summary": "This paper studies regularized data-enabled predictive control (DeePC) within a nonlinear framework and its relationship to subspace predictive control (SPC). The $Π$-regularization is extended to general basis functions and it is shown that, under suitable conditions, the resulting basis functions DeePC formulation constitutes a relaxation of basis functions SPC. To improve scalability, we introduce an SVD-based dimensionality reduction that preserves the equivalence with SPC, and we derive a reduced Π-regularization. A LASSO based sparse basis selection method is proposed to obtain a reduced basis from lifted data. Simulations on a nonlinear van der Pol oscillator model indicate that, in the absence of noise, DeePC and SPC yield equivalent absolute mean tracking errors (AMEs) when large penalties are applied. In contrast, under noisy measurements, careful tuning of the DeePC regularization results in a reduced AME, outperforming SPC."}
{"id": "2512.14450", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14450", "abs": "https://arxiv.org/abs/2512.14450", "authors": ["Riccardo Busetto", "Elia Cereda", "Marco Forgione", "Gabriele Maroni", "Dario Piga", "Daniele Palossi"], "title": "Nonlinear System Identification Nano-drone Benchmark", "comment": null, "summary": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics."}
{"id": "2512.14301", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.14301", "abs": "https://arxiv.org/abs/2512.14301", "authors": ["Rami Katz", "Dmitry Batenkov", "Giulia Giordano"], "title": "Separation-free exponential fitting with structured noise, with applications to inverse problems in parabolic PDEs", "comment": null, "summary": "We investigate the recovery of exponents and amplitudes of an exponential sum, where the exponents $\\left\\{λ_n \\right\\}_{n=1}^{N_1}$ are the first $N_1$ eigenvalues of a Sturm-Liouville operator, from finitely many measurements subject to measurement noise. This inverse problem is extremely ill-conditioned when the noise is arbitrary and unstructured. Surprisingly, however, the extreme ill-conditioning exhibited by this problem disappears when considering a \\emph{structured} noise term, taken as an exponential sum with exponents given by the subsequent eigenvalues $\\left\\{λ_n \\right\\}_{n=N_1+1}^{N_1+N_2}$ of the Sturm-Liouville operator, multiplied by a noise magnitude parameter $\\varepsilon>0$. In this case, we rigorously show that the exponents and amplitudes can be recovered with super-exponential accuracy: we both prove the theoretical result and show that it can be achieved numerically by a specific algorithm. By leveraging recent results on the mathematical theory of super-resolution, we show in this paper that the classical Prony's method attains the analytic optimal error decay also in the ``separation-free'' regime where $λ_n \\to \\infty$ as $n \\to \\infty$, thereby extending the applicability of Prony's method to new settings. As an application of our theoretical analysis, we show that the approximated eigenvalues obtained by our method can be used to recover an unknown potential in a linear reaction-diffusion equation from discrete solution traces."}
{"id": "2512.14492", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14492", "abs": "https://arxiv.org/abs/2512.14492", "authors": ["Martin Slawski"], "title": "Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error", "comment": null, "summary": "The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach."}
{"id": "2512.13882", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2512.13882", "abs": "https://arxiv.org/abs/2512.13882", "authors": ["Shilpa Mahato", "Rajibul Islam"], "title": "Achieving $10^{-5}$ level relative intensity crosstalk in optical holographic qubit addressing via a double-pass digital micromirror device", "comment": "7 pages, 3 figures", "summary": "Holographic beam shaping is a powerful approach for generating individually addressable optical spots for controlling atomic qubits, such as those in trapped-ion quantum processors. However, its application in qubit control is limited by residual intensity crosstalk at neighboring sites and by a nonzero background floor in the far wings of the addressing beam, leading to accumulated errors from many exposed qubits. Here, we present an all-optical scheme that mitigates both effects using a single digital micromirror device (DMD) operated in a double-pass configuration, in which light interacts with two separate regions of the same device. In the first pass, one region of the DMD is placed in a Fourier plane and implements a binary-amplitude hologram for individual addressing, while in the second pass a different region serves as a programmable intermediate image-plane aperture for spatial filtering. By multiplexing the Fourier-plane hologram to include secondary holograms, we generate weak auxiliary fields that interfere destructively with unwanted light at selected sites, while image-plane filtering suppresses the residual tail at larger distances. Together, these techniques maintain relative intensity crosstalk at or below $10^{-5}$ ($-50\\,\\mathrm{dB}$) across the full field of view relevant for qubit addressing, and further reduce the far-wing background to approximately $10^{-6}$ at large distances from the addressed qubit, approaching the detection limit. These results provide a compact, DMD-based solution for low-crosstalk optical holographic qubit addressing that is directly applicable to trapped ions and other spatially ordered quantum systems."}
{"id": "2512.14597", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.14597", "abs": "https://arxiv.org/abs/2512.14597", "authors": ["Frédéric Chassot", "Aki Pulkkinen", "Ján Minár", "Gunther Springholz", "Matthias Hengsberger", "Claude Monney"], "title": "Detection of Image Potential States above the vacuum level in GeTe", "comment": "10 pages, 9 figures", "summary": "The ferroelectric semiconductor α-GeTe(111) has attracted significant attention in the last decade due to its unique properties, with extensive studies focusing on its occupied electronic bandstructure. In contrast, its unoccupied states - particularly those near the conduction band minimum - remain largely unexplored. In an effort to characterize those states, we surprisingly observe three image potential states (IPS) in α-GeTe(111) extending up to 0.8 eV above the vacuum level. Using time and angle-resolved photoemission spectroscopy, we resolve the full parabolic dispersions of the first three IPS and determine their binding energies. Our analysis, combined with Bloch spectral function calculations, reveals that the unexpected persistence of IPS above the vacuum level originates from strong dipole transitions and the presence of large electron reservoirs in GeTe."}
{"id": "2512.14520", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14520", "abs": "https://arxiv.org/abs/2512.14520", "authors": ["Aihui Liu", "Magnus Jansson"], "title": "The Innovation Null Space of the Kalman Predictor: A Stochastic Perspective for DeePC", "comment": null, "summary": "Willems' fundamental lemma uses a key decision variable $g$ to combine measured input-output data and describe trajectories of a linear time-invariant system. In this paper, we ask: what is a good choice for this vector $g$ when the system is affected by noise? For a linear system with Gaussian noise, we show that there exists an optimal subspace for this decision variable $g$, which is the null space of the innovation Hankel matrix. If the decision vector lies in this null space, the resulting predictor gets closer to the Kalman predictor. To show this, we use a result that we refer to as the Kalman Filter Fundamental Lemma (KFFL), which applies Willems' lemma to the Kalman predictor. This viewpoint also explains several existing data-driven predictive control methods: regularized DeePC schemes act as soft versions of the innovation null-space constraint, instrumental-variable methods enforce it by construction, and ARX-based approaches explicitly estimate this innovation null space."}
{"id": "2512.14583", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.14583", "abs": "https://arxiv.org/abs/2512.14583", "authors": ["Cesar Lema", "Aleix Bou-Comas", "Atithi Acharya", "Vadim Oganesyan", "Anirvan Sengupta"], "title": "Reading Qubits with Sequential Weak Measurements: Limits of Information Extraction", "comment": "25 pages, 4 figures", "summary": "Quantum information processing and computation requires high accuracy qubit configuration readout. In many practical schemes, the initial qubit configuration has to be inferred from readout that is a time-dependent weak measurement record. However, a combination of the measurement scheme and intrinsic dynamics can end up scrambling the initial state and lose information irretrievably. Here, we study the information physics of quantum trajectories based on weak measurements in order to address the optimal achievable performance in qubit configuration readout for two realistic models of single qubit readout: (i) Model I is informationally complete, but without intrinsic dynamics; (ii) Model II is informationally incomplete weak measurements with intrinsic dynamics. We first use mutual information to characterize how much intrinsic information about the initial state is encoded in the measurement record. Using a fixed discrete time-step formulation, we compute the mutual information while varying the measurement strength, duration of measurement record, and the relative strength of intrinsic dynamics in our measurement schemes. We also exploit the emergence of continuum scaling and the Stochastic Master Equation in the weak measurement limit. We develop an asymptotic expansion in the measurement efficiency parameter to calculate mutual information, which captures qualitative and quantitative features of the numerical data. The bounds on information extraction are manifested as plateaux in mutual information, our analysis obtains these bounds and also optimal duration of measurement required to saturate them. Our results should be useful both for quantum device operation and optimization and also, possibly, for improving the performance of recent machine learning approaches for qubit and multiqubit configuration readout in current Noisy Intermediate-Scale Quantum (NISQ) experiment regimes."}
{"id": "2512.13890", "categories": ["quant-ph", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13890", "abs": "https://arxiv.org/abs/2512.13890", "authors": ["Charles Marrder", "Shuo Sun", "Murray J. Holland"], "title": "Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences", "comment": null, "summary": "Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise."}
{"id": "2512.14451", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14451", "abs": "https://arxiv.org/abs/2512.14451", "authors": ["Gil Serrano", "Marcelo Jacinto", "Bruno J. Guerreiro", "Rita Cunha"], "title": "Equivariant Observer for Bearing Estimation with Linear and Angular Velocity Inputs", "comment": "This work has been submitted to the 2026 European Control Conference", "summary": "This work addresses the problem of designing an equivariant observer for a first order dynamical system on the unit-sphere. Building upon the established case of unit bearing vector dynamics with angular velocity inputs, we introduce an additional linear velocity input projected onto the unit-sphere tangent space. This extended formulation is particularly useful in image-based visual servoing scenarios where stable bearing estimates are required and the relative velocity between the vehicle and target features must be accounted for. Leveraging lifted kinematics to the Special Orthogonal group, we design an observer for the bearing vector and prove its almost global asymptotic stability. Additionally, we demonstrate how the equivariant observer can be expressed in the original state manifold. Numerical simulation results validate the effectiveness of the proposed algorithm."}
{"id": "2512.14416", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14416", "abs": "https://arxiv.org/abs/2512.14416", "authors": ["Björn Liljegren-Sailer"], "title": "Reducing Training Complexity in Empirical Quadrature-Based Model Reduction via Structured Compression", "comment": null, "summary": "Model order reduction seeks to approximate large-scale dynamical systems by lower-dimensional reduced models. For linear systems, a small reduced dimension directly translates into low computational cost, ensuring online efficiency. This property does not generally hold for nonlinear systems, where an additional approximation of nonlinear terms -- known as complexity reduction -- is required. To achieve online efficiency, empirical quadrature and cell-based empirical cubature are among the most effective complexity reduction techniques. However, existing offline training algorithms can be prohibitively expensive because they operate on raw snapshot data of all nonlinear integrands associated with the reduced model. In this paper, we introduce a preprocessing approach based on a specific structured compression of the training data. Its key feature is that it scales only with the number of collected snapshots, rather than additionally with the reduced model dimension. Overall, this yields roughly an order-of-magnitude reduction in offline computational cost and memory requirements, thereby enabling the application of the complexity reduction methods to larger-scale problems. Accuracy is preserved, as indicated by our error analysis and demonstrated through numerical examples."}
{"id": "2512.14504", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14504", "abs": "https://arxiv.org/abs/2512.14504", "authors": ["Emanuele Giorgi", "Jonas Wallin"], "title": "A flexible class of latent variable models for the analysis of antibody response data", "comment": "This is a working paper, and updated versions will be released in the future. For further information about this research, please contact Emanuele Giorgi at e.giorgi@bham.ac.uk", "summary": "Existing approaches to modelling antibody concentration data are mostly based on finite mixture models that rely on the assumption that individuals can be divided into two distinct groups: seronegative and seropositive. Here, we challenge this dichotomous modelling assumption and propose a latent variable modelling framework in which the immune status of each individual is represented along a continuum of latent seroreactivity, ranging from minimal to strong immune activation. This formulation provides greater flexibility in capturing age-related changes in antibody distributions while preserving the full information content of quantitative measurements. We show that the proposed class of models can accommodate a great variety of model formulations, both mechanistic and regression-based, and also includes finite mixture models as a special case. We demonstrate the advantages of this approach using malaria serology data and its ability to develop joint analyses across all ages that account for changes in transmission patterns. We conclude by outlining extensions of the proposed modelling framework and its relevance to other omics applications."}
{"id": "2512.13887", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13887", "abs": "https://arxiv.org/abs/2512.13887", "authors": ["Xinfeng Gao", "Olivier Pfister", "Stefan Bekiranov"], "title": "Implementing the Koopman-von Neumann approach on continuous-variable photonic quantum computers", "comment": "9 pages, 3 figures, submitted for publication", "summary": "The Koopman-von Neumann (KvN) formalism recasts classical mechanics in a Hilbert space framework using complex wavefunctions and linear operators, akin to quantum mechanics. Instead of evolving probability densities in phase space (as in Liouville's equation), KvN uses a Schrödinger-like equation for a classical wavefunction, with commuting position and momentum operators. Mapped to quantum computing, KvN offers a promising route to simulate classical dynamical systems using quantum algorithms by leveraging unitary evolution and quantum linear algebra tools, potentially enabling efficient classical-to-quantum mappings without invoking full quantum uncertainty. In this work, we specifically explore the implementation of the KvN approach on continuous-variable photonic quantum computing architectures, with the goals of leveraging quantum simulation for both sampling and computing intractable nonlinear dynamics. We will demonstrate its implementation and feasibility with two problems: the harmonic oscillator and a 1D partial differential equation governing nonlinear dynamics."}
{"id": "2512.14682", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14682", "abs": "https://arxiv.org/abs/2512.14682", "authors": ["David O. Williams Rogers", "Hang Woon Lee"], "title": "Enhancing Orbital Debris Remediation with Reconfigurable Space-Based Laser Constellations", "comment": "Accepted to the 2026 IEEE Aerospace Conference", "summary": "Orbital debris poses an escalating threat to space missions and the long-term sustainability of Earth's orbital environment. The literature proposes various approaches for orbital debris remediation, including the use of multiple space-based lasers that collaboratively engage debris targets. While the proof of concept for this laser-based approach has been demonstrated, critical questions remain about its scalability and responsiveness as the debris population continues to expand rapidly. This paper introduces constellation reconfiguration as a system-level strategy to address these limitations. Through coordinated orbital maneuvers, laser-equipped satellites can dynamically adapt their positions to respond to evolving debris distributions and time-critical events. We formalize this concept as the Reconfigurable Laser-to-Debris Engagement Scheduling Problem (R-L2D-ESP), an optimization framework that determines the optimal sequence of constellation reconfigurations and laser engagements to maximize debris remediation capacity, which quantifies the constellation's ability to nudge, deorbit, or perform just-in-time collision avoidance maneuvers on debris objects. To manage the complexity of this combinatorial optimization problem, we employ a receding horizon approach. Our experiments reveal that reconfigurable constellations significantly outperform static ones, achieving greater debris remediation capacity and successfully deorbiting substantially more debris objects. Additionally, our sensitivity analyses identify the key parameters that influence remediation performance the most, providing essential insights for future system design. These findings demonstrate that constellation reconfiguration represents a promising advancement for laser-based debris removal systems, offering the adaptability and scalability necessary to enhance this particular approach to orbital debris remediation."}
{"id": "2512.14520", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14520", "abs": "https://arxiv.org/abs/2512.14520", "authors": ["Aihui Liu", "Magnus Jansson"], "title": "The Innovation Null Space of the Kalman Predictor: A Stochastic Perspective for DeePC", "comment": null, "summary": "Willems' fundamental lemma uses a key decision variable $g$ to combine measured input-output data and describe trajectories of a linear time-invariant system. In this paper, we ask: what is a good choice for this vector $g$ when the system is affected by noise? For a linear system with Gaussian noise, we show that there exists an optimal subspace for this decision variable $g$, which is the null space of the innovation Hankel matrix. If the decision vector lies in this null space, the resulting predictor gets closer to the Kalman predictor. To show this, we use a result that we refer to as the Kalman Filter Fundamental Lemma (KFFL), which applies Willems' lemma to the Kalman predictor. This viewpoint also explains several existing data-driven predictive control methods: regularized DeePC schemes act as soft versions of the innovation null-space constraint, instrumental-variable methods enforce it by construction, and ARX-based approaches explicitly estimate this innovation null space."}
{"id": "2512.14510", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14510", "abs": "https://arxiv.org/abs/2512.14510", "authors": ["Aihui Liu", "Magnus Jansson"], "title": "Closed-Loop Consistent, Causal Data-Driven Predictive Control via SSARX", "comment": null, "summary": "We propose a fundamental-lemma-free data-driven predictive control (DDPC) scheme for synthesizing model predictive control (MPC)-like policies directly from input-output data. Unlike the well-known DeePC approach and other DDPC methods that rely on Willems' fundamental lemma, our method avoids stacked Hankel representations and the DeePC decision variable g. Instead, we develop a closed-loop consistent, causal DDPC scheme based on the multi-step predictor Subspace-ARX (SSARX). The method first (i) estimates predictor/observer Markov parameters via a high-order ARX model to decouple the noise, then (ii) learns a multi-step past-to-future map by regression, optionally with a reduced-rank constraint. The SSARX predictor is strictly causal, which allows it to be integrated naturally into an MPC formulation. Our experimental results show that SSARX performs competitively with other methods when applied to closed-loop data affected by measurement and process noise."}
{"id": "2512.14419", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14419", "abs": "https://arxiv.org/abs/2512.14419", "authors": ["Xiaoqi Ma", "Jin Zhang"], "title": "Semi-robust equal-order hybridized discontinuous methods", "comment": null, "summary": "This paper introduces a unified analysis framework of equal-order hybridized discontinuous finite element (HDG) methods. The general framework covers standard HDG, embedded discontinuous finite element, and embedded-hybridized discontinuous finite element methods."}
{"id": "2512.14609", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14609", "abs": "https://arxiv.org/abs/2512.14609", "authors": ["Marc-Oliver Pohle", "Jan-Lukas Wermuth", "Christian H. Weiß"], "title": "Asymptotic Inference for Rank Correlations", "comment": null, "summary": "Kendall's tau and Spearman's rho are widely used tools for measuring dependence. Surprisingly, when it comes to asymptotic inference for these rank correlations, some fundamental results and methods have not yet been developed, in particular for discrete random variables and in the time series case, and concerning variance estimation in general. Consequently, asymptotic confidence intervals are not available. We provide a comprehensive treatment of asymptotic inference for classical rank correlations, including Kendall's tau, Spearman's rho, Goodman-Kruskal's gamma, Kendall's tau-b, and grade correlation. We derive asymptotic distributions for both iid and time series data, resorting to asymptotic results for U-statistics, and introduce consistent variance estimators. This enables the construction of confidence intervals and tests, generalizes classical results for continuous random variables and leads to corrected versions of widely used tests of independence. We analyze the finite-sample performance of our variance estimators, confidence intervals, and tests in simulations and illustrate their use in case studies."}
{"id": "2512.13890", "categories": ["quant-ph", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13890", "abs": "https://arxiv.org/abs/2512.13890", "authors": ["Charles Marrder", "Shuo Sun", "Murray J. Holland"], "title": "Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences", "comment": null, "summary": "Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise."}
{"id": "2512.13774", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2512.13774", "abs": "https://arxiv.org/abs/2512.13774", "authors": ["Rahel Lea Baumgartner", "Pietro Pelliconi", "Soumik Bandyopadhyay", "Francesca Orsi", "Philipp Hauke", "Jean-Philippe Brantut", "Julian Sonner"], "title": "Quantum simulation using Trotterized disorder Hamiltonians in a single-mode optical cavity", "comment": "54 pages, 11 figures, plus appendices", "summary": "All-to-all interacting and disordered many-body systems are notoriously hard to simulate on quantum platforms, as interactions are commonly mediated by auxiliary degrees of freedom that lower the amount of disorder, introducing undesired correlations. In this work, we show how a Trotterization scheme can be effectively utilized to densify the disorder of the model. In particular, we study the statistical properties of the resulting model, as well as Trotterization errors in the simulation that affect the time evolution and dynamical observables. As a concrete example, we propose an implementation via a single-mode cavity QED platform of the complex Sachdev-Ye-Kitaev model. We analyze several features of the effective model, such as the distribution of the effective couplings, the number of interacting sites, state preparation, and the behavior of quantum chaos probes. We conclude this work with a detailed investigation of the robustness of our findings against dissipation, both analytically and numerically."}
{"id": "2512.13836", "categories": ["eess.SY", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13836", "abs": "https://arxiv.org/abs/2512.13836", "authors": ["Ricardo Tapia", "Iman Soltani"], "title": "A Convex Obstacle Avoidance Formulation", "comment": "18 pages, 17 figures", "summary": "Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear."}
{"id": "2512.14682", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14682", "abs": "https://arxiv.org/abs/2512.14682", "authors": ["David O. Williams Rogers", "Hang Woon Lee"], "title": "Enhancing Orbital Debris Remediation with Reconfigurable Space-Based Laser Constellations", "comment": "Accepted to the 2026 IEEE Aerospace Conference", "summary": "Orbital debris poses an escalating threat to space missions and the long-term sustainability of Earth's orbital environment. The literature proposes various approaches for orbital debris remediation, including the use of multiple space-based lasers that collaboratively engage debris targets. While the proof of concept for this laser-based approach has been demonstrated, critical questions remain about its scalability and responsiveness as the debris population continues to expand rapidly. This paper introduces constellation reconfiguration as a system-level strategy to address these limitations. Through coordinated orbital maneuvers, laser-equipped satellites can dynamically adapt their positions to respond to evolving debris distributions and time-critical events. We formalize this concept as the Reconfigurable Laser-to-Debris Engagement Scheduling Problem (R-L2D-ESP), an optimization framework that determines the optimal sequence of constellation reconfigurations and laser engagements to maximize debris remediation capacity, which quantifies the constellation's ability to nudge, deorbit, or perform just-in-time collision avoidance maneuvers on debris objects. To manage the complexity of this combinatorial optimization problem, we employ a receding horizon approach. Our experiments reveal that reconfigurable constellations significantly outperform static ones, achieving greater debris remediation capacity and successfully deorbiting substantially more debris objects. Additionally, our sensitivity analyses identify the key parameters that influence remediation performance the most, providing essential insights for future system design. These findings demonstrate that constellation reconfiguration represents a promising advancement for laser-based debris removal systems, offering the adaptability and scalability necessary to enhance this particular approach to orbital debris remediation."}
{"id": "2512.14535", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14535", "abs": "https://arxiv.org/abs/2512.14535", "authors": ["Thomas O. de Jong", "Mircea Lazar", "Siep Weiland", "Florian Dörfler"], "title": "Scalable Nonlinear DeePC: Bridging Direct and Indirect Methods and Basis Reduction", "comment": null, "summary": "This paper studies regularized data-enabled predictive control (DeePC) within a nonlinear framework and its relationship to subspace predictive control (SPC). The $Π$-regularization is extended to general basis functions and it is shown that, under suitable conditions, the resulting basis functions DeePC formulation constitutes a relaxation of basis functions SPC. To improve scalability, we introduce an SVD-based dimensionality reduction that preserves the equivalence with SPC, and we derive a reduced Π-regularization. A LASSO based sparse basis selection method is proposed to obtain a reduced basis from lifted data. Simulations on a nonlinear van der Pol oscillator model indicate that, in the absence of noise, DeePC and SPC yield equivalent absolute mean tracking errors (AMEs) when large penalties are applied. In contrast, under noisy measurements, careful tuning of the DeePC regularization results in a reduced AME, outperforming SPC."}
{"id": "2512.14467", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14467", "abs": "https://arxiv.org/abs/2512.14467", "authors": ["Neelakantan Padmanabhan"], "title": "Ensemble Parameter Estimation for the LPLSP Framework: A Rapid Approach to Reduced-Order Modeling for Transient Thermal Systems", "comment": null, "summary": "This work introduces an ensemble parameter estimation framework that enables the Lumped Parameter Linear Superposition (LPLSP) method to generate reduced order thermal models from a single transient dataset. Unlike earlier implementations that relied on multiple parametric simulations to excite each heat source independently, the proposed approach simultaneously identifies all model coefficients using fully transient excitations. Two estimation strategies namely rank-reduction and two-stage decomposition are developed to further reduce computational cost and improve scalability for larger systems. The proposed strategies yield ROMs with mean temperature-prediction errors within 5% of CFD simulations while reducing model-development times to O(10^0 s)-O(10^1 s). Once constructed, the ROM evaluates new transient operating conditions in O(10^0 s), enabling rapid thermal analysis and enabling automated generation of digital twins for both simulated and physical systems."}
{"id": "2512.13891", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.13891", "abs": "https://arxiv.org/abs/2512.13891", "authors": ["ChunJun Cao", "Giuseppe Cotardo", "Brad Lackey"], "title": "Quantum Anticodes", "comment": null, "summary": "This work introduces a symplectic framework for quantum error correcting codes in which local structure is analyzed through an anticode perspective. In this setting, a code is treated as a symplectic space, and anticodes arise as maximal symplectic subspaces whose elements vanish on a prescribed set of components, providing a natural quantum analogue of their classical counterparts. This framework encompasses several families of quantum codes, including stabilizer and subsystem codes, provides a natural extension of generalized distances in quantum codes, and yields new invariants that capture local algebraic and combinatorial features. The notion of anticodes also naturally leads to operations such as puncturing and shortening for symplectic codes, which in turn provide algebraic interpretations of key phenomena in quantum error correction, such as the cleaning lemma and complementary recovery and yield new descriptions of weight enumerators."}
{"id": "2512.13777", "categories": ["quant-ph", "cond-mat.str-el", "hep-th", "math-ph"], "pdf": "https://arxiv.org/pdf/2512.13777", "abs": "https://arxiv.org/abs/2512.13777", "authors": ["Alison Warman", "Sakura Schafer-Nameki"], "title": "Transversal Clifford-Hierarchy Gates via Non-Abelian Surface Codes", "comment": "18 pages", "summary": "We present a purely 2D transversal realization of phase gates at any level of the Clifford hierarchy, and beyond, using non-Abelian surface codes. Our construction encodes a logical qubit in the quantum double $D(G)$ of a non-Abelian group $G$ on a triangular spatial patch. The logical gate is implemented transversally by stacking on the spatial region a symmetry-protected topological (SPT) phase specified by a group 2-cocycle. The Bravyi--König theorem limits the unitary gates implementable by constant-depth quantum circuits on Pauli stabilizer codes in $D$ dimensions to the $D$-th level of the Clifford hierarchy. We bypass this, by constructing transversal unitary gates at arbitrary levels of the Clifford hierarchy purely in 2D, without sacrificing locality or fault tolerance, however at the cost of using the quantum double of a non-Abelian group $G$. Specifically, for $G = D_{4N}$, the dihedral group of order $8N$, we realize the phase gate $T^{1/N} = \\mathrm{diag}(1, e^{iπ/(4N)})$ in the logical $\\overline{Z}$ basis. For $8N = 2^n$, this gate lies at the $n$-th level of the Clifford hierarchy and, importantly, has a qubit-only realization: we show that it can be constructed in terms of Clifford-hierarchy stabilizers for a code with $n$ physical qubits on each edge of the lattice. We also discuss code-switching to the $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ and $\\mathbb{Z}_2$ toric codes, which can be utilized for the quantum error correction in this setup."}
{"id": "2512.13868", "categories": ["eess.SY", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13868", "abs": "https://arxiv.org/abs/2512.13868", "authors": ["Tianyu Zhou", "Zihao Liang", "Zehui Lu", "Shaoshuai Mou"], "title": "Safe Online Control-Informed Learning", "comment": null, "summary": "This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems."}
{"id": "2512.13890", "categories": ["quant-ph", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13890", "abs": "https://arxiv.org/abs/2512.13890", "authors": ["Charles Marrder", "Shuo Sun", "Murray J. Holland"], "title": "Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences", "comment": null, "summary": "Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise."}
{"id": "2512.14570", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2512.14570", "abs": "https://arxiv.org/abs/2512.14570", "authors": ["Zhaonan Dong", "Tanvi Wadhawan"], "title": "On the constants in inverse trace inequalities for polynomials orthogonal to lower-order subspaces", "comment": null, "summary": "We derive sharp, explicit constants in inverse trace inequalities for polynomial functions belonging to $\\mathbb{P}_p(T)$ (polynomial space with total degree $p$) that are orthogonal to the lower-order subspace $\\mathbb{P}_n(T)$, $n\\leq p$, where $T$ denotes a $d$-dimensional simplex. The proofs rely on orthogonal polynomial expansions on reference simplices and on a careful analysis of the eigenvalues of the relevant blocks of the face mass matrices, following the arguments developed in [9]. These results are very useful in the $hp$-analysis of the hybrid Galerkin methods, e.g. hybridizable discontinuous Galerkin methods, hybrid high-order methods, etc."}
{"id": "2512.13900", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13900", "abs": "https://arxiv.org/abs/2512.13900", "authors": ["Dragomir Davidovic"], "title": "Regulated reconstruction of long-time spin--boson dynamics and emergent zero-bias transverse measurement primitive", "comment": "18 pages 10 figures", "summary": "Time--convolutionless (TCL) master equations can break down at long times: time-local perturbative generators develop secular growth in correlation-dominated regimes. We mitigate this by a regulated, partially resummed reconstruction of the dynamical map around a Davies reference semigroup, expressed through a non--Markovian density-matrix correlator C(t) that remains bounded at late times. An exactly solvable rotating-wave benchmark links generator growth to interference-induced near-zeros of the coherence and shows how the reconstruction regulates the map. Applying the method to the unbiased spin--boson model reveals an emergent transverse measurement primitive: bath memory and counter--rotating terms induce phase lock-in that irreversibly erases the relative phase between $σ_x$ eigenspaces on a finite timescale $t_P$, yielding an effective zero-bias transverse ($σ_x$) measurement channel. The selected transverse basis is not assumed a priori; it follows from the reconstructed reduced dynamics. The effect disappears in the rotating-wave approximation and in the Davies weak-coupling limit, demonstrating its non--Markovian interference origin."}
{"id": "2512.13809", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2512.13809", "abs": "https://arxiv.org/abs/2512.13809", "authors": ["Kabir Khanna", "Romain Vasseur"], "title": "Universal Statistics of Measurement-Induced Entanglement in Tomonaga-Luttinger liquids", "comment": null, "summary": "We study the statistics of measurement-induced entanglement (MIE) after partial measurement on a class of one-dimensional quantum critical states described by Tomonaga-Luttinger liquids at low energies. Using a replica trick to average over measurement outcomes in the charge basis and tools from conformal field theory (CFT), we derive closed-form expressions for the cumulants of MIE. We show that exact Born-averaging over microscopic measurement outcomes becomes equivalent at low energy to averaging over conformal boundary conditions weighted by their corresponding partition functions. Our results yield distinctive critical behavior across all cumulants in the regime where the unmeasured parts of the system are maximally separated. We also obtain the full distribution of the post-measurement entanglement entropy, finding that it is generically bimodal and exhibits fat-tails. We corroborate our analytical predictions by numerical calculations and find good agreement between them."}
{"id": "2512.13993", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13993", "abs": "https://arxiv.org/abs/2512.13993", "authors": ["Nicholas J. E. Richardson", "Noah Marusenko", "Michael P. Friedlander"], "title": "Multiple Scale Methods For Optimization Of Discretized Continuous Functions", "comment": "25 pages, 8 figures, supplemental materials is 28 pages", "summary": "A multiscale optimization framework for problems over a space of Lipschitz continuous functions is developed. The method solves a coarse-grid discretization followed by linear interpolation to warm-start project gradient descent on progressively finer grids. Greedy and lazy variants are analyzed and convergence guarantees are derived that show the multiscale approach achieves provably tighter error bounds at lower computational cost than single-scale optimization. The analysis extends to any base algorithm with iterate convergence at a fixed rate. Constraint modification techniques preserve feasibility across scales. Numerical experiments on probability density estimation problems, including geological data, demonstrate speedups of an order of magnitude or better."}
{"id": "2512.14520", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14520", "abs": "https://arxiv.org/abs/2512.14520", "authors": ["Aihui Liu", "Magnus Jansson"], "title": "The Innovation Null Space of the Kalman Predictor: A Stochastic Perspective for DeePC", "comment": null, "summary": "Willems' fundamental lemma uses a key decision variable $g$ to combine measured input-output data and describe trajectories of a linear time-invariant system. In this paper, we ask: what is a good choice for this vector $g$ when the system is affected by noise? For a linear system with Gaussian noise, we show that there exists an optimal subspace for this decision variable $g$, which is the null space of the innovation Hankel matrix. If the decision vector lies in this null space, the resulting predictor gets closer to the Kalman predictor. To show this, we use a result that we refer to as the Kalman Filter Fundamental Lemma (KFFL), which applies Willems' lemma to the Kalman predictor. This viewpoint also explains several existing data-driven predictive control methods: regularized DeePC schemes act as soft versions of the innovation null-space constraint, instrumental-variable methods enforce it by construction, and ARX-based approaches explicitly estimate this innovation null space."}
{"id": "2512.14590", "categories": ["math.NA", "cs.GR", "math.DG"], "pdf": "https://arxiv.org/pdf/2512.14590", "abs": "https://arxiv.org/abs/2512.14590", "authors": ["Henrik Schumacher", "Jannik Rönsch", "Thorsten Hohage", "Max Wardetzky"], "title": "Inverse obstacle scattering regularized by the tangent-point energy", "comment": "46 pages, 13 figures, 4 tables", "summary": "We employ the so-called tangent-point energy as Tikhonov regularizer for ill-conditioned inverse scattering problems in 3D. The tangent-point energy is a self-avoiding functional on the space of embedded surfaces that also penalizes surface roughness. Moreover, it features nice compactness and continuity properties. These allow us to show the well-posedness of the regularized problems and the convergence of the regularized solutions to the true solution in the limit of vanishing noise level. We also provide a reconstruction algorithm of iteratively regularized Gauss-Newton type. Our numerical experiments demonstrate that our method is numerically feasible and effective in producing reconstructions of unprecedented quality."}
{"id": "2512.13908", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13908", "abs": "https://arxiv.org/abs/2512.13908", "authors": ["Emma Rosenfeld", "Craig Gidney", "Gabrielle Roberts", "Alexis Morvan", "Nathan Lacroix", "Dvir Kafri", "Jeffrey Marshall", "Ming Li", "Volodymyr Sivak", "Dmitry Abanin", "Amira Abbas", "Rajeev Acharya", "Laleh Aghababaie Beni", "Georg Aigeldinger", "Ross Alcaraz", "Sayra Alcaraz", "Trond I. Andersen", "Markus Ansmann", "Frank Arute", "Kunal Arya", "Walt Askew", "Nikita Astrakhantsev", "Juan Atalaya", "Ryan Babbush", "Brian Ballard", "Joseph C. Bardin", "Hector Bates", "Andreas Bengtsson", "Majid Bigdeli Karimi", "Alexander Bilmes", "Simon Bilodeau", "Felix Borjans", "Jenna Bovaird", "Dylan Bowers", "Leon Brill", "Peter Brooks", "Michael Broughton", "David A. Browne", "Brett Buchea", "Bob B. Buckley", "Tim Burger", "Brian Burkett", "Nicholas Bushnell", "Jamal Busnaina", "Anthony Cabrera", "Juan Campero", "Hung-Shen Chang", "Silas Chen", "Zijun Chen", "Ben Chiaro", "Liang-Ying Chih", "Agnetta Y. Cleland", "Bryan Cochrane", "Matt Cockrell", "Josh Cogan", "Paul Conner", "Harold Cook", "Rodrigo G. Cortiñas", "William Courtney", "Alexander L. Crook", "Ben Curtin", "Martin Damyanov", "Sayan Das", "Dripto M. Debroy", "Sean Demura", "Paul Donohoe", "Ilya Drozdov", "Andrew Dunsworth", "Valerie Ehimhen", "Alec Eickbusch", "Aviv Moshe Elbag", "Lior Ella", "Mahmoud Elzouka", "David Enriquez", "Catherine Erickson", "Lara Faoro", "Vinicius S. Ferreira", "Marcos Flores", "Leslie Flores Burgos", "Sam Fontes", "Ebrahim Forati", "Jeremiah Ford", "Brooks Foxen", "Masaya Fukami", "Alan Wing Lun Fung", "Lenny Fuste", "Suhas Ganjam", "Gonzalo Garcia", "Christopher Garrick", "Robert Gasca", "Helge Gehring", "Robert Geiger", "Élie Genois", "William Giang", "Dar Gilboa", "James E. Goeders", "Edward C. Gonzales", "Raja Gosula", "Stijn J. de Graaf", "Alejandro Grajales Dau", "Dietrich Graumann", "Joel Grebel", "Alex Greene", "Jonathan A. Gross", "Jose Guerrero", "Loïck Le Guevel", "Tan Ha", "Steve Habegger", "Tanner Hadick", "Ali Hadjikhani", "Michael C. Hamilton", "Monica Hansen", "Matthew P. Harrigan", "Sean D. Harrington", "Jeanne Hartshorn", "Stephen Heslin", "Paula Heu", "Oscar Higgott", "Reno Hiltermann", "Jeremy Hilton", "Hsin-Yuan Huang", "Mike Hucka", "Christopher Hudspeth", "Ashley Huff", "William J. Huggins", "Lev B. Ioffe", "Evan Jeffrey", "Shaun Jevons", "Zhang Jiang", "Xiaoxuan Jin", "Chaitali Joshi", "Pavol Juhas", "Andreas Kabel", "Hui Kang", "Kiseo Kang", "Amir H. Karamlou", "Ryan Kaufman", "Kostyantyn Kechedzhi", "Tanuj Khattar", "Mostafa Khezri", "Seon Kim", "Paul V. Klimov", "Can M. Knaut", "Bryce Kobrin", "Alexander N. Korotkov", "Fedor Kostritsa", "John Mark Kreikebaum", "Ryuho Kudo", "Ben Kueffler", "Arun Kumar", "Vladislav D. Kurilovich", "Vitali Kutsko", "Tiano Lange-Dei", "Brandon W. Langley", "Pavel Laptev", "Kim-Ming Lau", "Emma Leavell", "Justin Ledford", "Joy Lee", "Kenny Lee", "Brian J. Lester", "Wendy Leung", "Lily Li", "Wing Yan Li", "Alexander T. Lill", "William P. Livingston", "Matthew T. Lloyd", "Aditya Locharla", "Laura De Lorenzo", "Erik Lucero", "Daniel Lundahl", "Aaron Lunt", "Sid Madhuk", "Aniket Maiti", "Ashley Maloney", "Salvatore Mandrà", "Leigh S. Martin", "Orion Martin", "Eric Mascot", "Paul Masih Das", "Dmitri Maslov", "Melvin Mathews", "Cameron Maxfield", "Jarrod R. McClean", "Matt McEwen", "Seneca Meeks", "Anthony Megrant", "Kevin C. Miao", "Zlatko K. Minev", "Reza Molavi", "Sebastian Molina", "Shirin Montazeri", "Charles Neill", "Michael Newman", "Anthony Nguyen", "Murray Nguyen", "Chia-Hung Ni", "Murphy Yuezhen Niu", "Nicholas Noll", "Logan Oas", "William D. Oliver", "Raymond Orosco", "Kristoffer Ottosson", "Alice Pagano", "Agustin Di Paolo", "Sherman Peek", "David Peterson", "Alex Pizzuto", "Elias Portoles", "Rebecca Potter", "Orion Pritchard", "Michael Qian", "Chris Quintana", "Ganesh Ramachandran", "Arpit Ranadive", "Matthew J. Reagor", "Rachel Resnick", "David M. Rhodes", "Daniel Riley", "Roberto Rodriguez", "Emma Ropes", "Lucia B. De Rose", "Eliott Rosenberg", "Dario Rosenstock", "Elizabeth Rossi", "Pedram Roushan", "David A. Rower", "Robert Salazar", "Kannan Sankaragomathi", "Murat Can Sarihan", "Max Schaefer", "Sebastian Schroeder", "Henry F. Schurkus", "Aria Shahingohar", "Michael J. Shearn", "Aaron Shorter", "Noah Shutty", "Vladimir Shvarts", "Spencer Small", "W. Clarke Smith", "David A. Sobel", "Barrett Spells", "Sofia Springer", "George Sterling", "Jordan Suchard", "Aaron Szasz", "Alexander Sztein", "Madeline Taylor", "Jothi Priyanka Thiruraman", "Douglas Thor", "Dogan Timucin", "Eifu Tomita", "Alfredo Torres", "M. Mert Torunbalci", "Hao Tran", "Abeer Vaishnav", "Justin Vargas", "Sergey Vdovichev", "Guifre Vidal", "Benjamin Villalonga", "Catherine Vollgraff Heidweiller", "Meghan Voorhees", "Steven Waltman", "Jonathan Waltz", "Shannon X. Wang", "Danni Wang", "Brayden Ware", "James D. Watson", "Yonghua Wei", "Travis Weidel", "Theodore White", "Kristi Wong", "Bryan W. K. Woo", "Christopher J. Wood", "Maddy Woodson", "Cheng Xing", "Z. Jamie Yao", "Ping Yeh", "Bicheng Ying", "Juhwan Yoo", "Noureldin Yosri", "Elliot Young", "Grayson Young", "Adam Zalcman", "Ran Zhang", "Yaxing Zhang", "Ningfeng Zhu", "Nicholas Zobrist", "Zhenjie Zou", "Hartmut Neven", "Sergio Boixo", "Cody Jones", "Julian Kelly", "Alexandre Bourassa", "Kevin J. Satzinger"], "title": "Magic state cultivation on a superconducting quantum processor", "comment": null, "summary": "Fault-tolerant quantum computing requires a universal gate set, but the necessary non-Clifford gates represent a significant resource cost for most quantum error correction architectures. Magic state cultivation offers an efficient alternative to resource-intensive distillation protocols; however, testing the proposal's assumptions represents a challenging departure from quantum memory experiments. We present an experimental study of magic state cultivation on a superconducting quantum processor. We implement cultivation, including code-switching into a surface code, and develop a fault-tolerant measurement protocol to bound the magic state fidelity. Cultivation reduces the error by a factor of 40, with a state fidelity of 0.9999(1) (retaining 8% of attempts). Our results experimentally establish magic state cultivation as a viable solution to one of quantum computing's most significant challenges."}
{"id": "2512.13812", "categories": ["hep-lat", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13812", "abs": "https://arxiv.org/abs/2512.13812", "authors": ["Itay Gomelski", "Jonathan Elyovich", "Ariel Kelman", "Erez Zohar", "Patrick Emonts"], "title": "Algorithmic aspects of gauged Gaussian fermionic projected entangled pair states", "comment": null, "summary": "Lattice gauge theories (LGTs) provide a powerful framework for studying non-perturbative phenomena in gauge theories. However, conventional approaches such as Monte Carlo (MC) simulations in imaginary time are limited, as they do not allow real time evolution and suffer from a sign problem in many important cases. Using Gauged Gaussian fermionic projected entangled pair states (GGFPEPS) as a variational ground state ansatz offers an alternative for studying LGTs through a sign-problem-free variational MC. As this method is extended to larger and more complex systems, understanding its numerical behavior becomes essential. While conventional action based MC has been extensively studied, the performance and characteristics of non-action-based MC within the GGFPEPS framework are far less explored. In this work, we investigate these algorithmic aspects, identifying an optimal update size for GGFPEPS-based MC simulations for $\\mathbb{Z}_2$ in $2+1$ dimensions. We show that gauge fixing generally slows convergence, and demonstrate that not exploiting the translation-invariance can, in some cases, improve the computational time scaling of error convergence. We expect that these improvements will allow advancing the simulation to larger and more complex systems."}
{"id": "2512.14286", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.14286", "abs": "https://arxiv.org/abs/2512.14286", "authors": ["Samuel Cruz Alegría", "Bindi Çapriqi", "Shega Likaj", "Ken Trotti", "Rolf Krause"], "title": "An Additively Preconditioned Trust Region Strategy for Machine Learning", "comment": "13 Pages", "summary": "Modern machine learning, especially the training of deep neural networks, depends on solving large-scale, highly nonconvex optimization problems, whose objective function exhibit a rough landscape. Motivated by the success of parallel preconditioners in the context of Krylov methods for large scale linear systems, we introduce a novel nonlinearly preconditioned Trust-Region method that makes use of an additive Schwarz correction at each minimization step, thereby accelerating convergence.\n  More precisely, we propose a variant of the Additively Preconditioned Trust-Region Strategy (APTS), which combines a right-preconditioned additive Schwarz framework with a classical Trust-Region algorithm. By decomposing the parameter space into sub-domains, APTS solves local non-linear sub-problems in parallel and assembles their corrections additively. The resulting method not only shows fast convergence; due to the underlying Trust-Region strategy, it furthermore largely obviates the need for hyperparameter tuning."}
{"id": "2512.14682", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14682", "abs": "https://arxiv.org/abs/2512.14682", "authors": ["David O. Williams Rogers", "Hang Woon Lee"], "title": "Enhancing Orbital Debris Remediation with Reconfigurable Space-Based Laser Constellations", "comment": "Accepted to the 2026 IEEE Aerospace Conference", "summary": "Orbital debris poses an escalating threat to space missions and the long-term sustainability of Earth's orbital environment. The literature proposes various approaches for orbital debris remediation, including the use of multiple space-based lasers that collaboratively engage debris targets. While the proof of concept for this laser-based approach has been demonstrated, critical questions remain about its scalability and responsiveness as the debris population continues to expand rapidly. This paper introduces constellation reconfiguration as a system-level strategy to address these limitations. Through coordinated orbital maneuvers, laser-equipped satellites can dynamically adapt their positions to respond to evolving debris distributions and time-critical events. We formalize this concept as the Reconfigurable Laser-to-Debris Engagement Scheduling Problem (R-L2D-ESP), an optimization framework that determines the optimal sequence of constellation reconfigurations and laser engagements to maximize debris remediation capacity, which quantifies the constellation's ability to nudge, deorbit, or perform just-in-time collision avoidance maneuvers on debris objects. To manage the complexity of this combinatorial optimization problem, we employ a receding horizon approach. Our experiments reveal that reconfigurable constellations significantly outperform static ones, achieving greater debris remediation capacity and successfully deorbiting substantially more debris objects. Additionally, our sensitivity analyses identify the key parameters that influence remediation performance the most, providing essential insights for future system design. These findings demonstrate that constellation reconfiguration represents a promising advancement for laser-based debris removal systems, offering the adaptability and scalability necessary to enhance this particular approach to orbital debris remediation."}
{"id": "2512.13845", "categories": ["cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.13845", "abs": "https://arxiv.org/abs/2512.13845", "authors": ["Lars T. Kyllingstad"], "title": "Co-simulation errors due to step size changes", "comment": "17 pages, 10 figures. Code to perform simulations and produce plots is available at https://doi.org/10.60609/6p8m-0713", "summary": "When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps."}
{"id": "2512.13931", "categories": ["quant-ph", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.13931", "abs": "https://arxiv.org/abs/2512.13931", "authors": ["Narasinga Rao Miniskar", "Mohammad Alaul Haque Monil", "Elaine Wong", "Vicente Leyton-Ortega", "Jeffrey S. Vetter", "Seth R. Johnson", "Travis S. Humble"], "title": "Q-IRIS: The Evolution of the IRIS Task-Based Runtime to Enable Classical-Quantum Workflows", "comment": null, "summary": "Extreme heterogeneity in emerging HPC systems are starting to include quantum accelerators, motivating runtimes that can coordinate between classical and quantum workloads. We present a proof-of-concept hybrid execution framework integrating the IRIS asynchronous task-based runtime with the XACC quantum programming framework via the Quantum Intermediate Representation Execution Engine (QIR-EE). IRIS orchestrates multiple programs written in the quantum intermediate representation (QIR) across heterogeneous backends (including multiple quantum simulators), enabling concurrent execution of classical and quantum tasks. Although not a performance study, we report measurable outcomes through the successful asynchronous scheduling and execution of multiple quantum workloads. To illustrate practical runtime implications, we decompose a four-qubit circuit into smaller subcircuits through a process known as quantum circuit cutting, reducing per-task quantum simulation load and demonstrating how task granularity can improve simulator throughput and reduce queueing behavior -- effects directly relevant to early quantum hardware environments. We conclude by outlining key challenges for scaling hybrid runtimes, including coordinated scheduling, classical-quantum interaction management, and support for diverse backend resources in heterogeneous systems."}
{"id": "2512.14065", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14065", "abs": "https://arxiv.org/abs/2512.14065", "authors": ["Y. S. Liu", "X. Z. Zhang"], "title": "Integrability Breaking and Coherent Dynamics in Hermitian and Non-Hermitian Spin Chains with Long-Range Coupling", "comment": "14 pages, 12 figures", "summary": "Unraveling the mechanisms of ergodicity breaking in complex quantum systems is a central pursuit in nonequilibrium physics. In this work, we investigate a one-dimensional spin model featuring a tunable long-range hopping term, $H_{n}$, which introduces nonlocal interactions and bridges the gap between Hermitian and non-Hermitian regimes. Through a systematic analysis of level-spacing statistics, Krylov complexity, and entanglement entropy, we demonstrate that $H_{n}$ acts as a universal control parameter driving the transition from integrability to quantum chaos. Specifically, increasing the strength of $H_{n}$ induces a crossover from Poissonian to Gaussian Orthogonal Ensemble statistics in the Hermitian limit, and similarly triggers chaotic dynamics in the non-Hermitian case. Most remarkably, despite the onset of global chaos, we identify a tower of exact nonthermal eigenstates that evade thermalization. These states survive as robust quantum many-body scars, retaining low entanglement and coherent dynamics even under strong non-Hermitian perturbations. Our findings reveal a universal mechanism by which long-range and non-Hermitian effects reshape quantum ergodicity, offering new pathways for preserving quantum coherence in complex many-body systems."}
{"id": "2512.14301", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.14301", "abs": "https://arxiv.org/abs/2512.14301", "authors": ["Rami Katz", "Dmitry Batenkov", "Giulia Giordano"], "title": "Separation-free exponential fitting with structured noise, with applications to inverse problems in parabolic PDEs", "comment": null, "summary": "We investigate the recovery of exponents and amplitudes of an exponential sum, where the exponents $\\left\\{λ_n \\right\\}_{n=1}^{N_1}$ are the first $N_1$ eigenvalues of a Sturm-Liouville operator, from finitely many measurements subject to measurement noise. This inverse problem is extremely ill-conditioned when the noise is arbitrary and unstructured. Surprisingly, however, the extreme ill-conditioning exhibited by this problem disappears when considering a \\emph{structured} noise term, taken as an exponential sum with exponents given by the subsequent eigenvalues $\\left\\{λ_n \\right\\}_{n=N_1+1}^{N_1+N_2}$ of the Sturm-Liouville operator, multiplied by a noise magnitude parameter $\\varepsilon>0$. In this case, we rigorously show that the exponents and amplitudes can be recovered with super-exponential accuracy: we both prove the theoretical result and show that it can be achieved numerically by a specific algorithm. By leveraging recent results on the mathematical theory of super-resolution, we show in this paper that the classical Prony's method attains the analytic optimal error decay also in the ``separation-free'' regime where $λ_n \\to \\infty$ as $n \\to \\infty$, thereby extending the applicability of Prony's method to new settings. As an application of our theoretical analysis, we show that the approximated eigenvalues obtained by our method can be used to recover an unknown potential in a linear reaction-diffusion equation from discrete solution traces."}
{"id": "2512.14015", "categories": ["quant-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.14015", "abs": "https://arxiv.org/abs/2512.14015", "authors": ["Limin Xu", "Zhen Huang", "Zhennan Zhou"], "title": "Frozen Gaussian sampling algorithms for simulating Markovian open quantum systems in the semiclassical regime", "comment": null, "summary": "Simulating Markovian open quantum systems in the semiclassical regime poses a grand challenge for computational physics, as the highly oscillatory nature of the dynamics imposes prohibitive resolution requirements on traditional grid-based methods. To overcome this barrier, this paper introduces an efficient Frozen Gaussian Sampling (FGS) algorithm based on the Wigner-Fokker-Planck phase-space formulation. The proposed algorithm exhibits two transformative advantages. First, for the computation of physical observables, its sampling error is independent of the semiclassical parameter $\\varepsilon$, thus fundamentally breaking the prohibitive computational scaling faced by grid methods in the semiclassical limit. Second, its mesh-free nature entirely eliminates the boundary-induced instabilities that constrain long-time grid-based simulations. Leveraging these capabilities, the FGS algorithm serves as a powerful investigatory tool for exploring the long-time behavior of open quantum systems. Specifically, we provide compelling numerical evidence for the existence of steady states in strongly non-harmonic potentials-a regime where rigorous analytical results are currently lacking."}
{"id": "2512.13949", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2512.13949", "abs": "https://arxiv.org/abs/2512.13949", "authors": ["Zachariah Malik", "Zain Saleem"], "title": "Coherence-Sensitive Readout Models for Quantum Devices: Beyond the Classical Assignment Matrix", "comment": "9 pages", "summary": "Readout error models for noisy quantum devices almost universally assume that measurement noise is classical: the measurement statistics are obtained from the ideal computational-basis populations by a column-stochastic assignment matrix $A$. This description is equivalent to assuming that the effective positive-operator-valued measurement (POVM) is diagonal in the measurement basis, and therefore completely insensitive to quantum coherences. We relax this assumption and derive a fully general expression for the observed measurement probabilities under arbitrary completely positive trace-preserving (CPTP) noise preceding a computational-basis measurement. Writing the ideal post-circuit stat $\\tildeρ$ in terms of its populations $x$ and coherences $y$, we show that the observed probability vector $z$ satisfies $z = A x + C y$, where $A$ is the familiar classical assignment matrix and $C$ is a coherence-response matrix constructed from the off-diagonal matrix elements of the effective POVM in the computational basis. The classical model $z = A x$ arises if and only if all POVM elements are diagonal; in this sense $C$ quantifies accessible information about coherent readout distortions and interference between computational-basis states, all of which are invisible to models that retain only $A$. This work therefore provides a natural, fully general framework for coherence-sensitive readout modeling on current and future quantum devices."}
{"id": "2512.14124", "categories": ["math.OC", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.14124", "abs": "https://arxiv.org/abs/2512.14124", "authors": ["Boris S. Mordukhovich", "Peipei Tang", "Chengjing Wang"], "title": "Complete Characterizations of Well-Posedness in Parametric Composite Optimization", "comment": null, "summary": "This paper provides complete characterization of well-posedness for Karush-Kuhn-Tucker (KKT) systems associated with general problems of perturbed composite optimization. Leveraging the property of parabolic regularity for composite models, we show that the second-order subderivative of the cost function reduces to the novel second-order variational function playing a crucial role in the subsequent analysis. This foundational result implies that the strong second-order sufficient condition (SSOSC) introduced in this work for the general class of composite optimization problems naturally extends the classical second-order sufficient condition in nonlinear programming. Then we obtain several equivalent characterizations of the second-order qualification condition (SOQC) and highlight its equivalence to the constraint nondegeneracy condition under the $\\mathcal{C}^{2}$-cone reducibility assumption. These insights lead us to multiple equivalent conditions for the major Lipschitz-like/Aubin property of KKT systems, including the SOQC combined with the new second-order subdifferential condition and the SOQC combined with tilt stability of local minimizers. Furthermore, under $\\mathcal{C}^{2}$-cone reducibility, we prove that the Lipschitz-like property of the reference KKT system is equivalent to its strong regularity. Finally, we demonstrate that the Lipschitz-like property is equivalent to the nonsingularity of the generalized Jacobian associated with the KKT system under a certain verifiable assumption. These results provide a unified and rigorous framework for analyzing stability and sensitivity of solutions to composite optimization problems, as well as for the design and justification of numerical algorithms."}
{"id": "2512.13971", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13971", "abs": "https://arxiv.org/abs/2512.13971", "authors": ["Adriano Macarone-Palmieri", "Alberto Ferrara", "Rosario Lo Franco"], "title": "A non-linear quantum neural network framework for entanglement engineering", "comment": null, "summary": "Multipartite entanglement is a key resource for quantum technologies, yet its scalable generation in noisy quantum devices remains challenging. Here, we propose a low-depth quantum neural network architecture with linear scaling, inspired by memory-enabled photonic components, for variational entanglement engineering. The network incorporates physically motivated non-linear activation functions, enhancing expressivity beyond linear variational circuits at fixed depth. By Monte Carlo sampling over circuit topologies, we identify architectures that efficiently generate highly entangled pure states, approaching the GHz limit, and demonstrate a clear advantage of non-linear networks up to 20 qubits. For the noisy scenario, we employ the experimentally accessible Meyer-Wallach global entanglement as a surrogate optimization cost and certify entanglement using bipartite negativity. For mixed states of up to ten qubits, the optimized circuits generate substantial entanglement across both symmetric and asymmetric bipartitions. These results establish an experimentally motivated and scalable variational framework for engineering multipartite entanglement on near-term quantum devices, highlighting the combined role of non-linearity and circuit topology."}
{"id": "2512.14468", "categories": ["math.OC", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.14468", "abs": "https://arxiv.org/abs/2512.14468", "authors": ["Xinhua Shen", "Hongpeng Sun"], "title": "A preconditioned second-order convex splitting algorithm with extrapolation", "comment": null, "summary": "Nonconvex optimization problems are widespread in modern machine learning and data science. We introduce an extrapolation strategy into a class of preconditioned second-order convex splitting algorithms for nonconvex optimization problems. The proposed algorithms combine second-order backward differentiation formulas (BDF2) with an extrapolation method. Meanwhile, the implicit-explicit scheme simplifies the subproblem through a preconditioned process. As a result, our approach solves nonconvex problems efficiently without significant computational overhead. Theoretical analysis establishes global convergence of the algorithms using Kurdyka-Łojasiewicz properties. Numerical experiments include a benchmark problem, the least squares problem with SCAD regularization, and an image segmentation problem. These results demonstrate that our algorithms are highly efficient, as they achieve reduced solution times and competitive performance."}
{"id": "2512.14004", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14004", "abs": "https://arxiv.org/abs/2512.14004", "authors": ["Isabela Gnasso", "Khadija Sarguroh", "Dorian Gangloff", "Sophia E. Economou", "Edwin Barnes"], "title": "Quantifying electron-nuclear spin entanglement dynamics in central-spin systems using one-tangles", "comment": "12+27 pages, 9 figures", "summary": "Optically-active solid-state systems such as self-assembled quantum dots, rare-earth ions, and color centers in diamond and SiC are promising candidates for quantum network, computing, and sensing applications. Although the nuclei in these systems naturally lead to electron spin decoherence, they can be repurposed, if they are controllable, as long-lived quantum memories. Prior work showed that a metric known as the one-tangling power can be used to quantify the entanglement dynamics of sparse systems of spin-1/2 nuclei coupled to color centers in diamond and SiC. Here, we generalize these findings to a wide range of electron-nuclear central-spin systems, including those with spin > 1/2 nuclei, such as in III-V quantum dots (QDs), rare-earth ions, and some color centers. Focusing on the example of an (In)GaAs QD, we offer a procedure for pinpointing physically realistic parameter regimes that yield maximal entanglement between the central electron and surrounding nuclei. We further harness knowledge of naturally-occurring degeneracies and the tunability of the system to generate maximal entanglement between target subsets of spins when the QD electron is subject to dynamical decoupling. We also leverage the one-tangling power as an exact and immediate method for computing QD electron spin dephasing times with and without the application of spin echo sequences, and use our analysis to identify coherence-sustaining conditions within the system."}
{"id": "2512.14015", "categories": ["quant-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.14015", "abs": "https://arxiv.org/abs/2512.14015", "authors": ["Limin Xu", "Zhen Huang", "Zhennan Zhou"], "title": "Frozen Gaussian sampling algorithms for simulating Markovian open quantum systems in the semiclassical regime", "comment": null, "summary": "Simulating Markovian open quantum systems in the semiclassical regime poses a grand challenge for computational physics, as the highly oscillatory nature of the dynamics imposes prohibitive resolution requirements on traditional grid-based methods. To overcome this barrier, this paper introduces an efficient Frozen Gaussian Sampling (FGS) algorithm based on the Wigner-Fokker-Planck phase-space formulation. The proposed algorithm exhibits two transformative advantages. First, for the computation of physical observables, its sampling error is independent of the semiclassical parameter $\\varepsilon$, thus fundamentally breaking the prohibitive computational scaling faced by grid methods in the semiclassical limit. Second, its mesh-free nature entirely eliminates the boundary-induced instabilities that constrain long-time grid-based simulations. Leveraging these capabilities, the FGS algorithm serves as a powerful investigatory tool for exploring the long-time behavior of open quantum systems. Specifically, we provide compelling numerical evidence for the existence of steady states in strongly non-harmonic potentials-a regime where rigorous analytical results are currently lacking."}
{"id": "2512.14065", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2512.14065", "abs": "https://arxiv.org/abs/2512.14065", "authors": ["Y. S. Liu", "X. Z. Zhang"], "title": "Integrability Breaking and Coherent Dynamics in Hermitian and Non-Hermitian Spin Chains with Long-Range Coupling", "comment": "14 pages, 12 figures", "summary": "Unraveling the mechanisms of ergodicity breaking in complex quantum systems is a central pursuit in nonequilibrium physics. In this work, we investigate a one-dimensional spin model featuring a tunable long-range hopping term, $H_{n}$, which introduces nonlocal interactions and bridges the gap between Hermitian and non-Hermitian regimes. Through a systematic analysis of level-spacing statistics, Krylov complexity, and entanglement entropy, we demonstrate that $H_{n}$ acts as a universal control parameter driving the transition from integrability to quantum chaos. Specifically, increasing the strength of $H_{n}$ induces a crossover from Poissonian to Gaussian Orthogonal Ensemble statistics in the Hermitian limit, and similarly triggers chaotic dynamics in the non-Hermitian case. Most remarkably, despite the onset of global chaos, we identify a tower of exact nonthermal eigenstates that evade thermalization. These states survive as robust quantum many-body scars, retaining low entanglement and coherent dynamics even under strong non-Hermitian perturbations. Our findings reveal a universal mechanism by which long-range and non-Hermitian effects reshape quantum ergodicity, offering new pathways for preserving quantum coherence in complex many-body systems."}
{"id": "2512.14091", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14091", "abs": "https://arxiv.org/abs/2512.14091", "authors": ["James Daniel Whitfield"], "title": "Group Theory and Representation Theory for Identical Particles", "comment": null, "summary": "Few, if any, applications of quantum technology are as widely known as the quantum simulation of quantum matter. Consequently, many interesting questions have been sparked at the intersection of condensed matter, quantum chemistry, and quantum computing. Given the common mathematical foundation of these subjects, we walk through the necessary group theory and representation theory serving as background in all of these fields. Our discussion will include a full development of the mathematics of identical particles and the mechanics of describing systems of identical particles in both first and second quantization schemes. This chapter is an offshoot of a larger work that provides a graduate-level introduction to quantum information science. This chapter is being released separately because it is not explicitly focused on quantum information. It has grown beyond a short digression into a full-fledged journey into the symmetries and representations of identical particles that we invite you, the reader, to join."}
{"id": "2512.14122", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2512.14122", "abs": "https://arxiv.org/abs/2512.14122", "authors": ["Christopher A. Fuchs", "Blake C. Stacey"], "title": "QBism, Polishing Some Points", "comment": "22 pages, 2 figures; chapter accepted for forthcoming quantum-centennial book; contains passages condensed from arXiv:1705.03483 and arXiv:2303.01446 for a less overspecialized audience, with a healthy proportion of new material", "summary": "QBism pursues the real by first eliminating the elements of quantum theory too fragile to be ontologies on their own. Thereafter, it seeks an \"ontological lesson\" from whatever remains. Here, we explore this program by highlighting three tenets of QBism. First, the Born Rule is a normative statement. It is about the decision-making behavior any individual agent should strive for, not a descriptive \"law of nature.\" Second, all probabilities, including all quantum probabilities, are so subjective they never tell nature what to do. This includes probability-1 assignments. Quantum states thus have no \"ontic hold\" on the world, which implies a more radical kind of indeterminism in quantum theory than other interpretations understand. Third, quantum measurement outcomes just are personal experiences for the agent gambling upon them. Thus all quantum measurement outcomes are local in the sense of the agent enacting them. Through these tenets, we explain four points better than previously: 1) how QBism contrasts with Bohr's concern over unambiguous language, 2) how QBism contrasts with the Everett interpretation, 3) how QBism understands the meaning of Bell inequality violations, and 4) how QBism responds to Wigner's \"suspended animation\" argument. Finally, we consider the ontological lesson of the tenets and ask what it might mean for the next one hundred years of quantum theory and humankind more generally."}
{"id": "2512.14155", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14155", "abs": "https://arxiv.org/abs/2512.14155", "authors": ["Precious Ogbonda Amadi", "Paphon Pewkhom", "Pruet Kalasuwan", "Norshamsuri Ali", "Syed Alwee Aljunid", "Rosdisham Endut"], "title": "Quantum Fisher Information Measure in a Strongly Confined Harmonic Paul Trap Lattice System", "comment": "22 pages, 8 figures, 1 table", "summary": "In this work, we examine how the informational and structural properties of a single ion respond to controlled changes of the effective potential in a Paul trap modified by an optical lattice. We consider the ground state of the system where confinement is strongest. And by treating the trap frequency $ω$ and lattice $κ$ as independent tunning parameters, we show that Fisher information, Shannon entropy, and Fisher-Shannon complexity track the curvature of the effective potential $ω_{\\mathrm{eff}}=ω^2\\,\\sqrt{1-κ}$. The $ω$ and $κ$ sweeps confirm that curvature and not the choice of control parameter determines the behaviour of the system. This gives the trapped-ion platform a clear advantage that the curvature can be engineered without altering the harmonic characteristics of the system. The interplay between $ω$ and $κ$ thus provides a practical route for precision quantum control and offers Information-theoretic framework for experiments that probe confinement, quantization scale, and information flow in engineered ion traps."}
{"id": "2512.14174", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14174", "abs": "https://arxiv.org/abs/2512.14174", "authors": ["Christian Saugbjerg Lange", "Ella Elisabeth Lassen", "Rasmus Vesterager Gothelf", "Lars Bojer Madsen"], "title": "High-Order Harmonic Generation with Beyond-Semiclassical Emitter Dynamics: A Strong-Field Quantum Optical Heisenberg Picture Approach", "comment": null, "summary": "Quantum-optical descriptions of strong-field processes have attracted significant attention in recent years. Typically, the theoretical modeling has been conducted in the Schrödinger picture, where results are only obtainable under certain approximations, while, in contrast, the Heisenberg picture has remained relatively unexplored. In this work, we develop an accurately controlled perturbative expansion of the time-evolution operator in the Heisenberg picture and derive beyond-semiclassical corrections to the emitter dynamics due to the coupling to the quantized electromagnetic field, capturing effects of the quantum fluctuations present in the latter. We focus on high-order harmonic generation (HHG), where the approach is accurate in parameter regimes of current interest and it gives closed-form expressions for key observables. This formulation not only simplifies numerical calculations compared to the Schrödinger-picture approach but also provides a clear correspondence between nonclassical features of the emitted light and the underlying induced dynamics of the generating medium including quantum fluctuations. Moreover, the Heisenberg framework naturally yields scaling relations with the number of independent emitters, enabling us to assess whether nonclassical behavior should persist under typical experimental conditions involving large emitter ensembles. Interestingly, we find that the degree of squeezing increases with the number of emitters, whereas the photon statistics approaches a classical Poissonian distribution in the many-emitter limit. We also find that the beyond-semiclassical emitter dynamics significantly enhances the degree of squeezing of the emitted light. Our work advances the theoretical understanding of quantum-optical HHG and introduces an accessible and well-controlled framework to describe realistic experiments."}
{"id": "2512.14181", "categories": ["quant-ph", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14181", "abs": "https://arxiv.org/abs/2512.14181", "authors": ["Shaolun Ruan", "Feng Liang", "Rohan Ramakrishna", "Chao Ren", "Rudai Yan", "Qiang Guan", "Jiannan Li", "Yong Wang"], "title": "Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization", "comment": "9 pages, 6 figures, accepted by TVCG 2026, not published yet", "summary": "Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping."}
{"id": "2512.14182", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2512.14182", "abs": "https://arxiv.org/abs/2512.14182", "authors": ["Ling-Zhi Tang", "Xiao Li", "Z. D. Wang", "Dan-Wei Zhang"], "title": "Discrete time crystals enabled by Floquet strong Hilbert space fragmentation", "comment": "9 pages, 5 figures", "summary": "Discrete time crystals (DTCs) are non-equilibrium phases of matter that break the discrete time-translation symmetry and is characterized by a robust subharmonic response in periodically driven quantum systems. Here, we explore the DTC in a disorder-free, periodically kicked XXZ spin chain, which is stabilized by the Floquet strong Hilbert space fragmentation. We numerically show the period-doubling response of the conventional DTC order, and uncover a multiple-period response with beating dynamics due to the coherent interplay of multiple $π$-pairs in the Floquet spectrum of small-size systems. The lifetime of the DTC order exhibits independence of the driving frequency and a power-law dependence on the ZZ interaction strength. It also grows exponentially with the system size, as a hallmark of the strong fragmentation inherent to the Floquet model. We analytically reveal the approximate conservation of the magnetization and domain-wall number in the Floquet operator for the emergent strong fragmentation, which is consistent with numerical results of the dimensionality ratio of symmetry subspaces. The rigidity and phase regime of the DTC order are identified through finite-size scaling of the Floquet-spectrum-averaged mutual information, as well as via dynamical probes. Our work establishes the Floquet Hilbert space fragmentation as a disorder-free mechanism for sustaining nontrivial temporal orders in out-of-equilibrium quantum many-body systems."}
{"id": "2512.14208", "categories": ["quant-ph", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.14208", "abs": "https://arxiv.org/abs/2512.14208", "authors": ["Mierk Schwabe", "Lorenzo Pastori", "Valentina Sarandrea", "Veronika Eyring"], "title": "Quantum Machine Learning for Climate Modelling", "comment": "Accepted at IEEE Quantum Artificial Intelligence 2025 conference. This version is a pre-publication and may differ from the final IEEE-published version. The final paper will be available in the IEEE Xplore digital library", "summary": "Quantum machine learning (QML) is making rapid progress, and QML-based models hold the promise of quantum advantages such as potentially higher expressivity and generalizability than their classical counterparts. Here, we present work on using a quantum neural net (QNN) to develop a parameterization of cloud cover for an Earth system model (ESM). ESMs are needed for predicting and projecting climate change, and can be improved in hybrid models incorporating both traditional physics-based components as well as machine learning (ML) models. We show that a QNN can predict cloud cover with a performance similar to a classical NN with the same number of free parameters and significantly better than the traditional scheme. We also analyse the learning capability of the QNN in comparison to the classical NN and show that, at least for our example, QNNs learn more consistent relationships than classical NNs."}
{"id": "2512.14255", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14255", "abs": "https://arxiv.org/abs/2512.14255", "authors": ["Long D. H. My", "Shao-Hen Chiew", "Jing Hao Chai", "Hui Khoon Ng"], "title": "Information-efficient decoding of surface codes", "comment": null, "summary": "Surface codes are a popular error-correction route to fault-tolerant quantum computation. The so-called exponential backlog problem that can arise when one has to do logical $T$-gates within the surface code demands real-time decoding of the syndrome information to diagnose the appropriate Pauli frame in which to do the gate. This in turn puts a minimum requirement on the communication rate between the quantum processing unit, where the syndrome information is collected, and the classical processor, where the decoding algorithm is run. This minimum communication rate can be difficult to achieve while preserving the quality of the quantum processor. Here, we present two decoders that make use of a reduced syndrome information volume, relying on a number of syndrome bits that scale only as the width -- and not the usual area -- of the surface-code patch. This eases the communication requirements necessary for real-time decoding."}
{"id": "2512.14276", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2512.14276", "abs": "https://arxiv.org/abs/2512.14276", "authors": ["S. Mojtaba Tabatabaei", "Babak Zare Rameshti", "Mohsen Akbari"], "title": "Engineering Anisotropic Rabi Model in Circuit QED", "comment": "9 pages, 5 figures", "summary": "The anisotropic Rabi model (ARM), which features tunable Jaynes-Cummings (JC) and anti-Jaynes-Cummings (AJC) interactions, has remained challenging to realize fully. We present a circuit QED implementation that provides static control over the ARM parameters. By simultaneously coupling a qubit to a resonator's voltage and current antinodes, we geometrically tune the interaction from pure JC to pure AJC. This control enables novel quantum measurement capabilities, including dispersive shift cancellation and Purcell-suppressed readout. Our work establishes a direct platform for exploring the ARM's full parameter space and its applications in quantum information processing."}
{"id": "2512.14280", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14280", "abs": "https://arxiv.org/abs/2512.14280", "authors": ["Saba Arife Bozpolat"], "title": "General Quantum Instruction for Communication via Maximally Entangled $n$-Qubit States", "comment": "10 pages, 5 figures", "summary": "This study presents a generalized $n$-bit superdense coding protocol that enables the transmission of n classical bits of information using an entangled n--qubit quantum system and the transmission of $n-1$ qubits. The protocol involves creating a maximally entangled n--qubit state, encoding the classical message with Pauli--Z and Pauli--X gates, and then transmitting and decoding the message via quantum communication, quantum operations, and measurements. The key novelty of this work lies in the proposed n--bit encoding routine, which, to the best of our knowledge, is the first explicit and scalable recipe for constructing quantum circuits for n--bit Superdense Coding, minimizing errors through a simple circuit design. The protocol was tested on real quantum hardware using Qiskit 2.0 and the IBM--Torino quantum computer for message lengths of 4, 6, 8, and 10 bits. Results show that success rates decrease as message length, circuit depth, and gate count increase, largely due to increased Pauli--X gate usage for messages with more ``1\" bits. Strategies to improve performance include sending messages in shorter segments and advances in qubit coherence and gate fidelity. This work offers a practical and easily scalable quantum communication instruction with potential applications in quantum networks and communication systems."}
{"id": "2512.14302", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14302", "abs": "https://arxiv.org/abs/2512.14302", "authors": ["Jia Bao", "Bin Guo", "Shu Qu", "Fanqin Xu", "Zhaoyu Sun"], "title": "Universal Structure of Nonlocal Operators for Deterministic Navigation and Geometric Locking", "comment": null, "summary": "We establish a universal geometric framework that transforms the search for optimal nonlocal operators from a combinatorial black box into a deterministic predict-verify operation. We discover that the principal eigenvalue governing nonlocality is rigorously dictated by a low-dimensional manifold parameterized by merely two fundamental angular variables, $θ$ and $φ$, whose symmetry leads to further simplification. This geometric distillation establishes a precise mapping connecting external control parameters directly to optimal measurement configurations. Crucially, a comparative analysis of the geometric angles against the principal eigenvalue spectrum, including its magnitude, susceptibility, and nonlocal gap, reveals a fundamental dichotomy in quantum criticality. While transitions involving symmetry sector rotation manifest as geometric criticality with drastic operator reorientation, transitions dominated by strong anisotropy exhibit geometric locking, where the optimal basis remains robust despite clear signatures of phase transitions in the spectral indicators. This distinction offers a novel structural classification of quantum phase transitions and provides a precision navigation chart for Bell experiments."}
{"id": "2512.14377", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14377", "abs": "https://arxiv.org/abs/2512.14377", "authors": ["Xiongfeng Ma"], "title": "Steering Alternative Realities through Local Quantum Memory Operations", "comment": "9 pages, 3 figures", "summary": "Quantum measurement resolves a superposition into a definite outcome by correlating it with an observer's memory -- a reality register. While the global quantum state remains coherent, the observer's local reality becomes singular and definite. This work introduces reality steering, a protocol that allows an observer to probabilistically access a different reality already supported by the initial quantum state, without reversing decoherence on the environment. The mechanism relies on locally erasing the 'which-outcome' information stored in the observer's brain. Here, 'local' means operations confined to the observer's memory, excluding the environment, which may be cosmically large. Reality steering nevertheless faces intrinsic constraints: successful navigation requires coherent participation from the observer's counterparts across the relevant branches, and any transition is operationally indistinguishable from non-transition. After arriving in a new reality, all memory records are perfectly consistent with that reality, leaving no internal evidence that a switch occurred. This makes conscious confirmation impossible within standard quantum mechanics. We show that nonlinear operations beyond the standard theory could, in principle, enable verifiable and deliberate navigation. Our results shift multi-reality exploration from philosophical speculation toward a concrete -- though fundamentally constrained -- quantum-informational framework."}
{"id": "2512.14383", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14383", "abs": "https://arxiv.org/abs/2512.14383", "authors": ["T. Pernambuco", "L. C. Céleri"], "title": "Geometric quantum thermodynamics: A fibre bundle approach", "comment": "9 pages, 1 figure. Comments are welcome", "summary": "Classical thermodynamics is a theory based on coarse-graining, meaning that the thermodynamic variables arise from discarding information related to the microscopic features of the system at hand. In quantum mechanics, however, where one has a high degree of control over microscopic systems, information theory plays an important role in describing the thermal properties of quantum systems. Recently, a new approach has been proposed in the form of a quantum thermodynamic gauge theory, where the notion of redundant information arises from a group of physically motivated gauge transformations called the thermodynamic group. In this work, we explore the geometrical structure of quantum thermodynamics. Particularly, we do so by explicitly constructing the relevant principal fibre bundle. We then show that there are two distinct (albeit related) geometric structures associated with the gauge theory of quantum thermodynamics. In this way, we express thermodynamics in the same mathematical (geometric) language as the fundamental theories of physics. Finally, we discuss how the geometric and topological properties of these structures may help explain fundamental properties of thermodynamics."}
{"id": "2512.14408", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14408", "abs": "https://arxiv.org/abs/2512.14408", "authors": ["Cagla Ozkan", "Lucas Alves Zischler", "Kadir Gumus", "Joao dos Reis Frazao", "Cristian Antonelli", "Chigo Okonkwo"], "title": "Capacity and SKR tradeoff in coexisting classical and CV-QKD metropolitan-reach optical links", "comment": null, "summary": "We demonstrate power-regime-dependent guardband optimization for quantum-classical coexistence in metropolitan DWDM. Quantum channel at band-edge with 100-150 GHz guardbands achieves 108% SKR improvement at -1.5 dBm/ch, incurring 3.4% capacity loss versus 6.8% for band-center."}
{"id": "2512.14415", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14415", "abs": "https://arxiv.org/abs/2512.14415", "authors": ["Ludwig Nützel", "Michael J. Hartmann", "Henrik Dreyer", "Etienne Granet"], "title": "Ground State Energy via Adiabatic Evolution and Phase Measurement for a Molecular Hamiltonian on an Ion-Trap Quantum Computer", "comment": null, "summary": "Estimating molecular ground-state energies is a central application of quantum computing, requiring both the preparation of accurate quantum states and efficient energy readout. Understanding the effect of hardware noise on these experiments is crucial to distinguish errors that have low impact, errors that can be mitigated, and errors that must be reduced at the hardware level. We ran a state preparation and energy measurement protocol on an ion-trap quantum computer, without any non-scalable off-loading of computational tasks to classical computers, and show that leakage errors are the main obstacle to chemical accuracy. More specifically, we apply adiabatic state preparation to prepare the ground state of a six-qubit encoding of the H3+ molecule and extract its energy using a noise-resilient variant of iterative quantum phase estimation. Our results improve upon the classical Hartree-Fock energy. Analyzing the effect of hardware noise on the result, we find that while coherent and incoherent noise have little influence, the hardware results are mainly impacted by leakage errors. Absent leakage errors, noisy numerical simulations show that with our experimental settings we would have achieved close to chemical accuracy, even shot noise included. These insights highlight the importance of targeting leakage suppression in future algorithm and hardware development."}
{"id": "2512.14449", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14449", "abs": "https://arxiv.org/abs/2512.14449", "authors": ["Sean Thrasher", "Ioannis Kolotouros", "Julien Michel", "Petros Wallden"], "title": "Adiabatic-Inspired Hybrid Quantum-Classical Methods for Molecular Ground State Preparation", "comment": null, "summary": "Quantum computing promises to efficiently and accurately solve many important problems in quantum chemistry which elude classical solvers, such as the electronic structure problem of highly correlated materials. Two leading methods in solving the ground state problem are the Variational Quantum Eigensolver (VQE) and Adiabatic Quantum Computing (AQC) algorithms. VQE often struggles with convergence due to the energy landscape being highly non-convex and the existence of barren plateaux, and implementing AQC is beyond the capabilities of current quantum devices as it requires deep circuits. Adiabatically-inspired algorithms aim to fill this gap. In this paper, we first present a unifying framework for these algorithms and then benchmark the following methods: the Adiabatically Assisted VQE (AAVQE) (Garcia-Saez and Latorre (2018)), the Variational Adiabatic Quantum Computing (VAQC) (Harwood et al (2022)), and the Adiabatic Quantum Computing with Parametrized Quantum Circuits (AQC-PQC) (Kolotouros et al (2025)) algorithms. Second, we introduce a novel hybrid approach termed G-AQC-PQC, which generalizes the AQC-PQC method, and combines adiabatic-inspired initialization with the low-memory BFGS optimizer, reducing the quantum computational cost of the method. Third, we compare the accuracy of the methods for chemistry applications using the beryllium hydride molecule (BeH$_2$). We compare the approaches across a number of different choices (ansätze types, depth, discretization steps, initial Hamiltonian, adiabatic schedules and method used). Our results show that the G-AQC-PQC outperforms conventional VQE. We further discuss limitations such as the zero-gradient problem and identify regimes where adiabatically-inspired methods offer a tangible advantage for near-term quantum chemistry applications."}
{"id": "2512.14463", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14463", "abs": "https://arxiv.org/abs/2512.14463", "authors": ["Xin Wang", "Zeyang Liao"], "title": "Super-Heisenberg-limited Sensing via Collective Subradiance in Waveguide QED", "comment": null, "summary": "We explore the quantum-metrological potential of subwavelength-spaced emitter arrays coupled to a one-dimensional nanophotonic waveguide. In this system, strong dipole--dipole interactions profoundly modify the collective optical response, leading to the emergence of ultranarrow subradiant resonances. Through an eigenmode analysis of the effective non-Hermitian Hamiltonian, we derive a universal scaling law for the decay rate of the most subradiant state, which exhibits an $ N^{-3} $ scaling with even-odd oscillatory behavior in the deep-subwavelength regime. This scaling is directly observable in the single-photon scattering spectrum, enabling the detection of minute changes in atomic separation with a figure of merit that scales as $ N^3 $. The quantum Fisher information (QFI) scales as $N^6$ and can be closely approached by measuring spectral shifts near the steepest slope of the most subradiant resonance. These enhancements remain robust under realistic positional disorder, confirming that dipole--dipole-engineered subradiance provides a viable resource for quantum metrology. Our work bridges many-body waveguide quantum electrodynamics and high-precision sensing, opening a route toward scalable quantum sensors on integrated nanophotonic platforms."}
{"id": "2512.14497", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14497", "abs": "https://arxiv.org/abs/2512.14497", "authors": ["B. Vigneshwar", "R. Sankaranarayanan"], "title": "Nonlocal contributions to ergotropy: A thermodynamic perspective", "comment": null, "summary": "Nonlocality is a defining feature of quantum mechanics and has long served as a key indicator of quantum resources since the formulation of Bell's inequalities. Identifying the contribution of nonlocality to extractable work remains a central problem in quantum thermodynamics. We address this by introducing a quantifier of nonlocal contributions to extractable work in bipartite systems. It is shown that closed form expressions can be calculated for our quantity in terms of the Schmidt coefficients. Further for strictly non-interacting Hamiltonian, the direct relationship between ergotropy and correlations is established. Our results reveal that nonlocal resources invariably enhance extractable work under non-interacting Hamiltonians, while in the presence of interactions, their contribution can either increase or diminish depending on the structure of the state and the Hamiltonian."}
{"id": "2512.14513", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14513", "abs": "https://arxiv.org/abs/2512.14513", "authors": ["Artemiy Burov", "Julien Baglio", "Clément Javerzac-Galy"], "title": "Large circuit execution for NMR spectroscopy simulation on NISQ quantum hardware", "comment": null, "summary": "With the latest advances in quantum computing technology, we are gradually moving from the noisy intermediate-scale quantum (NISQ) era characterized by hardware limited in the number of qubits and plagued with quantum noise, to the age of quantum utility where both the newest hardware and software methods allow for tackling problems which have been deemed difficult or intractable with conventional classical methods. One of these difficult problems is the simulation of one-dimensional (1D) nuclear magnetic resonance (NMR) spectra, a major tool to learn about the structure of molecules, helping the design of new materials or drugs. Using advanced error mitigation and error suppression techniques from Q-CTRL together with the latest commercially available superconducting-qubit quantum computer from IBM and trapped-ion quantum computer from IonQ, we present the quantum Hamiltonian simulation of liquid-state 1D NMR spectra in the high-field regime for spin systems up to 34 spins. Our pipeline has a major impact on the ability to execute deep quantum circuits with the reduction of quantum noise, improving mean square error by a factor of 22. It allows for the execution of deep quantum circuits and obtaining salient features of the 1D NMR spectra for both 16-spin and 22-spin systems, as well as a 34-spin system, which lies beyond the regime where unrestricted full Liouvillespace simulations are practical (32 spins, the Liouville limit). Our work is a step toward near-term quantum utility in NMR spectroscopy."}
{"id": "2512.14521", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2512.14521", "abs": "https://arxiv.org/abs/2512.14521", "authors": ["Riccardo Grazi", "Henrik Johannesson", "Dario Ferraro", "Niccolò Traverso Ziani"], "title": "Finite-Time Protocols Stabilize Charging in Noisy Ising Quantum Batteries", "comment": "25 pages, 8 figures", "summary": "Reliable charging protocols are crucial for advancing quantum batteries toward practical use. We investigate a transverse-field Ising chain as a quantum battery, focusing on the combined role of qubit interactions in the battery model and finite charging time. This interplay yields smoother and more controllable charging compared to sudden protocols or non-interacting batteries. Introducing stochastic noise reveals a strong dependence on the charging trajectory. Protocols that weakly excite the system gain energy under noise but lose extractable work. In contrast, protocols that strongly excite many modes show the opposite trend: noise reduces stored energy yet improves efficiency, defined as the ratio of ergotropy to stored energy. These findings demonstrate that finite-time ramps stabilize charging and highlight that noise can either hinder or enhance quantum-battery performance depending on the protocol."}
{"id": "2512.14528", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14528", "abs": "https://arxiv.org/abs/2512.14528", "authors": ["Edward Gheorghita", "Sebastian Wald", "Andrea Pupić", "Onur Hosten"], "title": "Continuous Accumulation of Cold Atoms in an Optical Cavity", "comment": "10 pages, 6 figures", "summary": "Continuously operating atom-light interfaces represent a key prerequisite for steady-state quantum sensors and efficient quantum processors. Here, we demonstrate continuous accumulation of sub-Doppler-cooled atoms in a shallow intracavity dipole trap, realizing this regime. The key ingredient is a light-shift manipulation that creates spatially varying cooling parameters, enabling efficient capture and accumulation of atoms within a cavity mode. Demonstrated with rubidium atoms, a continuous flux from a source cell is funneled through the magneto-optical trap into the cavity mode, where the atoms are cooled and maintained below $10~μ\\text{K}$ in steady state without time-sequenced operation. We characterize the resulting continuously maintained ensemble of millions of atoms and its collective coupling to the cavity field, establishing a route toward continuously operated cavity-QED systems and long-duration atomic and hybrid quantum sensors."}
{"id": "2512.14541", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.14541", "abs": "https://arxiv.org/abs/2512.14541", "authors": ["Subrata Das", "Archisman Ghosh", "Swaroop Ghosh"], "title": "A Graph-Based Forensic Framework for Inferring Hardware Noise of Cloud Quantum Backend", "comment": "11 pages, 5 figures, conference", "summary": "Cloud quantum platforms give users access to many backends with different qubit technologies, coupling layouts, and noise levels. The execution of a circuit, however, depends on internal allocation and routing policies that are not observable to the user. A provider may redirect jobs to more error-prone regions to conserve resources, balance load or for other opaque reasons, causing degradation in fidelity while still presenting stale or averaged calibration data. This lack of transparency creates a security gap: users cannot verify whether their circuits were executed on the hardware for which they were charged. Forensic methods that infer backend behavior from user-visible artifacts are therefore becoming essential. In this work, we introduce a Graph Neural Network (GNN)-based forensic framework that predicts per-qubit and per-qubit link error rates of an unseen backend using only topology information and aggregated features extracted from transpiled circuits. We construct a dataset from several IBM 27-qubit devices, merge static calibration features with dynamic transpilation features and train separate GNN regressors for one- and two-qubit errors. At inference time, the model operates without access to calibration data from the target backend and reconstructs a complete error map from the features available to the user. Our results on the target backend show accurate recovery of backend error rate, with an average mismatch of approximately 22% for single-qubit errors and 18% for qubit-link errors. The model also exhibits strong ranking agreement, with the ordering induced by predicted error values closely matching that of the actual calibration errors, as reflected by high Spearman correlation. The framework consistently identifies weak links and high-noise qubits and remains robust under realistic temporal noise drift."}
{"id": "2512.14543", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14543", "abs": "https://arxiv.org/abs/2512.14543", "authors": ["Changchun Feng", "Laifa Tao", "Lin Chen"], "title": "Physics-Informed Neural Networks with Adaptive Constraints for Multi-Qubit Quantum Tomography", "comment": null, "summary": "Quantum state tomography (QST) faces exponential measurement requirements and noise sensitivity in multi-qubit systems, bottlenecking practical quantum technologies. We present a physics-informed neural network (PINN) framework integrating quantum mechanical constraints via adaptive weighting, a residual-and-attention-enhanced architecture, and differentiable Cholesky parameterization for physical validity. Evaluations on 2--5 qubit systems and arbitrary-dimensional states show PINN consistently outperforms traditional neural networks (TNNs), achieving highest fidelity across all dimensions. PINN outperforms baselines, with marked improvements in moderately high-dimensional systems, superior noise robustness (slower performance degradation), and consistent dimensional robustness. Theoretical analysis shows physical constraints reduce Rademacher complexity and mitigate the curse of dimensionality via constraint-induced dimension and sample complexity reduction, effective regardless of qubit number. While experiments are limited to 5-qubit systems due to computational constraints, our theoretical framework (convergence guarantees, generalization bounds, scalability theorems) justifies PINN's advantages will persist and strengthen in larger systems (6+ qubits), where constraint-induced dimension reduction benefits grow with system size. Practically, this advances quantum error correction and gate calibration by reducing measurement requirements from O(4^n) to O(2^n) while maintaining high fidelity, enabling faster error correction cycles and accelerated calibration critical for scalable quantum computing."}
{"id": "2512.14552", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14552", "abs": "https://arxiv.org/abs/2512.14552", "authors": ["Yuichiro Nakano", "Keisuke Fujii"], "title": "Fair sampling of ground-state configurations using hybrid quantum-classical MCMC algorithms", "comment": "12 pages, 7 figures", "summary": "We study the fair sampling properties of hybrid quantum-classical Markov chain Monte Carlo (MCMC) algorithms for combinatorial optimization problems with degenerate ground states. While quantum optimization heuristics such as quantum annealing and the quantum approximate optimization algorithm (QAOA) are known to induce biased sampling, hybrid quantum-classical MCMC incorporates quantum dynamics only as a proposal transition and enforces detailed balance through classical acceptance steps. Using small Ising models, we show that MCMC post-processing corrects the sampling bias of quantum dynamics and restores near-uniform sampling over degenerate ground states. We then apply the method to random $k$-SAT problems near the satisfiability threshold. For random 2-SAT, a hybrid MCMC combining QAOA-assisted neural proposals with single spin-flip updates achieves fairness comparable to that of PT-ICM. For random 3-SAT, where such classical methods are no longer applicable, the hybrid MCMC still attains approximately uniform sampling. We also examine solution counting and find that the required number of transitions is comparable to that of WalkSAT. These results indicate that hybrid quantum-classical MCMC provides a viable framework for fair sampling and solution enumeration."}
{"id": "2512.14566", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14566", "abs": "https://arxiv.org/abs/2512.14566", "authors": ["Reza Hamzehofi"], "title": "Entanglement measure for the W-class states", "comment": null, "summary": "The structure and quantification of entanglement in the W-class states are investigated under physically motivated transformations that induce mixed-state dynamics. A rigorous condition is established linking global separability to the behavior of pairwise entanglement, showing that the absence of pairwise entanglement is sufficient to guarantee complete separability of the system, provided the Hilbert-space basis is preserved. This result motivates the identification of the sum of two-tangles as a natural and effective entanglement quantifier for the W-class states. Furthermore, the commonly used $π$-tangle becomes ineffective for the maximally entangled $n$-qubit W state as the system size increases, vanishing in the large-$n$ limit. To address this limitation, the sum of $π$-tangles is introduced, which, like the sum of two-tangles, successfully quantifies the entanglement of the maximally entangled $n$-qubit W state in the large-$n$ limit. In addition, a new condition for entanglement measures is introduced, which facilitates the formulation of a well-behaved and physically meaningful entanglement measure."}
{"id": "2512.14582", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.14582", "abs": "https://arxiv.org/abs/2512.14582", "authors": ["Jakub Szefer"], "title": "Exploiting Reset Operations in Cloud-based Quantum Computers to Run Quantum Circuits for Free", "comment": "14 pages, 13 figures", "summary": "This work presents the first thorough exploration of how reset operations in cloud-based quantum computers could be exploited to run quantum circuits for free. This forms a new type of attack on the economics of cloud-based quantum computers. All major quantum computing companies today offer access to their hardware through some type of cloud-based service. Due to the noisy nature of quantum computers, a quantum circuit is run many times to collect the output statistics, and each run is called a shot. The fees users pay for access to the machines typically depend on the number of these shots of a quantum circuit that are executed. Per-shot pricing is a clean and straightforward approach as users are charged a small fee for each shot of their circuit. This work demonstrates that per-shot pricing can be exploited to get circuits to run for free when users abuse recently implemented mid-circuit qubit measurement and reset operations. Through evaluation on real, cloud-based quantum computers this work shows how multiple circuits can be executed together within a shot, by separating each user circuit by set of reset operations and submitting all the circuits, and reset operations, as one larger circuit. As a result, the user is charged per-shot pricing, even though inside each shot are multiple circuits. Total per-shot cost to run certain circuits could be reduced by up to $900$\\% using methods proposed in this work, leading to significant financial losses to quantum computing companies. To address this novel finding, this work proposes a clear approach for how users should be charged for their execution, while maintaining the flexibility and usability of the mid-circuit measurement and reset~operations."}
{"id": "2512.14583", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2512.14583", "abs": "https://arxiv.org/abs/2512.14583", "authors": ["Cesar Lema", "Aleix Bou-Comas", "Atithi Acharya", "Vadim Oganesyan", "Anirvan Sengupta"], "title": "Reading Qubits with Sequential Weak Measurements: Limits of Information Extraction", "comment": "25 pages, 4 figures", "summary": "Quantum information processing and computation requires high accuracy qubit configuration readout. In many practical schemes, the initial qubit configuration has to be inferred from readout that is a time-dependent weak measurement record. However, a combination of the measurement scheme and intrinsic dynamics can end up scrambling the initial state and lose information irretrievably. Here, we study the information physics of quantum trajectories based on weak measurements in order to address the optimal achievable performance in qubit configuration readout for two realistic models of single qubit readout: (i) Model I is informationally complete, but without intrinsic dynamics; (ii) Model II is informationally incomplete weak measurements with intrinsic dynamics. We first use mutual information to characterize how much intrinsic information about the initial state is encoded in the measurement record. Using a fixed discrete time-step formulation, we compute the mutual information while varying the measurement strength, duration of measurement record, and the relative strength of intrinsic dynamics in our measurement schemes. We also exploit the emergence of continuum scaling and the Stochastic Master Equation in the weak measurement limit. We develop an asymptotic expansion in the measurement efficiency parameter to calculate mutual information, which captures qualitative and quantitative features of the numerical data. The bounds on information extraction are manifested as plateaux in mutual information, our analysis obtains these bounds and also optimal duration of measurement required to saturate them. Our results should be useful both for quantum device operation and optimization and also, possibly, for improving the performance of recent machine learning approaches for qubit and multiqubit configuration readout in current Noisy Intermediate-Scale Quantum (NISQ) experiment regimes."}
{"id": "2512.14588", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14588", "abs": "https://arxiv.org/abs/2512.14588", "authors": ["Soham Sau", "Michal Sedlák"], "title": "Sequential realization of Quantum Instruments", "comment": "25 pages, 4 figures", "summary": "In adaptive quantum circuits classical results of mid-circuit measurements determine the upcoming gates. This allows POVMs, quantum channels or more generally quantum instruments to be implemented sequentially, so that fewer qubits need to be used at each of the $N$ measurement steps. In this paper, we mathematically describe these problems via adaptive sequence of instruments (ASI) and show how any instrument can be decomposed into it. Number of steps $N$ and number of ancillary qubits $n_A$ needed for actual implementation are crucial parameters of any such ASI. We show an achievable lower bound on the product $N.n_A$ and we determine in which situations this tradeoff is likely to be optimal. Contrary to common intuition we show that for quantum instruments which transform $n$ to $m(>n)$ qubits, there exist $N$-step ASI implementing them just with $(m-n)$ ancillary qubits, which are remeasured $(N-1)$ times and finally used as output qubits."}
{"id": "2512.14643", "categories": ["quant-ph", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.14643", "abs": "https://arxiv.org/abs/2512.14643", "authors": ["Malvika Raj Joshi", "Avishay Tal", "Francisca Vasconcelos", "John Wright"], "title": "Improved Lower Bounds for QAC0", "comment": null, "summary": "In this work, we establish the strongest known lower bounds against QAC$^0$, while allowing its full power of polynomially many ancillae and gates. Our two main results show that:\n  (1) Depth 3 QAC$^0$ circuits cannot compute PARITY regardless of size, and require at least $Ω(\\exp(\\sqrt{n}))$ many gates to compute MAJORITY.\n  (2) Depth 2 circuits cannot approximate high-influence Boolean functions (e.g., PARITY) with non-negligible advantage in depth $2$, regardless of size.\n  We present new techniques for simulating certain QAC$^0$ circuits classically in AC$^0$ to obtain our depth $3$ lower bounds. In these results, we relax the output requirement of the quantum circuit to a single bit (i.e., no restrictions on input preservation/reversible computation), making our depth $2$ approximation bound stronger than the previous best bound of Rosenthal (2021). This also enables us to draw natural comparisons with classical AC$^0$ circuits, which can compute PARITY exactly in depth $2$ using exponential size. Our proof techniques further suggest that, for inherently classical decision problems, constant-depth quantum circuits do not necessarily provide more power than their classical counterparts. Our third result shows that depth $2$ QAC$^0$ circuits, regardless of size, cannot exactly synthesize an $n$-target nekomata state (a state whose synthesis is directly related to the computation of PARITY). This complements the depth $2$ exponential size upper bound of Rosenthal (2021) for approximating nekomatas (which is used as a sub-circuit in the only known constant depth PARITY upper bound)."}
{"id": "2512.14655", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14655", "abs": "https://arxiv.org/abs/2512.14655", "authors": ["Iman Ahmadabadi", "I-Te Lu", "Leonardo A. Cunha", "Michael Ruggenthaler", "Johannes Flick", "Angel Rubio"], "title": "Testing electron-photon exchange-correlation functional performance for many-electron systems under weak and strong light-matter coupling", "comment": null, "summary": "We present results of a photon-free exchange-correlation functional within the local density approximation (pxcLDA) for quantum electrodynamics density functional theory (QEDFT) that efficiently describes the electron density of many-electron systems across weak to strong light-matter coupling. Building on previous work [I-Te. Lu et al., Phys. Rev. A 109, 052823 (2024)] that captured electron-photon correlations via an exchange-correlation functional derived from the nonrelativistic Pauli-Fierz Hamiltonian and tested on one-electron systems, we use a simple procedure to compute a renormalization factor describing electron-photon correlations and inhomogeneity in the weak-coupling regime by comparing it with quantum electrodynamics coupled-cluster, and previous QEDFT optimized effective potential methods. Across various atoms and molecules, pxcLDA reproduces cavity-modified densities in close agreement with these references. The renormalization factor approaches unity as the system size or collective coupling increases, reflecting an electron-photon exchange-dominated behavior and improved accuracy for larger systems. This approach now offers a practical route to applying QEDFT functionals based on electron density to realistic electron systems."}
{"id": "2512.13812", "categories": ["hep-lat", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.13812", "abs": "https://arxiv.org/abs/2512.13812", "authors": ["Itay Gomelski", "Jonathan Elyovich", "Ariel Kelman", "Erez Zohar", "Patrick Emonts"], "title": "Algorithmic aspects of gauged Gaussian fermionic projected entangled pair states", "comment": null, "summary": "Lattice gauge theories (LGTs) provide a powerful framework for studying non-perturbative phenomena in gauge theories. However, conventional approaches such as Monte Carlo (MC) simulations in imaginary time are limited, as they do not allow real time evolution and suffer from a sign problem in many important cases. Using Gauged Gaussian fermionic projected entangled pair states (GGFPEPS) as a variational ground state ansatz offers an alternative for studying LGTs through a sign-problem-free variational MC. As this method is extended to larger and more complex systems, understanding its numerical behavior becomes essential. While conventional action based MC has been extensively studied, the performance and characteristics of non-action-based MC within the GGFPEPS framework are far less explored. In this work, we investigate these algorithmic aspects, identifying an optimal update size for GGFPEPS-based MC simulations for $\\mathbb{Z}_2$ in $2+1$ dimensions. We show that gauge fixing generally slows convergence, and demonstrate that not exploiting the translation-invariance can, in some cases, improve the computational time scaling of error convergence. We expect that these improvements will allow advancing the simulation to larger and more complex systems."}
{"id": "2512.14149", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14149", "abs": "https://arxiv.org/abs/2512.14149", "authors": ["Yuki Miyazaki", "Shiori Tanigawa", "Giacomo Marmorini", "Nobuo Furukawa", "Daisuke Yamamoto"], "title": "A sine-square deformation approach to quantum critical points in one-dimensional systems", "comment": "23 pages, 11 figures", "summary": "We propose a method to determine the quantum phase boundaries of one-dimensional systems using sine-square deformation (SSD). Based on the proposition, supported by several exactly solved cases though not proven in full generality, that ``if a one-dimensional system is gapless, then the expectation value of any local observable in the ground state of the Hamiltonian with SSD exhibits translational symmetry in the thermodynamic limit,\" we determine the quantum critical point as the location where a local observable becomes site-independent, identified through finite-size scaling analysis. As case studies, we consider two models: the antiferromagnetic Ising chain in mixed transverse and longitudinal magnetic fields with nearest-neighbor and long-range interactions. We calculate the ground state of these Hamiltonians with SSD using the density-matrix renormalization-group algorithm and evaluate the local transverse magnetization. For the nearest-neighbor model, we show that the quantum critical point can be accurately estimated by our procedure with systems of up to 84 sites, or even smaller, in good agreement with results from the literature. For the long-range model, we find that the phase boundary between the antiferromagnetic and paramagnetic phases is slightly shifted relative to the nearest-neighbor case, leading to a reduced region of antiferromagnetic order. Moreover, we propose an experimental procedure to implement the antiferromagnetic $J_1$-$J_2$ Ising couplings with SSD using Rydberg atom arrays in optical tweezers, which can be achieved within a very good approximation. Because multiple independent scaling conditions naturally emerge, our approach enables precise determination of quantum critical points and possibly even the extraction of additional critical phenomena, such as critical exponents, from relatively small system sizes."}
{"id": "2512.14169", "categories": ["nlin.CD", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14169", "abs": "https://arxiv.org/abs/2512.14169", "authors": ["Parsa Kafashi", "Mozhgan Orujlu"], "title": "Quantum-Inspired Approach to Analyzing Complex System Dynamics", "comment": null, "summary": "We present a quantum information-inspired framework for analyzing complex systems through multivariate time series. In this approach the system's state is encoded into a density matrix, providing a compact representation of higher-order correlations and dependencies. This formulation enables precise quantification of the relative influence among time series, tracking of their response to external perturbations and also the definition of a recovery timescale without need for dimensional reduction. By leveraging tools such as fidelity from quantum information theory, our method naturally captures higher-order co-fluctuations beyond pairwise statistics, offering a holistic characterization of resilience and similarity in high-dimensional dynamics. We validate this approach on synthetic data generated by a 9-dimensional modified Lorenz-96 model and demonstrate its utility on real-world climate data, analyzing global temperature anomalies across nine regions, quantifying the dissimilarity of each 288-month time window up to July 2025 relative to the 1850-1874 baseline period."}
{"id": "2512.14315", "categories": ["physics.hist-ph", "gr-qc", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.14315", "abs": "https://arxiv.org/abs/2512.14315", "authors": ["Karen Crowther"], "title": "Another 100 Years of Quantum Interpretation?", "comment": "An edited version is to appear in: How to Understand Quantum Mechanics - 100 Years of Ongoing Interpretation, edited by Lars-Göran Johansson and Jan Faye", "summary": "Interpretation is not the only way to explain a theory's success, form and features, and nor is it the only way to solve problems we see with a theory. This can also be done by giving a reductive explanation of the theory, by reference to a newer, more accurate, and/or more fundamental theory. We are seeking a theory of quantum gravity, a more fundamental theory than both quantum mechanics and general relativity, yet, while this theory is supposed to explain general relativity, it's not typically been thought to be necessary, or able, to explain quantum mechanics -- a task instead assigned to interpretation. Here, I question why this is. I also present a new way of assessing the various interpretations of quantum mechanics, in terms of their heuristic and unificatory potential in helping us find a more fundamental theory."}
