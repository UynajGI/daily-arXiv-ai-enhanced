<div id=toc></div>

# Table of Contents

- [physics.soc-ph](#physics.soc-ph) [Total: 5]
- [eess.SY](#eess.SY) [Total: 16]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 8]
- [quant-ph](#quant-ph) [Total: 140]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [cs.SI](#cs.SI) [Total: 4]
- [nlin.CG](#nlin.CG) [Total: 1]
- [math.NA](#math.NA) [Total: 17]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 5]
- [nlin.AO](#nlin.AO) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [stat.ME](#stat.ME) [Total: 9]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 4]
- [math.OC](#math.OC) [Total: 10]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [hep-lat](#hep-lat) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [cs.CE](#cs.CE) [Total: 5]
- [math.ST](#math.ST) [Total: 5]


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [1] [Monthly Rural-Urban Scaling of Road Accidents in England, Wales and Scotland (2019-2023)](https://arxiv.org/abs/2510.07351)
*Isabel Copsey,Quentin Hanley,Jack Sutton*

Main category: physics.soc-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Road traffic accidents remain a major public health challenge worldwide, with
urbanisation and population density identified as key factors influencing risk.
This study analyses monthly accident data from 2009 to 2023 across 632
parliamentary constituencies in England, Wales, and Scotland, using an
area-normalised approach based on population density. Segmented power law
models consistently identified breakpoints separating sublinear rural from
superlinear urban scaling behaviours. Seasonal variation in scaling exponents
was pronounced in rural regions but less evident in urban ones. Fourier-based
cross-spectral analysis of yearly cycles revealed systematic phase shifts:
rural exponents lagged pre-exponential factors by 4.5 months, while urban
exponents were 2.7 months out of phase, producing a 5.3 month shift between
rural and urban exponents. These findings highlight the importance of
pre-exponentials-defined as the expected density of accidents at unit
population density-as comparable descaled metrics, revealing both long-term
national declines and recurring seasonal peaks. Notably, the phase offsets
suggest structurally distinct causes of rural and urban accident risk, with
urban regions exhibiting increasing acceleration in accident scaling,
potentially linked to growth in vehicle numbers, size, and weight. Residuals,
modelled with the Type I Generalised Logistic Distribution (GLD), captured
skewness and heterogeneity more effectively than normal assumptions. Geospatial
mapping highlighted persistent urban hotspots alongside rural and coastal
constituencies with systematically lower accident densities than predicted.
Together, these findings advance understanding of how density and urbanisation
shape accident risk and provide evidence to support more targeted road safety
interventions and policy planning.

</details>


### [2] [Economic thermodynamics and inflation](https://arxiv.org/abs/2510.07544)
*İdris Demir,Ali İhsan Keskin*

Main category: physics.soc-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study presents a computational and theoretical framework inspired by
thermodynamic principles to analyze the dynamics of economic inflation within
adiabatic and non-adiabatic systems. In a framework referred to as
developmental symmetry, inflation is formulated as a scalar field evolving
through continuity equations, drawing an analogy with the Raychaudhuri equation
in gravitational dynamics. The results show that adiabatic systems fail to
reach equilibrium, while non-adiabatic systems can evolve toward stable states
over time. The model successfully reproduces observed inflationary regimes-from
hyperinflation to stable low-inflation phases-with characteristic transition
periods of about a decade. These results indicate that production continuity
and controlled monetary flow are crucial for achieving stability in complex
economic systems, linking thermodynamic balance to macroeconomic equilibrium.

</details>


### [3] [Pursuing decarbonization and competitiveness: a narrow corridor for European green industrial transformation](https://arxiv.org/abs/2510.08199)
*Alice Di Bella,Toni Seibold,Tom Brown,Massimo Tavoni*

Main category: physics.soc-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study analyzes how Europe can decarbonize its industrial sector while
remaining competitive. Using the open-source model PyPSA-Eur, it examines key
energy- and emission-intensive industries, including steel, cement, methanol,
ammonia, and high-value chemicals. Two development paths are explored: a
continued decline in industrial activity and a reindustrialization driven by
competitiveness policies. The analysis assesses cost gaps between European
green products and lower-cost imports, and evaluates strategies such as
intra-European relocation, selective imports of green intermediates, and
targeted subsidies. Results show that deep industrial decarbonization is
technically feasible, led by electrification, but competitiveness depends
strongly on policy choices. Imports of green intermediates can lower costs
while preserving jobs and production, whereas broad subsidies are economically
unsustainable. Effective policy should focus support on sectors like ammonia
and steel finishing while maintaining current production levels.

</details>


### [4] [Modelling and evaluating travel information during disruptions: An illustrative example from Swedish railways](https://arxiv.org/abs/2510.08254)
*Abderrahman Ait-Ali,Anders Peterson*

Main category: physics.soc-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate and timely travel information is an asset for enhancing passenger
travel experience during normal traffic, and for mitigating the discomforts
during disruptions. With longer and more frequent disruptions as well as
increasing ridership, traffic delays can incur substantial costs for passengers
and other transport stakeholders, e.g., operators and infrastructure managers.
Such costs can, however, be reduced thanks to effective travel information
strategies during traffic disruptions. In this paper, we introduce an
evaluation model to assess the value of travel information under different
scenarios. Focusing on real-time travel information to train passengers,
accessibility benefits are quantified in monetary terms based on historical
delay distributions, timing of travel information (pre/on-trip) and ridership.
Using a case study from the Swedish railways, the model is showcased and
applied to a commuter line in Stockholm. The experimental results indicate
individual valuations that are higher than references and savings at the system
level of at least 23% of the delay costs. Further testing of the model, e.g.,
on larger-scale scenarios, and including transfer trips, is a possible
direction for future works.

</details>


### [5] [Effectiveness of Quota Policies Across STEM, Biological, and Humanities Programs](https://arxiv.org/abs/2510.08261)
*Ricardo D. Matheus,Elmer M. Gennaro,Marcelo T. Yamashita*

Main category: physics.soc-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We examine more than a decade of quota policy at Unesp, analyzing Physics,
Biology, and Pedagogy as representative programs of distinct assessment styles.
Quotas show little impact in Physics, where the admission barrier is low, and
in Pedagogy, where high pass rates make it difficult to differentiate students,
but they reveal systematic differences in Biology. Focusing the analysis on
Calculus I - an introductory course in Physics and other Science, Technology,
Engineering, and Mathematics (STEM) programs - for which much larger statistics
are available, a clear hierarchy emerges: students admitted through open
competition perform best, those from public schools achieve intermediate
results, and students from racial quotas perform worst. When students are
divided directly by the admittance exam grade, the performance difference is
even clearer. Statistical analysis also shows that, contrary to expectation,
the probability of passing decreases as the number of attempts increases,
indicating that initial educational gaps are difficult to overcome within
higher education.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [6] [Auctioning Future Services in Edge Networks with Moving Vehicles: N-Step Look-Ahead Contracts for Sustainable Resource Provision](https://arxiv.org/abs/2510.07333)
*Ziqi Ling,Minghui Liwang,Xianbin Wang,Seyyedali Hosseinalipour,Zhipeng Cheng,Sai Zou,Wei Ni,Xiaoyu Xia*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Timely resource allocation in edge-assisted vehicular networks is essential
for compute-intensive services such as autonomous driving and navigation.
However, vehicle mobility leads to spatio-temporal unpredictability of resource
demands, while real-time double auctions incur significant latency. To address
these challenges, we propose a look-ahead contract-based auction framework that
shifts decision-making from runtime to planning time. Our approach establishes
N-step service contracts between edge servers (ESs) using demand forecasts and
modified double auctions. The system operates in two stages: first, an
LSTM-based prediction module forecasts multi-slot resource needs and determines
ES roles (buyer or seller), after which a pre-double auction generates
contracts specifying resource quantities, prices, and penalties. Second, these
contracts are enforced in real time without rerunning auctions. The framework
incorporates energy costs, transmission overhead, and contract breach risks
into utility models, ensuring truthful, rational, and energy-efficient trading.
Experiments on real-world (UTD19) and synthetic traces demonstrate that our
method improves time efficiency, energy use, and social welfare compared with
existing baselines.

</details>


### [7] [Nonlinear System Identification for Model-Based Control of Waked Wind Turbines](https://arxiv.org/abs/2510.07336)
*Sebastiano Randino,Lorenzo Schena,Nicolas Coudou,Emanuele Garone,Miguel Alfonso Mendez*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work presents a nonlinear system identification framework for modeling
the power extraction dynamics of wind turbines, including both freestream and
waked conditions. The approach models turbine dynamics using data-driven power
coefficient maps expressed as combinations of compact radial basis functions
and polynomial bases, parameterized in terms of tip-speed ratio and upstream
conditions. These surrogate models are embedded in a first-order dynamic system
suitable for model-based control. Experimental validation is carried out in two
wind tunnel configurations: a low-turbulence tandem setup and a high-turbulence
wind farm scenario. In the tandem case, the identified model is integrated into
an adapted K\omega^2 controller, resulting in improved tip-speed ratio tracking
and power stability compared to BEM-based and steady-state models. In the wind
farm scenario, the model captures the statistical behavior of the turbines
despite unresolved turbulence. The proposed method enables interpretable,
adaptive control across a range of operating conditions without relying on
black-box learning strategies.

</details>


### [8] [Techno-economic analysis of self-sustainable thermophotovoltaic systems for grid-scale energy generation](https://arxiv.org/abs/2510.07338)
*Jihun Lim,Sungwon Lee*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To facilitate the widespread adoption of renewable energy, dispatchable,
zero-emission power sources are essential for grid stability. This work
performs a comprehensive techno-economic analysis of a self-sustainable
thermophotovoltaic (TPV) system, an architecture that integrates solar charging
to function as a standalone power generation asset. Using theory-based models
for air-bridge InGaAs and Si diode cells, our analysis reveals that while the
system is not currently competitive from a pure levelized of storage cost
(LCOS) perspective due to the high capital expenditure for thermal battery
materials, its primary value lies in its competitive levelized cost of
electricity (LCOE). The results demonstrate that the LCOE of this
self-sustaining system can be competitive with conventional dispatchable
generators, such as gas turbines. Furthermore, at scales exceeding the
gigawatt-hour level, a Si-based system can also achieve an LCOE comparable to
that of traditional gas-turbine power plants, despite having a lower conversion
efficiency than its InGaAs counterpart. This highlights a practical engineering
pathway for leveraging silicon's immense manufacturing scalability, offering a
lower-risk route to deployment compared to III-V materials. Ultimately, this
work establishes the self-sustainable TPV architecture as a compelling pathway
toward providing grid-scale, on-demand, zero-emission power.

</details>


### [9] [Adaptive Control Allocation for Underactuated Time-Scale Separated Non-Affine Systems](https://arxiv.org/abs/2510.07507)
*Daniel M. Cherenson,Dimitra Panagou*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many robotic systems are underactuated, meaning not all degrees of freedom
can be directly controlled due to lack of actuators, input constraints, or
state-dependent actuation. This property, compounded by modeling uncertainties
and disturbances, complicates the control design process for trajectory
tracking. In this work, we propose an adaptive control architecture for
uncertain, nonlinear, underactuated systems with input constraints. Leveraging
time-scale separation, we construct a reduced-order model where fast dynamics
provide virtual inputs to the slower subsystem and use dynamic control
allocation to select the optimal control inputs given the non-affine dynamics.
To handle uncertainty, we introduce a state predictor-based adaptive law, and
through singular perturbation theory and Lyapunov analysis, we prove stability
and bounded tracking of reference trajectories. The proposed method is
validated on a VTOL quadplane with nonlinear, state-dependent actuation,
demonstrating its utility as a unified controller across various flight
regimes, including cruise, landing transition, and hover.

</details>


### [10] [Some Reflections on Sliding Mode Designs in Control Systems: An Example of Adaptive Tracking Control for Simple Mechanical Systems With Friction Without Measurement of Velocity](https://arxiv.org/abs/2510.07675)
*Romeo Ortega,Leyan Fang,Jose Guadalupe Romero*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The objective of this note is to share some reflections of the authors
regarding the use of sliding mode designs in control systems. We believe the
abundant, and ever increasing, appearance of this kind of works on our
scientific publications deserves some critical evaluation of their actual role,
relevance and pertinence. First, we discuss the procedure followed by most of
these designs -- illustrated with examples from the literature. Second, we
bring to the readers attention several aspects of the control problem, central
in classical designs, which are disregarded in the sliding mode literature.
Finally, to illustrate with an specific example our previous considerations, we
compare the performance of two adaptive tracking controllers for a simple one
degree of freedom mechanical systems with unknown parameters and static and
Coulomb friction -- that do not rely on the measurement of velocity.

</details>


### [11] [Space Logistics Analysis and Incentive Design for Commercialization of Orbital Debris Remediation](https://arxiv.org/abs/2510.07708)
*Asaad Abdul-Hamid,Brycen D. Pearl,Hang Woon Lee,Hao Chen*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As orbital debris continues to become a higher priority for the space
industry, there is a need to explore how partnerships between the public and
private space sector may aid in addressing this issue. This research develops a
space logistics framework for planning orbital debris remediation missions,
providing a quantitative basis for partnerships that are mutually beneficial
between space operators and debris remediators. By integrating network-based
space logistics and game theory, we illuminate the high-level costs of
remediating orbital debris, and the surplus that stands to be shared as a
result. These findings indicate significant progress toward the continued
development of a safe, sustainable, and profitable space economy.

</details>


### [12] [Multi-Level Multi-Fidelity Methods for Path Integral and Safe Control](https://arxiv.org/abs/2510.07756)
*Zhuoyuan Wang,Takashi Tanaka,Yongxin Chen,Yorie Nakahira*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sampling-based approaches are widely used in systems without analytic models
to estimate risk or find optimal control. However, gathering sufficient data in
such scenarios can be prohibitively costly. On the other hand, in many
situations, low-fidelity models or simulators are available from which samples
can be obtained at low cost. In this paper, we propose an efficient approach
for risk quantification and path integral control that leverages such data from
multiple models with heterogeneous sampling costs. A key technical novelty of
our approach is the integration of Multi-level Monte Carlo (MLMC) and
Multi-fidelity Monte Carlo (MFMC) that enable data from different time and
state representations (system models) to be jointly used to reduce variance and
improve sampling efficiency. We also provide theoretical analysis of the
proposed method and show that our estimator is unbiased and consistent under
mild conditions. Finally, we demonstrate via numerical simulation that the
proposed method has improved computation (sampling costs) vs. accuracy
trade-offs for risk quantification and path integral control.

</details>


### [13] [Multi-level informed optimization via decomposed Kriging for large design problems under uncertainty](https://arxiv.org/abs/2510.07904)
*Enrico Ampellio,Blazhe Gjorgiev,Giovanni Sansavini*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Engineering design involves demanding models encompassing many decision
variables and uncontrollable parameters. In addition, unavoidable aleatoric and
epistemic uncertainties can be very impactful and add further complexity. The
state-of-the-art adopts two steps, uncertainty quantification and design
optimization, to optimize systems under uncertainty by means of robust or
stochastic metrics. However, conventional scenario-based, surrogate-assisted,
and mathematical programming methods are not sufficiently scalable to be
affordable and precise in large and complex cases. Here, a multi-level approach
is proposed to accurately optimize resource-intensive, high-dimensional, and
complex engineering problems under uncertainty with minimal resources. A
non-intrusive, fast-scaling, Kriging-based surrogate is developed to map the
combined design/parameter domain efficiently. Multiple surrogates are
adaptively updated by hierarchical and orthogonal decomposition to leverage the
fewer and most uncertainty-informed data. The proposed method is statistically
compared to the state-of-the-art via an analytical testbed and is shown to be
concurrently faster and more accurate by orders of magnitude.

</details>


### [14] [A Stable, Accurate and Well-Conditioned Time-Domain PMCHWT Formulation](https://arxiv.org/abs/2510.07989)
*Van Chien Le,Cedric Munger,Francesco P. Andriulli,Kristof Cools*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces a new boundary element formulation for transient
electromagnetic scattering by homogeneous dielectric objects based on the
time-domain PMCHWT equation. To address dense-mesh breakdown, a multiplicative
Calderon preconditioner utilizing a modified static electric field integral
operator is employed. Large-timestep breakdown and late-time instability are
simultaneously resolved by rescaling the Helmholtz components leveraging the
quasi-Helmholtz projectors and using temporal differentiation and integration
as rescaling operators. This rescaling also balances the loop and star
components at large timesteps, improving solution accuracy. The resulting
discrete system is solved using a marching-on-in-time scheme and iterative
solvers. Numerical experiments for simply- and multiply-connected dielectric
scatterers, including highly non-smooth geometries, corroborate the accuracy,
stability, and efficiency of the proposed approach.

</details>


### [15] [General formulation of an analytic, Lipschitz continuous control allocation for thrust-vectored controlled rigid-bodies](https://arxiv.org/abs/2510.08119)
*Frank Mukwege,Tam Willy Nguyen,Emanuele Garone*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study introduces a systematic and scalable method for arbitrary
rigid-bodies equipped with vectorized thrusters. Two novel solutions are
proposed: a closed-form, Lipschitz continuous mapping that ensures smooth
actuator orientation references, and a convex optimization formulation capable
of handling practical actuator constraints such as thrust saturation and
angular rate limits. Both methods leverage the null-space structure of the
allocation mapping to perform singularity avoidance while generating
sub-optimal yet practical solutions. The effectiveness and generality of the
proposed framework are demonstrated through numerical simulations on a 3DOF
marine vessel and a 6DOF aerial quadcopter.

</details>


### [16] [Closed-loop control of sloshing fuel in a spinning spacecraft](https://arxiv.org/abs/2510.08121)
*Umberto Zucchelli,Miguel Alfonso Mendez,Annafederica Urbano,Sebastien Vincent-Bonnieu,Piotr Wenderski,Francesco Sanfedino*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: New-generation space missions require satellites to carry substantial amounts
of liquid propellant, making it essential to analyse the coupled
control-structure-propellant dynamics in detail. While Computational Fluid
Dynamics (CFD) offers high-fidelity predictions, its computational cost limits
its use in iterative design. Equivalent Mechanical Models (EMMs) provide a
faster alternative, though their predictive performance, especially in
closed-loop scenarios, remains largely unexplored. This work presents a
comparative analysis of a spacecraft under feedback control, using both CFD and
a reduced-order sloshing model. Results show good agreement, validating the
simplified model for the manoeuvrer considered. This validation enables
efficient sensitivity and stability studies, offering a practical tool for
early-stage spacecraft design.

</details>


### [17] [SecuLEx: a Secure Limit Exchange Market for Dynamic Operating Envelopes](https://arxiv.org/abs/2510.08172)
*Maurizio Vassallo,Adrien Bolland,Alireza Bahmanyar,Louis Wehenkel,Laurine Duchesne,Dong Liu,Sania Khaskheli,Alexis Ha Thuc,Pedro P. Vergara,Amjad Anvari-Moghaddam,Simon Gerard,Damien Ernst*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Distributed energy resources (DERs) are transforming power networks,
challenging traditional operational methods, and requiring new coordination
mechanisms. To address this challenge, this paper introduces SecuLEx (Secure
Limit Exchange), a new market-based paradigm to allocate power injection and
withdrawal limits that guarantee network security during time periods, called
dynamic operating envelopes (DOEs). Under this paradigm, distribution system
operators (DSOs) assign initial DOEs to customers. These limits can be
exchanged afterward through a market, allowing customers to reallocate them
according to their needs while ensuring network operational constraints. We
formalize SecuLEx and illustrate DOE allocation and market exchanges on a
small-scale low-voltage (LV) network, demonstrating that both procedures are
computationally tractable. In this example, SecuLEx reduces renewable
curtailment and improves grid utilization and social welfare compared to
traditional approaches.

</details>


### [18] [A Control Allocation Algorithm for Hypersonic Glide Vehicles with Input Limitations](https://arxiv.org/abs/2510.08275)
*Johannes Autenrieb,Patrick Gruhn*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hypersonic glide vehicles (HGVs) operate in challenging flight regimes
characterized by strong nonlinearities in actuation and stringent physical
constraints. These include state-dependent actuator limitations, asymmetric
control bounds, and thermal loads that vary with maneuvering conditions. This
paper introduces an iterative control allocation method to address these
challenges in real time. The proposed algorithm searches for control inputs
that achieve the desired moment commands while respecting constraints on input
magnitude and rate. For slender HGV configurations, thermal loads and drag
generation are strongly correlated-lower drag typically results in reduced
surface heating. By embedding drag-sensitive soft constraints, the method
improves energy efficiency and implicitly reduces surface temperatures,
lowering the vehicle's infrared signature. These features are particularly
advantageous for long-range military operations that require low observability.
The approach is demonstrated using the DLR's Generic Hypersonic Glide Vehicle 2
(GHGV-2) simulation model. The results confirm the method's effectiveness in
maintaining control authority under realistic, constrained flight conditions.

</details>


### [19] [CPU- and GPU-Based Parallelization of the Robust Reference Governor](https://arxiv.org/abs/2510.08288)
*Hamid R. Ossareh,William Shayne,Samuel Chevalier*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Constraint management is a central challenge in modern control systems. A
solution is the Reference Governor (RG), which is an add-on strategy to
pre-stabilized feedback control systems to enforce state and input constraints
by shaping the reference command. While robust formulations of RG exist for
linear systems, their extension to nonlinear systems is often computationally
intractable. This paper develops a scenario-based robust RG formulation for
nonlinear systems and investigates its parallel implementation on multi-core
CPUs and CUDA-enabled GPUs. We analyze the computational structure of the
algorithm, identify parallelization opportunities, and implement the resulting
schemes on modern parallel hardware. Benchmarking on a nonlinear hydrogen fuel
cell model demonstrates order-of-magnitude speedups (by as much as three orders
of magnitude) compared to sequential implementations.

</details>


### [20] [Underground Power Distribution System Restoration Using Inverter Based Resources](https://arxiv.org/abs/2510.08356)
*Wenlong Shi,Hongyi Li,Zhaoyu Wang*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Underground power distribution systems (PDSs) are increasingly deployed in
urban areas. The integration of smart devices including smart switchgears,
pad-mounted distribution transformers and inverter-based resources (IBRs)
enhance system resilience, however simultaneously introducing unique
challenges. The challenges include inrush currents caused by trapped charges in
underground cables, ferroresonance in distribution transformers during
energization, and three-phase load imbalance resulting from single-phase
underground laterals. To address these issues, this paper proposes an
underground PDS restoration framework using IBRs. Firstly, an underground cable
energization model is developed to quantify inrush current by analyzing voltage
differences across both switchgear terminals. Secondly, a distribution
transformer energization model is proposed to evaluate ferroresonance using
Q-factor constraints based on underground cable capacitance and damping
resistance. Thirdly, a phase-swapping model is proposed to improve load
balancing by dynamically reassigning lateral-phase connections through smart
switchgears. The proposed models are further integrated into a mixed-integer
nonlinear programming (MINLP) formulation to maximize the total weighted
restored load while constraining inrush currents, ferroresonance, and phase
imbalance. To address the nonlinearity induced by impedance matrix reordering
during phase swapping, a permutation-based linearization technique is proposed.
Finally, case studies on an underground PDS established based on IEEE 123-Node
Test Feeder validate the effectiveness of the proposed strategy in improving
uderground PDS restoration performance.

</details>


### [21] [Learning to Mitigate Post-Outage Load Surges: A Data-Driven Framework for Electrifying and Decarbonizing Grids](https://arxiv.org/abs/2510.08357)
*Wenlong Shi,Dingwei Wang,Liming Liu,Zhaoyu Wang*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Electrification and decarbonization are transforming power system demand and
recovery dynamics, yet their implications for post-outage load surges remain
poorly understood. Here we analyze a metropolitan-scale heterogeneous dataset
for Indianapolis comprising 30,046 feeder-level outages between 2020 and 2024,
linked to smart meters and submetering, to quantify the causal impact of
electric vehicles (EVs), heat pumps (HPs) and distributed energy resources
(DERs) on restoration surges. Statistical analysis and causal forest inference
demonstrate that rising penetrations of all three assets significantly increase
surge ratios, with effects strongly modulated by restoration timing, outage
duration and weather conditions. We develop a component-aware multi-task
Transformer estimator that disaggregates EV, HP and DER contributions, and
apply it to project historical outages under counterfactual 2035 adoption
pathways. In a policy-aligned pathway, evening restorations emerge as the
binding reliability constraint, with exceedance probabilities of 0.057 when
30\% of system load is restored within the first 15 minutes. Mitigation
measures, probabilistic EV restarts, short thermostat offsets and accelerated
DER reconnection, reduce exceedance to 0.019 and eliminate it entirely when
20\% or less of system load is restored. These results demonstrate that
transition-era surges are asset-driven and causally linked to electrification
and decarbonization, but can be effectively managed through integrated
operational strategies.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [22] [Localization of information driven by stochastic resetting](https://arxiv.org/abs/2510.07394)
*Camille Aron,Manas Kulkarni*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The dynamics of extended many-body systems are generically chaotic.
Classically, a hallmark of chaos is the exponential sensitivity to initial
conditions captured by positive Lyapunov exponents. Supplementing chaotic
dynamics with stochastic resetting drives a sharp dynamical phase transition:
we show that the Lyapunov spectrum, i.e., the complete set of Lyapunov
exponents, abruptly collapses to zero above a critical resetting rate. At
criticality, we find a sudden loss of analyticity of the velocity-dependent
Lyapunov exponent, which we relate to the transition from ballistic scrambling
of information to an arrested regime where information becomes exponentially
localized over a characteristic length diverging at criticality with an
exponent $\nu = 1/2$. We illustrate our analytical results on generic chaotic
dynamics by numerical simulations of coupled map lattices.

</details>


### [23] [Thermodynamically Consistent Continuum Theory of Magnetic Particles in High-Gradient Fields](https://arxiv.org/abs/2510.07552)
*Marko Tesanovic,Daniel M. Markiewitz,Marcus L. Popp,Martin Z. Bazant,Sonja Berensmeier*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Magnetic particles underpin a broad range of technologies, from water
purification and mineral processing to bioseparations and targeted drug
delivery. The dynamics of magnetic particles in high-gradient magnetic
fields-encompassing both their transport and eventual capture-arise from the
coupled interplay of field-driven drift, fluid advection, and particle-field
feedback. These processes remain poorly captured by existing models relying on
empirical closures or discrete particle tracking. Here, we present a
thermodynamically consistent continuum theory for collective magnetic particle
transport and capture in high-gradient fields. The framework derives from a
free-energy functional that couples magnetic energy, entropic mixing, and
steric interactions, yielding a concentration-dependent susceptibility via
homogenization theory. The resulting equations unify magnetism, mass transport,
and momentum balances without ad hoc shut-off criteria, allowing field
shielding, anisotropic deposition, and boundary-layer confinement to emerge
naturally. Simulations predict canonical capture morphologies-axially aligned
plumes, crescent-shaped deposits, and nonlinear shielding-across field
strengths and flow regimes, consistent with trends reported in prior
experimental and modeling studies. By organizing captured particle mass data
into a dimensionless phase diagram based on the Mason number, we reveal three
distinct regimes-thermodynamically controlled, transitional, and dynamically
controlled. This perspective provides a predictive platform for in silico
optimization and extension to three-dimensional geometries, and informing
digital twin development for industrial-scale high-gradient magnetic separation
processes.

</details>


### [24] [What is the most optimal diffusion?](https://arxiv.org/abs/2510.07571)
*Vasili Baranau*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: What is the fastest possible "diffusion"? A trivial answer would be "a
process that converts a Dirac delta-function into a uniform distribution
infinitely fast". Below, we consider a more reasonable formulation: a process
that maximizes differential entropy of a probability density function (pdf)
$f(\vec{x}, t)$ at every time $t$, under certain restrictions. Specifically, we
focus on a case when the rate of the Kullback-Leibler divergence
$D_{\text{KL}}$ is fixed. If $\Delta(\vec{x}, t, d{t}) = \frac{\partial f}{
\partial t} d{t}$ is the pdf change at a time step $d{t}$, we maximize the
differential entropy $H[f + \Delta]$ under the restriction $D_{\text{KL}}(f +
\Delta || f) = A^2 d{t}^2$, $A = \text{const} > 0$. It leads to the following
equation: $\frac{\partial f}{ \partial t} = - \kappa f (\ln{f} - \int f \ln{f}
d{\vec{x}})$, with $\kappa = \frac{A}{\sqrt{ \int f \ln^2{f} d{\vec{x}} -
\left( \int f \ln{f} d{\vec{x}} \right)^2 } }$. Notably, this is a non-local
equation, so the process is different from the It\^{o} diffusion and a
corresponding Fokker-Planck equation. We show that the normal and exponential
distributions are solutions to this equation, on $(-\infty; \infty)$ and $[0;
\infty)$, respectively, both with $\text{variance} \sim e^{2 A t}$, i.e.
diffusion is highly anomalous. We numerically demonstrate for sigmoid-like
functions on a segment that the entropy change rate $\frac{d H}{d t}$ produced
by such an optimal "diffusion" is, as expected, higher than produced by the
"classical" diffusion.

</details>


### [25] [Non-Kramers State Transitions in a Synthetic Toggle Switch Biosystem](https://arxiv.org/abs/2510.07797)
*Jianzhe Wei,Jingwen Zhu,Pan Chu,Liang Luo,Xiongfei Fu*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: State transitions are fundamental in biological systems but challenging to
observe directly. Here, we present the first single-cell observation of state
transitions in a synthetic bacterial genetic circuit. Using a mother machine,
we tracked over 1007 cells for 27 hours. First-passage analysis and dynamical
reconstruction reveal that transitions occur outside the small-noise regime,
challenging the applicability of classical Kramers' theory. The process lacks a
single characteristic rate, questioning the paradigm of transitions between
discrete cell states. We observe significant multiplicative noise that distorts
the effective potential landscape yet increases transition times. These
findings necessitate theoretical frameworks for biological state transitions
beyond the small-noise assumption.

</details>


### [26] [Phase Transitions Without Instability: A Universal Mechanism from Non-Normal Dynamics](https://arxiv.org/abs/2510.07938)
*Virgile Troude,Didier Sornette*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We identify a new universality class of phase transitions that arises in
non-normal systems, challenging the classical view that transitions require
eigenvalue instabilities. In traditional bifurcation theory, critical phenomena
emerge when spectral stability is lost; here, we show that transitions can
occur even when all equilibria are spectrally stable. The key mechanism is the
transient amplification induced by non-orthogonal eigenvectors: noise-driven
dynamics are enhanced not by lowering energy barriers, but by increasing the
effective shear of the flow, which renormalizes fluctuations and acts as an
emergent temperature. Once the non-normality index $\kappa$ exceeds a critical
threshold $\kappa_c$, stable equilibria lose practical relevance, enabling
escapes and abrupt transitions despite preserved spectral stability. This
pseudo-criticality generalizes Kramers' escape beyond potential barriers,
providing a fundamentally new route to critical phenomena. Its implications are
broad: in biology, DNA methylation dynamics reconcile long-term epigenetic
memory with rapid stochastic switching; in climate, ecology, finance, and
engineered networks, abrupt tipping points can arise from the same mechanism.
By demonstrating that phase transitions can emerge from non-normal
amplification rather than eigenvalue instabilities, we introduce a predictive,
compact framework for sudden transitions in complex systems, establishing
non-normality as a defining principle of a new universality class of phase
transitions.

</details>


### [27] [Kinetic description of one-dimensional stochastic dynamics with small inertia](https://arxiv.org/abs/2510.08502)
*Denis S. Goldobin,Lyudmila S. Klimenko,Irina V. Tyulkina,Vasily A. Kostin,Lev A. Smirnov*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study single-variable approaches for describing stochastic dynamics with
small inertia. The basic models we deal with describe passive Brownian
particles and phase elements (phase oscillators, rotators, superconducting
Josephson junctions) with an effective inertia in the case of a linear
dissipation term and active Brownian particles in the case of a nonlinear
dissipation. Elimination of a fast variable (velocity) reduces the
characterization of the system state to a single variable and is formulated in
four representations: moments, cumulants, the basis of Hermite functions, and
the formal cumulant variant of the last. This elimination provides rigorous
mathematical description for the overdamped limit in the case of linear
dissipation and the overactive limit of active Brownian particles. For the
former, we derive a low-dimensional equation system which generalizes the
Ott-Antonsen Ansatz to systems with small effective inertia. In the latter
case, we derive a Fokker-Planck-type equation with a forced drift term and an
effective diffusion in one dimension, where the standard two-/three-dimensional
mechanism is impossible. In the four considered representations, truncated
equation chains are demonstrated to be utilitary for numerical simulation for a
small finite inertia.

</details>


### [28] [Exact solution for the current generated by dissipative master equation in single-band tight-binding systems subject to time-dependent uniform electric fields](https://arxiv.org/abs/2510.08021)
*J. M. Alendouro Pinho,B. Amorim,J. M. Viana Parente Lopes*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The theory of open quantum systems is one of the most essential tools for the
development of quantum technologies. A particular area of interest is in the
optical response of solid state systems, where dissipation is introduced
phenomenologically through the relaxation time approximation and its effects
are usually gauged perturbatively. Analytical exact results for driven systems
under this approximation are scarce and tipically pertain only to the
stationary regime. Here, we obtain the analytical solution for the current
response of general single-band tight-binding system driven by a uniform
electric field with generic time-dependence under the relaxation time
approximation. We explore the effects of dissipation in two limiting cases: A
monochromatic field, where we analytically deduce the effect of dissipation on
High Harmonic Generation, and a constant electric field, where a generalization
for the Esaki-Tsu equation is presented for any single-band tight-binding
system. We specify the results for a simple 2D nearest neighbours tight-binding
lattice to emphasize the effect of the scale competition introduced by the two
different neighbours in both the monochromatic and constant field cases.
Finally, we compare our exact result for the constant field to the one obtained
from the usual perturbation theory calculation to probe the validity of the
latter.

</details>


### [29] [A nonequilibrium distribution for stochastic thermodynamics](https://arxiv.org/abs/2510.08315)
*Jean-Luc Garden*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The canonical distribution of Gibbs is extended to the case of systems
outside equilibrium. The distribution of probabilities of a discrete energy
levels system is used to provide a microscopic definition of work, along with a
microscopic definition of the uncompensated heat of Clausius involved in
nonequilibrium processes. The later is related to the presence of
non-conservatives forces with regards to the variation of the external
parameters. This new framework is used to investigate the nonequilibrium
relations in stochastic thermodynamics. A new relation is derived for the
random quantity of heat associated to the nonequilibrium work protocol. We
finally show that the distributions of probabilities of work, heat and
uncompensated heat are non-independent each other during a nonequilibrium
process.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [30] [What is Quantum Computer Security?](https://arxiv.org/abs/2510.07334)
*Sanjay Deshpande,Jakub Szefer*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum computing is rapidly emerging as one of the most transformative
technologies of our time. With the potential to tackle problems that remain
intractable for even the most powerful classical supercomputers, quantum
hardware has advanced at an extraordinary pace. Today, major platforms such as
IBM Quantum, Amazon Braket, and Microsoft Azure provide cloud-based access to
quantum processors, making them more widely available than ever before. While a
promising technology, quantum computing is not magically immune to security
threats. Much research has been done on post-quantum cryptography, which
addresses how to protect classical computers from attackers using quantum
computers. This article meanwhile introduces the dual idea of quantum computer
security: how to protect quantum computers from security attacks.

</details>


### [31] [Quantum Resources in Non-Abelian Lattice Gauge Theories: Nonstabilizerness, Multipartite Entanglement, and Fermionic Non-Gaussianity](https://arxiv.org/abs/2510.07385)
*Gopal Chandra Santra,Julius Mildenberger,Edoardo Ballini,Alberto Bottarelli,Matteo M. Wauters,Philipp Hauke*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Lattice gauge theories (LGTs) represent one of the most ambitious goals of
quantum simulation. From a practical implementation perspective, non-Abelian
theories present significantly tougher challenges than Abelian LGTs. However,
it is unknown whether this is also reflected in increased values of quantum
resources relating to the complexity of simulating quantum many-body models.
Here, we compare three paradigmatic measures of quantum resources -- stabilizer
R\'enyi entropy, generalized geometric measure of entanglement, and fermionic
antiflatness -- for pure-gauge theories on a ladder with Abelian $\mathbb{Z}_N$
as well as non-Abelian $D_3$ and SU(2) gauge symmetries. We find that
non-Abelian symmetries are not necessarily inherently harder to simulate than
Abelian ones, but rather the required quantum resources depend nontrivially on
the interplay between the group structure, superselection sector, and encoding
of the gauge constraints. Our findings help indicate where quantum advantage
could emerge in simulations of LGTs, both in NISQ and fault-tolerant eras.

</details>


### [32] [A Hardware-Efficient Mølmer-Sørensen Gate for Superconducting Quantum Computers](https://arxiv.org/abs/2510.07352)
*M. AbuGhanem*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The M{\o}lmer-S{\o}rensen gate, a cornerstone entangling operation in
trapped-ion systems, represents a promising alternative to standard entangling
gates in superconducting quantum architectures. However, its performance on
superconducting hardware has remained unverified. In this work, we present a
hardware-efficient implementation of the M{\o}lmer-S{\o}rensen gate and
characterize its performance using quantum process tomography (QPT) on IBM
Quantum's superconducting processors. Our implementation achieves a process
fidelity of 92.47\% on the real quantum hardware, a performance competitive
with the 93.02\% fidelity of the device's native controlled-NOT (CX) gate.
Furthermore, for the $|00\rangle$ input state, the gate prepares the target
Bell state with $94.2\%$ success probability, confirming its correct logical
operation. These results demonstrate that non-native entangling gates can be
optimized to perform on par with hardware-native operations. This work expands
the effective gate set for algorithm design on fixed-architecture processors
and provides a critical benchmark for cross-platform gate evaluation,
underscoring the role of hardware-aware compilation in advancing noisy
intermediate-scale quantum (NISQ) computing.

</details>


### [33] [Local active error correction from simulated confinement](https://arxiv.org/abs/2510.08056)
*Ethan Lake*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We refine an old idea for performing fault-tolerant error correction in
topological codes by simulating confining interactions between excitations. We
implement confinement using an array of local classical processors that measure
syndromes, broadcast messages to neighboring processors, and move excitations
using received messages. The dynamics of the resulting real-time decoder is
geometrically local, homogeneous in spacetime, and self-organized, operating
without any form of global control. On a system of linear size $L$, our decoder
requires access to $O({\rm polylog}(L))$ noiseless classical bits for each
qubit, and we prove that below a threshold error rate, it achieves a memory
lifetime scaling as a stretched exponential in $L$. When applied to the surface
code subject to depolarizing noise and measurement errors of equal strength,
numerics indicate a threshold at $p_c \approx 1.5\%$.

</details>


### [34] [Quantum Grid Path Planning Using Parallel QAOA Circuits Based on Minimum Energy Principle](https://arxiv.org/abs/2510.07413)
*Jun Liu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To overcome the bottleneck of classical path planning schemes in solving NP
problems and address the predicament faced by current mainstream quantum path
planning frameworks in the Noisy Intermediate-Scale Quantum (NISQ) era, this
study attempts to construct a quantum path planning solution based on parallel
Quantum Approximate Optimization Algorithm (QAOA) architecture. Specifically,
the grid path planning problem is mapped to the problem of finding the minimum
quantum energy state. Two parallel QAOA circuits are built to simultaneously
execute two solution processes, namely connectivity energy calculation and path
energy calculation. A classical algorithm is employed to filter out
unreasonable solutions of connectivity energy, and finally, the approximate
optimal solution to the path planning problem is obtained by merging the
calculation results of the two parallel circuits. The research findings
indicate that by setting appropriate filter parameters, quantum states
corresponding to position points with extremely low occurrence probabilities
can be effectively filtered out, thereby increasing the probability of
obtaining the target quantum state. Even when the circuit layer number p is
only 1, the theoretical solution of the optimal path coding combination can
still be found by leveraging the critical role of the filter. Compared with
serial circuits, parallel circuits exhibit a significant advantage, as they can
find the optimal feasible path coding combination with the highest probability.

</details>


### [35] [Quantum Algorithm for Binary Vector Encoding and Retrieval Utilizing the Permutation Trick](https://arxiv.org/abs/2510.07354)
*Andreas Wichert*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel quantum storage algorithm for k binary vectors of
dimension m into a superposition of a m qubit quantum state based on a
permutation technique. We compare this algorithm to the storage algorithm
proposed by Ventura and Martinez. The permutation technique is simpler and can
lead to an additional reduction through the reduce algorithm. To retrieve a
binary vector from the superposition of k vectors represented by a m qubit
quantum state, we must use a modified version of Grover algorithm, as Grover
algorithm does not function correctly for non uniform distributions. We
introduce the permutation trick that enables an exhaustive search by Grover
algorithm in square root of k steps for k patterns, independent of n equal two
power m. We compare this trick to the Ventura and Martinez trick, which
requires square root of n steps for k patterns.

</details>


### [36] [Proposals for experimentally realizing (mostly) quantum-autonomous gates](https://arxiv.org/abs/2510.07372)
*José Antonio Marín Guzmán,Yu-Xin Wang,Tom Manovitz,Paul Erker,Norbert M. Linke,Simone Gasparinetti,Nicole Yunger Halpern*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous quantum machines (AQMs) execute tasks without requiring
time-dependent external control. Motivations for AQMs include the restrictions
imposed by classical control on quantum machines' coherence times and
geometries. Most AQM work is theoretical and abstract; yet an experiment
recently demonstrated AQMs' usefulness in qubit reset, crucial to quantum
computing. To further reduce quantum computing's classical control, we propose
realizations of (fully and partially) quantum-autonomous gates on three
platforms: Rydberg atoms, trapped ions, and superconducting qubits. First, we
show that a Rydberg-blockade interaction or an ultrafast transition can
quantum-autonomously effect entangling gates on Rydberg atoms. One can perform
$Z$ or entangling gates on trapped ions mostly quantum-autonomously, by
sculpting a linear Paul trap or leveraging a ring trap. Passive lasers control
these gates, as well as the Rydberg-atom gates, quantum-autonomously. Finally,
circuit quantum electrodynamics can enable quantum-autonomous $Z$ and $XY$
gates on superconducting qubits. The gates can serve as building blocks for
(fully or partially) quantum-autonomous circuits, which may reduce
classical-control burdens.

</details>


### [37] [Entanglement Growth from Entangled States: A Unified Perspective on Entanglement Generation and Transport](https://arxiv.org/abs/2510.08344)
*Chun-Yue Zhang,Zi-Xiang Li,Shi-Xin Zhang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Studies of entanglement dynamics in quantum many-body systems have focused
largely on initial product states. Here, we investigate the far richer dynamics
from initial entangled states, uncovering universal patterns across diverse
systems ranging from many-body localization (MBL) to random quantum circuits.
Our central finding is that the growth of entanglement entropy can exhibit a
non-monotonic dependence on the initial entanglement in many non-ergodic
systems, peaking for moderately entangled initial states. To understand this
phenomenon, we introduce a conceptual framework that decomposes entanglement
growth into two mechanisms: ``build'' and ``move''. The ``build'' mechanism
creates new entanglement, while the ``move'' mechanism redistributes
pre-existing entanglement throughout the system. We model a pure ``move''
dynamics with a random SWAP circuit, showing it uniformly distributes
entanglement across all bipartitions. We find that MBL dynamics are
``move-dominated'', which naturally explains the observed non-monotonicity of
the entanglement growth. This ``build-move'' framework offers a unified
perspective for classifying diverse physical dynamics, deepening our
understanding of entanglement propagation and information processing in quantum
many-body systems.

</details>


### [38] [Quantum simulation of chemistry via quantum fast multipole method](https://arxiv.org/abs/2510.07380)
*Dominic W. Berry,Kianna Wan,Andrew D. Baczewski,Elliot C. Eklund,Arkin Tikku,Ryan Babbush*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Here we describe an approach for simulating quantum chemistry on quantum
computers with significantly lower asymptotic complexity than prior work. The
approach uses a real-space first-quantised representation of the molecular
Hamiltonian which we propagate using high-order product formulae. Essential for
this low complexity is the use of a technique similar to the fast multipole
method for computing the Coulomb operator with $\widetilde{\cal O}(\eta)$
complexity for a simulation with $\eta$ particles. We show how to modify this
algorithm so that it can be implemented on a quantum computer. We ultimately
demonstrate an approach with $t(\eta^{4/3}N^{1/3} + \eta^{1/3} N^{2/3} ) (\eta
Nt/\epsilon)^{o(1)}$ gate complexity, where $N$ is the number of grid points,
$\epsilon$ is target precision, and $t$ is the duration of time evolution. This
is roughly a speedup by ${\cal O}(\eta)$ over most prior algorithms. We provide
lower complexity than all prior work for $N<\eta^6$ (the regime of practical
interest), with only first-quantised interaction-picture simulations providing
better performance for $N>\eta^6$. As with the classical fast multipole method,
large numbers $\eta\gtrsim 10^3$ would be needed to realise this advantage.

</details>


### [39] [When Less is More: Approximating the Quantum Geometric Tensor with Block Structures](https://arxiv.org/abs/2510.08430)
*Ahmedeo Shokry,Alessandro Santini,Filippo Vicentini*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The natural gradient is central in neural quantum states optimizations but it
is limited by the cost of computing and inverting the quantum geometric tensor,
the quantum analogue of the Fisher information matrix. We introduce a
block-diagonal quantum geometric tensor that partitions the metric by network
layers, analogous to block-structured Fisher methods such as K-FAC. This
layer-wise approximation preserves essential curvature while removing noisy
cross-layer correlations, improving conditioning and scalability. Experiments
on Heisenberg and frustrated $J_1$-$J_2$ models show faster convergence, lower
energy, and improved stability.

</details>


### [40] [Average-case quantum complexity from glassiness](https://arxiv.org/abs/2510.08497)
*Alexander Zlokapa,Bobak T. Kiani,Eric R. Anschuetz*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Glassiness -- a phenomenon in physics characterized by a rough free-energy
landscape -- implies hardness for stable classical algorithms. For example, it
can obstruct constant-time Langevin dynamics and message-passing in random
$k$-SAT and max-cut instances. We provide an analogous framework for
average-case quantum complexity showing that a natural family of quantum
algorithms (e.g., Lindbladian evolution) fails for natural Hamiltonian
ensembles (e.g., random 3-local Hamiltonians). Specifically, we prove that the
standard notion of quantum glassiness based on replica symmetry breaking
obstructs stable quantum algorithms for Gibbs sampling, which we define by a
Lipschitz temperature dependence in quantum Wasserstein complexity. Our proof
relies on showing that such algorithms fail to capture a structural phase
transition in the Gibbs state, where glassiness causes the Gibbs state to
decompose into clusters extensively separated in quantum Wasserstein distance.
This yields average-case lower bounds for constant-time local Lindbladian
evolution and shallow variational circuits. Unlike mixing time lower bounds,
our results hold even when dynamics are initialized from the maximally mixed
state. We apply these lower bounds to non-commuting, non-stoquastic
Hamiltonians by showing a glass transition via the replica trick. We find that
the ensemble of all 3-local Pauli strings with independent Gaussian
coefficients is average-case hard, while providing analytical evidence that the
general $p$-local Pauli ensemble is non-glassy for sufficiently large constant
$p$, in contrast to its classical (Ising $p$-spin, always glassy) and fermionic
(SYK, never glassy) counterparts.

</details>


### [41] [Random unitaries from Hamiltonian dynamics](https://arxiv.org/abs/2510.08434)
*Laura Cui,Thomas Schuster,Liang Mao,Hsin-Yuan Huang,Fernando Brandao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The nature of randomness and complexity growth in systems governed by unitary
dynamics is a fundamental question in quantum many-body physics. This problem
has motivated the study of models such as local random circuits and their
convergence to Haar-random unitaries in the long-time limit. However, these
models do not correspond to any family of physical time-independent
Hamiltonians. In this work, we address this gap by studying the
indistinguishability of time-independent Hamiltonian dynamics from truly random
unitaries. On one hand, we establish a no-go result showing that for any
ensemble of constant-local Hamiltonians and any evolution times, the resulting
time-evolution unitary can be efficiently distinguished from Haar-random and
fails to form a $2$-design or a pseudorandom unitary (PRU). On the other hand,
we prove that this limitation can be overcome by increasing the locality
slightly: there exist ensembles of random polylog-local Hamiltonians in
one-dimension such that under constant evolution time, the resulting
time-evolution unitary is indistinguishable from Haar-random, i.e. it forms
both a unitary $k$-design and a PRU. Moreover, these Hamiltonians can be
efficiently simulated under standard cryptographic assumptions.

</details>


### [42] [Spectral properties and coding transitions of Haar-random quantum codes](https://arxiv.org/abs/2510.07396)
*Grace M. Sommers,J. Alexander Jacoby,Zack Weinstein,David A. Huse,Sarang Gopalakrishnan*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A quantum error-correcting code with a nonzero error threshold undergoes a
mixed-state phase transition when the error rate reaches that threshold. We
explore this phase transition for Haar-random quantum codes, in which the
logical information is encoded in a random subspace of the physical Hilbert
space. We focus on the spectrum of the encoded system density matrix as a
function of the rate of uncorrelated, single-qudit errors. For low error rates,
this spectrum consists of well-separated bands, representing errors of
different weights. As the error rate increases, the bands for high-weight
errors merge. The evolution of these bands with increasing error rate is well
described by a simple analytic ansatz. Using this ansatz, as well as an
explicit calculation, we show that the threshold for Haar-random quantum codes
saturates the hashing bound, and thus coincides with that for random
\emph{stabilizer} codes. For error rates that exceed the hashing bound, typical
errors are uncorrectable, but postselected error correction remains possible
until a much higher \emph{detection} threshold. Postselection can in principle
be implemented by projecting onto subspaces corresponding to low-weight errors,
which remain correctable past the hashing bound.

</details>


### [43] [Random unitaries that conserve energy](https://arxiv.org/abs/2510.08448)
*Liang Mao,Laura Cui,Thomas Schuster,Hsin-Yuan Huang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Random unitaries sampled from the Haar measure serve as fundamental models
for generic quantum many-body dynamics. Under standard cryptographic
assumptions, recent works have constructed polynomial-size quantum circuits
that are computationally indistinguishable from Haar-random unitaries,
establishing the concept of pseudorandom unitaries (PRUs). While PRUs have
found broad implications in many-body physics, they fail to capture the energy
conservation that governs physical systems. In this work, we investigate the
computational complexity of generating PRUs that conserve energy under a fixed
and known Hamiltonian $H$. We provide an efficient construction of
energy-conserving PRUs when $H$ is local and commuting with random
coefficients. Conversely, we prove that for certain translationally invariant
one-dimensional $H$, there exists an efficient quantum algorithm that can
distinguish truly random energy-conserving unitaries from any polynomial-size
quantum circuit. This establishes that energy-conserving PRUs cannot exist for
these Hamiltonians. Furthermore, we prove that determining whether
energy-conserving PRUs exist for a given family of one-dimensional local
Hamiltonians is an undecidable problem. Our results reveal an unexpected
computational barrier that fundamentally separates the generation of generic
random unitaries from those obeying the basic physical constraint of energy
conservation.

</details>


### [44] [Classical-quantum oscillators as diffusive processes in phase space](https://arxiv.org/abs/2510.07402)
*Emanuele Panella*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The dynamics of hybrid systems -- i.e. ones in which classical and quantum
degrees of freedom co-exist and interact -- feature both diffusion in the
classical sector and decoherence in the quantum state. In this article, we will
consider the simple setup of a classical damped oscillator interacting with its
quantum counterpart and show that, for any initial state, the dynamics flows to
a unique (non-equilibrium) steady state, which we compute explicitly. To do so,
we make use of a useful mapping between hybrid and classical diffusive
dynamics, which we characterise in detail in the master equation formalism.

</details>


### [45] [Quantum Probe Tomography](https://arxiv.org/abs/2510.08499)
*Sitan Chen,Jordan Cotler,Hsin-Yuan Huang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Characterizing quantum many-body systems is a fundamental problem across
physics, chemistry, and materials science. While significant progress has been
made, many existing Hamiltonian learning protocols demand digital quantum
control over the entire system, creating a disconnect from many real-world
settings that provide access only through small, local probes. Motivated by
this, we introduce and formalize the problem of quantum probe tomography, where
one seeks to learn the parameters of a many-body Hamiltonian using a single
local probe access to a small subsystem of a many-body thermal state undergoing
time evolution. We address the identifiability problem of determining which
Hamiltonians can be distinguished from probe data through a new combination of
tools from algebraic geometry and smoothed analysis. Using this approach, we
prove that generic Hamiltonians in various physically natural families are
identifiable up to simple, unavoidable structural symmetries. Building on these
insights, we design the first efficient end-to-end algorithm for probe
tomography that learns Hamiltonian parameters to accuracy $\varepsilon$, with
query complexity scaling polynomially in $1/\varepsilon$ and classical
post-processing time scaling polylogarithmically in $1/\varepsilon$. In
particular, we demonstrate that translation- and rotation-invariant
nearest-neighbor Hamiltonians on square lattices in one, two, and three
dimensions can be efficiently reconstructed from single-site probes of the
Gibbs state, up to inversion symmetry about the probed site. Our results
demonstrate that robust Hamiltonian learning remains achievable even under
severely constrained experimental access.

</details>


### [46] [Optimal Distillation of Qubit Clocks](https://arxiv.org/abs/2510.08493)
*Sujay Kazi,Iman Marvian*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study coherence distillation under time-translation-invariant operations:
given many copies of a quantum state containing coherence in the energy
eigenbasis, the aim is to produce a purer coherent state while respecting the
time-translation symmetry. This symmetry ensures that the output remains
synchronized with the input and that the process can be realized by
energy-conserving unitaries coupling the system to a reservoir initially in an
energy eigenstate, thereby modeling thermal operations supplemented by a work
reservoir or battery. For qubit systems, we determine the optimal asymptotic
fidelity and show that it is governed by the purity of coherence, a measure of
asymmetry derived from the right logarithmic derivative (RLD) Fisher
information. In particular, we find that the lowest achievable infidelity (one
minus fidelity) scales as $1/N$ times the reciprocal of the purity of coherence
of each input qubit, where $N$ is the number of copies, giving this quantity a
clear operational meaning. We additionally study many other interesting aspects
of the coherence distillation problem for qubits, including computing
higher-order corrections to the lowest achievable infidelity up to $O(1/N^3)$,
and expressing the optimal channel as a boundary value problem that can be
solved numerically.

</details>


### [47] [Hardness of recognizing phases of matter](https://arxiv.org/abs/2510.08503)
*Thomas Schuster,Dominik Kufel,Norman Y. Yao,Hsin-Yuan Huang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We prove that recognizing the phase of matter of an unknown quantum state is
quantum computationally hard. More specifically, we show that the quantum
computational time of any phase recognition algorithm must grow exponentially
in the range of correlations $\xi$ of the unknown state. This exponential
growth renders the problem practically infeasible for even moderate correlation
ranges, and leads to super-polynomial quantum computational time in the system
size $n$ whenever $\xi = \omega(\log n)$. Our results apply to a substantial
portion of all known phases of matter, including symmetry-breaking phases and
symmetry-protected topological phases for any discrete on-site symmetry group
in any spatial dimension. To establish this hardness, we extend the study of
pseudorandom unitaries (PRUs) to quantum systems with symmetries. We prove that
symmetric PRUs exist under standard cryptographic conjectures, and can be
constructed in extremely low circuit depths. We also establish hardness for
systems with translation invariance and purely classical phases of matter. A
key technical limitation is that the locality of the parent Hamiltonians of the
states we consider is linear in $\xi$; the complexity of phase recognition for
Hamiltonians with constant locality remains an important open question.

</details>


### [48] [Alpha-bit state merging](https://arxiv.org/abs/2510.07418)
*Jessica Yeh,Jinzhao Wang,Patrick Hayden*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: State merging is a fundamental protocol in quantum information theory that
generalizes quantum teleportation. Traditionally, it is achieved by local
operations on shared entanglement and classical communication. In this work, we
study state merging done with alpha-bits, a versatile quantum communication
resource weaker than qubits. We study alpha-bit state merging with and without
catalytic entanglement, and we find a potential gap between the rates of
alpha-bits consumed. In light of our result, we discuss how to interpret
entanglement wedge reconstruction in AdS/CFT in terms of alpha-bit state
merging

</details>


### [49] [Quantum Filtering and Analysis of Multiplicities in Eigenvalue Spectra](https://arxiv.org/abs/2510.07439)
*Zhiyan Ding,Lin Lin,Yilun Yang,Ruizhe Zhang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-grained spectral properties of quantum Hamiltonians, including both
eigenvalues and their multiplicities, provide useful information for
characterizing many-body quantum systems as well as for understanding phenomena
such as topological order. Extracting such information with small additive
error is $\#\textsf{BQP}$-complete in the worst case. In this work, we
introduce QFAMES (Quantum Filtering and Analysis of Multiplicities in
Eigenvalue Spectra), a quantum algorithm that efficiently identifies clusters
of closely spaced dominant eigenvalues and determines their multiplicities
under physically motivated assumptions, which allows us to bypass worst-case
complexity barriers. QFAMES also enables the estimation of observable
expectation values within targeted energy clusters, providing a powerful tool
for studying quantum phase transitions and other physical properties. We
validate the effectiveness of QFAMES through numerical demonstrations,
including its applications to characterizing quantum phases in the
transverse-field Ising model and estimating the ground-state degeneracy of a
topologically ordered phase in the two-dimensional toric code model. Our
approach offers rigorous theoretical guarantees and significant advantages over
existing subspace-based quantum spectral analysis methods, particularly in
terms of the sample complexity and the ability to resolve degeneracies.

</details>


### [50] [Angular Geometry of Atomic Multipole Transitions](https://arxiv.org/abs/2510.07451)
*Wesley C. Campbell*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A simple way to calculate Rabi frequencies is outlined for interactions of
atomic or nuclear multipole moments with laser fields that focuses on their
relative geometry. The resulting expression takes the form of a dot product
between the laser polarization and a vector spherical harmonic, thereby
naturally connecting to the multipole's far-field spontaneous-emission pattern
and providing a way to visualize the interaction. Since the vector spherical
harmonics are not yet a standard tool in quantum science, their relevant
properties are reviewed. This approach is illustrated in the calculation of a
variety of beam effects, yielding both perturbative corrections and some
nontrivial cases with non-vanishing coupling.

</details>


### [51] [Simulation of Quantum Repeater Networks under Decoherence and Purification Constraints](https://arxiv.org/abs/2510.07471)
*Wenhan Li,Shiyu Zhang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Long-distance quantum communication requires reliable entanglement
distribution, but direct generation with protocols such as Barrett--Kok suffers
from exponentially decreasing success probability with distance, making it
impractical over hundreds of kilometers. Quantum repeaters address this by
segmenting the channel and combining entanglement generation, swapping, and
purification. In this work, we present a simulation framework for chain-based
repeaters under continuous-time depolarizing noise. Our model implements
heralded entanglement generation, Bell-state swapping, and multi-round
purification, with configurable chain length, noise levels, and purification
depth. Numerical results highlight how memory decoherence constrains
performance, how purification mitigates fidelity loss, and how time and
entanglement costs scale with distance. While simplified, the framework offers
a flexible tool for exploring trade-offs in repeater design and provides a
basis for extensions toward more complex network scenarios.

</details>


### [52] [A quantum state transfer protocol with Ising Hamiltonians](https://arxiv.org/abs/2510.07481)
*Oscar Michel,Matthias Werner,Arnau Riera*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum state transfer is a fundamental requirement for scalable quantum
computation, where fast and reliable communication between distant spins is
essential. In this work, we present a protocol for quantum state transfer in
linear spin chains tailored to superconducting flux qubits. Starting from a
perfect state transfer scheme via a Heisenberg Hamiltonian with inhomogeneous
couplings, we adapt it for architectures implementing the transverse-field
Ising model by encoding the information in domain walls. The resulting linear
Ising chain makes quantum transport experiments accessible to many platforms
for analog quantum simulation. We test the protocol for 1-, 2-, and 3- spin
states, obtaining high transfer fidelities of up to 0.99 and study the accuracy
dependence on the domain wall approximation. These results are the first step
in paving the way for an experimental implementation of the protocol.

</details>


### [53] [3-Local Hamiltonian Problem and Constant Relative Error Quantum Partition Function Approximation: $O(2^{\frac{n}{2}})$ Algorithm Is Nearly Optimal under QSETH](https://arxiv.org/abs/2510.07495)
*Nai-Hui Chia,Yu-Ching Shen*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the computational complexity of the Local Hamiltonian (LH)
problem and the approximation of the Quantum Partition Function (QPF), two
central problems in quantum many-body physics and quantum complexity theory.
Both problems are known to be QMA-hard, and under the widely believed
assumption that $\mathsf{BQP} \neq \mathsf{QMA}$, no efficient quantum
algorithm exits. The best known quantum algorithm for LH runs in
$O\bigl(2^{\frac{n}{2}(1 - o(1))}\bigr)$ time, while for QPF, the
state-of-the-art algorithm achieves relative error $\delta$ in
$O^\ast\bigl(\frac{1}{\delta}\sqrt{\frac{2^n}{Z}}\bigr)$ time, where $Z$
denotes the value of the partition function. A nature open question is whether
more efficient algorithms exist for both problems.
  In this work, we establish tight conditional lower bounds showing that these
algorithms are nearly optimal. Under the plausible Quantum Strong Exponential
Time Hypothesis (QSETH), we prove that no quantum algorithm can solve either LH
or approximate QPF significantly faster than $O(2^{n/2})$, even for 3-local
Hamiltonians. In particular, we show: 1) 3-local LH cannot be solved in time
$O(2^{\frac{n}{2}(1-\varepsilon)})$ for any $\varepsilon > 0$ under QSETH; 2)
3-local QPF cannot be approximated up to any constant relative error in
$O(2^{\frac{n}{2}(1-\varepsilon)})$ time for any $\varepsilon > 0$ under QSETH;
and 3) we present a quantum algorithm that approximates QPF up to relative
error $1/2 + 1/\mathrm{poly}(n)$ in $O^\ast(2^{n/2})$ time, matching our
conditional lower bound.
  Notably, our results provide the first fine-grained lower bounds for both LH
and QPF with fixed locality. This stands in sharp contrast to QSETH and the
trivial fine-grained lower bounds for LH, where the locality of the SAT
instance and the Hamiltonian depends on the parameter $\varepsilon$ in the
$O(2^{\frac{n}{2}(1-\varepsilon)})$ running time.

</details>


### [54] [Exponential Speed-ups for Structured Goemans-Williamson relaxations via Quantum Gibbs States and Pauli Sparsity](https://arxiv.org/abs/2510.08292)
*Haomu Yuan,Daniel Stilck França,Ilia Luchnikov,Egor Tiunov,Tobias Haug,Leandro Aolita*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) problems are prevalent in
various applications and are known to be NP-hard. The seminal work of Goemans
and Williamson introduced a semidefinite programming (SDP) relaxation for such
problems, solvable in polynomial time that upper bounds the optimal value.
Their approach also enables randomized rounding techniques to obtain feasible
solutions with provable performance guarantees.
  In this work, we identify instances of QUBO problems where matrix
multiplicative weight methods lead to quantum and quantum-inspired algorithms
that approximate the Goemans-Williamson SDP exponentially faster than existing
methods, achieving polylogarithmic time complexity relative to the problem
dimension. This speedup is attainable under the assumption that the QUBO cost
matrix is sparse when expressed as a linear combination of Pauli strings
satisfying certain algebraic constraints, and leverages efficient quantum and
classical simulation results for quantum Gibbs states.
  We demonstrate how to verify these conditions efficiently given the
decomposition. Additionally, we explore heuristic methods for randomized
rounding procedures and extract the energy of a feasible point of the QUBO in
polylogarithmic time. While the practical relevance of instances where our
methods excel remains to be fully established, we propose heuristic algorithms
with broader applicability and identify Kronecker graphs as a promising class
for applying our techniques. We conduct numerical experiments to benchmark our
methods. Notably, by utilizing tensor network methods, we solve an SDP with $D
= 2^{50}$ variables and extract a feasible point which is certifiably within
$0.15\%$ of the optimum of the QUBO through our approach on a desktop, reaching
dimensions millions of times larger than those handled by existing SDP or QUBO
solvers, whether heuristic or rigorous.

</details>


### [55] [Efficient Radiofrequency Sensing with Fluorescence Encoding](https://arxiv.org/abs/2510.07510)
*Nicole Voce,Paul Stevenson*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Optically-active spin qubits have emerged as powerful quantum sensors capable
of nanoscale magnetometry, yet conventional coherent sensing approaches are
ultimately limited by the coherence time of the sensor, typically precluding
detection in the sub-MHz regime. We present a broadly applicable
fluorescence-encoding method that circumvents coherence-time constraints by
transducing time-varying magnetic fields directly into modulated fluorescence
signals. Using nitrogen-vacancy centers in diamond as a model system, we
demonstrate shot-noise-limited sensitivity for AC magnetic fields spanning
near-DC to MHz frequencies, with detection bandwidth tunable via optical
excitation power. The technique captures complete spectral information in a
single measurement, eliminating the need for point-by-point frequency scanning,
and allows phase-sensitive multi-frequency detection with Hz-level resolution.
This approach transforms quantum sensors into atomic-scale spectrum analyzers,
with immediate applications for low-frequency RF communication, zero-field NMR,
and bioelectronic sensing. Our approach is broadly applicable to the expanding
class of optically-active spin qubits, including molecular systems and
fluorescent proteins, opening new sensing regimes previously inaccessible to
coherent techniques

</details>


### [56] [Quantum memory optimisation using finite-horizon, decoherence time and discounted mean-square performance criteria](https://arxiv.org/abs/2510.08299)
*Igor G. Vladimirov,Ian R. Petersen,Guodong Shi*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper is concerned with open quantum memory systems for approximately
retaining quantum information, such as initial dynamic variables or quantum
states to be stored over a bounded time interval. In the Heisenberg picture of
quantum dynamics, the deviation of the system variables from their initial
values lends itself to closed-form computation in terms of tractable moment
dynamics for open quantum harmonic oscillators and finite-level quantum systems
governed by linear or quasi-linear Hudson-Parthasarathy quantum stochastic
differential equations, respectively. This tractability is used in a recently
proposed optimality criterion for varying the system parameters so as to
maximise the memory decoherence time when the mean-square deviation achieves a
given critical threshold. The memory decoherence time maximisation approach is
extended beyond the previously considered low-threshold asymptotic
approximation and to Schr\"{o}dinger type mean-square deviation functionals for
the reduced system state governed by the Lindblad master equation. We link this
approach with the minimisation of the mean-square deviation functionals at a
finite time horizon and with their discounted version which quantifies the
averaged performance of the quantum system as a temporary memory under a
Poisson flow of storage requests.

</details>


### [57] [Error correction phase transition in noisy random quantum circuits](https://arxiv.org/abs/2510.07512)
*Jon Nelson,Joel Rajakumar,Michael J. Gullans*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we study the task of encoding logical information via a noisy
quantum circuit. It is known that at superlogarithmic depth, the output of any
noisy circuit without reset gates or intermediate measurements becomes
indistinguishable from the maximally mixed state, implying that all input
information is destroyed. This raises the question of whether there is a
low-depth regime where information is preserved as it is encoded into an
error-correcting codespace by the circuit. When considering noisy random
encoding circuits, our numerical simulations show that there is a sharp phase
transition at a critical depth of order $p^{-1}$, where $p$ is the noise rate,
such that below this depth threshold quantum information is preserved, whereas
after this threshold it is lost. Furthermore, we rigorously prove that this is
the best achievable trade-off between depth and noise rate for any noisy
circuit encoding a constant rate of information. Thus, random circuits are
optimal noisy encoders in this sense.

</details>


### [58] [Quenching, Fast and Slow: Breaking Kibble-Zurek Universal Scaling by Jumping along Geodesics](https://arxiv.org/abs/2510.08528)
*Thi Ha Kyaw,Guillermo Romero,Gaurav Saxena*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A major drawback of adiabatic quantum computing (AQC) is fulfilling the
energy gap constraint, which requires the total evolution time to scale
inversely with the square of the minimum energy gap. Failure to satisfy this
condition violates the adiabatic approximation, potentially undermining
computational accuracy. Recently, several approaches have been proposed to
circumvent this constraint. One promising approach is to use the family of
adiabatic shortcut procedures to fast-forward AQC. One caveat, however, is that
it requires an additional Hamiltonian that is very challenging to implement
experimentally. Here, we investigate an alternate pathway that avoids any extra
Hamiltonian in the evolution to fast-forward the adiabatic dynamics by
traversing geodesics of a quantum system. We find that jumping along geodesics
offers a striking mechanism to highly suppress the density of excitations in
many-body systems. Particularly, for the spin-$1/2$ XY model, we analytically
prove and numerically demonstrate a rate-independent defect plateau, which
contrasts with well-established results for the Kibble-Zurek and
anti-Kibble-Zurek mechanisms.

</details>


### [59] [No exponential quantum speedup for $\mathrm{SIS}^\infty$ anymore](https://arxiv.org/abs/2510.07515)
*Robin Kothari,Ryan O'Donnell,Kewen Wu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In 2021, Chen, Liu, and Zhandry presented an efficient quantum algorithm for
the average-case $\ell_\infty$-Short Integer Solution ($\mathrm{SIS}^\infty$)
problem, in a parameter range outside the normal range of cryptographic
interest, but still with no known efficient classical algorithm. This was
particularly exciting since $\mathrm{SIS}^\infty$ is a simple problem without
structure, and their algorithmic techniques were different from those used in
prior exponential quantum speedups.
  We present efficient classical algorithms for all of the
$\mathrm{SIS}^\infty$ and (more general) Constrained Integer Solution problems
studied in their paper, showing there is no exponential quantum speedup
anymore.

</details>


### [60] [A Structural Theory of Quantum Metastability: Markov Properties and Area Laws](https://arxiv.org/abs/2510.08538)
*Thiago Bergamaschi,Chi-Fang Chen,Umesh Vazirani*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Statistical mechanics assumes that a quantum many-body system at low
temperature can be effectively described by its Gibbs state. However, many
complex quantum systems exist only as metastable states of dissipative open
system dynamics, which appear stable and robust yet deviate substantially from
true thermal equilibrium. In this work, we model metastable states as
approximate stationary states of a quasi-local, (KMS)-detailed-balanced master
equation representing Markovian system-bath interaction, and unveil a universal
structural theory: all metastable states satisfy an area law of mutual
information and a Markov property. The more metastable the states are, the
larger the regions to which these structural results apply. Therefore, the
hallmark correlation structure and noise resilience of Gibbs states are not
exclusive to true equilibrium but emerge dynamically. Behind our structural
results lies a systematic framework encompassing sharp equivalences between
local minima of free energy, a non-commutative Fisher information, and
approximate detailed balance conditions. Our results build towards a
comprehensive theory of thermal metastability and, in turn, formulate a
well-defined, feasible, and repeatable target for quantum thermal simulation.

</details>


### [61] [On Quantum Computation Using Bias-Preserving Gates](https://arxiv.org/abs/2510.07532)
*Debadrito Roy,Aryaman Manish Kolhe,V. Lalitha,Navin Kashyap*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Certain types of quantum computing platforms, such as those realized using
Rydberg atoms or Kerr-cat qubits, are natively more susceptible to Pauli-Z
noise than Pauli-X noise, or vice versa. On such hardware, it is useful to
ensure that computations use only gates that maintain the Z-bias (or X-bias) in
the noise. This is so that quantum error-correcting codes tailored for
biased-noise models can be used to provide fault-tolerance on these platforms.
In this paper, we follow up on the recent work of Fellous-Asiani et al. (npj
Quantum Inf., 2025) in studying the structure and properties of bias-preserving
gates. Our main contributions are threefold: (1) We give a novel
characterization of Z-bias-preserving gates based on their decomposition as a
linear combination of Pauli operators. (2) We show that any Z-bias-preserving
gate can be approximated arbitrarily well using only gates from the set
{X,R_z(\theta),CNOT,CCNOT}, where \theta is any irrational multiple of 2\pi.
(3) We prove, by drawing a connection with coherence resource theory, that any
Z-bias-preserving logical operator acting on the logical qubits of a
Calderbank-Shor-Steane (CSS) code can be realized by applying Z-bias-preserving
gates on the physical qubits. Along the way, we also demonstrate that
Z-bias-preserving gates are far from being universal for quantum computation.

</details>


### [62] [Polyhedral Classical Simulators for Quantum Computation](https://arxiv.org/abs/2510.07540)
*Cihan Okay*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum advantage in computation refers to the existence of computational
tasks that can be performed efficiently on a quantum computer but cannot be
efficiently simulated on any classical computer. Identifying the precise
boundary of efficient classical simulability is a central challenge and
motivates the development of new simulation paradigms. In this paper, we
introduce polyhedral classical simulators, a framework for classical simulation
grounded in polyhedral geometry. This framework encompasses well-known methods
such as the Gottesman-Knill algorithm, while also extending naturally to more
recent models of quantum computation, including those based on magic states and
measurement-based quantum computation. We show how this framework unifies and
extends existing simulation methods while at the same time providing a
geometric roadmap for pushing the boundary of efficient classical simulation
further.

</details>


### [63] [Constructive counterexamples to the additivity of minimum output Rényi entropy of quantum channels for all $p>1$](https://arxiv.org/abs/2510.07547)
*Harm Derksen,Benjamin Lovitz*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present explicit quantum channels with strictly sub-additive minimum
output R\'enyi entropy for all $p>1$, improving upon prior constructions which
handled $p>2$. Our example is provided by explicit constructions of linear
subspaces with high geometric measure of entanglement. This construction
applies in both the bipartite and multipartite settings. As further
applications, we use our construction to find entanglement witnesses with many
highly negative eigenvalues, and to construct entangled mixed states that
remain entangled after perturbation.

</details>


### [64] [Correlation Lengths for Stochastic Matrix Product States](https://arxiv.org/abs/2510.07561)
*Lubashan Pathirana,Albert H. Werner*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a general model of stochastically generated matrix product
states (MPS) in which the local tensors share a common distribution and form a
strictly stationary sequence, without requiring spatial independence. Under
natural conditions on the associated transfer operators, we prove the existence
of thermodynamic limits of expectations of local observables and establish
almost-sure exponential decay of two-point correlations. In the homogeneous
(random translation-invariant) case, for any error tolerance in probability,
the two-point function decays exponentially in the distance between the two
sites, with a deterministic rate. In the i.i.d. case, the exponential decay
still holds with a deterministic rate, with the probability approaching one
exponentially fast in the distance. For strictly stationary ensembles with
decaying spatial dependence, the correlation decay quantitatively reflects the
mixing profile: ($\rho$)-mixing yields polynomial bounds with high probability,
while stretched-exponential (resp. exponential) decay in ($\rho$) (resp.
($\beta$)) yields stretched-exponential (resp. exponential) decay of the
two-point function, again with correspondingly strong high-probability
guarantees. Altogether, the framework unifies and extends recent progress on
stationary ergodic and Gaussian translation-invariant ensembles, providing a
transfer-operator route to typical correlation decay in random MPS.

</details>


### [65] [Entanglement in von Neumann Algebraic Quantum Information Theory](https://arxiv.org/abs/2510.07563)
*Lauritz van Luijk*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In quantum systems with infinitely many degrees of freedom, states can be
infinitely entangled across a pair of subsystems, but are there different forms
of infinite entanglement? To understand entanglement in such systems, we use a
framework in which subsystems are described by von Neumann algebras on the full
system's Hilbert space. Although this approach has been known for over 50
years, an operational justification has been missing so far. We resolve this by
deriving the von Neumann algebraic description of subsystems from operational
axioms. This raises the question of how physical properties of the subsystem
relate to algebraic properties. Our main result shows a surprisingly strong
connection: The type classification of von Neumann algebras (types I, II, III,
and their respective subtypes) is in one-to-one correspondence with a family of
operational entanglement properties. For instance, Connes' classification of
type III factors can be formulated in terms of the smallest achievable error
when "embezzling" entanglement from the system. Our findings promote the type
classification from algebraic bookkeeping to a classification of infinite
quantum systems based on the kind of infinite entanglement that they support.

</details>


### [66] [Nonlinear Coupling between Motional Modes in Trapped Ion Quantum Processors](https://arxiv.org/abs/2510.07590)
*Wes Johnson,Brandon Ruzic*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Trapped-ion crystals are a leading platform for quantum information science,
but achieving the high-fidelity entangling gates required for fault-tolerant
quantum computing becomes harder as system size increases. As systems scale,
spectral crowding makes low-order nonlinear resonances between collective
motional modes increasingly common and can limit gate performance, especially
in monolithic or global-mode architectures. We develop a general model to
identify and simulate nonlinear motional-mode coupling (NoMoCou) arising from
third-order Coulomb terms and quantify its impact on the Molmer-Sorensen gate
across linear chains and 2D crystals in rf and Penning traps. We delineate the
regimes where NoMoCou dominates the error budget and provide design rules:
detune operating points from low-order resonances, tune trap anisotropy to
reshape spectra, and shape gate waveforms.

</details>


### [67] [Beyond Hoeffding and Chernoff: Trading conclusiveness for advantages in quantum hypothesis testing](https://arxiv.org/abs/2510.07601)
*Kaiyuan Ji,Bartosz Regula*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The ultimate limits of quantum state discrimination are often thought to be
captured by asymptotic bounds that restrict the achievable error probabilities,
notably the quantum Chernoff and Hoeffding bounds. Here we study hypothesis
testing protocols that are permitted a probability of producing an inconclusive
discrimination outcome, and investigate their performance when this probability
is suitably constrained. We show that even by allowing an arbitrarily small
probability of inconclusiveness, the limits imposed by the quantum Hoeffding
and Chernoff bounds can be significantly exceeded, completely circumventing the
conventional trade-offs between error exponents in hypothesis testing.
Furthermore, such improvements over standard state discrimination are robust
and can be obtained even when an exponentially vanishing probability of
inconclusive outcomes is demanded. Relaxing the constraints on the inconclusive
probability can enable even larger advantages, but this comes at a price. We
show a 'strong converse' property of this setting: targeting error exponents
beyond those achievable with vanishing inconclusiveness necessarily forces the
probability of inconclusive outcomes to converge to one. By exactly quantifying
the rate of this convergence, we give a complete characterisation of the
trade-offs between error exponents and rates of conclusive outcome
probabilities. Overall, our results provide a comprehensive asymptotic picture
of how allowing inconclusive measurement outcomes reshapes optimal quantum
hypothesis testing.

</details>


### [68] [Noisy-Syndrome Decoding of Hypergraph Product Codes](https://arxiv.org/abs/2510.07602)
*Venkata Gandikota,Elena Grigorescu,Vatsal Jha,S. Venkitesh*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hypergraph product codes are a prototypical family of quantum codes with
state-of-the-art decodability properties. Recently, Golowich and Guruswami
(FOCS 2024) showed a reduction from quantum decoding to syndrome decoding for a
general class of codes, which includes hypergraph product codes. In this work
we consider the "noisy" syndrome decoding problem for hypergraph product codes,
and show a similar reduction in the noisy setting, addressing a question posed
by Golowich and Guruswami. Our results hold for a general family of codes
wherein the code and the dual code are "simultaneously nice"; in particular,
for codes admitting good syndrome decodability and whose duals look "similar".
These include expander codes, Reed-Solomon codes, and variants.

</details>


### [69] [Reconfigurable dissipative entanglement between many spin ensembles: from robust quantum sensing to many-body state engineering](https://arxiv.org/abs/2510.07616)
*Anjun Chu,Mikhail Mamaev,Martin Koppenhöfer,Ming Yuan,Aashish A. Clerk*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: An attractive approach for stabilizing entangled many-body spin states is to
employ engineered dissipation. Most existing proposals either target relatively
simple collective spin states, or require numerous independent and complex
dissipative processes. Here, we show a surprisingly versatile scheme for
many-body reservoir engineering that relies solely on fully collective
single-excitation decay, augmented with local Hamiltonian terms. Crucially, all
these ingredients are readily available in cavity QED setups. Our method is
based on splitting the spin system into groups of sub-ensembles, and provides
an easily tunable setup for stabilizing a broad family of pure, highly
entangled states with closed-form analytic descriptions. Our results have
immediate application to multi-ensemble quantum metrology, enabling
Heisenberg-limited sensing of field gradients and curvatures. Notably, the
generated states have robustness against common-mode phase noise, and only
require simple Ramsey-style measurements. The same setup also allows the
stabilization of entangled states in a 1D chain of spin ensembles with
symmetry-protected topological (SPT) order, and have a direct connection to the
outputs of sequential unitary circuits. In particular, we present an efficient
method for engineering the celebrated spin-1 Affleck-Kennedy-Lieb-Tasaki (AKLT)
state.

</details>


### [70] [Conjugate queries can help](https://arxiv.org/abs/2510.07622)
*Ewin Tang,John Wright,Mark Zhandry*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We give a natural problem over input quantum oracles $U$ which cannot be
solved with exponentially many black-box queries to $U$ and $U^\dagger$, but
which can be solved with constant many queries to $U$ and $U^*$, or $U$ and
$U^{\mathrm{T}}$. We also demonstrate a quantum commitment scheme that is
secure against adversaries that query only $U$ and $U^\dagger$, but is insecure
if the adversary can query $U^*$. These results show that conjugate and
transpose queries do give more power to quantum algorithms, lending credence to
the idea put forth by Zhandry that cryptographic primitives should prove
security against these forms of queries.
  Our key lemma is that any circuit using $q$ forward and inverse queries to a
state preparation unitary for a state $\sigma$ can be simulated to
$\varepsilon$ error with $n = \mathcal{O}(q^2/\varepsilon)$ copies of $\sigma$.
Consequently, for decision tasks, algorithms using (forward and inverse) state
preparation queries only ever perform quadratically better than sample access.
These results follow from straightforward combinations of existing techniques;
our contribution is to state their consequences in their strongest, most
counter-intuitive form. In doing so, we identify a motif where generically
strengthening a quantum resource can be possible if the output is allowed to be
random, bypassing no-go theorems for deterministic algorithms. We call this the
acorn trick.

</details>


### [71] [Asymptotic Gate Count Bounds for Ancilla-Free Single-Qubit Synthesis with Arithmetic Gates](https://arxiv.org/abs/2510.07627)
*Kaoru Sano,Hayata Morisaki,Seiseki Akibue*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study ancilla-free approximation of single-qubit unitaries $U\in {\rm
SU}(2)$ by gate sequences over Clifford+$G$, where $G\in\{T,V\}$ or their
generalization. Let $p$ denote the characteristic factor of the gate set (e.g.,
$p=2$ for $G=T$ and $p=5$ for $G=V$). We prove three asymptotic bounds on the
minimum $G$-count required to achieve approximation error at most
$\varepsilon$. First, for Haar-almost every $U$, we show that
$3\log_{p}(1/\varepsilon)$ $G$-count is both necessary and sufficient;
moreover, probabilistic synthesis improves the leading constant to $3/2$.
Second, for unitaries whose ratio of matrix elements lies in a specified number
field, $4\log_p(1/\varepsilon)$ $G$-count is necessary. Again, the leading
constant can be improved to $2$ by probabilistic synthesis. Third, there exist
unitaries for which the $G$-count per $\log_{p}(1/\varepsilon)$ fails to
converge as $\varepsilon\to 0^+$. These results partially resolve a generalized
form of the Ross--Selinger conjecture.

</details>


### [72] [Generating Entangled Steady States in Multistable Open Quantum Systems via Initial State Control](https://arxiv.org/abs/2510.07628)
*Diego Fallas Padilla,Raphael Kaubruegger,Adrianna Gillman,Stephen Becker,Ana Maria Rey*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Entanglement underpins the power of quantum technologies, yet it is fragile
and typically destroyed by dissipation. Paradoxically, the same dissipation,
when carefully engineered, can drive a system toward robust entangled steady
states. However, this engineering task is nontrivial, as dissipative many-body
systems are complex, particularly when they support multiple steady states.
Here, we derive analytic expressions that predict how the steady state of a
system evolving under a Lindblad equation depends on the initial state, without
requiring integration of the dynamics. These results extend the frameworks
developed in Refs. [Phys. Rev. A 89, 022118 (2014) and Phys. Rev. X 6, 041031
(2016)], showing that while the steady-state manifold is determined by the
Liouvillian kernel, the weights within it depend on both the Liouvillian and
the initial state. We identify a special class of Liouvillians for which the
steady state depends only on the initial overlap with the kernel. Our framework
provides analytical insight and a computationally efficient tool for predicting
steady states in open quantum systems. As an application, we propose schemes to
generate metrologically useful entangled steady states in spin ensembles via
balanced collective decay.

</details>


### [73] [Towards deterministic non-Gaussianity on a chip](https://arxiv.org/abs/2510.07658)
*Samuel E. Fontaine,J. E. Sipe,Marco Liscidini,Milica Banic*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose cascaded spontaneous four-wave mixing (SFWM) in microring
resonators as a scalable and efficient approach for directly generating
non-Gaussian states of light. Focusing on the well-understood "low-gain"
regime, we demonstrate that triplet generation through cascaded SFWM can be
achieved with high efficiency and favorable spectral characteristics using
realistic microring sources in AlGaAs. The ability to achieve the generation of
light in a single set of supermodes -- and the accessibility of the "high-gain"
regime at realistic pump powers -- makes this source a promising candidate as a
direct and deterministic source of non-Gaussian light for photonic quantum
information processing.

</details>


### [74] [Optimal lower bounds for quantum state tomography](https://arxiv.org/abs/2510.07699)
*Thilo Scharnhorst,Jack Spilecki,John Wright*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We show that $n = \Omega(rd/\varepsilon^2)$ copies are necessary to learn a
rank $r$ mixed state $\rho \in \mathbb{C}^{d \times d}$ up to error
$\varepsilon$ in trace distance. This matches the upper bound of $n =
O(rd/\varepsilon^2)$ from prior work, and therefore settles the sample
complexity of mixed state tomography. We prove this lower bound by studying a
special case of full state tomography that we refer to as projector tomography,
in which $\rho$ is promised to be of the form $\rho = P/r$, where $P \in
\mathbb{C}^{d \times d}$ is a rank $r$ projector. A key technical ingredient in
our proof, which may be of independent interest, is a reduction which converts
any algorithm for projector tomography which learns to error $\varepsilon$ in
trace distance to an algorithm which learns to error $O(\varepsilon)$ in the
more stringent Bures distance.

</details>


### [75] [Speed of evolution in qutrit systems](https://arxiv.org/abs/2510.07724)
*Jesica Espino-González,Francisco J. Sevilla,Andrea Valdés-Hernández*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The speed of evolution between perfectly distinguishable states is thoroughly
analyzed in a closed three-level (qutrit) quantum system. Considering an
evolution under an arbitrary time-independent Hamiltonian, we fully
characterize the relevant parameters according to whether the corresponding
quantum speed limit is given by the Mandelstam-Tamm, the Margolus-Levitin, or
the Ness-Alberti-Sagi (dual) bound, thereby elucidating their hierarchy and
relative importance. We revisit the necessary and sufficient conditions that
guarantee the evolution of the initial state towards an orthogonal one in a
finite time, and pay special attention to the full characterization of the
speed of evolution, offering a speed map in parameter space that highlights
regions associated with faster or slower dynamics. The general analysis is
applied to concrete physical settings, particularly a pair of bosons governed
by an extended Bose-Hubbard Hamiltonian, and a single particle in a triple-well
potential. Our findings provide a framework to explore how the energetic
resources and the initial configurations shape the pace of the dynamics in
higher-dimensional systems.

</details>


### [76] [Quantum Shannon Information Theory -Design of communication, cipher and sensor-](https://arxiv.org/abs/2510.07726)
*Osamu Hirota*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: One of the key aspects of Shannon's theory is that it provides guidance for
designing the most efficient systems, such as minimizing errors and clarifying
the limits of coding. Such theories have made great developments in the 50
years since 1948. It has played a vital role in enabling the development of
modern ultra-fast, stable, and highly dependable information and communication
systems. The Shannon theory is supported by the statistical communication
theory such as detection and estimation theory. The theory of communication
systems that transmit Shannon information using quantum media is called quantum
Shannon information theory, and research began in the 1960s. The theoretical
formulation comparable to conventional Shannon theory has been completed. Its
important role is to suggest that application of quantum effect will surpass
existing communication performance. It would be meaningless if performance,
efficiency, and utility were to deteriorate due to quantum effects, even if
certain new function is given. This paper suggests that there are various
limitations to utilizing quantum Shannon information theory to benefit
real-world communication systems and presents a theoretical framework for
achieving the ultimate goal. Finally, we introduce the perfect secure cipher
that overcome the Shannon impossibility theorem without degrading communication
performance and sensor et al as the examples.

</details>


### [77] [Speed limits of two-qutrit gates](https://arxiv.org/abs/2510.07742)
*Bora Basyildiz,Zhexuan Gong,Sahel Ashhab*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The speed of elementary quantum gates sets a limit on the speed at which
quantum circuits can be applied and, as a result, the size of the computations
that can be performed on a quantum computer. This limitation stems from the
fact that present-day quantum hardware systems have finite coherence times that
limit the total computation time. The speeds of qubit gates in various hardware
settings have been well studied over the past few decades. The recent interest
in multi-level quantum systems naturally creates a need for similar
investigations of the speeds of multi-level or qudit gates. In this work, we
perform an empirical study of the speed limit for the three-level or qutrit CZ
gate. Our analysis focuses on a theoretical model for capacitively coupled
superconducting transmons but can be extended to other systems. We generate CZ
gate protocols using optimal control theory techniques and observe when the
fidelity crosses certain thresholds. In addition to the empirical approach, we
derive an analytical speed limit for the qutrit CZ gate using traditional
quantum speed limit techniques. We compare the speed limits derived using these
two different approaches and discuss the gap that remains between them. We also
compare the time needed to implement the qutrit CZ gate with its qubit
counterpart.

</details>


### [78] [Beyond AME: A Novel Connection between Quantum Secret Sharing Schemes and $k$-Uniform States](https://arxiv.org/abs/2510.07753)
*Xuhong Liu,Shuai Shao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the connection between quantum secret sharing (QSS) schemes and
$k$-uniform states of qubits beyond the equivalence between threshold QSS
schemes and AME states. Specifically, we focus on homogeneous access structures
and show that $3$-uniformity is a necessary but not sufficient condition for
constructing a $3$-homogeneous QSS scheme using states of qubits. This gives a
novel connection between non-threshold QSS schemes and $k$-uniform states. As
an application of our result, we classify QSS schemes for up to 7 players and
provide explicit characterizations of their existence. Our results offer new
insights into the role of $k$-uniform states in the design of QSS schemes (not
necessarily threshold) and provide a foundation for future classifications of
QSS schemes with more complex structures.

</details>


### [79] [The debiased Keyl's algorithm: a new unbiased estimator for full state tomography](https://arxiv.org/abs/2510.07788)
*Angelos Pelecanos,Jack Spilecki,John Wright*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the problem of quantum state tomography, one is given $n$ copies of an
unknown rank-$r$ mixed state $\rho \in \mathbb{C}^{d \times d}$ and asked to
produce an estimator of $\rho$. In this work, we present the debiased Keyl's
algorithm, the first estimator for full state tomography which is both unbiased
and sample-optimal. We derive an explicit formula for the second moment of our
estimator, with which we show the following applications.
  (1) We give a new proof that $n = O(rd/\varepsilon^2)$ copies are sufficient
to learn a rank-$r$ mixed state to trace distance error $\varepsilon$, which is
optimal.
  (2) We further show that $n = O(rd/\varepsilon^2)$ copies are sufficient to
learn to error $\varepsilon$ in the more challenging Bures distance, which is
also optimal.
  (3) We consider full state tomography when one is only allowed to measure $k$
copies at once. We show that $n =O\left(\max
\left(\frac{d^3}{\sqrt{k}\varepsilon^2}, \frac{d^2}{\varepsilon^2} \right)
\right)$ copies suffice to learn in trace distance. This improves on the prior
work of Chen et al. and matches their lower bound.
  (4) For shadow tomography, we show that $O(\log(m)/\varepsilon^2)$ copies are
sufficient to learn $m$ given observables $O_1, \dots, O_m$ in the "high
accuracy regime", when $\varepsilon = O(1/d)$, improving on a result of Chen et
al. More generally, we show that if $\mathrm{tr}(O_i^2) \leq F$ for all $i$,
then $n = O\Big(\log(m) \cdot \Big(\min\Big\{\frac{\sqrt{r F}}{\varepsilon},
\frac{F^{2/3}}{\varepsilon^{4/3}}\Big\} + \frac{1}{\varepsilon^2}\Big)\Big)$
copies suffice, improving on existing work.
  (5) For quantum metrology, we give a locally unbiased algorithm whose mean
squared error matrix is upper bounded by twice the inverse of the quantum
Fisher information matrix in the asymptotic limit of large $n$, which is
optimal.

</details>


### [80] [Unified Framework for Direct Characterization of Kraus Operators, Observables, Density Matrices, and Weak Values Without Weak Interaction](https://arxiv.org/abs/2510.07789)
*Sahil,Sohail*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generalized quantum measurements, described by positive operator-valued
measures (POVMs), are essential for modeling realistic processes in open
quantum systems. While quantum process tomography can fully characterize a
POVM, it is resource-intensive and impractical when only specific POVM elements
or matrix elements of a particular POVM element are of interest. Direct quantum
measurement tomography offers a more efficient alternative but typically relies
on weak interactions and complex structures of the system, environment, and
probe as the dimension of the system increases, limiting its precision and
scalability. Furthermore, characterizing a POVM element alone is insufficient
to determine the underlying physical mechanism, as multiple Kraus operators can
yield the same measurement statistics. In this work, we present a unified
framework for the direct characterization of individual matrix elements of
Kraus operators associated with specific POVM elements and arbitrary input
states without requiring weak interaction, complex structures of the
system-environment-probe or full process and state tomography. This framework
naturally extends to projective measurements, enabling direct observable
tomography, and to the characterization of unitary operations. Our method also
captures modular and weak values of observables and Kraus operators, without
invoking weak interaction approximations. We demonstrate potential
implementations in optical systems, highlighting the experimental feasibility
of our approach.

</details>


### [81] [Efficient Closest Matrix Product State Learning in Logarithmic Depth](https://arxiv.org/abs/2510.07798)
*Chia-Ying Lin,Nai-Hui Chia,Shih-Han Hung*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning the closest matrix product state (MPS) representation of a quantum
state is known to enable useful tools for prediction and analysis of complex
quantum systems.
  In this work, we study the problem of learning MPS in following setting:
given many copies of an input MPS, the task is to recover a classical
description of the state. The best known polynomial-time algorithm, introduced
by [LCLP10, CPF+10], requires linear circuit depth and $O(n^5)$ samples, and
has seen no improvement in over a decade. The strongest known lower bound is
only $\Omega(n)$. The combination of linear depth and high sample complexity
renders existing algorithms impractical for near-term or even early
fault-tolerant quantum devices.
  We show a new efficient MPS learning algorithm that runs in $O(\log n)$ depth
and has sample complexity $O(n^3)$. Also, we can generalize our algorithm to
learn closest MPS state, in which the input state is not guaranteed to be close
to the MPS with a fixed bond dimension. Our algorithms also improve both sample
complexity and circuit depth of previous known algorithm.

</details>


### [82] [Learning to steer quantum many-body dynamics with tree optimization](https://arxiv.org/abs/2510.07802)
*Jixing Zhang,Bo Peng,Yang Wang,Cheuk Kit Cheung,Guodong Bian,Andrew M. Edmonds,Matthew Markham,Zhe Zhao,Durga Bhaktavatsala Rao Dasari,Ruoming Peng,Ye Wei,Jörg Wrachtrup*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-quality control over complex quantum systems is a key to achieving
practical quantum technologies. However, progress is hindered by the
exponential growth of quantum state spaces and the challenges posed by
realistic experimental conditions. Here, we present an AI framework that learns
to design pulse sequences for optimized quantum control over many-body spin
systems, providing a powerful alternative to theory-driven methods. The
framework combines customized tree search, neural network filtering, and
numerical simulation guidance to navigate highly nonlinear optimization
landscapes, using only desktop-level computational resources and minimal
experimental input. The objective function is set to preserve coherence, a key
prerequisite for quantum information processing. Our framework identifies over
900 high-performing sequences that exhibit non-intuitive structures and hence
challenge long-standing design principles, while established optimization
methods struggle to find such solutions. Experiments in a diamond spin ensemble
show that the best AI-designed sequences achieve coherence times exceeding 200
microseconds, representing a 100% improvement over state-of-the-art baselines
and approaching the temperature-imposed limit. Beyond spin coherence
preservation, our framework is readily extendable through modified objective
functions and incorporation of appropriate training data. This work highlights
AI's potential to steer complex quantum many-body dynamics, marking a paradigm
shift toward data-driven sequence design with broad applicability across
spin-based quantum technologies and beyond.

</details>


### [83] [Optimal and Robust In-situ Quantum Hamiltonian Learning through Parallelization](https://arxiv.org/abs/2510.07818)
*Suying Liu,Xiaodi Wu,Murphy Yuezhen Niu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hamiltonian learning is a cornerstone for advancing accurate many-body
simulations, improving quantum device performance, and enabling
quantum-enhanced sensing. Existing readily deployable quantum metrology
techniques primarily focus on achieving Heisenberg-limited precision in one- or
two-qubit systems. In contrast, general Hamiltonian learning theories address
broader classes of unknown Hamiltonian models but are highly inefficient due to
the absence of prior knowledge about the Hamiltonian. There remains a lack of
efficient and practically realizable Hamiltonian learning algorithms that
directly exploit the known structure and prior information of the Hamiltonian,
which are typically available for a given quantum computing platform. In this
work, we present the first Hamiltonian learning algorithm that achieves both
Cramer-Rao lower bound saturated optimal precision and robustness to realistic
noise, while exploiting device structure for quadratic reduction in
experimental cost for fully connected Hamiltonians. Moreover, this approach
enables simultaneous in-situ estimation of all Hamiltonian parameters without
requiring the decoupling of non-learnable interactions during the same
experiment, thereby allowing comprehensive characterization of the system's
intrinsic contextual errors. Notably, our algorithm does not require deep
circuits and remains robust against both depolarizing noise and time-dependent
coherent errors. We demonstrate its effectiveness with a detailed experimental
proposal along with supporting numerical simulations on Rydberg atom quantum
simulators, showcasing its potential for high-precision Hamiltonian learning in
the NISQ era.

</details>


### [84] [Product testing with single-copy measurements](https://arxiv.org/abs/2510.07820)
*Jacob Beckey,Luke Coffman,Ariel Shlosberg,Louis Schatzki,Felix Leditzky*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we study the sample complexity of two variants of product
testing when restricted to single-copy measurements. In particular, we consider
both bipartite product testing (i.e., does there exist at least one non-trivial
cut across which the state is product) and multipartite product testing (i.e.,
is the state fully product across every cut). For the first variant, we prove
an exponential lower bound on the sample complexity of any algorithm for this
task which utilizes only single-copy measurements. When comparing this with
known efficient algorithms that utilize multi-copy measurements, this
establishes an exponential separation for this and several related entanglement
learning tasks. For the second variant, we prove another sample lower bound
that establishes a separation between single- and multi-copy strategies. To
obtain our results, we prove a crucial technical lemma that gives a lower bound
on the overlap between tensor products of permutation operators acting on
subsystems of states that themselves carry a tensor structure. Finally, we
provide an algorithm for multipartite product testing using only single-copy,
local measurements, and we highlight several interesting open questions arising
from this work.

</details>


### [85] [The detection of Planck-scale physics facilitated by nonlinear quantum optics](https://arxiv.org/abs/2510.07844)
*Wenlin Li,Chengsong Zhao,Najmeh Eshaqi-Sani,Zhiyu Jiang,Xingli Li*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A tenet of contemporary physics is that novel physics beyond the Standard
Model lurks at a scale related to the Planck length. The development and
validation of a unified framework that merges general relativity and quantum
physics is contingent upon the observation of Planck-scale physics. Here, we
present a fully quantum model for measuring the nonstationary dynamics of a
ng-mass mechanical resonator, which will slightly deviate from the predictions
of standard quantum mechanics induced by modified commutation relations
associated with quantum gravity effects at low-energy scalar. The deformed
commutator is quantified by the oscillation frequency deviation, which is
amplified by the nonlinear mechanism of the detection field. The measurement
resolution is optimized to a precision level that is $15$ orders of magnitude
below the electroweak scale.

</details>


### [86] [A Meta-Complexity Characterization of Minimal Quantum Cryptography](https://arxiv.org/abs/2510.07859)
*Bruno Cavalar,Boyang Chen,Andrea Coladangelo,Matthew Gray,Zihan Hu,Zhengfeng Ji,Xingjian Li*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We give a meta-complexity characterization of EFI pairs, which are considered
the "minimal" primitive in quantum cryptography (and are equivalent to quantum
commitments). More precisely, we show that the existence of EFI pairs is
equivalent to the following: there exists a non-uniformly samplable
distribution over pure states such that the problem of estimating a certain
Kolmogorov-like complexity measure is hard given a single copy.
  A key technical step in our proof, which may be of independent interest, is
to show that the existence of EFI pairs is equivalent to the existence of
non-uniform single-copy secure pseudorandom state generators (nu 1-PRS). As a
corollary, we get an alternative, arguably simpler, construction of a universal
EFI pair.

</details>


### [87] [Balanced ternary formalism of second quantization](https://arxiv.org/abs/2510.07863)
*Yao Yao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We construct a second-quantized representation with a structure of balanced
ternary formalism, which involves three substances in organic molecular
materials, namely electron, hole and charge-transfer exciton, into a uniform
framework. The quantum thermodynamic of excitons is investigated in a closed
and compact manner, benefitting from the interplay of the three substances. In
order to be friendly with quantum simulations, the interactions among them are
all described with unitary transformations. Significantly, the nonconserving
dynamics of particle numbers, such as the generation of charge current and the
exciton fission in organic semiconductors, is consistently expressed by this
unitary formalism on the basis of bosonic coherent states. The spin degree of
freedom is further taken into account, and an exotic molecular ferromagnetic
ordering is induced in a specific configuration of excitons. This balanced
ternary formalism establishes a solid bridge to connect thermodynamics and
quantum simulations.

</details>


### [88] [Quantum Tanner Color Codes on Qubits with Transversal Gates](https://arxiv.org/abs/2510.07864)
*Kyle Gulshen,Tali Kaufman*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work focuses on growing our understanding of how high dimensional
expanders (HDX) can be utilized to construct highly performant quantum codes.
While asymptotically good qLDPC codes have been constructed on 2D HDX built
from products of graphs, these constructions have a number of limitations, like
lack of structure useful for fault-tolerant logic. We develop a framework for
transversal logical gates that can naturally utilize symmetric non-product
simplicial HDX, and we demonstrate a particular code in this framework that
offers various advantages over prior constructions. Specifically, we study the
generalization of color codes to \emph{Tanner color codes}, which encompass
color, pin, and rainbow codes, and should enable constructions with better
parameters. We prove an `unfolding' theorem that characterizes the logical
operators of the Tanner color code in terms of logical operators from several
colored copies of the companion sheaf code. We leverage this understanding of
the logical operators to identify a local condition that ensures such a code on
a $D$-dimensional complex has a strictly-transversal $\frac{2 \pi}{2^D}$-phase
gate on a single block, $\frac{2 \pi}{2^\ell}$-phase gates on subsets of a
single block for $\ell<D$, and $C^{D-1}Z$ across $D$ blocks that preserve the
code space. We explicitly instantiate our paradigm in every dimension with
codes on highly-symmetric expanding coset complexes. These are the first qubit
codes explicitly defined on expanding (non-product) simplicial complexes. We
investigate in detail the self-dual 2D family, which has large rate $\geq
\frac{7}{64}$ and transversal $CZ$, $S$, and $H$ gates, among many other
fault-tolerant (generalizations of) fold-transversal gates arising from the
symmetry of the complex. We conjecture that it has constant relative distance.
We conclude by describing a Floquet variant of this code with check weight 4.

</details>


### [89] [Learning T-conjugated stabilizers: The multiple-squares dihedral StateHSP](https://arxiv.org/abs/2510.07872)
*Gideon Lee,Jonathan A. Gross,Masaya Fukami,Zhang Jiang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The state hidden subgroup problem (StateHSP) is a recent generalization of
the hidden subgroup problem. We present an algorithm that solves the
non-abelian StateHSP over $N$ copies of the dihedral group of order $8$ (the
symmetries of a square). This algorithm is of interest for learning non-Pauli
stabilizers, as well as related symmetries relevant for the problem of
Hamiltonian spectroscopy. Our algorithm is polynomial in the number of samples
and computational time, and requires only constant depth circuits. This result
extends previous work on the abelian StateHSP and, as a special case, provides
a solution for the ordinary hidden subgroup problem on this specific
non-abelian group.

</details>


### [90] [A Feasible Quantum Walk-Enabled Blockchain with Quantum Delegated Proof-of-Stake Consensus](https://arxiv.org/abs/2510.07874)
*Chong-Qiang Ye,Heng-Ji Li,Jian Li,Xiao-Yu Chen*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum blockchains provide inherent resilience against quantum adversaries
and represent a promising alternative to classical blockchain systems in the
quantum era. However, existing quantum blockchain architectures largely depend
on entanglement to maintain inter-block connections, facing challenges in
stability, consensus efficiency, and system verification. To address these
issues, this work proposes a novel quantum blockchain framework based on
quantum walks, which reduces reliance on entanglement while improving stability
and connection efficiency. We further propose a quantum consensus mechanism
based on a weighted quantum voting protocol, which enables a fairer voting
process while reflecting the weights of different nodes. To validate the
proposed framework, we conduct circuit simulations to evaluate the correctness
and effectiveness of both the quantum walk-based block construction and the
quantum voting consensus mechanism. Compared with existing
entanglement-dependent approaches, our framework achieves stronger stability
and enables simpler verification of block integrity, making it a practical
candidate for quantum-era blockchain applications.

</details>


### [91] [Measuring gravitational lensing time delays with quantum information processing](https://arxiv.org/abs/2510.07898)
*Zhenning Liu,William DeRocco,Shiming Gu,Emil T. Khabiboulline,Soonwon Choi,Andrew M. Childs,Anson Hook,Alexey V. Gorshkov,Daniel Gottesman*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The gravitational fields of astrophysical bodies bend the light around them,
creating multiple paths along which light from a distant source can arrive at
Earth. Measuring the difference in photon arrival time along these different
paths provides a means of determining the mass of the lensing system, which is
otherwise difficult to constrain. This is particularly challenging in the case
of microlensing, where the images produced by lensing cannot be individually
resolved; existing proposals for detecting time delays in microlensed systems
are significantly constrained due to the need for large photon flux and the
loss of signal coherence when the angular diameter of the light source becomes
too large.
  In this work, we propose a novel approach to measuring astrophysical time
delays. Our method uses exponentially fewer photons than previous schemes,
enabling observations that would otherwise be impossible. Our approach, which
combines a quantum-inspired algorithm and quantum information processing
technologies, saturates a provable lower bound on the number of photons
required to find the time delay. Our scheme has multiple applications: we
explore its use both in calibrating optical interferometric telescopes and in
making direct mass measurements of ongoing microlensing events. To demonstrate
the latter, we present a fiducial example of microlensed stellar flares sources
in the Galactic Bulge. Though the number of photons produced by such events is
small, we show that our photon-efficient scheme opens the possibility of
directly measuring microlensing time delays using existing and near-future
ground-based telescopes.

</details>


### [92] [Hamiltonian Decoded Quantum Interferometry](https://arxiv.org/abs/2510.07913)
*Alexander Schmidhuber,Jonathan Z. Lu,Noah Shutty,Stephen Jordan,Alexander Poremba,Yihui Quek*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce Hamiltonian Decoded Quantum Interferometry (HDQI), a quantum
algorithm that utilizes coherent Bell measurements and the symplectic
representation of the Pauli group to reduce Gibbs sampling and Hamiltonian
optimization to classical decoding. For a signed Pauli Hamiltonian $H$ and any
degree-$\ell$ polynomial ${P}$, HDQI prepares a purification of the density
matrix $\rho_{P}(H) \propto {P}^2(H)$ by solving a combination of two tasks:
decoding $\ell$ errors on a classical code defined by $H$, and preparing a
pilot state that encodes the anti-commutation structure of $H$. Choosing $P(x)$
to approximate $\exp(-\beta x/2)$ yields Gibbs states at inverse temperature
$\beta$; other choices prepare approximate ground states, microcanonical
ensembles, and other spectral filters.
  For local Hamiltonians, the corresponding decoding problem is that of LDPC
codes. Preparing the pilot state is always efficient for commuting
Hamiltonians, but highly non-trivial for non-commuting Hamiltonians.
Nevertheless, we prove that this state admits an efficient matrix product state
representation for Hamiltonians whose anti-commutation graph decomposes into
connected components of logarithmic size.
  We show that HDQI efficiently prepares Gibbs states at arbitrary temperatures
for a class of physically motivated commuting Hamiltonians -- including the
toric code and Haah's cubic code -- but we also develop a matching efficient
classical algorithm for this task. For a non-commuting semiclassical spin glass
and commuting stabilizer Hamiltonians with quantum defects, HDQI prepares Gibbs
states up to a constant inverse-temperature threshold using polynomial quantum
resources and quasi-polynomial classical pre-processing. These results position
HDQI as a versatile algorithmic primitive and the first extension of Regev's
reduction to non-abelian groups.

</details>


### [93] [Quantum Random Feature Method for Solving Partial Differential Equations](https://arxiv.org/abs/2510.07945)
*Junpeng Hu,Shi Jin,Nana Liu,Lei Zhang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum computing holds significant promise for scientific computing due to
its potential for polynomial to even exponential speedups over classical
methods, which are often hindered by the curse of dimensionality. While neural
networks present a mesh-free alternative to solve partial differential
equations (PDEs), their accuracy is difficult to achieve since one needs to
solve a high-dimensional non-convex optimization problem using the stochastic
gradient descent method and its variants, the convergence of which is difficult
to prove and cannot be guaranteed. The classical random feature method (RFM)
effectively merges advantages from both classical numerical analysis and neural
network based techniques, achieving spectral accuracy and a natural
adaptability to complex geometries. In this work, we introduce a quantum random
feature method (QRFM) that leverages quantum computing to accelerate the
classical RFM framework. Our method constructs PDE solutions using
quantum-generated random features and enforces the governing equations via a
collocation approach. A complexity analysis demonstrates that this hybrid
quantum-classical algorithm can achieve a quadratic speedup over the classical
RFM.

</details>


### [94] [Quantum channel discrimination against jammers](https://arxiv.org/abs/2510.07977)
*Kun Fang,Michael X. Cao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of quantum channel discrimination between two channels
with an adversary input party (a.k.a. a jammer). This setup interpolates
between the best-case channel discrimination as studied by (Wang & Wilde, 2019)
and the worst-case channel discrimination as studied by (Fang, Fawzi, & Fawzi,
2025), thereby generalizing both frameworks. To address this problem, we
introduce the notion of minimax channel divergence and establish several of its
key mathematical properties. We prove the Stein's lemma in this new setting,
showing that the optimal type-II error exponent in the asymptotic regime under
parallel strategies is characterized by the regularized minimax channel
divergence.

</details>


### [95] [Quantum Max-Cut is NP hard to approximate](https://arxiv.org/abs/2510.07995)
*Stephen Piddock*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We unconditionally prove that it is NP-hard to compute a constant
multiplicative approximation to the QUANTUM MAX-CUT problem on an unweighted
graph of constant bounded degree. The proof works in two stages: first we
demonstrate a generic reduction to computing the optimal value of a quantum
problem, from the optimal value over product states. Then we prove an
approximation preserving reduction from MAX-CUT to PRODUCT-QMC the product
state version of QUANTUM MAX-CUT. More precisely, in the second part, we
construct a PTAS reduction from MAX-CUT$_k$ (the rank-k constrained version of
MAX-CUT) to MAX-CUT$_{k+1}$, where MAX-CUT and PRODUCT-QMC coincide with
MAX-CUT$_1$ and MAX-CUT$_3$ respectively. We thus prove that Max-Cut$_k$ is
APX-complete for all constant $k$.

</details>


### [96] [Integer Factoring with Unoperations](https://arxiv.org/abs/2510.08027)
*Paul Kohl*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work introduces the notion of unoperation $\mathfrak{Un}(\hat{O})$ of
some operation $\hat{O}$. Given a valid output of $\hat{O}$, the corresponding
unoperation produces a set of all valid inputs to $\hat{O}$ that produce the
given output. Further, the working principle of unoperations is illustrated
using the example of addition. A device providing that functionality is
constructed utilising a quantum circuit performing the unoperation of addition
- referred to as unaddition. To highlight the potential of the approach the
unaddition quantum circuit is employed to construct a device for factoring
integer numbers $N$, which is then called unmultiplier. This approach requires
only a number of qubits $\in \mathcal{O}((\log{N})^2)$, rivalling the best
known factoring algorithms to date.

</details>


### [97] [Quantum Limits of LEO Satellite Beacon Reading](https://arxiv.org/abs/2510.08040)
*Pere Munar-Vallespir,Marc Geitz,Ángeles Vázquez-Castro,Janis Nötzel*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the quantum limits of the ELROI beacon concept introduced by Holmes,
Weaver, and Palmer. In this concept, a satellite continuously emits a weak
optical signal to broadcast its identity. Via analysis of the fundamental
limits on communication introduced by Shannon, Gordon, and Holevo, we
demonstrate that in such scenarios, incorporating quantum technology into the
design of a ground station significantly enhances performance. Specifically,
the Time-To-Read the beacon signal and thereby identify the satellite is
greatly reduced in situations where weather conditions obstruct the signal,
allowing the Active Time Window, during which the satellite can be utilized for
subsequent network operations, to be extended by nearly a factor of 20. In this
particular case, the quantum technology concept that is employed is the
so-called Joint Detector Receiver, which is a system aiming to operate at the
Gordon-Holevo limit by performing joint quantum operations on sequences of
incoming signals.

</details>


### [98] [Optimization of Quadratic Constraints by Decoded Quantum Interferometry](https://arxiv.org/abs/2510.08061)
*Daniel Cohen Hillel*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A recent paper by Jordan et al. introduced Decoded Quantum Interferometry
(DQI), a novel quantum algorithm that uses the quantum Fourier transform to
reduce linear optimization problems -- max-XORSAT and max-LINSAT -- to decoding
problems. In this paper, we extend DQI to optimization problems involving
quadratic constraints, which we call max-QUADSAT. Leveraging a connection to
quadratic Gauss sums, we give an efficient algorithm to prepare the DQI state
for max-QUADSAT. To demonstrate that our algorithm achieves a quantum
advantage, we introduce the Quadratic Optimal Polynomial Intersection
(quadratic-OPI) problem, a restricted variant of OPI for which, to our
knowledge, the standard DQI framework offers no algorithmic speedup. We show
that quadratic-OPI is an instance of max-QUADSAT and use our algorithm to
optimize it. Lastly, we present a new generalized proof of the "semicircle law"
for the fraction of satisfied constraints, generalizing it to any DQI state of
problems where the distribution of the number of satisfied constraints for a
random assignment is sufficiently close to a binomial distribution. This
condition holds exactly for the DQI state of max-LINSAT, and approximately
holds in the max-QUADSAT case, with the approximation becoming exponentially
better as the problem size increases. This establishes performance guarantees
for our algorithm.

</details>


### [99] [An infinite hierarchy of multi-copy quantum learning tasks](https://arxiv.org/abs/2510.08070)
*Jan Nöller,Viet T. Tran,Mariami Gachechiladze,Richard Kueng*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning properties of quantum states from measurement data is a fundamental
challenge in quantum information. The sample complexity of such tasks depends
crucially on the measurement primitive. While shadow tomography achieves
sample-efficient learning by allowing entangling measurements across many
copies, it requires prohibitively deep circuits. At the other extreme, two-copy
measurements already yield exponential advantages over single-copy strategies
in tasks such as Pauli tomography. In this work we show that such sharp
separations extend far beyond the two-copy regime: for every prime c we
construct explicit learning tasks of degree c, which are exponentially hard
with (c - 1)-copy measurements but efficiently solvable with c-copy
measurements. Our protocols are not only sample-efficient but also realizable
with shallow circuits. Extending further, we show that such finite-degree tasks
exist for all square-free integers c, pointing toward a general principle
underlying their existence. Together, our results reveal an infinite hierarchy
of multi-copy learning problems, uncovering new phase transitions in sample
complexity and underscoring the role of reliable quantum memory as a key
resource for exponential quantum advantage.

</details>


### [100] [A Unified Approach to Quantum Key Leasing with a Classical Lessor](https://arxiv.org/abs/2510.08079)
*Fuyuki Kitagawa,Jiahui Liu,Shota Yamada,Takashi Yamakawa*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Secure key leasing allows a cryptographic key to be leased as a quantum state
in such a way that the key can later be revoked in a verifiable manner. In this
work, we propose a modular framework for constructing secure key leasing with a
classical-lessor, where the lessor is entirely classical and, in particular,
the quantum secret key can be both leased and revoked using only classical
communication. Based on this framework, we obtain classical-lessor secure key
leasing schemes for public-key encryption (PKE), pseudorandom function (PRF),
and digital signature. We adopt the strong security notion known as security
against verification key revealing attacks (VRA security) proposed by Kitagawa
et al. (Eurocrypt 2025) into the classical-lessor setting, and we prove that
all three of our schemes satisfy this notion under the learning with errors
assumption. Our PKE scheme improves upon the previous construction by Goyal et
al. (Eurocrypt 2025), and our PRF and digital signature schemes are
respectively the first PRF and digital signature with classical-lessor secure
key leasing property.

</details>


### [101] [On the modeling of irreversibility by relaxator Liouville dynamics](https://arxiv.org/abs/2510.08083)
*Janos Hajdu,Martin Janßen*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A general approach to modeling irreversibility starting from microscopic
reversibility is presented. The time $t_s$ up to which relevant degrees of
freedom of a system are tracked is extremely much shorter than the spectral
resolution time $t_e$ that would be necessary to resolve the spectrum of all
degrees of freedom involved. A relaxator that breaks reversibility condenses in
the Liouville operator of the relevant degrees of freedom. The irrelevant
degrees of freedom act as an environment to the system. The irreversible
relaxator Liouville equation contains memory effects and initial correlations
of all degrees of freedom. Stationary states turn out to be generically unique
and independent of the initial conditions and exceptions are due to
degeneracies. Equilibrium states lie in the relaxator's kernel yielding a
stationary Pauli master equation. Kinetic equations for oneparticle densities
are constructed as special cases of relaxator Liouville dynamics. Kubo's linear
response theory is generalized to relaxator Liouville dynamics and related to
irreversibility within the system. In a weak coupling approximation between
system and environment the relaxator can be reduced to environmental
correlations and bilinear system operators. Markov approximation turns the
relaxator Liouville dynamics into a semi-group dynamics.

</details>


### [102] [Quantum Algorithm for Low Energy Effective Hamiltonian and Quasi-Degenerate Eigenvalue Problem](https://arxiv.org/abs/2510.08088)
*Chun-Tse Li,Tzen Ong,Chih-Yun Lin,Yu-Cheng Chen,Hsin Lin,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quasi-degenerate eigenvalue problems are central to quantum chemistry and
condensed-matter physics, where low-energy spectra often form manifolds of
nearly degenerate states that determine physical properties. Standard quantum
algorithms, such as phase estimation and QSVT-based eigenvalue filtering, work
well when a unique ground state is separated by a moderate spectral gap, but in
the quasi-degenerate regime they require resolution finer than the
intra-manifold splitting; otherwise, they return an uncontrolled superposition
within the low-energy span and fail to detect or resolve degeneracies. In this
work, we propose a quantum algorithm that directly diagonalizes such
quasi-degenerate manifolds by solving an effective-Hamiltonian eigenproblem in
a low-dimensional reference subspace. This reduced problem is exactly
equivalent to the full eigenproblem, and its solutions are lifted to the full
Hilbert space via a block-encoded wave operator. Our analysis provides provable
bounds on eigenvalue accuracy and subspace fidelity, together with total query
complexity, demonstrating that quasi-degenerate eigenvalue problems can be
solved efficiently without assuming any intra-manifold splitting. We benchmark
the algorithm on several systems (the Fermi-Hubbard model, LiH, and the
transition-metal complex [Ru(bpy)$_3$]$^{2+}$), demonstrating robust
performance and reliable resolution of (quasi-)degeneracies.

</details>


### [103] [Three-dimensional optical characterization of magnetostrictive deformation in magnomechanical systems](https://arxiv.org/abs/2510.08099)
*Xiaomin Liu,Jing Zhang,Jie Li,Rongguo Yang,Jiangrui Gao,Tiancai Zhang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Magnomechanical systems with YIG spheres have been proven to be an ideal
system for studying magnomechanically induced transparency, dynamical
backaction, and rich nonlinear effects, such as the magnon-phonon cross-Kerr
effect. Accurate characterization of the magnetostriction induced deformation
displacement is important as it can be used for, e.g., estimating the magnon
excitation number and the strength of the dynamical backaction. Here we propose
an optical approach for detecting the magnetostrictive deformation of a YIG
sphere in three dimensions (3Ds) with high precision. It is based on the
deformation induced spatial high-order modes of the scattered field,
postselection, and balanced homodyne detection. With feasible parameters, we
show that the measurement precision of the deformation in $x$, $y$, and $z$
directions can reach the picometer level. We further reveal the advantages of
our scheme using a higher-order probe beam and balanced homodyne detection by
means of quantum and classical Fisher information. The real-time and
high-precision measurement of the YIG sphere's deformation in 3Ds can be used
to determinate specific mechanical modes, characterize the magnomechanical
dynamical backaction and the 3D cooling of the mechanical vibration, and thus
finds a wide range of applications in magnomechanics.

</details>


### [104] [Adaptive Quantum Homeopathy](https://arxiv.org/abs/2510.08129)
*Lennart Bittel,Lorenzo Leone*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Randomness is a fundamental resource in quantum information, with crucial
applications in cryptography, algorithms, and error correction. A central
challenge is to construct unitary $k$-designs that closely approximate
Haar-random unitaries while minimizing the costly use of non-Clifford
operations. In this work, we present a protocol, named Quantum Homeopathy, able
to generate unitary $k$-designs on $n$ qubits, secure against any adversarial
quantum measurement, with a system-size-independent number of non-Clifford
gates. Inspired by the principle of homeopathy, our method applies a $k$-design
only to a subsystem of size $\Theta(k)$, independent of $n$. This "seed" design
is then "diluted" across the entire $n$-qubit system by sandwiching it between
two random Clifford operators. The resulting ensemble forms an
$\varepsilon$-approximate unitary $k$-design on $n$ qubits. We prove that this
construction achieves full quantum security against adaptive adversaries using
only $\tilde{O}(k^2 \log\varepsilon^{-1})$ non-Clifford gates. If one requires
security only against polynomial-time adaptive adversaries, the non-Clifford
cost decreases to $\tilde{O}(k + \log^{1+c} \varepsilon^{-1})$. This is
optimal, since we show that at least $\Omega(k)$ non-Clifford gates are
required in this setting. Compared to existing approaches, our method
significantly reduces non-Clifford overhead while strengthening security
guarantees to adaptive security as well as removing artificial assumptions
between $n$ and $k$. These results make high-order unitary designs practically
attainable in near-term fault-tolerant quantum architectures.

</details>


### [105] [Existence of universal resource and uselessness of too entangled states for quantum metrology](https://arxiv.org/abs/2510.08133)
*Rina Miyajima,Yuki Takeuchi,Seiseki Akibue*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We show (i) the existence of universal resource states for a certain class of
linear Hamiltonians and (ii) the uselessness of highly entangled states for
quantum metrology of linear Hamiltonians. We also show that random pure states
are basically not useful even if we consider more general Hamiltonians. Since
random pure states have high entanglement, this result strengthens the
uselessness of highly entangled states for quantum metrology.

</details>


### [106] [Enhancing Hybrid Methods in Parameterized Quantum Circuit Optimization](https://arxiv.org/abs/2510.08142)
*Joona V. Pankkonen,Matti Raasakka,Andrea Marchesin,Ilkka Tittonen*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Parameterized quantum circuits (PQCs) play an essential role in the
application of variational quantum algorithms (VQAs) in noisy
intermediate-scale quantum (NISQ) devices. The PQCs are a leading candidate to
achieve a quantum advantage in NISQ devices and have already been applied in
various domains such as quantum chemistry, quantum machine learning,
combinatorial optimization, and many others. There is no single definitive way
to optimize PQCs. The most commonly used methods are based on computing the
gradient via the parameter-shift rule to use classical gradient descent (GD)
optimizers like Adam, stochastic GD, and others. In addition, sequential
single-qubit optimizers have been proposed, such as Rotosolve, Free-Axis
Selection (Fraxis), Free-Quaternion Selection (FQS), and hybrid algorithms from
the aforementioned optimizers. We further develop hybrid algorithms than those
represented in the previous work by drawing inspiration from the early stopping
method used in classical machine learning. The switch between the optimizers
depends on the previous cost function values compared to the previous ones. We
introduce two new hybrid algorithms that are more robust and scalable, and they
outperform previous hybrid methods in terms of convergence towards the global
minima across various cost functions. In addition, we find that they are
feasible for NISQ devices with different noise profiles.

</details>


### [107] [Pattern or Not? QAOA Parameter Heuristics and Potentials of Parsimony](https://arxiv.org/abs/2510.08153)
*Vincent Eichenseher,Maja Franz,Christian Wolff,Wolfgang Mauerer*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Structured variational quantum algorithms such as the Quantum Approximate
Optimisation Algorithm (QAOA) have emerged as leading candidates for exploiting
advantages of near-term quantum hardware. They interlace classical computation,
in particular optimisation of variational parameters, with quantum-specific
routines, and combine problem-specific advantages -- sometimes even provable --
with adaptability to the constraints of noisy, intermediate-scale quantum
(NISQ) devices. While circuit depth can be parametrically increased and is
known to improve performance in an ideal (noiseless) setting, on realistic
hardware greater depth exacerbates noise: The overall quality of results
depends critically on both, variational parameters and circuit depth. Although
identifying optimal parameters is NP-hard, prior work has suggested that they
may exhibit regular, predictable patterns for increasingly deep circuits and
depending on the studied class of problems. In this work, we systematically
investigate the role of classical parameters in QAOA performance through
extensive numerical simulations and suggest a simple, yet effective heuristic
scheme to find good parameters for low-depth circuits. Our results demonstrate
that: (i) optimal parameters often deviate substantially from expected
patterns; (ii) QAOA performance becomes progressively less sensitive to
specific parameter choices as depth increases; and (iii) iterative
component-wise fixing performs on par with, and at shallow depth may even
outperform, several established parameter-selection strategies. We identify
conditions under which structured parameter patterns emerge, and when
deviations from the patterns warrant further consideration. These insights for
low-depth circuits may inform more robust pathways to harnessing QAOA in
realistic quantum compute scenarios.

</details>


### [108] [Classification and implementation of unitary-equivariant and permutation-invariant quantum channels](https://arxiv.org/abs/2510.08154)
*Laura Mančinska,Elias Theil*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many quantum information tasks use inputs of the form $\rho^{\otimes m}$,
which naturally induce permutation and unitary symmetries. We classify all
quantum channels that respect both symmetries - i.e. unitary-equivariant and
permutation-invariant quantum channels from $(\mathbb{C}^{d})^{\otimes m}$ to
$(\mathbb{C}^{d})^{\otimes n}$ - via their extremal points. Operationally, each
extremal quantum channel factors as unitary Schur sampling $\rightarrow$ an
irrep-level unitary-equivariant quantum channel $\rightarrow$ the adjoint
unitary Schur sampling. We give a streaming implementation ansatz that uses an
efficient streaming implementation of unitary Schur sampling together with a
resource-state primitive, and we apply it to state symmetrization, symmetric
cloning, and purity amplification. In these applications we obtain
polynomial-time algorithms with exponential memory improvements in $m,n$.
Further, for symmetric cloning we present, to our knowledge, the first
efficient (polynomial-time) algorithm with explicit memory and gate bounds.

</details>


### [109] [Efficient Fidelity Estimation with Few Local Pauli Measurements](https://arxiv.org/abs/2510.08155)
*Mingyu Sun,Gabriel Waite,Michael Bremner,Christopher Ferrie*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As quantum devices scale, quantifying how close an experimental state aligns
with a target becomes both vital and challenging. Fidelity is the standard
metric, but existing estimators either require full tomography or apply only to
restricted state/measurement families. Huang, Preskill, and Soleimanifar
(Nature Physics, 2025) introduced an efficient certification protocol for
Haar-random states using only a polynomial number of non-adaptive, single-copy,
local Pauli measurements. Here, we adopt the same data collection routine but
recast it as a fidelity estimation protocol with rigorous performance
guarantees and broaden its applicability. We analyze the bias in this
estimator, linking its performance to the mixing time $\tau$ of a Markov chain
induced by the target state, and resolve the three open questions posed by
Huang, Preskill, and Soleimanifar (Nature Physics, 2025). Our analysis extends
beyond Haar-random states to state $t$-designs, states prepared by low-depth
random circuits, physically relevant states and families of mixed states. We
introduce a $k$-generalized local escape property that identifies when the
fidelity estimation protocol is both efficient and accurate, and design a
practical empirical test to verify its applicability for arbitrary states. This
work enables scalable benchmarking, error characterization, and tomography
assistance, supports adaptive quantum algorithms in high dimensions, and
clarifies fundamental limits of learning from local measurements.

</details>


### [110] [Characterizing Liouvillian Exceptional Points Through Newton Polygons and Tropical Geometry](https://arxiv.org/abs/2510.08156)
*Sayooj P,Awadhesh Narayan*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The dynamics of open quantum systems described by the Lindblad master
equation follows according to non-Hermitian operators. As a result, such
systems can host non-Hermitian degeneracies called Liouvillian exceptional
points (EPs). In this work, we show that Newton polygons and tropical geometric
approach allow identification and characterization of Liouvillian EPs. We use
two models -- dissipative spin$-1/2$ system and dissipative superconducting
qubit system -- to illustrate our method. We demonstrate that our approach
captures the anisotropy and order of the Liouvillian EPs, while also revealing
the subtle dependence on the form of the perturbation. Our analytical analysis
is supplemented by direct numerical calculations of the scaling and exchange of
eigenvalues around Liouvillian EPs. Our analytical approach could be useful in
understanding and designing Liouvillian EPs of desired order.

</details>


### [111] [Quantum Agents for Algorithmic Discovery](https://arxiv.org/abs/2510.08159)
*Iordanis Kerenidis,El-Amine Cherrat*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce quantum agents trained by episodic, reward-based reinforcement
learning to autonomously rediscover several seminal quantum algorithms and
protocols. In particular, our agents learn: efficient logarithmic-depth quantum
circuits for the Quantum Fourier Transform; Grover's search algorithm; optimal
cheating strategies for strong coin flipping; and optimal winning strategies
for the CHSH and other nonlocal games. The agents achieve these results
directly through interaction, without prior access to known optimal solutions.
This demonstrates the potential of quantum intelligence as a tool for
algorithmic discovery, opening the way for the automated design of novel
quantum algorithms and protocols.

</details>


### [112] [Generalised fractional Rabi problem](https://arxiv.org/abs/2510.08167)
*Alexander Lopez,Sébastien Fumeron,Malte Henkel,Trifce Sandev,Esther D. Gutiérrez*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fractional quantum dynamics provides a natural framework to capture nonlocal
temporal behavior and memory effects in quantum systems. In this work, we
analyze the physical consequences of fractional-order quantum evolution using a
Green's function formulation based on the Caputo fractional derivative.
Explicit iterative expressions for the evolved state are derived and applied to
an extended two-level Rabi model, a paradigmatic setting for coherent quantum
control. We find that even in the absence of external driving, the static
Hamiltonian term induces non-trivial spin dynamics with damping features
directly linked to the fractional temporal nonlocality. When a periodically
varying driving field is introduced, the competition between energy injection
and memory effects gives rise to a richer dynamical behavior, manifest in the
evolution of spin polarization, autocorrelation function, and fidelity. Unlike
the standard Rabi oscillations characterized by a fixed frequency, the
fractional regime introduces controllable damping and dephasing governed by the
degree of fractionality. These distinctive signatures could be observable
through the Loschmidt echo and autocorrelation function, and would offer
potential routes to probe fractional quantum dynamics experimentally. Our
findings open pathways toward exploring memory-induced dynamical phenomena in
other systems effectively described by a two-level approximation, such as
graphene-like materials and topological SSH chains, where non-integer order
evolution may reveal novel topological or relaxation effects.

</details>


### [113] [Hyper-optimized Quantum Lego Contraction Schedules](https://arxiv.org/abs/2510.08210)
*Balint Pato,June Vanlerberghe,Kenneth R. Brown*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Calculating the quantum weight enumerator polynomial (WEP) is a valuable tool
for characterizing quantum error-correcting (QEC) codes, but it is
computationally hard for large or complex codes. The Quantum LEGO (QL)
framework provides a tensor network approach for WEP calculation, in some cases
offering superpolynomial speedups over brute-force methods, provided the code
exhibits area law entanglement, that a good QL layout is used, and an efficient
tensor network contraction schedule is found. We analyze the performance of a
hyper-optimized contraction schedule framework across QL layouts for diverse
stabilizer code families. We find that the intermediate tensors in the QL
networks for stabilizer WEPs are often highly sparse, invalidating the
dense-tensor assumption of standard cost functions. To address this, we
introduce an exact, polynomial-time Sparse Stabilizer Tensor (SST) cost
function based on the rank of the parity check matrices for intermediate
tensors. The SST cost function correlates perfectly with the true contraction
cost, providing a significant advantage over the default cost function, which
exhibits large uncertainty. Optimizing contraction schedules using the SST cost
function yields substantial performance gains, achieving up to orders of
magnitude improvement in actual contraction cost compared to using the dense
tensor cost function. Furthermore, the precise cost estimation from the SST
function offers an efficient metric to decide whether the QL-based WEP
calculation is computationally superior to brute force for a given QL layout.
These results, enabled by PlanqTN, a new open-source QL implementation,
validate hyper-optimized contraction as a crucial technique for leveraging the
QL framework to explore the QEC code design space.

</details>


### [114] [Enhancing optomechanical force sensing utilizing synthetic magnetism](https://arxiv.org/abs/2510.08234)
*Ding-hui Xu,Zheng Liu,Chang-shui Yu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In precision force sensing of multi-mechanical mode optomechanical systems,
coherent interference can decouple certain degenerate vibrational modes from
the cavity field, leading to incomplete information regarding the measured
signal. In this paper, we propose a scheme to enhance and control the detection
bandwidth in optomechanical force sensing by exploiting synthetic magnetism
achieved through tuning phonon hopping interactions. By toggling between broken
and unbroken dark mode, this approach effectively manages the response
bandwidth and exhibits intriguing additional noise characteristics.
Specifically, when the dark mode remains unbroken, the thermal noise is robust
and reduced to half of that of a standard device. In contrast, when the dark
mode is broken, thermal noise increases substantially at mechanical resonance
but remains the same as when the dark mode is unbroken at effective detection
frequencies. Moreover, our scheme offers the dual benefit of amplifying the
mechanical response while suppressing additional noise, with the potential to
surpass the standard quantum limit.

</details>


### [115] [Operational protocols cannot certify classicality](https://arxiv.org/abs/2510.08239)
*Chris Fields,James F. Glazebrook,Antonino Marcianò,Emanuele Zappala*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The existence and practical utility of operational protocols that certify
entanglement raises the question of whether operational protocols exist that
certify the absence of entanglement, i.e. that certify separability. We show,
within a purely topological, interpretation-independent representation, that
such protocols do not exist. Classicality is therefore, as Bohr suggested,
purely a pragmatic notion.

</details>


### [116] [Work distribution of quantum fields in static curved spacetimes](https://arxiv.org/abs/2510.08265)
*Rafael L. S. Costa,Marcos L. W. Basso,Jonas Maziero,Lucas C. Céleri*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the formulation of work distributions for quantum scalar
fields in static curved spacetimes by extending the Ramsey interferometric
protocol originally developed in previous works for flat spacetimes. The use of
Unruh-DeWitt particle detectors provides a causally consistent framework to
define and measure work statistics, avoiding the limitations of the two-time
projective measurement scheme in relativistic quantum field theory. We derive a
non-perturbative expression for the characteristic function of the quantum
field and apply it to thermal Kubo-Martin-Schwinger (KMS) states, showing that
the resulting work distributions satisfy both the Crooks fluctuation theorem
and the Jarzynski equality. Furthermore, we analyse the case of a pointlike
detector, obtaining compact expressions for the first two moments of the work
distribution, allowing us to recover the standard fluctuation-dissipation
relation in the high-temperature limit. Our results demonstrate that
fluctuation theorems hold for quantum fields interacting with Unruh-DeWitt
particle detectors in static curved spacetimes.

</details>


### [117] [Low-depth measurement-based deterministic quantum state preparation](https://arxiv.org/abs/2510.08267)
*Roselyn Nmaju,Fiona Speirits,Sarah Croke*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a low-depth amplitude encoding method for arbitrary quantum state
preparation. Building on the foundation of an existing divide-and-conquer
algorithm, we propose a method to disentangle the ancillary qubits from the
final state. Our method is measurement-based but deterministic, and offers an
alternative approach to existing state preparation algorithms. It has circuit
depth O(n), which is known to be optimal, and O(2^n) ancilla qubits, which is
close to optimal. We illustrate our method through detailed worked examples of
both a ``dense'' state and a W-state. We discuss extensions to the algorithm
resetting qubits mid-circuit, and construct hybrid algorithms with varying
space and circuit depth complexities.

</details>


### [118] [A Binary Optimisation Algorithm for Near-Term Photonic Quantum Processors](https://arxiv.org/abs/2510.08274)
*Alexander Makarovskiy,Mateusz Slysz,Łukasz Grodzki,Dawid Siera,Thorin Farnsworth,William R. Clements,Piotr Rydlichowski,Krzysztof Kurowski*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Binary optimisation tasks are ubiquitous in areas ranging from logistics to
cryptography. The exponential complexity of such problems means that the
performance of traditional computational methods decreases rapidly with
increasing problem sizes. Here, we propose a new algorithm for binary
optimisation, the Bosonic Binary Solver, designed for near-term photonic
quantum processors. This variational algorithm uses samples from a quantum
optical circuit, which are post-processed using trainable classical bit-flip
probabilities, to propose candidate solutions. A gradient-based training loop
finds progressively better solutions until convergence. We perform ablation
tests that validate the structure of the algorithm. We then evaluate its
performance on an illustrative range of binary optimisation problems, using
both simulators and real hardware, and perform comparisons to classical
algorithms. We find that this algorithm produces high-quality solutions to
these problems. As such, this algorithm is a promising method for leveraging
the scalable nature of photonic quantum processors to solve large-scale
real-world optimisation problems.

</details>


### [119] [Transversal gates for probabilistic implementation of multi-qubit Pauli rotations](https://arxiv.org/abs/2510.08290)
*Nobuyuki Yoshioka,Alireza Seif,Andrew Cross,Ali Javadi-Abhari*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a general framework for weak transversal gates -- probabilistic
implementation of logical unitaries realized by local physical unitaries -- and
propose a novel partially fault-tolerant quantum computing architecture that
surpasses the standard Clifford+T architecture on workloads with million-scale
Clifford+T gate counts. First, we prove the existence of weak transversal gates
on the class of Calderbank-Shor-Steane codes, covering high-rate qLDPC and
topological codes such as surface code or color codes, and present an efficient
algorithm to determine the physical multi-qubit Pauli rotations required for
the desired logical rotation. Second, we propose a partially fault-tolerant
Clifford+$\phi$ architecture that performs in-place Pauli rotations via a
repeat-until-success strategy; phenomenological simulations indicate that a
rotation of 0.003 attains logical error of $9.5\times10^{-5}$ on a surface code
with $d=7$ at physical error rate of $10^{-4}$, while avoiding the spacetime
overheads of magic state factories, small angle synthesis, and routing.
Finally, we perform resource estimation on surface and gross codes for a
Trotter-like circuit with $N=108$ logical qubits to show that the
Clifford+$\phi$ architecture outperforms the conventional Clifford+T approach
by a factor of tens to a hundred in runtime due to natural rotation-gate
parallelism. This work open a novel paradigm for realizing logical operations
beyond the constraints of conventional design.

</details>


### [120] [Adversarial Thermodynamics](https://arxiv.org/abs/2510.08298)
*Maite Arcos,Philippe Faist,Takahiro Sagawa,Jonathan Oppenheim*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In thermodynamics, an agent's ability to extract work is fundamentally
constrained by their environment. Traditional frameworks struggle to capture
how strategic decision-making under uncertainty -- particularly an agent's
tolerance for risk -- determines the trade-off between extractable work and
probability of success in finite-scale experiments. Here, we develop a
framework for non-equilibrium thermodynamics based on adversarial resource
theories, in which work extraction is modelled as an adversarial game for an
agent extracting work. Within this perspective, we recast the Szilard engine as
a game isomorphic to Kelly gambling, an information-theoretic model of optimal
betting under uncertainty -- but with a thermodynamic utility function.
Extending the framework to finite-size regimes, we apply a risk-reward
trade-off to find an interpretation of the Renyi-divergences, in terms of
extractable work for a given failure probability. By incorporating risk
sensitivity via utility functions, we show that the guaranteed amount of work a
rational agent would accept instead of undertaking a risky protocol is given by
a R\'enyi divergence. This provides a unified picture of thermodynamics and
gambling, and highlights how generalized free energies emerge from an
adversarial setup.

</details>


### [121] [Clifford+V synthesis for multi-qubit unitary gates](https://arxiv.org/abs/2510.08312)
*Soichiro Yamazaki,Seiseki Akibue*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We developed a general framework for synthesizing target gates by using a
finite set of basic gates, which is a crucial step in quantum compilation. When
approximating a gate in SU($n$), a naive brute-force search requires a
computational complexity of $O(1/\varepsilon^{(n^2 - 1)})$ to achieve an
approximation with error $\varepsilon$. In contrast, by using our method, the
complexity can be reduced to $O(-n^2 \log\varepsilon/\varepsilon^{((n^2 -
1)/2)})$. This method requires almost no assumptions and can be applied to a
variety of gate sets, including Clifford+$T$ and Clifford+$V$. Further, we
introduce a suboptimal but short run-time algorithm for synthesizing
multi-qubit controlled gates. This approach highlights the role of subgroup
structures in reducing synthesis complexity and opens a new direction of study
on the compilation of multi-qubit gates. The framework is broadly applicable to
different universal gate sets, and our analysis suggests that it can serve as a
foundation for resource-efficient quantum compilation in near-term
architectures.

</details>


### [122] [Characterizing Space-Constrained Implementability of Quantum Instruments via Signaling Conditions](https://arxiv.org/abs/2510.08313)
*Kosuke Matsui,Jun-Yi Wu,Hayata Yamasaki,Min-Hsiu Hsieh,Mio Murao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scaling up the number of qubits available on quantum processors remains
technically demanding even in the long term; it is therefore crucial to clarify
the number of qubits required to implement a given quantum operation. For the
most general class of quantum operations, known as quantum instruments, the
qubit requirements are not well understood, especially when mid-circuit
measurements and delayed input preparation are permitted. In this work, we
characterize lower and upper bounds on the number of qubits required to
implement a given quantum instrument in terms of the causal structure of the
instrument. We further apply our results to entanglement distillation protocols
based on stabilizer codes and show that, in these cases, the lower and upper
bounds coincide, so the optimal qubit requirement is determined. In particular,
we compute that the optimal number of qubits is 3 for the
$[[9,1,3]]$-code-based protocol and 4 for the $[[5,1,3]]$-code-based protocol.

</details>


### [123] [The power of quantum catalytic local operations](https://arxiv.org/abs/2510.08320)
*Patryk Lipka-Bartosik,Jessica Bavaresco,Nicolas Brunner,Pavel Sekatski*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A key result in entanglement theory is that the addition of a catalyst
dramatically enlarges the set of possible state transformations via local
operations and classical communication (LOCC). However, it remains unclear what
is the interplay between classical communication and quantum catalysis. Here
our aim is to disentangle the effect of the catalyst from that of classical
communication. To do so, we explore a class of state transformations termed
catalytic local operations (CLO) and compare it to LOCC and to stochastic LOCC
augmented by bounded quantum communication. We show that these classes are
incomparable and capture different facets of quantum state transformations.

</details>


### [124] [Adaptive Sparsification for Linear Programming](https://arxiv.org/abs/2510.08348)
*Étienne Objois,Adrian Vladu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a generic framework for solving linear programs (LPs) with many
constraints $(n \gg d)$ via adaptive sparsification. Our approach provides a
principled generalization of the techniques of [Assadi '23] from matching
problems to general LPs and robustifies [Clarkson's '95] celebrated algorithm
for the exact setting. The framework reduces LP solving to a sequence of calls
to a ``low-violation oracle'' on small, adaptively sampled subproblems, which
we analyze through the lens of the multiplicative weight update method.
  Our main results demonstrate the versatility of this paradigm. First, we
present a quantum version of Clarkson's algorithm that finds an exact solution
to an LP using $\tilde{O}(\sqrt{n} d^3)$ row-queries to the constraint matrix.
This is achieved by accelerating the classical bottleneck (the search for
violated constraints) with a generalization of Grover search, decoupling the
quantum component from the classical solver. Second, our framework yields new
state-of-the-art algorithms for mixed packing and covering problems when the
packing constraints are ``simple''. By retaining all packing constraints while
sampling only from the covering constraints, we achieve a significant width
reduction, leading to faster solvers in both the classical and quantum query
models. Our work provides a modular and powerful approach for accelerating LP
solvers.

</details>


### [125] [Selective high-order topological states and tunable chiral emission in atomic metasurfaces](https://arxiv.org/abs/2510.08349)
*Yi-Xin Wang,Yan Zhang,Lei Du,Lingzhen Guo,Jin-Hui Wu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Atomic metasurfaces (AMs) provide a powerful nanophotonic platform for
integrating topological effects into quantum many-body systems. In this Letter,
we investigate the quantum optical and topological properties of a
two-dimensional Kagome AM, going beyond the tight-binding approximation and
incorporating all-to-all interactions. We reveal selective higher-order
topological states with a unique dynamical ``chasing" behavior, protected by a
generalized chiral symmetry and enabling efficient topological directional
transfer. By introducing an impurity atom -- a giant atom -- coupled to all
array atoms, we observe chiral emission patterns strongly dependent on the
atomic polarization. This nonlocal coupling structure allows exploration of
self-interference effects at subwavelength scales. Our findings establish AMs
as a versatile platform for engineering tunable topological states and chiral
quantum optical phenomena, with potential applications in customized light
sources and photonic devices.

</details>


### [126] [Entanglement of mechanical oscillators mediated by a Rydberg tweezer chain](https://arxiv.org/abs/2510.08371)
*Cedric Wind,Chris Nill,Julia Gamper,Samuel Germer,Valerie Mauth,Wolfgang Alt,Igor Lesanovsky,Sebastian Hofferberth*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mechanical systems provide a unique test bed for studying quantum phenomena
at macroscopic length scales. However, realizing quantum states that feature
quantum correlations among macroscopic mechanical objects remains an
experimental challenge. Here, we propose a quantum system in which two
micro-electromechanical oscillators interact through a chain of Rydberg atoms
confined in optical tweezers. We demonstrate that the coherent dynamics of the
system generate entanglement between the oscillators. Furthermore, we utilize
the tunability of the radiative decay of the Rydberg atoms for dissipative
entanglement generation. Our results highlight the potential to exploit the
flexibility and tunability of Rydberg atom chains to generate nonclassical
correlations between distant mechanical oscillators.

</details>


### [127] [Uniform mixing in continuous-time quantum walks on oriented, nonabelian Cayley graphs](https://arxiv.org/abs/2510.08376)
*Peter Sin*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A family of oriented, normal, nonabelian Cayley graphs is presented, whose
continuous-time quantum walks exhibit uniform mixing.

</details>


### [128] [Optimal quantum spectroscopy using single-photon pulses](https://arxiv.org/abs/2510.08386)
*Sourav Das,Aiman Khan,Francesco Albarelli,Animesh Datta*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We provide the ultimate precision attainable in spectroscopy of a quantum
emitter using single-photon pulses. We find the maximum for estimating the
linewidth to be independent of the details of the emitter's bare Hamiltonian
while that for the detunings not to be so. We also identify optimal pulse
shapes attaining these precisions.

</details>


### [129] [Emergent continuous symmetry and ground-state factorization induced by long-range interactions](https://arxiv.org/abs/2510.08391)
*Yue Yu,Myung-Joong Hwang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The spontaneous breaking of a $Z_2$ symmetry typically gives rise to emergent
excitations possessing the same symmetry with a renormalized mass. Contrary to
this conventional wisdom, we present a theory in which the low-lying excitation
in the broken-symmetry phase acquires a continuous symmetry, even when the
underlying symmetry of the system is discrete. In the presence of anisotropic
long-range interactions, the order parameter renormalizes the relative strength
of the particle-conserving and particle-nonconserving interactions. When one of
the two renormalized interactions vanishes, a conservation law absent in the
original Hamiltonian emerges, giving rise to a continuous symmetry. A striking
consequence of the emergent continuous symmetry and conservation law is that it
constrains quantum correlations in the ground-state to be zero, leading to the
ground-state factorization in the presence of strong interactions. Our finding
is a universal feature of quantum phase transitions in fully-connected systems
and in their lattice generalizations; therefore, it can be observed in a wide
range of physical systems.

</details>


### [130] [Classical Obfuscation of Quantum Circuits via Publicly-Verifiable QFHE](https://arxiv.org/abs/2510.08400)
*James Bartusek,Aparna Gupte,Saachi Mutreja,Omri Shmueli*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A classical obfuscator for quantum circuits is a classical program that,
given the classical description of a quantum circuit $Q$, outputs the classical
description of a functionally equivalent quantum circuit $\hat{Q}$ that hides
as much as possible about $Q$. Previously, the only known feasibility result
for classical obfuscation of quantum circuits (Bartusek and Malavolta, ITCS
2022) was limited to circuits that always reject. On the other hand, if the
obfuscator is allowed to compile the quantum circuit $Q$ into a quantum state
$|\hat{Q}\rangle$, there exist feasibility results for obfuscating all
pseudo-deterministic quantum circuits (Bartusek, Kitagawa, Nishimaki and
Yamakawa, STOC 2023, Bartusek, Brakerski and Vaikuntanathan, STOC 2024), and
all unitaries (Huang and Tang, FOCS 2025).
  We show that (relative to a classical oracle) there exists a classical
obfuscator for all pseudo-deterministic quantum circuits. We do this by giving
the first construction of a compact quantum fully-homomorphic encryption (QFHE)
scheme that supports public verification of (pseudo-deterministic) quantum
evaluation, relative to a classical oracle.
  To construct our QFHE scheme, we improve on the approach of Bartusek,
Kitagawa, Nishimaki and Yamakawa (STOC 2023), which required ciphertexts that
are both quantum and non-compact due to the use of quantum coset states and
their publicly-verifiable properties. We introduce new techniques for analyzing
coset states that can be generated ''on the fly'', by proving new cryptographic
properties of the one-shot signature scheme of Shmueli and Zhandry (CRYPTO
2025). Our techniques allow us to produce QFHE ciphertexts that are purely
classical, compact, and publicly-verifiable. This also yields the first
classical verification of quantum computation protocol for BQP that
simultaneously satisfies blindness and public-verifiability.

</details>


### [131] [Universal Fault Tolerance with Non-Transversal Clifford Gates](https://arxiv.org/abs/2510.08402)
*Benjamin Anker,Milad Marvian*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a scheme for the fault-tolerant implementation of arbitrary
Clifford circuits. To achieve this, we extend previous work on flag gadgets for
syndrome extraction to a general framework that flags any Clifford circuit.
This framework opens new pathways toward universal fault tolerance by allowing
transversal implementation of $T$ gates alongside fault-tolerant realization of
selected non-transversal Clifford gates using flags. The construction we
present allows a Clifford circuit consisting of $n$ two-qubit gates and $O(n)$
single-qubit gates acting upon physical qubits in a code of distance $d$ to be
made fault tolerant to distance $d$ using $O(d^2 \log(nd^2\log n))$ ancilla
qubits and $O(nd^2 \log(nd^2 \log n))$ extra CNOTs. Beyond asymptotic analysis,
we demonstrate our construction by implementing the non-transversal logical
Hadamard gate for the [[15,1,3]] code, which has transversal T, and compare to
alternative approaches for universality using this code. We also apply our
construction to magic-state preparation, general state preparation using
Clifford circuits, and data-syndrome codes.

</details>


### [132] [A Quantum Time-Space Tradeoff for Directed $st$-Connectivity](https://arxiv.org/abs/2510.08403)
*Stacey Jeffery,Galina Pass*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Directed $st$-connectivity (DSTCON) is the problem of deciding if there
exists a directed path between a pair of distinguished vertices $s$ and $t$ in
an input directed graph. This problem appears in many algorithmic applications,
and is also a fundamental problem in complexity theory, due to its ${\sf
NL}$-completeness. We show that for any $S\geq \log^2(n)$, there is a quantum
algorithm for DSTCON using space $S$ and time $T\leq
2^{\frac{1}{2}\log(n)\log(n/S)+o(\log^2(n))}$, which is an (up to quadratic)
improvement over the best classical algorithm for any $S=o(\sqrt{n})$. Of the
$S$ total space used by our algorithm, only $O(\log^2(n))$ is quantum space -
the rest is classical. This effectively means that we can trade off classical
space for quantum time.

</details>


### [133] [Routed Bell tests with arbitrarily many local parties](https://arxiv.org/abs/2510.08405)
*Gereon Koßmann,Mario Berta,René Schwonnek*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Device-independent quantum key distribution (DIQKD) promises cryptographic
security based solely on observed quantum correlations, yet its implementation
over long distances remains limited by the detection-efficiency loophole.
Routed Bell tests have recently re-emerged as a promising strategy to mitigate
this limitation by enabling local self-testing of one party's device. However,
extending this idea to self-testing both communicating parties has remained
unclear. Here, we introduce a modified setup that enables local self-tests for
both Alice and Bob and analyze its security against potential attacks.
Employing modern tools from robust self-testing, we show that in a BB84-type
protocol between the self-tested devices, the achievable key rate varies
continuously with the winning probability of the local tests. In particular, we
find that perfect local Bell tests can, in principle, overcome the
detection-efficiency barrier, rendering the asymptotic key rate limited only by
standard bit-flip errors, as in the device-dependent case.

</details>


### [134] [Dynamical error reshaping for dual-rail erasure qubits](https://arxiv.org/abs/2510.08416)
*Filippos Dakis,Sophia E. Economou,Edwin Barnes*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Erasure qubits -- qubits designed to have an error profile that is dominated
by detectable leakage errors -- are a promising way to cut down the resources
needed for quantum error correction. There have been several recent experiments
demonstrating erasure qubits in superconducting quantum processors, most
notably the dual-rail qubit defined by the one-photon subspace of two coupled
cavities. An outstanding challenge is that the ancillary transmons needed to
facilitate erasure checks and two-qubit gates introduce a substantial amount of
noise, limiting the benefits of working with erasure-biased qubits. Here, we
show how to suppress the adverse effects of transmon-induced noise while
performing erasure checks or two-qubit gates. We present control schemes for
these operations that suppress erasure check errors by two orders of magnitude
and reduce the logical two-qubit gate infidelities by up to three orders of
magnitude.

</details>


### [135] [A resource theory of gambling](https://arxiv.org/abs/2510.08418)
*Maite Arcos,Renato Renner,Jonathan Oppenheim*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Betting games provide a natural setting to capture how information yields
strategic advantage. The Kelly criterion for betting, long a cornerstone of
portfolio theory and information theory, admits an interpretation in the limit
of infinitely many repeated bets. We extend Kelly's seminal result into the
single-shot and finite-betting regimes, recasting it as a resource theory of
adversarial information. This allows one to quantify what it means for the
gambler to have more information than the odds-maker. Given a target rate of
return, after a finite number of bets, we compute the optimal strategy which
maximises the probability of successfully reaching the target, revealing a
risk-reward trade-off characterised by a hierarchy of R\'enyi divergences
between the true distribution and the odds. The optimal strategies in the
one-shot regime coincide with strategies maximizing expected utility, and
minimising hypothesis testing errors, thereby bridging economic and
information-theoretic viewpoints. We then generalize this framework to a
distributed side-information game, in which multiple players observe correlated
signals about an unknown state. Recasting gambling as an adversarial resource
theory provides a unifying lens that connects economic and
information-theoretic perspectives, and allows for generalisation to the
quantum domain, where quantum side-information and entanglement play analogous
roles.

</details>


### [136] [Continuous Variable Hamiltonian Learning at Heisenberg Limit via Displacement-Random Unitary Transformation](https://arxiv.org/abs/2510.08419)
*Xi Huang,Lixing Zhang,Di Luo*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Characterizing the Hamiltonians of continuous-variable (CV) quantum systems
is a fundamental challenge laden with difficulties arising from
infinite-dimensional Hilbert spaces and unbounded operators. Existing protocols
for achieving the Heisenberg limit precision are often restricted to specific
Hamiltonian structures or demand experimentally challenging resources. In this
work, we introduce an efficient and experimentally accessible protocol, the
Displacement-Random Unitary Transformation (D-RUT), that learns the
coefficients of general, arbitrary finite-order bosonic Hamiltonians with a
total evolution time scaling as $O(1/\epsilon)$ for a target precision
$\epsilon$ robust to SPAM error. For multi-mode systems, we develop a
hierarchical coefficients recovering strategy with superior statistical
efficiency. Furthermore, we extend our protocol to first quantization, enabling
the learning of fundamental physical parameters from Hamiltonians expressed in
position and momentum operators at the Heisenberg limit.

</details>


### [137] [Computational Bell Inequalities](https://arxiv.org/abs/2510.08423)
*Ilya Merkulov,Rotem Arnon*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a systematic approach for analyzing device-independent
single-prover interactive protocols under computational assumptions. This is
done by establishing an explicit correspondence with Bell inequalities and
nonlocal games and constructing a computational space of correlations.We show
how computational assumptions are converted to computational Bell inequalities,
in their rigorous mathematical sense, a hyperplane that separates the sets of
classical and quantum verifier-prover interactions. We reveal precisely how the
nonsignaling assumption in standard device-independent setups interchanges with
the computational challenge of learning a hidden input (that we define). We
further utilize our fundamental results to study explicit protocols using the
new perspective. We take advantage of modular tools for studying nonlocality,
deriving tighter Tsirelson bounds for single-prover protocols and bounding the
entropy generated in the interaction, improving on previous results. Our work
thus establishes a modular approach to analyzing single-prover quantum
certification protocols based on computational assumptions through the
fundamental lens of Bell inequalities, removing many layers of technical
overhead. The link that we draw between single-prover protocols and Bell
inequalities goes far beyond the spread intuitive understanding or known
results about "compiled nonlocal games"; Notably, it captures the exact way in
which the correspondence between computational assumptions and locality should
be understood also in protocols based on, e.g., trapdoor claw-free functions
(in which there is no clear underlying nonlocal game).

</details>


### [138] [A convergent hierarchy of spectral gap certificates for qubit Hamiltonians](https://arxiv.org/abs/2510.08427)
*Sujit Rao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We give a convergent hierarchy of SDP certificates for bounding the spectral
gap of local qubit Hamiltonians from below. Our approach is based on the NPA
hierarchy applied to a polynomially-sized system of constraints defining the
universal enveloping algebra of the Lie algebra $\mathfrak{su}(2^{n})$, as well
as additional constraints which put restrictions on the corresponding
representations of the algebra. We also use as input an upper bound on the
ground state energy, either using a hierarchy introduced by Fawzi, Fawzi, and
Scalet, or an analog for qubit Hamiltonians of the Lasserre hierarchy of upper
bounds introduced by Klep, Magron, Mass\'{e}, and Vol\v{c}i\v{c}. The
convergence of the certificates does not require that the Hamiltonian be
frustration-free.
  We prove that the resulting certificates have polynomial size at fixed degree
and converge asymptotically (in fact, at level $n$), by showing that all
allowed representations of the algebra correspond to the second exterior power
$\wedge^2(\mathbb{C}^{2^n})$, which encodes the sum of the two smallest
eigenvalues of the original Hamiltonian. We also give an example showing that
for a commuting 1-local Hamiltonian, the hierarchy certifies a nontrivial lower
bound on the spectral gap.

</details>


### [139] [Parallel Spooky Pebbling Makes Regev Factoring More Practical](https://arxiv.org/abs/2510.08432)
*Gregory D. Kahanamoku-Meyer,Seyoon Ragavan,Katherine Van Kirk*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: "Pebble games," an abstraction from classical reversible computing, have
found use in the design of quantum circuits for inherently sequential tasks.
Gidney showed that allowing Hadamard basis measurements during pebble games can
dramatically improve costs -- an extension termed "spooky pebble games" because
the measurements leave temporary phase errors called ghosts. In this work, we
define and study parallel spooky pebble games. Previous work by Blocki, Holman,
and Lee (TCC 2022) and Gidney studied the benefits offered by either
parallelism or spookiness individually; here we show that these resources can
yield impressive gains when used together. First, we show by construction that
a line graph of length $\ell$ can be pebbled in depth $2\ell$ (which is exactly
optimal) using space $\leq 2.47\log \ell$. Then, to explore pebbling schemes
using even less space, we use a highly optimized $A^*$ search implemented in
Julia to find the lowest-depth parallel spooky pebbling possible for a range of
concrete line graph lengths $\ell$ given a constant number of pebbles $s$.
  We show that these techniques can be applied to Regev's factoring algorithm
(Journal of the ACM 2025) to significantly reduce the cost of its arithmetic.
For example, we find that 4096-bit integers $N$ can be factored in
multiplication depth 193, which outperforms the 680 required of previous
variants of Regev and the 444 reported by Eker{\aa} and G\"artner for Shor's
algorithm (IACR Communications in Cryptology 2025). While space-optimized
implementations of Shor's algorithm remain likely the best candidates for first
quantum factorization of large integers, our results show that Regev's
algorithm may have practical importance in the future, especially given the
possibility of further optimization. Finally, we believe our pebbling
techniques will find applications in quantum cryptanalysis beyond integer
factorization.

</details>


### [140] [Code Swendsen-Wang Dynamics](https://arxiv.org/abs/2510.08446)
*Dominik Hangleiter,Nathan Ju,Umesh Vazirani*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: An important open question about Markov chains for preparing quantum Gibbs
states is proving rapid mixing. However, rapid mixing at low temperatures has
only been proven for Gibbs states with no thermally stable phases, e.g., the 2D
toric code. Inspired by Swendsen-Wang dynamics, in this work we give a simple
Markov chain, Code Swendsen-Wang dynamics, for preparing Gibbs states of
commuting Hamiltonians. We prove rapid mixing of this chain for classes of
quantum and classical Hamiltonians with thermally stable phases, including the
4D toric code, at any temperature. We conjecture its efficiency for all code
Hamiltonians away from first-order phase transition points.

</details>


### [141] [Unifying Quantum Smoothing Theories with Extended Retrodiction](https://arxiv.org/abs/2510.08447)
*Mingxuan Liu,Ge Bai,Valerio Scarani*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Estimating the state of an open quantum system monitored over time requires
incorporating information from past measurements (filtering) and, for improved
accuracy, also from future measurements (smoothing). While classical smoothing
is well-understood within Bayesian framework, its quantum generalization has
been challenging, leading to distinct and seemingly incompatible approaches. In
this work, we resolve this conceptual divide by developing a comprehensive
retrodictive framework for quantum state smoothing. We demonstrate that
existing theories are special cases within our formalism, corresponding to
different extended prior beliefs. Our theory unifies the field and naturally
extends it to a broader class of scenarios. We also explore the behavior of
updates when using different priors with the same marginal and prove that the
upper and lower bounds on average entropy of smoothed states are achieved by
the Petz-Fuchs smoothed state and the CLHS smoothed state, respectively. Our
results establish that quantum state smoothing is fundamentally a retrodictive
process, finally bringing it into a closer analogy with classical smoothing.

</details>


### [142] [Non-Clifford Gates are Required for Long-Term Memory](https://arxiv.org/abs/2510.08451)
*Jon Nelson,Joel Rajakumar,Michael J. Gullans*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We show that all Clifford circuits under interspersed depolarizing noise lose
memory of their input exponentially quickly, even when given access to a
constant supply of fresh qubits in arbitrary states. This is somewhat
surprising given the result of Aharonov et al. [STOC1997] which gives a
fault-tolerant protocol for general quantum circuits using a supply of fresh
qubits. Our result shows that such a protocol is impossible using only Clifford
gates demonstrating that non-Clifford gates are fundamentally required to store
information for long periods of time.

</details>


### [143] [Wavefunction Flows: Efficient Quantum Simulation of Continuous Flow Models](https://arxiv.org/abs/2510.08462)
*David Layden,Ryan Sweke,Vojtěch Havlíček,Anirban Chowdhury,Kirill Neklyudov*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flow models are a cornerstone of modern machine learning. They are generative
models that progressively transform probability distributions according to
learned dynamics. Specifically, they learn a continuous-time Markov process
that efficiently maps samples from a simple source distribution into samples
from a complex target distribution. We show that these models are naturally
related to the Schr\"odinger equation, for an unusual Hamiltonian on continuous
variables. Moreover, we prove that the dynamics generated by this Hamiltonian
can be efficiently simulated on a quantum computer. Together, these results
give a quantum algorithm for preparing coherent encodings (a.k.a., qsamples)
for a vast family of probability distributions--namely, those expressible by
flow models--by reducing the task to an existing classical learning problem,
plus Hamiltonian simulation. For statistical problems defined by flow models,
such as mean estimation and property testing, this enables the use of quantum
algorithms tailored to qsamples, which may offer advantages over classical
algorithms based only on samples from a flow model. More broadly, these results
reveal a close connection between state-of-the-art machine learning models,
such as flow matching and diffusion models, and one of the main expected
capabilities of quantum computers: simulating quantum dynamics.

</details>


### [144] [Approximating quantum states by states of low rank](https://arxiv.org/abs/2510.08463)
*Nathaniel Johnston,Chi-Kwong Li*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Given a positive integer k, it is natural to ask for a formula for the
distance between a given density matrix (i.e., mixed quantum state) and the set
of density matrices of rank at most k. This problem has already been solved
when "distance" is measured in the trace or Frobenius norm. We solve it for all
other unitary similarity invariant norms. We also present some consequences of
our formula. For example, in the trace and Frobenius norms, the density matrix
that is farthest from the set of low-rank density matrices is the maximally
mixed state, but this is not true in many other unitary similarity invariant
norms.

</details>


### [145] [Stability of digital and analog quantum simulations under noise](https://arxiv.org/abs/2510.08467)
*Jayant Rao,Jens Eisert,Tommaso Guaita*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum simulation is a central application of near-term quantum devices,
pursued in both analog and digital architectures. A key challenge for both
paradigms is the effect of imperfections and noise on predictive power. In this
work, we present a rigorous and physically transparent comparison of the
stability of digital and analog quantum simulators under a variety of
perturbative noise models. We provide rigorous worst- and average-case error
bounds for noisy quantum simulation of local observables. We find that the two
paradigms show comparable scaling in the worst case, while exhibiting different
forms of enhanced error cancellation on average. We further analyze Gaussian
and Brownian noise processes, deriving concentration bounds that capture
typical deviations beyond worst-case guarantees. These results provide a
unified framework for quantifying the robustness of noisy quantum simulations
and identify regimes where digital methods have intrinsic advantages and when
we can see similar behavior.

</details>


### [146] [Platform-Agnostic Modular Architecture for Quantum Benchmarking](https://arxiv.org/abs/2510.08469)
*Neer Patel,Anish Giri,Hrushikesh Pramod Patil,Noah Siekierski,Avimita Chatterjee,Sonika Johri,Timothy Proctor,Thomas Lubinski,Siyuan Niu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a platform-agnostic modular architecture that addresses the
increasingly fragmented landscape of quantum computing benchmarking by
decoupling problem generation, circuit execution, and results analysis into
independent, interoperable components. Supporting over 20 benchmark variants
ranging from simple algorithmic tests like Bernstein-Vazirani to complex
Hamiltonian simulation with observable calculations, the system integrates with
multiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse
workflows. We validate the architecture through successful integration with
Sandia's $\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for
multi-GPU HPC simulations. Extensibility of the system is demonstrated by
implementing dynamic circuit variants of existing benchmarks and a new quantum
reinforcement learning benchmark, which become readily available across
multiple execution and analysis modes. Our primary contribution is identifying
and formalizing modular interfaces that enable interoperability between
incompatible benchmarking frameworks, demonstrating that standardized
interfaces reduce ecosystem fragmentation while preserving optimization
flexibility. This architecture has been developed as a key enhancement to the
continually evolving QED-C Application-Oriented Performance Benchmarks for
Quantum Computing suite.

</details>


### [147] [Learning Coulomb Potentials and Beyond with Fermions in Continuous Space](https://arxiv.org/abs/2510.08471)
*Andreas Bluhm,Marius Lemm,Tim Möbus,Oliver Siebert*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a modular algorithm for learning external potentials in
continuous-space free-fermion models including Coulomb potentials in any
dimension. Compared to the lattice-based approaches, the continuum presents new
mathematical challenges: the state space is infinite-dimensional and the
Hamiltonian contains the Laplacian, which is unbounded in the continuum and
thus produces an unbounded speed of information propagation. Our framework
addresses these difficulties through novel optimization methods or
information-propagation bounds in combination with a priori regularity
assumptions on the external potential. The resulting algorithm provides a
unified and robust approach that covers both Coulomb interactions and other
classes of physically relevant potentials. One possible application is the
characterization of charge and position of nuclei and ions in quantum
chemistry. Our results thus lay the foundation for a scalable and generalizable
toolkit to explore fermionic systems governed by continuous-space interactions.

</details>


### [148] [Agnostic Product Mixed State Tomography via Robust Statistics](https://arxiv.org/abs/2510.08472)
*Alvan Arulandu,Ilias Diakonikolas,Daniel Kane,Jerry Li*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the problem of agnostic tomography with \emph{mixed state}
ansatz, and specifically, the natural ansatz class of product mixed states. In
more detail, given $N$ copies of an $n$-qubit state $\rho$ which is
$\epsilon$-close to a product mixed state $\pi$, the goal is to output a
nearly-optimal product mixed state approximation to $\rho$. While there has
been a flurry of recent work on agnostic tomography, prior work could only
handle pure state ansatz, such as product states or stabilizer states. Here we
give an algorithm for agnostic tomography of product mixed states which finds a
product state which is $O(\epsilon \log 1 / \epsilon)$ close to $\rho$ which
uses polynomially many copies of $\rho$, and which runs in polynomial time.
Moreover, our algorithm only uses single-qubit, single-copy measurements. To
our knowledge, this is the first efficient algorithm that achieves any
non-trivial agnostic tomography guarantee for any class of mixed state ansatz.
  Our algorithm proceeds in two main conceptual steps, which we believe are of
independent interest. First, we demonstrate a novel, black-box efficient
reduction from agnostic tomography of product mixed states to the classical
task of \emph{robustly learning binary product distributions} -- a textbook
problem in robust statistics. We then demonstrate a nearly-optimal efficient
algorithm for the classical task of robustly learning a binary product,
answering an open problem in the literature. Our approach hinges on developing
a new optimal certificate of closeness for binary product distributions that
can be leveraged algorithmically via a carefully defined convex relaxation.
Finally, we complement our upper bounds with a lower bound demonstrating that
adaptivity is information-theoretically necessary for our agnostic tomography
task, so long as the algorithm only uses single-qubit two-outcome projective
measurements.

</details>


### [149] [An Improved Quantum Algorithm for 3-Tuple Lattice Sieving](https://arxiv.org/abs/2510.08473)
*Lynn Engelberts,Yanlin Chen,Amin Shiraz Gilani,Maya-Iggy van Hoof,Stacey Jeffery,Ronald de Wolf*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The assumed hardness of the Shortest Vector Problem in high-dimensional
lattices is one of the cornerstones of post-quantum cryptography. The fastest
known heuristic attacks on SVP are via so-called sieving methods. While these
still take exponential time in the dimension $d$, they are significantly faster
than non-heuristic approaches and their heuristic assumptions are verified by
extensive experiments. $k$-Tuple sieving is an iterative method where each
iteration takes as input a large number of lattice vectors of a certain norm,
and produces an equal number of lattice vectors of slightly smaller norm, by
taking sums and differences of $k$ of the input vectors. Iterating these
''sieving steps'' sufficiently many times produces a short lattice vector. The
fastest attacks (both classical and quantum) are for $k=2$, but taking larger
$k$ reduces the amount of memory required for the attack. In this paper we
improve the quantum time complexity of 3-tuple sieving from $2^{0.3098 d}$ to
$2^{0.2846 d}$, using a two-level amplitude amplification aided by a
preprocessing step that associates the given lattice vectors with nearby
''center points'' to focus the search on the neighborhoods of these center
points. Our algorithm uses $2^{0.1887d}$ classical bits and QCRAM bits, and
$2^{o(d)}$ qubits. This is the fastest known quantum algorithm for SVP when
total memory is limited to $2^{0.1887d}$.

</details>


### [150] [High-Sensitivity Optical Detection of Electron-Nuclear Spin Clusters in Diamond](https://arxiv.org/abs/2510.08474)
*Louis Chambard,Alrik Durand,Julien Voisin,Maxime Perdriat,Vincent Jacques,Gabriel Hétet*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We perform sensitive nuclear magnetic resonance (NMR) with spin ensembles
which are polarized by nitrogen vacancy centers (NV centers) in diamond at
room-temperature. With a near shot-noise-limited photoluminescence detection
and a highly uniform magnetic field, we resolve sharp NMR features arising from
multiple spin clusters. In particular, we investigate the coupling between
nuclear spins and NV centers in the neutral and negatively charged states.
Further, we perform high precision NMR and coherent control of families of
carbon 13 nuclear spin ensembles in the $m_s$=0 level of the NV ground state.
Applying an off-axis magnetic field reveals the various sites associated with
the otherwise degenerate couplings of the carbon 13 sites around the NV
electronic spin providing access to all the hyperfine tensor components. Last,
we observe spectroscopic signatures of pairs of nuclear spins coupled to the
same NV center. These results are relevant for ensemble measurements of
dynamical polarization that currently rely on expensive nuclear magnetic
resonance systems as well as for recently proposed nuclear spin gyroscopes.

</details>


### [151] [Universality and kernel-adaptive training for classically trained, quantum-deployed generative models](https://arxiv.org/abs/2510.08476)
*Andrii Kurkin,Kevin Shen,Susanne Pielawa,Hao Wang,Vedran Dunjko*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The instantaneous quantum polynomial (IQP) quantum circuit Born machine
(QCBM) has been proposed as a promising quantum generative model over
bitstrings. Recent works have shown that the training of IQP-QCBM is
classically tractable w.r.t. the so-called Gaussian kernel maximum mean
discrepancy (MMD) loss function, while maintaining the potential of a quantum
advantage for sampling itself. Nonetheless, the model has a number of aspects
where improvements would be important for more general utility: (1) the basic
model is known to be not universal - i.e. it is not capable of representing
arbitrary distributions, and it was not known whether it is possible to achieve
universality by adding hidden (ancillary) qubits; (2) a fixed Gaussian kernel
used in the MMD loss can cause training issues, e.g., vanishing gradients. In
this paper, we resolve the first question and make decisive strides on the
second. We prove that for an $n$-qubit IQP generator, adding $n + 1$ hidden
qubits makes the model universal. For the latter, we propose a kernel-adaptive
training method, where the kernel is adversarially trained. We show that in the
kernel-adaptive method, the convergence of the MMD value implies weak
convergence in distribution of the generator. We also analytically analyze the
limitations of the MMD-based training method. Finally, we verify the
performance benefits on the dataset crafted to spotlight improvements by the
suggested method. The results show that kernel-adaptive training outperforms a
fixed Gaussian kernel in total variation distance, and the gap increases with
the dataset dimensionality. These modifications and analyses shed light on the
limits and potential of these new quantum generative methods, which could offer
the first truly scalable insights in the comparative capacities of classical
versus quantum models, even without access to scalable quantum computers.

</details>


### [152] [Completeness for Fault Equivalence of Clifford ZX Diagrams](https://arxiv.org/abs/2510.08477)
*Maximilian Rüsch,Benjamin Rodatz,Aleks Kissinger*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Two circuits are considered to be equivalent under noise if the effect of
faults on one circuit is no worse than the effect of faults on the other
circuit. We call this relationship fault equivalence. Fault equivalence offers
a way to transform circuits while provably preserving their fault-tolerant
properties, enabling a framework for fault-tolerant circuit synthesis and
optimisation that is correct by construction. The ZX calculus, a set of
graphical rewrite rules for quantum computations, provides a useful tool for
manipulating circuits while preserving fault equivalence. For this, the usual
set of ZX rewrites has to be restricted to not only preserve the underlying
linear map represented by the diagram but also fault equivalence.
  In this work, we provide a set of ZX rewrites that are sound and complete for
fault equivalence of Clifford ZX diagrams. This means that any equivalence that
can be derived using the proposed rules is certain to be correct, and any
correct equivalence can be derived using only these rules. For this, we utilise
diagrammatic constructions called fault gadgets to reason about arbitrary,
possibly correlated Pauli faults in ZX diagrams. Fault gadgets allow us to
separate the diagram into a fault-free part, which captures the noise-free
behaviour of a diagram, and a noisy part that enumerates the effects of all
possible faults. Using this, we provide a unique normal form for ZX diagrams
under noise and show that any diagram can be brought into this normal form
using our proposed rule set.

</details>


### [153] [Guess your neighbor's input: Quantum advantage in Feige's game](https://arxiv.org/abs/2510.08484)
*Simon Schmidt,Sigurd A. L. Storgaard,Michael Walter,Yuming Zhao*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this article, we study a nonlocal game with two questions and three
answers per player, which was first considered by Feige in 1991, and show that
there is quantum advantage in this game. We prove that the game is a robust
self-test for the $3$-dimensional maximally entangled state. Furthermore, we
show that the game can be seen as the "or" of two games that each do not have
quantum advantage. Lastly, we investigate the behavior of the game with respect
to parallel repetition in the classical, quantum and non-signalling case and
obtain perfect parallel repetition of the non-signalling value if Feige's game
is repeated an even amount of times.

</details>


### [154] [Quartic quantum speedups for community detection](https://arxiv.org/abs/2510.08494)
*Alexander Schmidhuber,Alexander Zlokapa*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Community detection is a foundational problem in data science. Its natural
extension to hypergraphs captures higher-order correlations beyond pairwise
interactions. In this work, we develop a quantum algorithm for hypergraph
community detection that achieves a quartic quantum speedup over the best known
classical algorithm, along with superpolynomial savings in space. Our algorithm
is based on the Kikuchi method, which we extend beyond previously considered
problems such as Tensor PCA and $p$XORSAT to a broad family of generalized
stochastic block models. To demonstrate (near) optimality of this method, we
prove matching lower bounds (up to logarithmic factors) in the low-degree
framework, showing that the algorithm saturates a smooth
statistical-computational tradeoff. The quantum speedup arises from a quantized
version of the Kikuchi method and is based on the efficient preparation of a
guiding state correlated with the underlying community structure. Our work
suggests that prior quantum speedups using the Kikuchi method are sufficiently
robust to encompass a broader set of problems than previously believed; we
conjecture that a quantity known as marginal order characterizes the existence
of these quantum speedups.

</details>


### [155] [Compiling Any $\mathsf{MIP}^{*}$ into a (Succinct) Classical Interactive Argument](https://arxiv.org/abs/2510.08495)
*Andrew Huang,Yael Tauman Kalai*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a generic compiler that converts any $\mathsf{MIP}^{*}$ protocol
into a succinct interactive argument where the communication and the verifier
are classical, and where post-quantum soundness relies on the post-quantum
sub-exponential hardness of the Learning with Errors ($\mathsf{LWE}$) problem.
Prior to this work, such a compiler for $\mathsf{MIP}^{*}$ was given by Kalai,
Lombardi, Vaikuntanathan and Yang (STOC 2022), but the post-quantum soundness
of this compiler is still under investigation.
  More generally, our compiler can be applied to any $\mathsf{QIP}$ protocol
which is sound only against semi-malicious provers that follow the prescribed
protocol, but with possibly malicious initial state. Our compiler consists of
two steps. We first show that if a language $\mathcal{L}$ has a $\mathsf{QIP}$
with semi-malicious soundness, where the prover runs in time $T$, then
$\mathcal{L} \in \mathsf{QMATIME}(T)$. Then we construct a succinct classical
argument for any such language, where the communication complexity grows
polylogarithmically with $T$, under the post-quantum sub-exponential hardness
of $\mathsf{LWE}$.

</details>


### [156] [Learning and certification of local time-dependent quantum dynamics and noise](https://arxiv.org/abs/2510.08500)
*Daniel Stilck França,Tim Möbus,Cambyse Rouzé,Albert H. Werner*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hamiltonian learning protocols are essential tools to benchmark quantum
computers and simulators. Yet rigorous methods for time-dependent Hamiltonians
and Lindbladians remain scarce despite their wide use. We close this gap by
learning the time-dependent evolution of a locally interacting $n$-qubit system
on a graph of effective dimension $D$ using only preparation of product Pauli
eigenstates, evolution under the time-dependent generator for given times, and
measurements in product Pauli bases. We assume the time-dependent parameters
are well approximated by functions in a known space of dimension $m$ admitting
stable interpolation, e.g. by polynomials. Our protocol outputs functions
approximating these coefficients to accuracy $\epsilon$ on an interval with
success probability $1-\delta$, requiring only
$O\big(\epsilon^{-2}poly(m)\log(n\delta^{-1})\big)$ samples and $poly(n,m)$
pre/postprocessing. Importantly, the scaling in $m$ is polynomial, whereas
naive extensions of previous methods scale exponentially. The method estimates
time derivatives of observable expectations via interpolation, yielding
well-conditioned linear systems for the generator's coefficients. The main
difficulty in the time-dependent setting is to evaluate these coefficients at
finite times while preserving a controlled link between derivatives and
dynamical parameters. Our innovation is to combine Lieb-Robinson bounds,
process shadows, and semidefinite programs to recover the coefficients
efficiently at constant times. Along the way, we extend state-of-the-art
Lieb-Robinson bounds on general graphs to time-dependent, dissipative dynamics,
a contribution of independent interest. These results provide a scalable tool
to verify state-preparation procedures (e.g. adiabatic protocols) and
characterize time-dependent noise in quantum devices.

</details>


### [157] [Localizing entanglement in high-dimensional states](https://arxiv.org/abs/2510.08501)
*Christopher Vairogs,Akanksha Chablani,Leo Lee,Hanyang Sha,Abigail Vaughan-Lee,Jacob L. Beckey*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we study the asymptotic behavior of protocols that localize
entanglement in large multi-qubit states onto a subset of qubits by measuring
the remaining qubits. We use the maximal average n-tangle that can be generated
on a fixed subsystem by measuring its complement -- either with local or global
measurements -- as our key figure of merit. These quantities are known
respectively as the localizable entanglement (LE) and the entanglement of
assistance (EA). We build upon the work of [arXiv:2411.04080] that proposed a
polynomial-time test, based on the EA, for whether it is possible to transform
certain graph states into others using local measurements. We show, using
properties of the EA, that this test is effective and useful in large systems
for a wide range of sizes of the measured subsystem. In particular, we use this
test to demonstrate the surprising result that general local unitaries and
global measurements will typically not provide an advantage over the more
experimentally feasible local Clifford unitaries and local Pauli measurements
in transforming large linear cluster states into GHZ states. Finally, we derive
concentration inequalities for the LE and EA over Haar-random states which
indicate that the localized entanglement structure has a striking dependence on
the locality of the measurement. In deriving these concentration inequalities,
we develop several technical tools that may be of independent interest.

</details>


### [158] [The quantum communication power of indefinite causal order](https://arxiv.org/abs/2510.08507)
*Xuanqiang Zhao,Benchi Zhao,Giulio Chiribella*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum theory is in principle compatible with scenarios where physical
processes take place in an indefinite causal order, a possibility that was
shown to yield advantages in several information processing tasks. However,
advantages in communication, the most basic form of information processing,
have so far remained controversial and hard to prove. Here we develop a
framework that can be used to rigorously assess the role of causal order in a
scenario where communication links are built by assembling multiple quantum
devices. In this setting, we establish a clear-cut advantage of indefinite
order in the one-shot transmission of classical messages. On the other hand, we
also show that the advantage is not generic to all communication tasks.
Notably, we find that indefinite order does not offer any advantage over shared
entanglement in the asymptotic scenario where a large number of uses of the
same communication device is employed. Overall, our results unveil non-trivial
relations between communication, causal order, entanglement, and no-signaling
resources in quantum mechanics.

</details>


### [159] [Randomized and quantum approximate matrix multiplication](https://arxiv.org/abs/2510.08509)
*Simon Apers,Arjan Cornelissen,Samson Wang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The complexity of matrix multiplication is a central topic in computer
science. While the focus has traditionally been on exact algorithms, a long
line of literature also considers randomized algorithms, which return an
approximate solution in faster time. In this work, we adopt a unifying
perspective that frames these randomized algorithms in terms of mean
estimation. Using it, we first give refined analyses of classical algorithms
based on random walks by Cohen-Lewis (`99), and based on sketching by Sarl\'os
(`06) and Drineas-Kannan-Mahoney (`06). We then propose an improvement on
Cohen-Lewis that yields a single classical algorithm that is faster than all
the other approaches, if we assume no use of (exact) fast matrix multiplication
as a subroutine. Second, we demonstrate a quantum speedup on top of these
algorithms by using the recent quantum multivariate mean estimation algorithm
by Cornelissen-Hamoudi-Jerbi (`22).

</details>


### [160] [How hard is it to verify a classical shadow?](https://arxiv.org/abs/2510.08515)
*Georgios Karaiskos,Dorian Rudolph,Johannes Jakob Meyer,Jens Eisert,Sevag Gharibian*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Classical shadows are succinct classical representations of quantum states
which allow one to encode a set of properties P of a quantum state rho, while
only requiring measurements on logarithmically many copies of rho in the size
of P. In this work, we initiate the study of verification of classical shadows,
denoted classical shadow validity (CSV), from the perspective of computational
complexity, which asks: Given a classical shadow S, how hard is it to verify
that S predicts the measurement statistics of a quantum state? We show that
even for the elegantly simple classical shadow protocol of [Huang, Kueng,
Preskill, Nature Physics 2020] utilizing local Clifford measurements, CSV is
QMA-complete. This hardness continues to hold for the high-dimensional
extension of said protocol due to [Mao, Yi, and Zhu, PRL 2025]. Among other
results, we also show that CSV for exponentially many observables is complete
for a quantum generalization of the second level of the polynomial hierarchy,
yielding the first natural complete problem for such a class.

</details>


### [161] [Randomized truncation of quantum states](https://arxiv.org/abs/2510.08518)
*Aram W. Harrow,Angus Lowe,Freek Witteveen*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A fundamental task in quantum information is to approximate a pure quantum
state in terms of sparse states or, for a bipartite system, states of bounded
Schmidt rank. The optimal deterministic approximation in each case is
straightforward, and maximizes the fidelity: keep the largest entries or
singular values. On the other hand, random mixtures of sparse states can
achieve quadratically improved trace distances, and yield nontrivial bounds on
other distance measures like the robustness. In this work, we give efficient
algorithms for finding mixtures of sparse states that optimally approximate a
given pure state in either trace distance or robustness. These algorithms also
yield descriptions of efficiently samplable ensembles of sparse, or
less-entangled, states that correspond to these optimal mixed approximations.
This can be used for the truncation step of algorithms for matrix product
states, improving their accuracy while using no extra memory, and we
demonstrate this improvement numerically. Our proofs use basic facts about
convex optimization and zero-sum games, as well as rigorous guarantees for
computing maximum-entropy distributions.

</details>


### [162] [High-Rate Surgery: towards constant-overhead logical operations](https://arxiv.org/abs/2510.08523)
*Guo Zheng,Liang Jiang,Qian Xu*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scalable quantum computation requires not only quantum codes with low memory
overhead but also encoded operations with low space-time overhead. High rate
quantum low-density parity-check (qLDPC) codes address the former by achieving
a high information-encoding rate, yet existing methods for implementing logical
operations often suffer from a low information-processing rate, leading to
substantial space-time costs. Here, we introduce high-rate surgery, a general
scheme that can perform extensive, addressable logical Pauli-product
measurements in parallel on arbitrary qLDPC codes using a shared ancilla
system, attaining nearly constant space-time overhead. We develop both
algebraic and randomized ancilla constructions and demonstrate, using the
$[[144, 12, 12]]$ Gross code and new instances of qLDPC codes (e.g., $[[1125,
245, \leq 10]]$) with encoding rate up to $25\%$, that up to hundreds of
randomly sampled logical measurements can be executed simultaneously with a
total space-time overhead around a factor of two of that of memory experiments.
Our results address a major bottleneck for performing complex, addressable
logical operations on qLDPC codes in practice, advancing the prospect of
scalable, constant-overhead fault-tolerant quantum computation.

</details>


### [163] [Quantum Spin Chains Thermalize at All Temperatures](https://arxiv.org/abs/2510.08533)
*Thiago Bergamaschi,Chi-Fang Chen*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: It is shown that every one-dimensional Hamiltonian with short-range
interaction admits a quantum Gibbs sampler [CKG23] with a system-size
independent spectral gap at all finite temperatures. Consequently, their Gibbs
states can be prepared in polylogarithmic depth, and satisfy exponential
clustering of correlations, generalizing [Ara69].

</details>


### [164] [Quantum Relative Entropy Decay Composition Yields Shallow, Unstructured k-Designs](https://arxiv.org/abs/2510.08537)
*Nicholas Laracuente*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A major line of questions in quantum information and computing asks how
quickly locally random circuits converge to resemble global randomness. In
particular, approximate k-designs are random unitary ensembles that resemble
random circuits up to their first k moments. It was recently shown that on n
qudits, random circuits with slightly structured architectures converge to
k-designs in depth O(log n), even on one-dimensional connectivity. It has
however remained open whether the same shallow depth applies more generally
among random circuit architectures and connectivities, or if the structure is
truly necessary. We recall the study of exponential relative entropy decay,
another topic with a long history in quantum information theory. We show that a
constant number of layers of a parallel random circuit on a family of
architectures including one-dimensional `brickwork' has O(1 / logn) per-layer
multiplicative entropy decay. We further show that on general connectivity
graphs of bounded degree, randomly placed gates achieve O(1 / nlogn)-decay
(consistent with logn depth). Both of these results imply that random circuit
ensembles with O(polylog(n)) depth achieve approximate k-designs in diamond
norm. Hence our results address the question of whether extra structure is
truly necessary for sublinear-depth convergence. Furthermore, the relative
entropy recombination techniques might be of independent interest.

</details>


### [165] [A Dobrushin condition for quantum Markov chains: Rapid mixing and conditional mutual information at high temperature](https://arxiv.org/abs/2510.08542)
*Ainesh Bakshi,Allen Liu,Ankur Moitra,Ewin Tang*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A central challenge in quantum physics is to understand the structural
properties of many-body systems, both in equilibrium and out of equilibrium.
For classical systems, we have a unified perspective which connects structural
properties of systems at thermal equilibrium to the Markov chain dynamics that
mix to them. We lack such a perspective for quantum systems: there is no
framework to translate the quantitative convergence of the Markovian evolution
into strong structural consequences.
  We develop a general framework that brings the breadth and flexibility of the
classical theory to quantum Gibbs states at high temperature. At its core is a
natural quantum analog of a Dobrushin condition; whenever this condition holds,
a concise path-coupling argument proves rapid mixing for the corresponding
Markovian evolution. The same machinery bridges dynamic and structural
properties: rapid mixing yields exponential decay of conditional mutual
information (CMI) without restrictions on the size of the probed subsystems,
resolving a central question in the theory of open quantum systems. Our key
technical insight is an optimal transport viewpoint which couples quantum
dynamics to a linear differential equation, enabling precise control over how
local deviations from equilibrium propagate to distant sites.

</details>


### [166] [Energy, Bosons and Computational Complexity](https://arxiv.org/abs/2510.08545)
*Ulysse Chabaud,Sevag Gharibian,Saeed Mehraban,Arsalan Motamedi,Hamid Reza Naeij,Dorian Rudolph,Dhruva Sambrani*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the role of energy, i.e. average photon number, as a resource
in the computational complexity of bosonic systems. We show three sets of
results: (1. Energy growth rates) There exist bosonic gate sets which increase
energy incredibly rapidly, obtaining e.g. infinite energy in finite/constant
time. We prove these high energies can make computing properties of bosonic
computations, such as deciding whether a given computation will attain infinite
energy, extremely difficult, formally undecidable. (2. Lower bounds on
computational power) More energy ``='' more computational power. For example,
certain gate sets allow poly-time bosonic computations to simulate PTOWER, the
set of deterministic computations whose runtime scales as a tower of
exponentials with polynomial height. Even just exponential energy and $O(1)$
modes suffice to simulate NP, which, importantly, is a setup similar to that of
the recent bosonic factoring algorithm of [Brenner, Caha, Coiteux-Roy and
Koenig (2024)]. For simpler gate sets, we show an energy hierarchy theorem. (3.
Upper bounds on computational power) Bosonic computations with polynomial
energy can be simulated in BQP, ``physical'' bosonic computations with
arbitrary finite energy are decidable, and the gate set consisting of Gaussian
gates and the cubic phase gate can be simulated in PP, with exponential bound
on energy, improving upon the previous PSPACE upper bound. Finally, combining
upper and lower bounds yields no-go theorems for a continuous-variable
Solovay--Kitaev theorem for gate sets such as the Gaussian and cubic phase
gates.

</details>


### [167] [Equivalence of continuous- and discrete-variable gate-based quantum computers with finite energy](https://arxiv.org/abs/2510.08546)
*Alex Maltesson,Ludvig Rodung,Niklas Budinger,Giulia Ferrini,Cameron Calcluth*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We examine the ability of gate-based continuous-variable quantum computers to
outperform qubit or discrete-variable quantum computers. Gate-based
continuous-variable operations refer to operations constructed using a
polynomial sequence of elementary gates from a specific finite set, i.e., those
selected from the set of Gaussian operations and cubic phase gates. Our results
show that for a fixed energy of the system, there is no superpolynomial
computational advantage in using gate-based continuous-variable quantum
computers over discrete-variable ones. The proof of this result consists of
defining a framework - of independent interest - that maps quantum circuits
between the paradigms of continuous- to discrete-variables. This framework
allows us to conclude that a realistic gate-based model of continuous-variable
quantum computers, consisting of states and operations that have a total energy
that is polynomial in the number of modes, can be simulated efficiently using
discrete-variable devices. We utilize the stabilizer subsystem decomposition
[Shaw et al., PRX Quantum 5, 010331] to map continuous-variable states to
discrete-variable counterparts, which allows us to find the error of
approximating continuous-variable quantum computers with discrete-variable ones
in terms of the energy of the continuous-variable system and the dimension of
the corresponding encoding qudits.

</details>


### [168] [Verifiable blind observable estimation: A composably secure protocol for near-term quantum advantage tasks](https://arxiv.org/abs/2510.08548)
*Bo Yang,Elham Kashefi,Harold Ollivier*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid advance of quantum hardware is spotlighting pre-fault-tolerant
tasks that may no longer be efficiently validated by classical means and are
likely to run on potentially untrusted remote quantum servers. This motivates
problem-independent verification protocols with rigorous guarantees. The
Verifiable Blind Quantum Computation (VBQC) protocol provides delegated
computation where the composable security spans the confidentiality and
integrity of the computation.
  However, the success of these cryptographic protocols, especially their low
space overhead, is unfortunately confined to problems that admit an algorithm
whose output can be amplified through majority voting toward the correct
solution. This leaves various notable near-term applications relying on
observable estimation without efficient verification protocols.
  To address these needs, we introduce a protocol implementing Secure Delegated
Observable Estimation (SDOE), which efficiently verifies observable estimation
performed on an untrusted quantum machine. More precisely, it guarantees that
the computed estimate is within some $\epsilon>0$ of the true expectation value
or else it aborts. The required overhead is limited to adding test rounds that
are not more complex than the unprotected computation that needs to be
performed to implement the desired measurement on a given fiducial state; and
in addition, the security error is negligible in the total number of rounds of
the protocol.

</details>


### [169] [Single-Shot Universality in Quantum LDPC Codes via Code-Switching](https://arxiv.org/abs/2510.08552)
*Shi Jie Samuel Tan,Yifan Hong,Ting-Chun Lin,Michael J. Gullans,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Code-switching is a powerful technique in quantum error correction that
allows one to leverage the complementary strengths of different codes to
achieve fault-tolerant universal quantum computation. However, existing
code-switching protocols that encapsulate recent generalized lattice surgery
approaches often either require many rounds of measurements to ensure
fault-tolerance or suffer from low code rates. We present a single-shot,
universal protocol that uses code-switching between high-rate quantum codes to
perform fault-tolerant quantum computation. To our best knowledge, our work
contains the first universal fault-tolerant quantum computation protocol that
achieves what we term single-shot universality on high-rate codes that is
characterized by (i) single-shot error correction, (ii) single-shot state
preparation, as well as (iii) universal logical gates and logical measurements
with constant depth circuits. We achieve this feat with single-shot
code-switching between constant-rate 2D hypergraph product (HGP) codes and
high-rate 3D HGP codes that can be viewed as a generalization of Bombin's
dimensional jump for color codes and Hillmann et al.'s single-shot lattice
surgery for higher-dimensional topological codes. In addition, we prove the
fault-tolerance of our code-switching protocol under both the adversarial and
local-stochastic noise models. We introduce a vastly simpler recipe to
construct high-rate 3D HGP codes with transversal CCZ gates that grants immense
flexibility in the choice of expander graphs and local codes, allowing us to
expand the search space for codes with good parameters and interesting logical
gates. Our work opens an alternative path towards universal fault-tolerant
quantum computation with low space-time overhead by circumventing the need for
magic state distillation.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [170] [Minimizing the Value-at-Risk of Loan Portfolio via Deep Neural Networks](https://arxiv.org/abs/2510.07444)
*Albert Di Wang,Ye Du*

Main category: q-fin.CP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Risk management is a prominent issue in peer-to-peer lending. An investor may
naturally reduce his risk exposure by diversifying instead of putting all his
money on one loan. In that case, an investor may want to minimize the
Value-at-Risk (VaR) or Conditional Value-at-Risk (CVaR) of his loan portfolio.
We propose a low degree of freedom deep neural network model, DeNN, as well as
a high degree of freedom model, DSNN, to tackle the problem. In particular, our
models predict not only the default probability of a loan but also the time
when it will default. The experiments demonstrate that both models can
significantly reduce the portfolio VaRs at different confidence levels,
compared to benchmarks. More interestingly, the low degree of freedom model,
DeNN, outperforms DSNN in most scenarios.

</details>


### [171] [Multi-Agent Analysis of Off-Exchange Public Information for Cryptocurrency Market Trend Prediction](https://arxiv.org/abs/2510.08268)
*Kairan Hong,Jinling Gan,Qiushi Tian,Yanglinxuan Guo,Rui Guo,Runnan Li*

Main category: q-fin.CP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cryptocurrency markets present unique prediction challenges due to their
extreme volatility, 24/7 operation, and hypersensitivity to news events, with
existing approaches suffering from key information extraction and poor sideways
market detection critical for risk management. We introduce a
theoretically-grounded multi-agent cryptocurrency trend prediction framework
that advances the state-of-the-art through three key innovations: (1) an
information-preserving news analysis system with formal theoretical guarantees
that systematically quantifies market impact, regulatory implications, volume
dynamics, risk assessment, technical correlation, and temporal effects using
large language models; (2) an adaptive volatility-conditional fusion mechanism
with proven optimal properties that dynamically combines news sentiment and
technical indicators based on market regime detection; (3) a distributed
multi-agent coordination architecture with low communication complexity
enabling real-time processing of heterogeneous data streams. Comprehensive
experimental evaluation on Bitcoin across three prediction horizons
demonstrates statistically significant improvements over state-of-the-art
natural language processing baseline, establishing a new paradigm for financial
machine learning with broad implications for quantitative trading and risk
management systems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [172] [From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024](https://arxiv.org/abs/2510.07821)
*Raisa M. Simoes,Timoteo Kelly,Eduardo J. Simoes,Praveen Rao*

Main category: cs.SI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper aims to explore two competing data science methodologies to
attempt answering the question, "Which issues contributed most to voters'
choice in the 2024 presidential election?" The methodologies involve novel
empirical evidence driven by artificial intelligence (AI) techniques. By using
two distinct methods based on natural language processing and clustering
analysis to mine over eight thousand user comments on election-related YouTube
videos from one right leaning journal, Wall Street Journal, and one left
leaning journal, New York Times, during pre-election week, we quantify the
frequency of selected issue areas among user comments to infer which issues
were most salient to potential voters in the seven days preceding the November
5th election. Empirically, we primarily demonstrate that immigration and
democracy were the most frequently and consistently invoked issues in user
comments on the analyzed YouTube videos, followed by the issue of identity
politics, while inflation was significantly less frequently referenced. These
results corroborate certain findings of post-election surveys but also refute
the supposed importance of inflation as an election issue. This indicates that
variations on opinion mining, with their analysis of raw user data online, can
be more revealing than polling and surveys for analyzing election outcomes.

</details>


### [173] [Do We Really Need SFT? Prompt-as-Policy over Knowledge Graphs for Cold-start Next POI Recommendation](https://arxiv.org/abs/2510.08012)
*Jinze Wang,Lu Zhang,Yiyang Cui,Zhishu Shen,Xingjun Ma,Jiong Jin,Tiehua Zhang*

Main category: cs.SI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Next point-of-interest (POI) recommendation is crucial for smart urban
services such as tourism, dining, and transportation, yet most approaches
struggle under cold-start conditions where user-POI interactions are sparse.
Recent efforts leveraging large language models (LLMs) address this challenge
through either supervised fine-tuning (SFT) or in-context learning (ICL).
However, SFT demands costly annotations and fails to generalize to inactive
users, while static prompts in ICL cannot adapt to diverse user contexts. To
overcome these limitations, we propose Prompt-as-Policy over knowledge graphs,
a reinforcement-guided prompting framework that learns to construct prompts
dynamically through contextual bandit optimization. Our method treats prompt
construction as a learnable policy that adaptively determines (i) which
relational evidences to include, (ii) the number of evidence per candidate, and
(iii) their organization and ordering within prompts. More specifically, we
construct a knowledge graph (KG) to discover candidates and mine relational
paths, which are transformed into evidence cards that summarize rationales for
each candidate POI. The frozen LLM then acts as a reasoning engine, generating
recommendations from the KG-discovered candidate set based on the
policy-optimized prompts. Experiments on three real-world datasets demonstrate
that Prompt-as-Policy consistently outperforms state-of-the-art baselines,
achieving average 7.7\% relative improvements in Acc@1 for inactive users,
while maintaining competitive performance on active users, without requiring
model fine-tuning.

</details>


### [174] [Geometric opinion exchange polarizes in every dimension](https://arxiv.org/abs/2510.08190)
*Abdou Majeed Alidou,Júlia Baligács,Jan Hązła*

Main category: cs.SI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A recent line of work studies models of opinion exchange where agent opinions
about $d$ topics are tracked simultaneously. The opinions are represented as
vectors on the unit $(d-1)$-sphere, and the update rule is based on the overall
correlation between the relevant vectors. The update rule reflects the
assumption of biased assimilation, i.e., a pair of opinions is brought closer
together if their correlation is positive and further apart if the correlation
is negative.
  This model seems to induce the polarization of opinions into two antipodal
groups. This is in contrast to many other known models which tend to achieve
consensus. The polarization property has been recently proved for $d=2$, but
the general case of $d \ge 3$ remained open. In this work, we settle the
general case, using a more detailed understanding of the model dynamics and
tools from the theory of random processes.

</details>


### [175] [Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM Reasoning](https://arxiv.org/abs/2510.08481)
*Yifei Xu,Jiaying Wu,Herun Wan,Yang Li,Zhen Hou,Min-Yen Kan*

Main category: cs.SI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hashtag trends ignite campaigns, shift public opinion, and steer millions of
dollars in advertising spend, yet forecasting which tag goes viral is elusive.
Classical regressors digest surface features but ignore context, while large
language models (LLMs) excel at contextual reasoning but misestimate numbers.
We present BuzzProphet, a reasoning-augmented hashtag popularity prediction
framework that (1) instructs an LLM to articulate a hashtag's topical virality,
audience reach, and timing advantage; (2) utilizes these popularity-oriented
rationales to enrich the input features; and (3) regresses on these inputs. To
facilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated
from social media. Across diverse regressor-LLM combinations, BuzzProphet
reduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while
producing human-readable rationales. Results demonstrate that using LLMs as
context reasoners rather than numeric predictors injects domain insight into
tabular models, yielding an interpretable and deployable solution for social
media trend forecasting.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [176] [Self-replication and Computational Universality](https://arxiv.org/abs/2510.08342)
*Jordan Cotler,Clément Hongler,Barbora Hudcová*

Main category: nlin.CG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-replication is central to all life, and yet how it dynamically emerges
in physical, non-equilibrium systems remains poorly understood. Von Neumann's
pioneering work in the 1940s and subsequent developments suggest a natural
hypothesis: that any physical system capable of Turing-universal computation
can support self-replicating objects. In this work, we challenge this
hypothesis by clarifying what computational universality means for physical
systems and constructing a cellular automaton that is Turing-universal but
cannot sustain non-trivial self-replication. By analogy with biology, such
dynamics manifest transcription and translation but cannot instantiate
replication. More broadly, our work emphasizes that the computational
complexity of translating between physical dynamics and symbolic computation is
inseparable from any claim of universality (exemplified by our analysis of Rule
110) and builds mathematical foundations for identifying self-replicating
behavior. Our approach enables the formulation of necessary dynamical and
computational conditions for a physical system to constitute a living organism.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [177] [Determination of Range Conditions for General Projection Pair Operators](https://arxiv.org/abs/2510.07480)
*Richard Huber,Rolf Clackdoyle,Laurent Desbat*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Tomographic techniques are vital in modern medicine, allowing doctors to
observe patients' interior features. Individual steps in the measurement
process are modeled by `single projection operators' $p$. These are line
integral operators over a collection of curves that covers the regions of
interest. Then, the entire measurement process can be understood as a finite
collection of such single projections, and thus be modeled by an
$N$-projections operator $P=(p_1,\dots,p_N)$. The most well-known example of an
$N$-projections operator is the restriction of the Radon transform to finitely
many projection angles. Characterizations of the range of $N$-projections
operators are of intrinsic mathematical interest and can also help in practical
applications such as geometric calibration, motion detection, or model
parameter identification. In this work, we investigate the range of projection
pair operators $\mathcal{P}$ in the plane, i.e., operators formed by two
projections ($N=2$) applied to functions in $\mathbb{R}^2$. We find that the
set of annihilators to $\mathrm{rg}(\mathcal{P})$ that are regular
distributions contains at most one dimension and a range condition can be
explicitly determined by what we refer to as `kernel conditions'. With this
tool, we examine the exponential fanbeam transform for which no range
conditions were known, finding that no (regular) range condition exists, and
therefore, arbitrary data can be approximated in an $L^2$ sense by projections
of smooth functions. We also illustrate the use of this theory on a mixed
parallel-fanbeam projection pair operator.

</details>


### [178] [A control-based spatial source reconstruction in fractional heat equations](https://arxiv.org/abs/2510.07528)
*Galina García,Joaquín Vidal,Sebastián Zamorano*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This article addresses the inverse source problem for a nonlocal heat
equation involving the fractional Laplacian. The primary goal is to reconstruct
the spatial component of the source term from partial observations of the
system's state and its time derivative over a subset of the domain. A
reconstruction formula for the Fourier coefficients of the unknown source is
derived, leveraging the null controllability property of the fractional heat
equation when the fractional order lies in the interval $s\in(1/2,1)$. The
methodology builds on spectral analysis and Volterra integral equations,
providing a robust framework for recovering spatial sources under limited
measurement data. Numerical experiments confirm the accuracy and stability of
the proposed approach.

</details>


### [179] [Semi-implicit strategies for the Serre-Green-Naghdi equations in hyperbolic form. Is hyperbolic relaxation really a good idea?](https://arxiv.org/abs/2510.07539)
*Emanuele Macca,Walter Boscheri,Mario Ricchiuto*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Serre-Green-Naghdi (SGN) equations provide a valuable framework for
modelling fully nonlinear and weakly dispersive shallow-water flows. However,
their elliptic formulation can considerably increase the computational cost
compared to the Saint-Venant equations. To overcome this difficulty, hyperbolic
models (hSGN) have been proposed that replace the elliptic operators with
first-order hyperbolic formulations augmented by relaxation terms, which
recover the original elliptic formulation in the stiff limit. Yet, as the
relaxation parameter \lambda increases, explicit schemes face restrictive
stability constraints that may offset these advantages. To mitigate this
limitation, we introduce a semi-implicit (SI) integration strategy for the hSGN
system, where the stiff acoustic terms are treated implicitly within an IMEX
Runge-Kutta framework, while the advective components remain explicit. The
proposed approach mitigates the CFL stability restriction and maintains
dispersive accuracy at a moderate computational cost. Numerical results confirm
that the combination of hyperbolization and semi-implicit time integration
provides an efficient and accurate alternative to both classical SGN and fully
explicit hSGN solvers.

</details>


### [180] [Stochastic Gradient Descent for Incomplete Tensor Linear Systems](https://arxiv.org/abs/2510.07630)
*Anna Ma,Deanna Needell,Alexander Xue*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Solving large tensor linear systems poses significant challenges due to the
high volume of data stored, and it only becomes more challenging when some of
the data is missing. Recently, Ma et al. showed that this problem can be
tackled using a stochastic gradient descent-based method, assuming that the
missing data follows a uniform missing pattern. We adapt the technique by
modifying the update direction, showing that the method is applicable under
other missing data models. We prove convergence results and experimentally
verify these results on synthetic data.

</details>


### [181] [Parallel-in-Time Solution of Allen-Cahn Equations by Integrating Operator Learning into the Parareal Method](https://arxiv.org/abs/2510.07672)
*Yuwei Geng,Junqi Yin,Eric C. Cyr,Guannan Zhang,Lili Ju*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While recent advances in deep learning have shown promising efficiency gains
in solving time-dependent partial differential equations (PDEs), matching the
accuracy of conventional numerical solvers still remains a challenge. One
strategy to improve the accuracy of deep learning-based solutions for
time-dependent PDEs is to use the learned solution as the coarse propagator in
the Parareal method and a traditional numerical method as the fine solver.
However, successful integration of deep learning into the Parareal method
requires consistency between the coarse and fine solvers, particularly for PDEs
exhibiting rapid changes such as sharp transitions. To ensure such consistency,
we propose to use the convolutional neural networks (CNNs) to learn the fully
discrete time-stepping operator defined by the traditional numerical scheme
used as the fine solver. We demonstrate the effectiveness of the proposed
method in solving the classical and mass-conservative Allen-Cahn (AC)
equations. Through iterative updates in the Parareal algorithm, our approach
achieves a significant computational speedup compared to traditional fine
solvers while converging to high-accuracy solutions. Our results highlight that
the proposed Parareal algorithm effectively accelerates simulations,
particularly when implemented on multiple GPUs, and converges to the desired
accuracy in only a few iterations. Another advantage of our method is that the
CNNs model is trained on trajectories-based on random initial conditions, such
that the trained model can be used to solve the AC equations with various
initial conditions without re-training. This work demonstrates the potential of
integrating neural network methods into the parallel-in-time frameworks for
efficient and accurate simulations of time-dependent PDEs.

</details>


### [182] [Ergodicity and error estimate of laws for a random splitting Langevin Monte Carlo](https://arxiv.org/abs/2510.07676)
*Lei Li,Chen Wang,Mengchao Wang*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The random splitting Langevin Monte Carlo could mitigate the first order bias
in Langevin Monte Carlo with little extra work compared other high order
schemes. We develop in this work an analysis framework for the sampling error
under Wasserstein distance regarding the random splitting Langevin Monte Carlo.
First, the sharp local truncation error is obtained by the relative entropy
approach together with the explicit formulas for the commutator of related
semi-groups. The necessary pointwise estimates of the gradient and Hessian of
the logarithmic density are established by the Bernstein type approach in PDE
theory. Second, the geometric ergodicity is established by accommodation of the
reflection coupling. Combining the ergodicity with the local error estimate, we
establish a uniform-in-time sampling error bound, showing that the invariant
measure of the method approximates the true Gibbs distribution with $O(\tau^2)$
accuracy where $\tau$ is the time step. Lastly, we perform numerical
experiments to validate the theoretical results.

</details>


### [183] [Smoother-type a posteriori error estimates for finite element methods](https://arxiv.org/abs/2510.07677)
*Yuwen Li,Han Shui*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work develops user-friendly a posteriori error estimates of finite
element methods, based on smoothers of linear iterative solvers. The proposed
method employs simple smoothers, such as Jacobi or Gauss--Seidel iteration, on
an auxiliary finer mesh to process the finite element residual for a posteriori
error control. The implementation requires only a coarse-to-fine prolongation
operator. For symmetric problems, we prove the reliability and efficiency of
smoother-type error estimators under a saturation assumption. Numerical
experiments for various PDEs demonstrate that the proposed smoother-type error
estimators outperform residual-type estimators in accuracy and exhibit
robustness with respect to parameters and polynomial degrees.

</details>


### [184] [Elastic-plastic cell-based smoothed finite element method solving geotechnical problems](https://arxiv.org/abs/2510.07687)
*Yang Yang,Mingjiao Yan,Zongliang Zhang,Miao Zhang,Feidong Zheng,Dong Pana,Xiaozi Lina*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: An elastic-plastic cell-based smoothed finite element method (CSFEM) is
proposed for geotechnical analysis of soils and rocks exhibiting nonlinear and
path-dependent behaviors. By introducing strain smoothing over subcell domains
and employing a consistent stress return-mapping algorithm, the method enhances
stress accuracy, alleviates volumetric locking, and reduces sensitivity to mesh
distortion while retaining the flexibility of polygonal elements. The
formulation is implemented in ABAQUS via a user-defined element and validated
through benchmark and practical problems, including a pressurized thick
cylinder, biaxial soil test, strip footing bearing capacity, tunnel excavation,
and slope stability. Numerical results show excellent agreement with analytical
solutions and conventional FEM, with smoother stress fields, improved
convergence, and higher accuracy in ultimate load prediction. These findings
demonstrate that CSFEM provides a stable and efficient framework for
elastic-plastic analysis of complex geotechnical problems.

</details>


### [185] [Scaling crossover of the generalized Jeffreys-type law](https://arxiv.org/abs/2510.07930)
*Fugui Ma*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The generalized Jeffreys-type law is formulated as a multi-term
time-fractional Jeffreys-type equation, whose dynamics exhibit rich scaling
crossover phenomena entailing different diffusion mechanisms. In this work, we
provide a novel physical explanation for the equation from first principles,
beginning with a microscopic description based on the continuous-time random
walk framework with a generalized waiting time distribution and further
deriving the equation from an overdamped Langevin equation subject to a
stochastic time-change (subordination). Employing the Laplace transform method,
we conduct a rigorous analysis of the equation, establishing its well-posedness
and providing a detailed Sobolev regularity analysis. We also develop a novel
numerical scheme, termed the CIM-CLG algorithm, which achieves spectral
accuracy in both time and space while substantially relaxing the temporal
regularity requirements on the solution. The algorithm reduces the
computational complexity to $\mathcal{O}(N)$ in time and $\mathcal{O}(M\log M)$
in space and is fully parallelizable. Detailed implementation guidelines and
new technical error estimates are provided. Extensive numerical experiments in
1D and 2D settings validate the efficiency, robustness, and accuracy of the
proposed method. By integrating stochastic modeling, mathematical analysis, and
numerical computation, this work advances the understanding of the generalized
Jeffreys-type law and offers a mathematically rigorous and computationally
efficient framework for tackling complex nonlocal problems.

</details>


### [186] [Likelihood-informed Model Reduction for Bayesian Inference of Static Structural Loads](https://arxiv.org/abs/2510.07950)
*Jakob Scheffels,Elizabeth Qian,Iason Papaioannou,Elisabeth Ullmann*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian inverse problems use data to update a prior probability distribution
on uncertain parameter values to a posterior distribution. Such problems arise
in many structural engineering applications, but computational solution of
Bayesian inverse problems is often expensive because standard solution
approaches require many evaluations of the forward model mapping the parameter
value to predicted observations. In many settings, this forward model is
expensive because it requires the solution of a high-dimensional discretization
of a partial differential equation. However, Bayesian inverse problems often
exhibit low-dimensional structure because the available data are primarily
informative (relative to the prior) in a low-dimensional subspace, sometimes
called the likelihood-informed subspace (LIS). This paper proposes a new
projection-based model reduction method for static linear systems that exploits
this low-dimensional structure in the setting where the unknown parameter is
the right-hand-side forcing. The proposed method projects the governing partial
differential equation onto the likelihood-informed subspace, yielding a
computationally efficient reduced model that can be used to accelerate the
solution of the inverse problem. Numerical experiments on two structural
engineering model problems demonstrate that the proposed approach can
successfully exploit the intrinsic low-dimensionality of the problem, obtaining
relative errors of O(10^{-10}) in the inverse problem solution with a 10x-100x
lower-dimensional model.

</details>


### [187] [LDMD with Temporally Adaptive Segmentation](https://arxiv.org/abs/2510.08065)
*Qiuqi Li,Chang Liu,Yifei Yang*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic mode decomposition (DMD) is a widely used data-driven algorithm for
predicting the future states of dynamical systems. However, its standard
formulation often struggles with poor long-term predictive accuracy. To address
this limitation, we propose a localized DMD (LDMD) framework that improves
prediction performance by integrating DMD's strong linear forecasting
capabilities with time-domain segmentation techniques. In this framework, the
temporal domain is segmented into multiple subintervals, within which snapshot
matrices are constructed and localized predictions are performed. We first
present the localized DMD method with predefined segmentation, and then explore
an adaptive segmentation strategy to further enhance computational efficiency
and prediction robustness. Furthermore, we conduct an error analysis that
provides the upper bound of the local and global truncation error for the
proposed framework. The effectiveness of LDMD is demonstrated on four benchmark
problems-Burgers', Allen-Cahn, nonlinear Schrodinger, and Maxwell's equations.
Numerical results show that LDMD significantly enhances long-term predictive
accuracy while preserving high computational efficiency.

</details>


### [188] [Semi-Implicit Central scheme for Hyperbolic Systems of Balance Laws with Relaxed Source Term](https://arxiv.org/abs/2510.08134)
*Sudipta Sahu,Emanuele Macca,Rathan Samala*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quasi-linear hyperbolic systems with source terms introduce significant
computational challenges due to the presence of a stiff source term. To address
this, a finite volume Nessyahu-Tadmor (NT) central numerical scheme is explored
and applied to benchmark models such as the Jin-Xin relaxation model, the
shallow-water model, the Broadwell model, the Euler equations with heat
transfer, and the Euler system with stiff friction to assess their
effectiveness. The core part of this numerical scheme lies in developing a new
implicit-explicit (IMEX) scheme, where the stiff source term is handled in an
semi-implicit manner constructed by combining the midpoint rule in space, the
trapezoidal rule in time with a backward semi-implicit Taylor expansion. The
advantage of the proposed method lies in its stability region and maintains
robustness near stiffness and discontinuities, while asymptotically preserving
second-order accuracy.
  Theoretical analysis and numerical validation confirm the stability and
accuracy of the method, highlighting its potential for efficiently solving the
stiff hyperbolic systems of balance laws.

</details>


### [189] [Dual-primal Isogeometric Tearing and Interconnecting Solvers for adaptively refined multi-patch configurations](https://arxiv.org/abs/2510.08148)
*Stefan Takacs,Stefan Tyoler*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Isogeometric Analysis is a variant of the finite element method, where spline
functions are used for the representation of both the geometry and the
solution. Splines, particularly those with higher degree, achieve their full
approximation power only if the solution is sufficiently regular. Since
solutions are usually not regular everywhere, adaptive refinement is essential.
Recently, a multi-patch-based adaptive refinement strategy based on recursive
patch splitting has been proposed, which naturally generates hierarchical,
non-matching multi-patch configurations with T-junctions, but preserves the
tensor-product structure within each patch.
  In this work, we investigate the application of the dual-primal Isogeometric
Tearing and Interconnecting method (IETI-DP) to such adaptive multi-patch
geometries. We provide sufficient conditions for the solvability of the local
problems and propose a preconditioner for the overall iterative solver. We
establish a condition number bound that coincides with the bound previously
shown for the fully matching case. Numerical experiments confirm the
theoretical findings and demonstrate the efficiency of the proposed approach in
adaptive refinement scenarios.

</details>


### [190] [Full moment error estimates in strong norms for numerical approximations of stochastic Navier-Stokes equations with multiplicative noise, Part I: time discretization](https://arxiv.org/abs/2510.08291)
*Xiaobing Feng,Liet Vo*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper focuses on deriving optimal-order full moment error estimates in
strong norms for both velocity and pressure approximations in the
Euler-Maruyama time discretization of the stochastic Navier-Stokes equations
with multiplicative noise. Additionally, it introduces a novel approach and
framework for the numerical analysis of nonlinear stochastic partial
differential equations (SPDEs) with multiplicative noise in general. The main
ideas of this approach include establishing exponential stability estimates for
the SPDE solution, leveraging a discrete stochastic Gronwall inequality, and
employing a bootstrap argument.

</details>


### [191] [Surface finite element approximation of parabolic SPDEs with Whittle--Matérn noise](https://arxiv.org/abs/2510.08443)
*Øyvind Stormark Auestad,Geir-Arne Fuglstad,Annika Lang*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose and analyse a new type of fully discrete surface finite element
approximation of a class of linear parabolic stochastic evolution equations
with additive noise. Our discretization uses a surface finite element
approximation of the noise, and is tailored for equations with noise having
covariance operator defined by (negative powers of) elliptic operators, like
Whittle--Mat\'ern random fields. We derive strong and pathwise convergence
rates of our approximation, and verify these by numerical experiments.

</details>


### [192] [Refinement-based Christoffel sampling for least squares approximation in non-orthogonal bases](https://arxiv.org/abs/2510.08461)
*Astird Herremans,Ben Adcock*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a refinement-based Christoffel sampling (RCS) algorithm for
least squares approximation in the span of a given, generally non-orthogonal
set of functions $\Phi_n = \{\phi_1, \dots, \phi_n\}$. A standard sampling
strategy for this problem is Christoffel sampling, which achieves near-best
approximations in probability using only $\mathcal{O}(n \log(n))$ samples.
However, it requires i.i.d.\ sampling from a distribution whose density is
proportional to the inverse Christoffel function $k_n$, the computation of
which requires an orthonormal basis. As a result, existing approaches for
non-orthogonal bases $\Phi_n$ typically rely on costly discrete
orthogonalization. We propose a new iterative algorithm, inspired by recent
advances in approximate leverage score sampling, that avoids this bottleneck.
Crucially, while the computational cost of discrete orthogonalization grows
proportionally with $\|k_n\|_{L^\infty(X)}$, the cost of our approach increases
only logarithmically in $\|k_n\|_{L^\infty(X)}$. In addition, we account for
finite-precision effects by considering a numerical variant of the Christoffel
function, ensuring that the algorithm relies only on computable quantities.
Alongside a convergence proof, we present extensive numerical experiments
demonstrating the efficiency and robustness of the proposed method.

</details>


### [193] [Where Have All the Kaczmarz Iterates Gone?](https://arxiv.org/abs/2510.08563)
*El Houcine Bergou,Soumia Boucherouite,Aritra Dutta,Xin Li,Anna Ma*

Main category: math.NA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The randomized Kaczmarz (RK) algorithm is one of the most computationally and
memory-efficient iterative algorithms for solving large-scale linear systems.
However, practical applications often involve noisy and potentially
inconsistent systems. While the convergence of RK is well understood for
consistent systems, the study of RK on noisy, inconsistent linear systems is
limited. This paper investigates the asymptotic behavior of RK iterates in
expectation when solving noisy and inconsistent systems, addressing the
locations of their limit points. We explore the roles of singular vectors of
the (noisy) coefficient matrix and derive bounds on the convergence horizon,
which depend on the noise levels and system characteristics. Finally, we
provide extensive numerical experiments that validate our theoretical findings,
offering practical insights into the algorithm's performance under realistic
conditions. These results establish a deeper understanding of the RK
algorithm's limitations and robustness in noisy environments, paving the way
for optimized applications in real-world scientific and engineering problems.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [194] [Emergent spacetime supersymmetry at 2D fractionalized quantum criticality](https://arxiv.org/abs/2510.07383)
*Zhengzhi Wu,Zhou-Quan Wan,Shao-Kai Jian,Hong Yao*

Main category: cond-mat.str-el

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While experimental evidence for spacetime supersymmetry (SUSY) in particle
physics remains elusive, condensed matter systems offer a promising arena for
its emergence at quantum critical points (QCPs). Although there have been a
variety of proposals for emergent SUSY at symmetry-breaking QCPs, the emergence
of SUSY at fractionalized QCPs remains largely unexplored. Here, we demonstrate
emergent space-time SUSY at a fractionalized QCP in the Kitaev honeycomb model
with Su-Schrieffer-Heeger (SSH) spin-phonon coupling. Specifically, through
numerical computations and analytical analysis, we show that the anisotropic
SSH-Kitaev model hosts a fractionalized QCP between a Dirac spin liquid and an
incommensurate/commensurate valence-bond-solid phase coexisting with
$\mathbb{Z}_2$ topological order. A low-energy field theory incorporating
phonon quantum fluctuations reveals that this fractionalized QCP features an
emergent $\mathcal{N}=2$ spacetime SUSY. We further discuss their universal
experimental signatures in thermal transport and viscosity, highlighting the
concrete lattice realization of emergent SUSY at a fractionalized QCP in 2D.

</details>


### [195] [Quantizing Bosonized Fermi Surfaces](https://arxiv.org/abs/2510.07583)
*Sihan Chen,Luca V. Delacretaz*

Main category: cond-mat.str-el

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bosonization describes Fermi surface dynamics in terms of a collective field
that lives on a part of phase space. While sensible semiclassically, the
challenge of treating such a field quantum mechanically has prevented
bosonization from providing as powerful a nonperturbative tool as in one
dimension. We show that general Fermi surfaces can be exactly described by a
particular $N\to \infty$ limit of a $U(N)_1$ WZW model, with a tower of
irrelevant corrections. This matrix-valued description encodes the
noncommutative nature of phase space, and its (solvable) strongly coupled
dynamics resolves the naive overcounting of degrees of freedom of the
collective field without the need to cut the Fermi surface into patches. This
approach furthermore provides a quantitative tool to systematically study
power-law corrections to Fermi surface dynamics.

</details>


### [196] [Surface band-selective moiré effect induces flat band in mixed-dimensional heterostructures](https://arxiv.org/abs/2510.07704)
*Shuming Yu,Zhentao Fu,Dingkun Qin,Enting Li,Hao Zhong,Xingzhe Wang,Keming Zhao,Shangkun Mo,Qiang Wan,Yiwei Li,Jie Li,Jianxin Zhong,Hong Ding,Nan Xu*

Main category: cond-mat.str-el

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we reveal a curious type of moir\'e effect that selectively
modifies the surface states of bulk crystal. We synthesize mixed-dimensional
heterostructures consisting of a noble gas monolayer grow on the surface of
bulk Bi(111), and determine the electronic structure of the heterostructures
using angle-resolved photoemission spectroscopy. We directly observe moir\'e
replicas of the Bi(111) surface states, while the bulk states remain barely
changed. Meanwhile, we achieve control over the moir\'e period in the range of
25 {\AA} to 80 {\AA} by selecting monolayers of different noble gases and
adjusting the annealing temperature. At large moir\'e periods, we observe
hybridization between the surface band replicas, which leads to the formation
of a correlated flat band. Our results serve as a bridge for understanding the
moir\'e modulation effect from 2D to 3D systems, and provide a feasible
approach for the realization of correlated phenomena through the engineering of
surface states via moir\'e effects.

</details>


### [197] [Chiral Edge Excitations of Fractional Chern Insulators](https://arxiv.org/abs/2510.07795)
*Xiao-Han Yang,Ji-Yao Chen,Xiao-Yu Dong*

Main category: cond-mat.str-el

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Edge excitations are the defining signature of chiral topologically ordered
systems. In continuum fractional quantum Hall (FQH) states, these excitations
are described by the chiral Luttinger liquid ($\chi$LL) theory. Whether this
effective description remains valid for fractional Chern insulators (FCIs) on
discrete lattices has been a longstanding open question. Here we numerically
demonstrate that the charge-one edge spectral function of a $\nu=1/2$ FCI on an
infinitely long strip with width $L_y=10$ quantitatively follows the
predictions of $\chi$LL theory. The edge spectrum is gapless, chiral, and
linear, with spectral weight increasing linearly with both momentum and energy.
We further analyze the influence of lattice size, particle number, trapping
potential, and charge sector of excitations on the edge properties. Our results
establish a clear correspondence between lattice FCIs and continuum FQH systems
and provide guidance for future experimental detection of chiral edge modes.

</details>


### [198] [Fractional quantum Hall states under density decoherence](https://arxiv.org/abs/2510.08490)
*Zijian Wang,Ruihua Fan,Tianle Wang,Samuel J. Garratt,Ehud Altman*

Main category: cond-mat.str-el

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fractional quantum Hall states are promising platforms for topological
quantum computation due to their capacity to encode quantum information in
topologically degenerate ground states and in the fusion space of non-abelian
anyons. We investigate how the information encoded in two paradigmatic states,
the Laughlin and Moore-Read states, is affected by density decoherence --
coupling of local charge density to non-thermal noise. We identify a critical
filling factor $\nu_c$, above which the quantum information remains fully
recoverable for arbitrarily strong decoherence. The $\nu=1/3$ Laughlin state
and $\nu = 1/2$ Moore-Read state both lie within this range. Below $\nu_c$ both
classes of states undergo a decoherence induced
Berezinskii-Kosterlitz-Thousless (BKT) transition into a critical decohered
phase. For Laughlin states, information encoded in the topological ground state
manifold degrades continuously with decoherence strength inside this critical
phase, vanishing only in the limit of infinite decoherence strength. On the
other hand, quantum information encoded in the fusion space of non-abelian
anyons of the Moore-Read states remains fully recoverable for arbitrary strong
decoherence even beyond the BKT transition. These results lend further support
to the promise of non-Abelian FQH states as platforms for topological quantum
computation and raises the question of how errors in such states can be
corrected.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [199] [Three-state coevolutionary game dynamics with environmental feedback](https://arxiv.org/abs/2510.07946)
*Yi-Duo Chen,Zhi-Xi Wu,Jian-Yue Guan*

Main category: nlin.AO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Environmental feedback mechanisms are ubiquitous in real-world complex
systems. In this study, we incorporate a homogeneous environment into the
evolutionary dynamics of a three-state system comprising cooperators,
defectors, and empty nodes. Both coherence resonance and equilibrium states,
resulting from the tightly clustering of cooperator agglomerates, enhance
population survival and environmental quality. The resonance phenomenon arises
at the transition between cooperative and defective payoff parameters in the
prisoner's dilemma game.

</details>


### [200] [Spike-frequency and h-current based adaptation are dynamically equivalent in a Wilson-Cowan field model](https://arxiv.org/abs/2510.08436)
*Ronja Strömsdörfer,Klaus Obermayer*

Main category: nlin.AO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: During slow-wave sleep, the brain produces traveling waves of slow
oscillations (SOs; $\leq 2$ Hz), characterized by the propagation of
alternating high- and low-activity states. The question of internal mechanisms
that modulate traveling waves of SOs is still unanswered although it is
established that it is an adaptation mechanism that mediates them. One
mechanism investigated is spike-frequency adaptation, a hyperpolarizing
feedback current that is activated during periods of high-activity. An
alternative mechanism is based on hyperpolarization-activated currents, which
are positive feedback currents that are activated in low-activity states. Both
adaptation mechanisms were shown to feature SO-like dynamics in neuronal
populations, and the inclusion of a spatial domain seems to enhance observable
differences in their effects. To investigate this in detail, we examine a
spatially extended two-population Wilson-Cowan model with local spatial
coupling and the excitatory populations equipped with either one of the two
adaptation mechanisms. We describe them with the same dynamical equation and
include the inverse mode of action by changing the signs of adaptation strength
and gain. We show that the dynamical systems are mathematically equivalent
under a compensatory external input, which depends on the adaptation strength,
leading to a shift in state space of the otherwise equivalent bifurcation
structure. Strong enough adaptation is required to induce traveling waves.
Additionally, adaptation modulates the properties of the spatio-temporal
activity patterns, such as temporal and spatial frequencies, and the speed of
the traveling waves, all of which increase with increasing strength. Though
being dynamically equivalent, our results also explain why location-dependent
variations in feedback strength cause differences in the propagation of
traveling waves between both adaptation mechanisms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [201] [SEPhIA: <1 laser/neuron Spiking Electro-Photonic Integrated Multi-Tiled Architecture for Scalable Optical Neuromorphic Computing](https://arxiv.org/abs/2510.07427)
*Matěj Hejda,Aishwarya Natarajan,Chaerin Hong,Mehmet Berkay On,Sébastien d'Herbais de Thun,Raymond G. Beausoleil,Thomas Van Vaerenbergh*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Research into optical spiking neural networks (SNNs) has primarily focused on
spiking devices, networks of excitable lasers or numerical modelling of large
architectures, often overlooking key constraints such as limited optical power,
crosstalk and footprint. We introduce SEPhIA, a photonic-electronic,
multi-tiled SNN architecture emphasizing implementation feasibility and
realistic scaling. SEPhIA leverages microring resonator modulators (MRMs) and
multi-wavelength sources to achieve effective sub-one-laser-per-spiking neuron
efficiency. We validate SEPhIA at both device and architecture levels by
time-domain co-simulating excitable CMOS-MRR coupled circuits and by devising a
physics-aware, trainable optoelectronic SNN model, with both approaches
utilizing experimentally derived device parameters. The multi-layer
optoelectronic SNN achieves classification accuracies over 90% on a four-class
spike-encoded dataset, closely comparable to software models. A design space
study further quantifies how photonic device parameters impact SNN performance
under constrained signal-to-noise conditions. SEPhIA offers a scalable,
expressive, physically grounded solution for neuromorphic photonic computing,
capable of addressing spike-encoded tasks.

</details>


### [202] [A Distributed Emulation Environment for In-Memory Computing Systems](https://arxiv.org/abs/2510.08257)
*Eleni Bougioukou,Anastasios Petropoulos,Nikolaos Toulgaridis,Theodoros Chatzimichail,Theodore Antonakopoulos*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-memory computing technology is used extensively in artificial intelligence
devices due to lower power consumption and fast calculation of matrix-based
functions. The development of such a device and its integration in a system
takes a significant amount of time and requires the use of a real-time
emulation environment, where various system aspects are analyzed, microcode is
tested, and applications are deployed, even before the real chip is available.
In this work, we present the architecture, the software development tools, and
experimental results of a distributed and expandable emulation system for rapid
prototyping of integrated circuits based on in-memory computing technologies.
Presented experimental results demonstrate the usefulness of the proposed
emulator.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [203] [Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization](https://arxiv.org/abs/2510.07518)
*Blake Hansen,Dafne Zorzetto,Valeria Edefonti,Roberta De Vito*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding the association between dietary patterns and health outcomes,
such as the cancer risk, is crucial to inform public health guidelines and
shaping future dietary interventions. However, dietary intake data present
several statistical challenges: they are high-dimensional, often sparse with
excess zeros, and exhibit heterogeneity driven by individual-level covariates.
Non-Negative Matrix Factorization (NMF), commonly used to estimate patterns in
high-dimensional count data, typically relies on Poisson assumptions and lacks
the flexibility to fully address these complexities. Additionally, integrating
data across multiple studies, such as case-control studies on cancer risk,
requires models that can share information across sources while preserving
study-specific structure.
  In this paper, we introduce a novel Bayesian NMF model that (i) jointly
models multi-study count data to enable cross-study information sharing, (ii)
incorporate a mixture component to account for zero inflation, and (iii)
leverage flexible Bayesian non-parametric priors for characterizing the
heterogeneity in pattern scores induced by the individual covariates. This
structure allows for clustering of individuals based on dietary profiles,
enabling downstream association analyses with health outcomes. Through
extensive simulation studies, we demonstrate that our model significantly
improves estimation accuracy compared to existing Bayesian NMF methods.
  We further illustrate its utility through an application to multiple
case-control studies on diet and upper aero-digestive tract cancers,
identifying nutritionally meaningful dietary patterns. An R package
implementing our approach is available at
https://github.com/blhansen/ZIMultiStudyNMF.

</details>


### [204] [Integrating smart surveys with traditional surveys](https://arxiv.org/abs/2510.07521)
*Danielle Mccool,Peter Lugtig,Bella Struminskaya*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Smart surveys are surveys that make use of sensors and machine intelligence
to reduce respondent burden and increase data quality. Smart surveys have been
tests as a way to improve diary surveys in official statistics, where data are
collected on topics such as travel, time use and household expenditures. There
are often inherent differences both in measurement and representation between
smart surveys and traditional diaries, which makes it difficult to integrate
both data sources in producing statistics over time, or within a mixed- or
multi-source context. This paper distinguishes two different approaches to
integration: the mixed-mode approach, which prioritizes outcome alignment and
minimizes measurement differences for straightforward data merging, and the
multisource approach, which maintains inherent mode differences and integrates
data at the modeling stage, allowing exploitation of the strengths of each
source. Using travel surveys as an illustrative example, we explore the
benefits and drawbacks of each approach, and propose a decision framework to
guide researchers in selecting the appropriate integration strategy.

</details>


### [205] [Density estimation for compositional data using nonparametric mixtures](https://arxiv.org/abs/2510.07608)
*Jiajin Xie,Yong Wang,Eduardo García-Portugués*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compositional data, representing proportions constrained to the simplex,
arise in diverse fields such as geosciences, ecology, genomics, and microbiome
research. Existing nonparametric density estimation methods often rely on
transformations, which may induce substantial bias near the simplex boundary.
We propose a nonparametric mixture-based framework for density estimation on
compositions. Nonparametric Dirichlet mixtures are employed to naturally
accommodate boundary values, thereby avoiding the transformation or
zero-replacement, while also identifying components supported on the boundary,
providing reliable estimates for data with zero or near-zero values. Bandwidth
selection and initialization schemes are addressed. For comparison,
nonparametric Gaussian mixtures, coupled with log-ratio transformations, are
also considered. Extensive simulations show that the proposed estimators
outperform existing approaches. Three real data applications, including GDP
data analysis, handwritten digit recognition, and skin detection, demonstrate
the usefulness of nonparametric Dirichlet mixtures in practice.

</details>


### [206] [Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data](https://arxiv.org/abs/2510.07770)
*Zhi Yang Tho,Raymond Chambers,A. H. Welsh*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Clustered data arise naturally in many scientific and applied research
settings where units are grouped within clusters. They are commonly analyzed
using linear mixed models to account for within-cluster correlations. This
article focuses on the scenario in which cluster sizes might be highly
unbalanced and proposes a proportional random effect block bootstrap and a
modified random effect block bootstrap, which are applicable in such cases and
accommodate general distributions of random effects and error terms. These
methods generalize the random effect block bootstrap, originally designed for
the balanced case, and can be used for inference on parameters of linear mixed
models or functions thereof. Both proposed bootstraps are shown to enjoy Fisher
consistency under general cluster sizes, while the original random effect block
bootstrap is consistent only for balanced clusters. Simulations demonstrate
strong finite sample inferential performance of the proposed bootstraps
relative to the random effect block bootstrap and other existing bootstrap
methods for clustered data. Application to the Oman rainfall enhancement trial
dataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap
confidence intervals using the proposed bootstraps over the random effect block
bootstrap and a statistically significant effect of the ionization technology
on rainfall.

</details>


### [207] [Detection of mean changes in partially observed functional data](https://arxiv.org/abs/2510.07854)
*Šárka Hudecová,Claudia Kirch*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a test for a change in the mean for a sequence of functional
observations that are only partially observed on subsets of the domain, with no
information available on the complement. The framework accommodates important
scenarios, including both abrupt and gradual changes. The significance of the
test statistic is assessed via a permutation test. In addition to the classical
permutation approach with a fixed number of permutation samples, we also
discuss a variant with controlled resampling risk that relies on a random
(data-driven) number of permutation samples. The small sample performance of
the proposed methodology is illustrated in a Monte Carlo simulation study and
an application to real data.

</details>


### [208] [Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles](https://arxiv.org/abs/2510.08204)
*Soham Ghosh,Saloni Bhogale,Sameer K. Deshpande*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: By allowing the effects of $p$ covariates in a linear regression model to
vary as functions of $R$ additional effect modifiers, varying-coefficient
models (VCMs) strike a compelling balance between interpretable-but-rigid
parametric models popular in classical statistics and flexible-but-opaque
methods popular in machine learning. But in high-dimensional settings where $p$
and/or $R$ exceed the number of observations, existing approaches to fitting
VCMs fail to identify which covariates have a non-zero effect and which effect
modifiers drive these effects. We propose sparseVCBART, a fully Bayesian model
that approximates each coefficient function in a VCM with a regression tree
ensemble and encourages sparsity with a global--local shrinkage prior on the
regression tree leaf outputs and a hierarchical prior on the splitting
probabilities of each tree. We show that the sparseVCBART posterior contracts
at a near-minimax optimal rate, automatically adapting to the unknown sparsity
structure and smoothness of the true coefficient functions. Compared to
existing state-of-the-art methods, sparseVCBART achieved competitive predictive
accuracy and substantially narrower and better-calibrated uncertainty
intervals, especially for null covariate effects. We use sparseVCBART to
investigate how the effects of interpersonal conversations on prejudice could
vary according to the political and demographic characteristics of the
respondents.

</details>


### [209] [Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data](https://arxiv.org/abs/2510.08304)
*Matteo Amestoy,Mark van de Wiel,Jeroen Lakerveld,Wessel van Wieringen*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Exposure to diverse non-genetic factors, known as the exposome, is a critical
determinant of health outcomes. However, analyzing the exposome presents
significant methodological challenges, including: high collinearity among
exposures, the longitudinal nature of repeated measurements, and potential
complex interactions with individual characteristics. In this paper, we address
these challenges by proposing a novel statistical framework that extends
Bayesian profile regression. Our method integrates profile regression, which
handles collinearity by clustering exposures into latent profiles, into a
linear mixed model (LMM), a framework for longitudinal data analysis. This
profile-LMM approach effectively accounts for within-person variability over
time while also incorporating interactions between the latent exposure clusters
and individual characteristics. We validate our method using simulated data,
demonstrating its ability to accurately identify model parameters and recover
the true latent exposure cluster structure. Finally, we apply this approach to
a large longitudinal data set from the Lifelines cohort to identify
combinations of exposures that are significantly associated with diastolic
blood pressure.

</details>


### [210] [Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials](https://arxiv.org/abs/2510.08359)
*Jinho Cha,Eunchan Cha*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Micro-randomized trials (MRTs) are increasingly used to evaluate mobile
health interventions with binary proximal outcomes. Standard inverse
probability weighting (IPW) estimators are unbiased but unstable in small
samples or under extreme randomization. Estimated mean excursion effect (EMEE)
improves efficiency but lacks double robustness. We propose a doubly robust
EMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision
IPW and outcome regression. We prove double robustness, asymptotic efficiency,
and provide finite-sample variance corrections, with extensions to machine
learning nuisance estimators. In simulations, DR-EMEE reduces root mean squared
error, improves coverage, and achieves up to twofold efficiency gains over IPW
and five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and
mHealth datasets confirm stable and efficient inference across both randomized
and observational settings.

</details>


### [211] [Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes](https://arxiv.org/abs/2510.08438)
*Xi Fang,Bingkai Wang,Liangyuan Hu,Fan Li*

Main category: stat.ME

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cluster-randomized trials (CRTs) are experimental designs where groups or
clusters of participants, rather than the individual participants themselves,
are randomized to intervention groups. Analyzing CRT requires distinguishing
between treatment effects at the cluster level and the individual level, which
requires a clear definition of the estimands under the potential outcomes
framework. For analyzing survival outcomes, it is common to assess the
treatment effect by comparing survival functions or restricted mean survival
times between treatment groups. In this article, we formally characterize
cluster-level and individual-level treatment effect estimands with
right-censored survival outcomes in CRTs and propose doubly robust estimators
for targeting such estimands. Under covariate-dependent censoring, our
estimators ensure consistency when either the censoring model or the outcome
model is correctly specified, but not necessarily both. We explore different
modeling options for the censoring and outcome models to estimate the censoring
and survival distributions, and investigate a deletion-based jackknife method
for variance and interval estimation. Extensive simulations demonstrate that
the proposed methods perform adequately in finite samples. Finally, we
illustrate our method by analyzing a completed CRT with survival endpoints.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [212] [Bayesian Optimization of Multi-Bit Pulse Encoding in In2O3/Al2O3 Thin-film Transistors for Temporal Data Processing](https://arxiv.org/abs/2510.07421)
*Javier Meza-Arroyo,Benius Dunn,Weijie Xu,Yu-Chieh Chen,Jen-Sue Chen,Julia W. P. Hsu*

Main category: cond-mat.dis-nn

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Utilizing the intrinsic history-dependence and nonlinearity of hardware,
physical reservoir computing is a promising neuromorphic approach to encode
time-series data for in-sensor computing. The accuracy of this encoding
critically depends on the distinguishability of multi-state outputs, which is
often limited by suboptimal and empirically chosen reservoir operation
conditions. In this work, we demonstrate a machine learning approach, Bayesian
optimization, to improve the encoding fidelity of solution-processed
Al2O3/In2O3 thin-film transistors (TFTs). We show high-fidelity 6-bit temporal
encoding by exploring five key pulse parameters and using the normalized degree
of separation (nDoS) as the metric of output state separability. Additionally,
we show that a model trained on simpler 4-bit data can effectively guide
optimization of more complex 6-bit encoding tasks, reducing experimental cost.
Specifically, for the encoding and reconstruction of binary-patterned images of
a moving car across 6 sequential frames, we demonstrate that the encoding is
more accurate when operating the TFT using optimized pulse parameters and the
4-bit optimized operating condition performs almost as well as the 6-bit
optimized condition. Finally, interpretability analysis via Shapley Additive
Explanations (SHAP) reveals that gate pulse amplitude and drain voltage are the
most influential parameters in achieving higher state separation. This work
presents the first systematic method to identify optimal operating conditions
for reservoir devices, and the approach can be extended to other physical
reservoir implementations across different material platforms.

</details>


### [213] [Ergodicity Breaking and High-Dimensional Chaos in Random Recurrent Networks](https://arxiv.org/abs/2510.07932)
*Carles Martorell,Rubén Calvo,Adrián Roig,Alessia Annibale,Miguel A. Muñoz*

Main category: cond-mat.dis-nn

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The neural model introduced by Sompolinsky, Crisanti, and Sommers (SCS)
nearly four decades ago has become a paradigmatic framework for studying
complex dynamics in random recurrent networks. In its original formulation,
with balanced positive and negative couplings, the model exhibits two phases: a
quiescent regime, where all activity ceases, and a regime of ongoing irregular
collective activity, termed asynchronous chaos (AC), in which state variables
fluctuate strongly in time and across units but average to zero across the
network. Building on recent work, we analyze an extension of the SCS model that
breaks this coupling balance, yielding a richer phase diagram. In addition to
the classical quiescent and AC phases, two additional regimes emerge, marked by
spontaneous symmetry breaking. In the persistent-activity (PA) phase, each unit
settles into a distinct, stable activation state. In the synchronous-chaotic
(SC) phase, dynamics remain irregular and chaotic but fluctuate around a
nonzero mean, generating sustained long-time autocorrelations. Using analytical
techniques based on dynamical mean-field theory, complemented by extensive
numerical simulations, we show how structural disorder gives rise to symmetry
and ergodicity breaking. Remarkably, the resulting phase diagram closely
parallels that of the Sherrington-Kirkpatrick spin-glass model, with the onset
of the SC phase coinciding with the transition associated with replica-symmetry
breaking. All key features of spin glasses, including ergodicity breaking, have
clear counterparts in this recurrent network context, albeit with crucial
idiosyncratic differences, highlighting a unified perspective on complexity in
disordered systems.

</details>


### [214] [Non-Hermitian many-body localization in asymmetric chains with long-range interaction](https://arxiv.org/abs/2510.08277)
*Wen Wang,Han-Ze Li,Jian-Xin Zhong*

Main category: cond-mat.dis-nn

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding the relationship between many-body localization and spectra in
non-Hermitian many-body systems is crucial. In a one-dimensional clean,
long-range interaction-induced non-Hermitian many-body localization system, we
have discovered the coexistence of static and dynamic spectral real-complex
phase transitions, along with many-body ergodic-localized phase transitions.
The phase diagrams of these two types of transitions show similar non-monotonic
boundary trends but do not overlap, highlighting properties distinct from
conventional disorder-induced non-Hermitian many-body localization. We also
propose a potential experimental realization of this model in cold-atom
systems. Our findings provide valuable insights for further understanding the
relationship between non-Hermitian many-body localization and non-Hermitian
spectra in long-range interacting systems.

</details>


### [215] [The Phase Diagram for Percolating Free Surfaces in Disordered Assemblies of Faceted Grains](https://arxiv.org/abs/2510.08296)
*D. J. Priour Jr*

Main category: cond-mat.dis-nn

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Percolation in systems made up of randomly placed impermeable grains is often
examined in the context of system spanning clusters of connected solids forming
above a relatively low critical grain density $\rho_{c1}$ or networks of
interstitial void volumes ceasing to exist above a signficantly higher
threshold $\rho_{c2}$. In this work, we interpret these percolation transitions
as, respectively, the low and high density boundaries of percolating exposed
surfaces which either ensheath clusters of impermeable particles or line
tunnel-like voids. Moreover, we find in the thermodynamic limit exposed
surfaces are either sheaths or tunnels with a second order phase transition
from the former to the latter at a density threshold $\rho_{c*}$ intermediate
between $\rho_{c1}$ and $\rho_{c2}$. We calculate critical inclusion densities
with a new method which identifies exposed free surfaces in a geometrically
exact manner with a computational cost scaling only linearly in the system
volume. We obtain $\rho_{c1}$, $\rho_{c2}$, and $\rho_{c*}$ for a variety of
grain geometries, including each of the Platonic solids, truncated icosahedra,
and structurally disordered inclusions formed from cubes subject to a random
sequence of slicing planes. In the case of the latter, we find a limiting value
of $5\%$ for the critical porosity at the void percolation threshold as the
number of sustained slices per cube becomes large.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [216] [State Constrained Optimal Control Problems With Control On The Acceleration. Applications To Kinetic Mean Field Games](https://arxiv.org/abs/2510.07332)
*Yves Achdou*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Relying on the careful study of a related problem in the calculus of
variations, we study a class of optimal control problems in which the control
lies on the acceleration, with state constraints on the position variable. In
dimension one, we find explicit formulas in the special case when the running
cost is a power of the acceleration (in absolute value) and the terminal cost
is zero. For more general costs or/and higher dimensions, we study the
singularities of the value function. We also prove the closedness (in the C 1
topology) of the graph of the multivalued mapping which maps a point in the
state space to the set of optimal trajectories which start from this point. A
consequence of the latter is the existence, under general assumptions, of
relaxed equilibria for a class of kinetic mean field games with state
constraints.

</details>


### [217] [Variable aggregation-based formulations for pumped storage hydro model in the day-ahead unit commitment problem](https://arxiv.org/abs/2510.07751)
*Shaoze Li,Junhao Wu,Zhibin Deng*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Pumped storage hydro (PSH) plants can improve the flexibility of power
systems. A well-designed formulation for a PSH model is essential when
incorporating the PSH units into a day-ahead unit commitment model. In the
literature, the formulation of a PSH model is generally based on the individual
PSH unit. This formulation is tight if there is only one PSH unit in the
reservoir. However, when there are multiple units sharing the same reservoir in
a PSH plant, the existing formulation may introduce some symmetric structures
which degrade the efficiency of a mixed-integer programming solver
significantly. In this paper, to cope with the symmetric structure in the PSH
plants that have multiple units, we propose two new formulations. The first
formulation considers the case in which there are multiple identical units
sharing the same reservoir. The second formulation considers a general case in
which the units sharing the same reservoir have the same generating and pumping
efficiency. Using the new formulations, the symmetric structures in the problem
can be effectively broken. Numerical results are presented to study the
computational efficiency of the new formulations.

</details>


### [218] [Accelerated Price Adjustment for Fisher Markets with Exact Recovery of Competitive Equilibrium](https://arxiv.org/abs/2510.07759)
*He Chen,Chonghe Jiang,Anthony Man-Cho So*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The canonical price-adjustment process, t\^atonnement, typically fails to
converge to the exact competitive equilibrium (CE) and requires a high
iteration complexity of $\tilde{\mathcal{O}}(1/\epsilon)$ to compute
$\epsilon$-CE prices in widely studied linear and quasi-linear Fisher markets.
This paper proposes refined price-adjustment processes to overcome these
limitations. By formulating the task of finding CE of a (quasi-)linear Fisher
market as a strongly convex nonsmooth minimization problem, we develop a novel
accelerated price-adjustment method (APM) that finds an $\epsilon$-CE price in
$\tilde{\mathcal{O}}(1/\sqrt{\epsilon})$ lightweight iterations, which
significantly improves upon the iteration complexities of t\^atonnement
methods. Furthermore, through our new formulation, we construct a recovery
oracle that maps approximate CE prices to exact CE prices at a low
computational cost. By coupling this recovery oracle with APM, we obtain an
adaptive price-adjustment method whose iterates converge to CE prices in finite
steps. To the best of our knowledge, this is the first convergence guarantee to
exact CE for price-adjustment methods in linear and quasi-linear Fisher
markets. Our developments pave the way for efficient lightweight computation of
CE prices. We also present numerical results to demonstrate the fast
convergence of the proposed methods and the efficient recovery of CE prices.

</details>


### [219] [A unified optimal control framework: time-optimal control and stochastic optimal control](https://arxiv.org/abs/2510.07765)
*Shuzhen Yang*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we propose a unified stochastic optimal control framework that
bridges time-optimal control problems and classical stochastic optimal control
problems. Unlike traditional deterministic time-optimal control formulations,
our framework incorporates a generalized stochastic control structure under
minimum-time constraints. Here, the minimum-time condition characterizes the
earliest achievable moment for reaching a target state in expectation,
rendering the terminal time an endogenous control-dependent variable. The main
contributions of this study are: deriving an extended stochastic maximum
principle for the proposed model, and establishing a bang-bang type optimal
control for the linear time-optimal control problem. This unified stochastic
optimal control framework enables optimal strategy design across finance,
autonomous systems, and supply chains by simultaneously minimizing operational
costs and achieving statistically-defined targets at the earliest feasible
time.

</details>


### [220] [Degradation-Aware Model Predictive Control for Battery Swapping Stations under Energy Arbitrage](https://arxiv.org/abs/2510.07902)
*Ruochen Li,Zhichao Chen,Zhaoting Zhang,Renjie Guo,Zhankun Sun,Jiwei Yao,Jiaze Ma*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Battery swapping stations (BSS) offer a fast and scalable alternative to
conventional electric vehicle (EV) charging, gaining growing policy support
worldwide. However, existing BSS control strategies typically rely on
heuristics or low-fidelity degradation models, limiting profitability and
service level. This paper proposes BSS-MPC: a real-time, degradation-aware
Model Predictive Control (MPC) framework for BSS operations to trade off
economic incentives from energy market arbitrage and long-term battery
degradation effects. BSS-MPC integrates a high-fidelity, physics informed
battery aging model that accurately predicts the degradation level and the
remaining capacity of battery packs. The resulting multiscale
optimization-jointly considering energy arbitrage, swapping logistics, and
battery health-is formulated as a mixed-integer optimal control problem and
solved with tailored algorithms. Simulation results show that BSS-MPC
outperforms rule-based and low-fidelity baselines, achieving lower energy cost,
reduced capacity fade, and strict satisfaction of EV swapping demands.

</details>


### [221] [An efficient algorithm for kernel quantile regression](https://arxiv.org/abs/2510.07929)
*Shengxiang Deng,Xudong Li,Yangjing Zhang*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Kernel Quantile Regression (KQR) extends classical quantile regression to
nonlinear settings using kernel methods, offering powerful tools for modeling
conditional distributions. However, its application to large-scale datasets is
severely limited by the computational burden of the large, dense kernel matrix
and the need to efficiently solve large, often ill-conditioned linear systems.
Existing state-of-the-art solvers usually struggle with scalability. In this
paper, we propose a novel and highly efficient two-phase optimization algorithm
tailored for large-scale KQR. In the first phase, we employ an inexact
alternating direction method of multipliers (ADMM) to compute a high-quality
warm-start solution. The second phase refines this solution using an efficient
semismooth Newton augmented Lagrangian method (ALM). A key innovation of our
approach is a specialized preconditioning strategy that leverages low-rank
approximations of the kernel matrix to effectively mitigate the
ill-conditioning of the linear systems in the Newton steps of the ALM. This can
significantly accelerate iterative solvers for linear systems. Extensive
numerical experiments demonstrate that our algorithm substantially outperforms
existing state-of-the-art commercial and specialized KQR solvers in terms of
speed and scalability.

</details>


### [222] [On the Complexity of Lower-Order Implementations of Higher-Order Methods](https://arxiv.org/abs/2510.07992)
*Nikita Doikov,Geovani Nunes Grapiglia*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we propose a method for minimizing non-convex functions with
Lipschitz continuous $p$th-order derivatives, starting from $p \geq 1$. The
method, however, only requires derivative information up to order $(p-1)$,
since the $p$th-order derivatives are approximated via finite differences. To
ensure oracle efficiency, instead of computing finite-difference approximations
at every iteration, we reuse each approximation for $m$ consecutive iterations
before recomputing it, with $m \geq 1$ as a key parameter. As a result, we
obtain an adaptive method of order $(p-1)$ that requires no more than
$O(\epsilon^{-\frac{p+1}{p}})$ iterations to find an $\epsilon$-approximate
stationary point of the objective function and that, for the choice $m=(p-1)n +
1$, where $n$ is the problem dimension, takes no more than
$O(n^{1/p}\epsilon^{-\frac{p+1}{p}})$ oracle calls of order $(p-1)$. This
improves previously known bounds for tensor methods with finite-difference
approximations in terms of the problem dimension.

</details>


### [223] [Optimal domains for the Cheeger inequality](https://arxiv.org/abs/2510.08032)
*Dorin Bucur,Giuseppe Buttazzo,Alexis de Villeroché*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we consider the scale invariant shape functional
$${\mathcal{F}}_{p,q}(\Omega)=\frac{\lambda_p^{1/p}(\Omega)}{\lambda_q^{1/q}(\Omega)},$$
where $1\le q<p\le+\infty$ and $\lambda_p(\Omega)$ (respectively
$\lambda_q(\Omega)$) is the first eigenvalue of the $p$-Laplacian $-\Delta_p$
(respectively $-\Delta_q$) with Dirichlet boundary condition on
$\partial\Omega$. We study both the maximization and minimization problems for
${\mathcal{F}}_{p,q}$, and show the existence of optimal domains in
${\mathbb{R}}^d$, along with some of their qualitative properties.
Surprisingly, the case of a bounded box $D$ constraint
$$\max\Big\{\lambda_q(\Omega)\ :\ \Omega\subset D,\
\lambda_p(\Omega)=1\Big\},$$ leads to a problem of different nature, for which
the existence of a solution is shown by analyzing optimal capacitary measures.
In the last section we list some interesting questions that, in our opinion,
deserve to be investigated.

</details>


### [224] [Frictional martingale optimal transport and robust hedging](https://arxiv.org/abs/2510.08182)
*Pratik Rai*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study martingale optimal transport (MOT) with state-dependent trading
frictions and develop a geometric-duality framework that extends from one
time-step to the multi-marginal setting. Building on the frictionless
left-monotone structure (Beiglb\"ock and Juillet, Ann. Probab. 2016;
Henry-Labord\`ere and Touzi, Finance Stoch. 2016; Beiglb\"ock et al, Ann.
Probab. 2017), we penalize trade increments by a convex cost and prove a
frictional monotonicity principle. Optimal couplings admit bi-atomic
disintegrations on active components, while a trade band emerges as a
no-transaction region where the identity coupling is optimal. The band is
characterized in dual variables by a subgradient condition at the origin; off
the band, transport moves along two monotone graphs whose endpoints solve an
equal-slope system balancing continuation values and marginal trading costs.
  We establish strong duality with attainment for state-dependent frictions, a
dynamic programming identity that aggregates the one time-step problem, and
stability of optimal couplings and endpoints under perturbations of marginals
and friction. In the vanishing-friction limit the transport maps converge to
the frictionless left-curtain coupling. For linear-quadratic frictions,
motivated by spread and transient impact (Almgren and Chriss, J. Risk 2001;
Obizhaeva and Wang, J. Financ. Mark. 2013), we obtain explicit off-band
displacements and comparative statics in liquidity parameters. Applications
include model-independent pricing/superhedging of lookback, barrier, and Asian
options. Overall, the results link robust superhedging duality (Beiglb\"ock et
al, Finance Stoch. 2013; Dolinsky and Soner, Finance Stoch. 2014) with the fine
geometry of frictional MOT, providing a unified basis for analysis, stability,
and computation in multi-marginal robust pricing.

</details>


### [225] [Composite Lyapunov Criteria for Stability and Convergence with Applications to Optimization Dynamics](https://arxiv.org/abs/2510.08259)
*Hassan Saoud*

Main category: math.OC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a composite Lyapunov framework for nonlinear autonomous systems
that ensures strict decay through a pair of differential inequalities. The
approach yields integral estimates, quantitative convergence rates, vanishing
of dissipation measures, convergence to a critical set, and semistability under
mild conditions, without relying on invariance principles or compactness
assumptions. The framework unifies convergence to points and sets and is
illustrated through applications to inertial gradient systems and Primal--Dual
gradient flows.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [226] [A Virtual Fields Method-Genetic Algorithm (VFM-GA) calibration framework for isotropic hyperelastic constitutive models with application to an elastomeric foam material](https://arxiv.org/abs/2510.07683)
*Zicheng Yan,Jialiang Tao,Christian Franck,David L. Henann*

Main category: physics.comp-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work introduces a calibration framework for material parameter
identification in isotropic hyperelastic constitutive models. The framework
synergizes the Virtual Fields Method (VFM) to define an objective function with
a Genetic Algorithm (GA) as the optimization method to facilitate automated
calibration. The formulation of the objective function uses experimental
displacement fields measured from Digital Image Correlation (DIC) synchronized
with load cell data and can accommodate data from experiments involving
homogeneous or inhomogeneous deformation fields. The framework places no
restrictions on the target isotropic hyperelastic constitutive model,
accommodating models with coupled dependencies on deformation invariants and
specialized functional forms with a number of material parameters, and assesses
material stability, eliminating sets of material parameters that potentially
lead to non-physical behavior for the target hyperelastic constitutive model.
To minimize the objective function, a GA is deployed as the optimization tool
due to its ability to navigate the intricate landscape of material parameter
space. The VFM-GA framework is evaluated by applying it to a hyperelastic
constitutive model for compressible elastomeric foams. The evaluation process
entails a number of tests that employ both homogeneous and inhomogeneous
displacement fields collected from DIC experiments on open-cell foam specimens.
The results outperform manual fitting, demonstrating the framework's robust and
efficient capability to handle material parameter identification for complex
hyperelastic constitutive models.

</details>


### [227] [Iterated Agent for Symbolic Regression](https://arxiv.org/abs/2510.08317)
*Zhuo-Yang Song,Zeyu Cai,Shutao Zhang,Jiashen Wei,Jichen Pan,Shi Qiu,Qing-Hong Cao,Tie-Jiun Hou,Xiaohui Liu,Ming-xing Luo,Hua Xing Zhu*

Main category: physics.comp-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Symbolic regression (SR), the automated discovery of mathematical expressions
from data, is a cornerstone of scientific inquiry. However, it is often
hindered by the combinatorial explosion of the search space and a tendency to
overfit. Popular methods, rooted in genetic programming, explore this space
syntactically, often yielding overly complex, uninterpretable models. This
paper introduces IdeaSearchFitter, a framework that employs Large Language
Models (LLMs) as semantic operators within an evolutionary search. By
generating candidate expressions guided by natural-language rationales, our
method biases discovery towards models that are not only accurate but also
conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's
efficacy across diverse challenges: it achieves competitive, noise-robust
performance on the Feynman Symbolic Regression Database (FSReD), outperforming
several strong baselines; discovers mechanistically aligned models with good
accuracy-complexity trade-offs on real-world data; and derives compact,
physically-motivated parametrizations for Parton Distribution Functions in a
frontier high-energy physics application. IdeaSearchFitter is a specialized
module within our broader iterated agent framework, IdeaSearch, which is
publicly available at https://www.ideasearch.cn/.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [228] [The large-$N$ limit of the topological susceptibility of $\mathrm{SU}(N)$ Yang-Mills theories via Parallel Tempering on Boundary Conditions](https://arxiv.org/abs/2510.08006)
*Claudio Bonanno*

Main category: hep-lat

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: I present a large-$N$ determination of the topological susceptibility $\chi$
of $\mathrm{SU}(N)$ Yang--Mills theories using non-perturbative numerical Monte
Carlo simulations of the lattice-discretized theory for $3\le N \le 6$, and
adopting the Parallel Tempering on Boundary Conditions (PTBC) algorithm to
bypass topological freezing for $N>3$. Thanks to this algorithm I am able to
explore a uniform range of lattice spacing across all values of $N$, and to
precisely determine $\chi$ for finer lattice spacings compared to previous
studies with periodic or open boundary conditions. By taking the continuum
limit at fixed smoothing radius in physical units, I am also able to show the
independence of the continuum limit of $\chi$ from this choice. I conclude
providing a comprehensive comparison of my new PTBC results with previous
determinations of the topological susceptibility in the literature, both at
finite $N$ and in the large-$N$ limit.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [229] [Single and Multi-Objective Optimization of Distributed Acoustic Sensing Cable Layouts for Geophysical Applications](https://arxiv.org/abs/2510.07531)
*Dominik Strutz,Tjeerd Kiers,Andrew Curtis*

Main category: physics.geo-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a systematic approach to optimise distributed acoustic sensing
(DAS) fibre-optic cable layouts using global optimisation techniques. Our
method represents cable geometries using splines, enabling efficient
exploration of layouts while respecting physical deployment constraints. The
use of evolutionary algorithms enables single and multi-objective optimisation,
taking into account complex design constraints such as terrain, accesibility,
exclusion zones and cable length, while allowing efficient parallelisation of
the optimisation process. We demonstrate the approach on a real-world case
study, optimising the layout of a DAS cable for monitoring slope stability in
the Cuolm da Vi area of Switzerland. We adapt design criteria for seismic
source location problems, and for ambient noise surface wave tomography, to
account for the unique characteristics of DAS, such as directional sensitivity
patterns. The results show significant potential for improvements in source
location accuracy and surface wave tomographic resolution by optimising cable
layouts, highlighting the potential of this approach for optimising DAS
deployments in various geophysical applications.

</details>


### [230] [A Geomechanically-Informed Framework for Wellbore Trajectory Prediction: Integrating First-Principles Kinematics with a Rigorous Derivation of Gated Recurrent Networks](https://arxiv.org/abs/2510.07564)
*Shubham Kumar,Anshuman Sahoo*

Main category: physics.geo-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate wellbore trajectory prediction is a paramount challenge in
subsurface engineering, governed by complex interactions between the drilling
assembly and heterogeneous geological formations. This research establishes a
comprehensive, mathematically rigorous framework for trajectory prediction that
moves beyond empirical modeling to a geomechanically-informed, data-driven
surrogate approach.The study leverages Log ASCII Standard (LAS) and wellbore
deviation (DEV) data from 14 wells in the Gulfaks oil field, treating
petrophysical logs not merely as input features, but as proxies for the
mechanical properties of the rock that fundamentally govern drilling dynamics.
A key contribution of this work is the formal derivation of wellbore kinematic
models, including the Average Angle method and Dogleg Severity, from the first
principles of vector calculus and differential geometry, contextualizing them
as robust numerical integration schemes. The core of the predictive model is a
Gated Recurrent Unit (GRU) network, for which we provide a complete,
step-by-step derivation of the forward propagation dynamics and the
Backpropagation Through Time (BPTT) training algorithm. This detailed
theoretical exposition, often omitted in applied studies, clarifies the
mechanisms by which the network learns temporal dependencies. The methodology
encompasses a theoretically justified data preprocessing pipeline, including
feature normalization, uniform depth resampling, and sequence generation.
Trajectory post-processing and error analysis are conducted using Mean Absolute
Error (MAE), Root Mean Square Error (RMSE), and the Coefficient of
Determination (R2).

</details>


### [231] [Diffusion-Based Probabilistic Modeling for Hourly Streamflow Prediction and Assimilation](https://arxiv.org/abs/2510.08488)
*Wencong Yang,Haoyu Ji,Leo Lonzarich,Yalan Song,Chaopeng Shen*

Main category: physics.geo-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hourly predictions are critical for issuing flood warnings as the flood peaks
on the hourly scale can be distinctly higher than the corresponding daily ones.
Currently a popular hourly data-driven prediction scheme is multi-time-scale
long short-term memory (MTS-LSTM), yet such models face challenges in
probabilistic forecasts or integrating observations when available. Diffusion
artificial intelligence (AI) models represent a promising method to predict
high-resolution information, e.g., hourly streamflow. Here we develop a
denoising diffusion probabilistic model (h-Diffusion) for hourly streamflow
prediction that conditions on either observed or simulated daily discharge from
hydrologic models to generate hourly hydrographs. The model is benchmarked on
the CAMELS hourly dataset against record-holding MTS-LSTM and multi-frequency
LSTM (MF-LSTM) baselines. Results show that h-Diffusion outperforms baselines
in terms of general performance and extreme metrics. Furthermore, the
h-Diffusion model can utilize the inpainting technique and recent observations
to accomplish data assimilation that largely improves flood forecasting
performance. These advances can greatly reduce flood forecasting uncertainty
and provide a unified probabilistic framework for downscaling, prediction, and
data assimilation at the hourly scale, representing risks where daily models
cannot.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [232] [IKNet: Interpretable Stock Price Prediction via Keyword-Guided Integration of News and Technical Indicators](https://arxiv.org/abs/2510.07661)
*Jinwoong Kim,Sangjin Park*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing influence of unstructured external information, such as news
articles, on stock prices has attracted growing attention in financial markets.
Despite recent advances, most existing newsbased forecasting models represent
all articles using sentiment scores or average embeddings that capture the
general tone but fail to provide quantitative, context-aware explanations of
the impacts of public sentiment on predictions. To address this limitation, we
propose an interpretable keyword-guided network (IKNet), which is an
explainable forecasting framework that models the semantic association between
individual news keywords and stock price movements. The IKNet identifies
salient keywords via FinBERTbased contextual analysis, processes each embedding
through a separate nonlinear projection layer, and integrates their
representations with the time-series data of technical indicators to forecast
next-day closing prices. By applying Shapley Additive Explanations the model
generates quantifiable and interpretable attributions for the contribution of
each keyword to predictions. Empirical evaluations of S&P 500 data from 2015 to
2024 demonstrate that IKNet outperforms baselines, including recurrent neural
networks and transformer models, reducing RMSE by up to 32.9% and improving
cumulative returns by 18.5%. Moreover, IKNet enhances transparency by offering
contextualized explanations of volatility events driven by public sentiment.

</details>


### [233] [Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching](https://arxiv.org/abs/2510.07957)
*Shihe Zhou,Ruikun Li,Huandong Wang,Yong Li*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Forecasting state evolution of network systems, such as the spread of
information on social networks, is significant for effective policy
interventions and resource management. However, the underlying propagation
dynamics constantly shift with new topics or events, which are modeled as
changing coefficients of the underlying dynamics. Deep learning models struggle
to adapt to these out-of-distribution shifts without extensive new data and
retraining. To address this, we present Zero-Shot Forecasting of Network
Dynamics through Weight Flow Matching (FNFM), a generative,
coefficient-conditioned framework that generates dynamic model weights for an
unseen target coefficient, enabling zero-shot forecasting. Our framework
utilizes a Variational Encoder to summarize the forecaster weights trained in
observed environments into compact latent tokens. A Conditional Flow Matching
(CFM) module then learns a continuous transport from a simple Gaussian
distribution to the empirical distribution of these weights, conditioned on the
dynamical coefficients. This process is instantaneous at test time and requires
no gradient-based optimization. Across varied dynamical coefficients, empirical
results indicate that FNFM yields more reliable zero-shot accuracy than
baseline methods, particularly under pronounced coefficient shift.

</details>


### [234] [Reverse Supply Chain Network Design of a Polyurethane Waste Upcycling System](https://arxiv.org/abs/2510.08097)
*Dalga Merve Özkan,Sergio Lucia,Sebastian Engell*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a general mathematical programming framework for the
design and optimization of supply chain infrastructures for the upcycling of
plastic waste. For this purpose, a multi-product, multi-echelon, multi-period
mixed-integer linear programming (MILP) model has been formulated. The
objective is to minimize the cost of the entire circular supply chain starting
from the collection of post-consumer plastic waste to the production of
virgin-equivalent high value polymers, satisfying a large number of constraints
from collection quota to the quality of the feedstock. The framework aims to
support the strategic planning of future circular supply chains by determining
the optimal number, locations and sizes of various types of facilities as well
as the amounts of materials to be transported between the nodes of the supply
chain network over a specified period. The functionality of the framework has
been tested with a case study for the upcycling of rigid polyurethane foam
waste coming from construction sites in Germany. The economic potential and
infrastructure requirements are evaluated, and it has been found that from a
solely economic perspective, the current status of the value chain is not
competitive with fossil-based feedstock or incineration. However, with the
right economic incentives, there is a considerable potential to establish such
value chains, once the upcycling technology is ready and the economic framework
conditions have stabilized.

</details>


### [235] [Poisson Energy Formulation for Floorplanning: Variational Analysis and Mathematical Foundations](https://arxiv.org/abs/2510.08126)
*Wenxing Zhu,Hao Ai*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Arranging many modules within a bounded domain without overlap, central to
the Electronic Design Automation (EDA) of very large-scale integrated (VLSI)
circuits, represents a broad class of discrete geometric optimization problems
with physical constraints. This paper develops a variational and spectral
framework for Poisson energy-based floorplanning and placement in physical
design. We show that the Poisson energy, defined via a Neumann Poisson
equation, is exactly the squared H^{-1} Sobolev norm of the density residual,
providing a functional-analytic interpretation of the classical electrostatic
analogy. Through spectral analysis, we demonstrate that the energy acts as an
intrinsic low-pass filter, suppressing high-frequency fluctuations while
enforcing large-scale uniformity. Under a mild low-frequency dominance
assumption, we establish a quantitative linear lower bound relating the Poisson
energy to the geometric overlap area, thereby justifying its use as a smooth
surrogate for the hard nonoverlap constraint. We further show that projected
gradient descent converges globally to stationary points and exhibits local
linear convergence near regular minima. Finally, we interpret the
continuous-time dynamics as a Wasserstein-2 gradient flow, revealing the
intrinsic nonlocality and global balancing behavior of the model. These results
provide a mathematically principled foundation for PDE-regularized optimization
in large-scale floorplanning and related geometric layout problems.

</details>


### [236] [Design of chemical recycling processes for PUR foam under uncertainty](https://arxiv.org/abs/2510.08301)
*Patrick Lotz,Luca Bosetti,André Bardow,Sergio Lucia,Sebastian Engell*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Optimization problems in chemical process design involve a significant number
of discrete and continuous decisions. When taking into account uncertainties,
the search space is very difficult to explore, even for experienced engineers.
Moreover, it should be taken into account that while some decisions are fixed
at the design stage, other parameters can be adapted to the realization of the
uncertainty during the operation of the plant. This leads to a two-stage
optimization problem which is difficult to solve. To address this challenge, we
propose to combine commercial process simulation software with an evolutionary
strategy. This approach is applied to designing a downstream process to isolate
valuable products from pyrolysis oil produced by the catalytic pyrolysis of
rigid polyurethane foam. The suggested algorithm consistently performed better
than a manually designed robust process. Additionally, the analysis of
different scenarios provided insight into promising changes in the overall
layout of the recycling process.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [237] [Beyond independent component analysis: identifiability and algorithms](https://arxiv.org/abs/2510.07525)
*Alvaro Ribot,Anna Seigal,Piotr Zwiernik*

Main category: math.ST

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Independent Component Analysis (ICA) is a classical method for recovering
latent variables with useful identifiability properties. For independent
variables, cumulant tensors are diagonal; relaxing independence yields tensors
whose zero structure generalizes diagonality. These models have been the
subject of recent work in non-independent component analysis. We show that
pairwise mean independence answers the question of how much one can relax
independence: it is identifiable, any weaker notion is non-identifiable, and it
contains the models previously studied as special cases. Our results apply to
distributions with the required zero pattern at any cumulant tensor. We propose
an algebraic recovery algorithm based on least-squares optimization over the
orthogonal group. Simulations highlight robustness: enforcing full independence
can harm estimation, while pairwise mean independence enables more stable
recovery. These findings extend the classical ICA framework and provide a
rigorous basis for blind source separation beyond independence.

</details>


### [238] [Adaptive Thresholds for Monitoring and Screening in Imbalanced Samples: Optimality and Boosting Sensitivity](https://arxiv.org/abs/2510.08035)
*Ansgar Steland*

Main category: math.ST

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Suppose (standardized) measurements or statistics are monitored to raise an
alarm when a threshold is exceeded. Often, the underlying population is
heterogenous with respect to important discrete variables and thus samples may
consist of imbalanced classes. We propose to use thresholds which depend on
such covariates to boost the sensitivity for rare classes, which otherwise tend
to be ignored. Under mild conditions, we identify optimal threshold functions
and develop a feasible procedure for their computation. Further, for the
proportional rule a nonparametric estimator of the threshold function is
proposed and a central limit theorem is shown, including the case that
conditional mean and variance used for standardization are estimated. For
feasible uncertainty quantification a bootstrap scheme is proposed. The
approach is illustrated and evaluated by a real data analysis.

</details>


### [239] [Structured covariance estimation via tensor-train decomposition](https://arxiv.org/abs/2510.08174)
*Artsiom Patarusau,Nikita Puchkin,Maxim Rakhuba,Fedor Noskov*

Main category: math.ST

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider a problem of covariance estimation from a sample of i.i.d.
high-dimensional random vectors. To avoid the curse of dimensionality we impose
an additional assumption on the structure of the covariance matrix $\Sigma$. To
be more precise we study the case when $\Sigma$ can be approximated by a sum of
double Kronecker products of smaller matrices in a tensor train (TT) format.
Our setup naturally extends widely known Kronecker sum and CANDECOMP/PARAFAC
models but admits richer interaction across modes. We suggest an iterative
polynomial time algorithm based on TT-SVD and higher-order orthogonal iteration
(HOOI) adapted to Tucker-2 hybrid structure. We derive non-asymptotic
dimension-free bounds on the accuracy of covariance estimation taking into
account hidden Kronecker product and tensor train structures. The efficiency of
our approach is illustrated with numerical experiments.

</details>


### [240] [Navigating Sparsities in High-Dimensional Linear Contextual Bandits](https://arxiv.org/abs/2510.08435)
*Rui Zhao,Zihan Chen,Zemin Zheng*

Main category: math.ST

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-dimensional linear contextual bandit problems remain a significant
challenge due to the curse of dimensionality. Existing methods typically
consider either the model parameters to be sparse or the eigenvalues of context
covariance matrices to be (approximately) sparse, lacking general applicability
due to the rigidity of conventional reward estimators. To overcome this
limitation, a powerful pointwise estimator is introduced in this work that
adaptively navigates both kinds of sparsity. Based on this pointwise estimator,
a novel algorithm, termed HOPE, is proposed. Theoretical analyses demonstrate
that HOPE not only achieves improved regret bounds in previously discussed
homogeneous settings (i.e., considering only one type of sparsity) but also,
for the first time, efficiently handles two new challenging heterogeneous
settings (i.e., considering a mixture of two types of sparsity), highlighting
its flexibility and generality. Experiments corroborate the superiority of HOPE
over existing methods across various scenarios.

</details>


### [241] [Computational and statistical lower bounds for low-rank estimation under general inhomogeneous noise](https://arxiv.org/abs/2510.08541)
*Debsurya De,Dmitriy Kunisky*

Main category: math.ST

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work has generalized several results concerning the well-understood
spiked Wigner matrix model of a low-rank signal matrix corrupted by additive
i.i.d. Gaussian noise to the inhomogeneous case, where the noise has a variance
profile. In particular, for the special case where the variance profile has a
block structure, a series of results identified an effective spectral algorithm
for detecting and estimating the signal, identified the threshold signal
strength required for that algorithm to succeed, and proved
information-theoretic lower bounds that, for some special signal distributions,
match the above threshold. We complement these results by studying the
computational optimality of this spectral algorithm. Namely, we show that, for
a much broader range of signal distributions, whenever the spectral algorithm
cannot detect a low-rank signal, then neither can any low-degree polynomial
algorithm. This gives the first evidence for a computational hardness
conjecture of Guionnet, Ko, Krzakala, and Zdeborov\'a (2023). With similar
techniques, we also prove sharp information-theoretic lower bounds for a class
of signal distributions not treated by prior work. Unlike all of the above
results on inhomogeneous models, our results do not assume that the variance
profile has a block structure, and suggest that the same spectral algorithm
might remain optimal for quite general profiles. We include a numerical study
of this claim for an example of a smoothly-varying rather than
piecewise-constant profile. Our proofs involve analyzing the graph sums of a
matrix, which also appear in free and traffic probability, but we require new
bounds on these quantities that are tighter than existing ones for non-negative
matrices, which may be of independent interest.

</details>
