{"id": "2602.20108", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20108", "abs": "https://arxiv.org/abs/2602.20108", "authors": ["L. Brodoloni", "G. E. Astrakharchik", "S. Giorgini", "S. Pilati"], "title": "Energy gap of quantum spin glasses: a projection quantum Monte Carlo study", "comment": "6 pages plus additional material", "summary": "The performance of quantum annealing for combinatorial optimization is fundamentally limited by the minimum energy gap $Δ$ encountered at quantum phase transitions. We investigate the scaling of $Δ$ with system size $N$ for two paradigmatic quantum spin-glass models: the two-dimensional Edwards-Anderson (2D-EA) and the all-to-all Sherrington-Kirkpatrick (SK) models. Utilizing a newly proposed unbiased energy-gap estimator for continuous-time projection quantum Monte Carlo simulations, complemented by high-performance sparse eigenvalue solvers, we characterize the gap distributions across disorder realizations. It is found that, in the 2D-EA case, the inverse-gap distribution develops a fat tail with infinite variance as $N$ increases. This indicates that the unfavorable super-algebraic scaling of $Δ$, recently reported for binary couplings [Nature 631, 749 (2024)], persists for the Gaussian disorder considered here, pointing to a universal feature of 2D spin glasses. Conversely, the SK model retains a finite-variance distribution, with the disorder-averaged gap following a rather slow power law, close to $Δ\\propto N^{-1/3}$. This finding provides a promising outlook for the potential efficiency of quantum annealers for optimization problems with dense connectivity."}
{"id": "2602.18441", "categories": ["physics.comp-ph", "cs.CE", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18441", "abs": "https://arxiv.org/abs/2602.18441", "authors": ["Dimitrios Voulanas", "Eduardo Gildin"], "title": "Updating DMD Operators for Changes in Domain Properties", "comment": null, "summary": "Fast and reliable surrogate models are critical for optimization, control and uncertainty analysis in geological carbon-storage projects, yet high-fidelity multiphase simulators remain too expensive. Dynamic Mode Decomposition (DMD) offers an attractive data-driven reduction framework, but its operators are trained for a single set of reservoir properties. When permeability or well location changes, conventional practice is to regenerate snapshots and retrain the surrogate, erasing most of the speed advantage. This work presents a lightweight alternative that updates an existing DMD or DMD-with-control model without incorporating new simulation data or retraining. Two complementary update strategies are introduced. For cases where permeability changes uniformly across the domain, the proposed updates adjust the models internal dynamics and control response to match the new flow timescale. When permeability varies in space, the approach modifies the spatial representation so that high-permeability zones are given greater influence on the models reduced basis. Numerical experiments demonstrate that the proposed updates recover plume migration and pressure build-up within three percent of a freshly trained surrogate yet execute hundreds of times faster than full retraining. These methods therefore enable real-time optimization and rapid what-if studies while preserving the physical fidelity demanded by carbon-storage workflows."}
{"id": "2602.18714", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.18714", "abs": "https://arxiv.org/abs/2602.18714", "authors": ["Hitoshi Yamada"], "title": "Universal Basic Income with Time-Decaying Currency: Structural Effects on Essential Labor and Long-Term Formation", "comment": "13 pages, 3 figures. Simulation-based study of dual-currency UBI mechanism", "summary": "Time-decaying currencies have long been discussed in economic theory as a means to discourage hoarding and promote circulation. However, their modern digital implementation as a universal basic income (UBI) mechanism raises unresolved structural questions regarding labor participation and long-term social reproduction. In this study, we analyze a dual-currency model in which a time-decaying currency is distributed exclusively as UBI, while labor income and savings are denominated in a standard currency. Through agent-based simulations, we identify the acceptance ratio of the time-decaying currency for necessities as a critical design parameter. Our results show that essential labor does not necessarily collapse under such a system. Nevertheless, beyond a threshold acceptance ratio, delayed labor participation and weakened human capital formation emerge even in the absence of material deprivation. These findings suggest that time-decaying currency can stabilize short-term living conditions while distorting long-term formation incentives, depending on system design."}
{"id": "2602.18646", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.18646", "abs": "https://arxiv.org/abs/2602.18646", "authors": ["Yingkai Sha", "Tracy Hertneky", "Ethan Gutmann", "Seth McGinnis", "Lulin Xue", "David John Gagne", "Kathryn Newman", "Andrew Newman"], "title": "AI-based Regional Emulation for Kilometer-Scale Dynamical Downscaling", "comment": null, "summary": "An AI-based Limited-Area Model (LAM) is developed for dynamical downscaling over the Southern Great Plains and the southeastern United States, with strong generalization abilities under diverse boundary conditions. The model is trained using 0.25-degree, 3-hourly ERA5 as forcings and CONUS404 as targets in 1980--2019, producing 4-km, hourly dynamical downscaling outputs; it is also connected to a post-processing model to derive additional diagnostic variables. The model is evaluated across multiple forcing datasets, time periods, and climate regimes. For present-day downscaling in the 2021--2024 water years, the model produces stable multi-year simulations with no unrealistic drift; its deterministic verification scores are comparable to other weather-forecasting-oriented AI models. The model also generalizes robustly to a 1.0-degree, 6-hourly non-ERA5 forcing dataset, yielding only minor performance changes. Frontal cyclone and hurricane case studies further demonstrate that the model reconstructs realistic, interpretable weather-scale dynamical and thermodynamic structure from coarse boundary information. The AI-based LAM is further tested by downscaling 30-year global climate model runs in 1980--2010 and 2070--2100, and climate model ensembles in 2025-2027. In this application, the model remains stable at hourly downscaling frequencies for all 30 years and effectively captures future climate-change signals, indicating meaningful generalization across different climate regimes. When downscaling ensembles, the model produces well-posed ensemble distributions without collapsing the ensemble spread. Overall, the AI-based LAM of this study offers good downscaling performance and generalization abilities. It provides a practical and transferable example of adapting AI weather prediction models for regional climate applications."}
{"id": "2602.19481", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.19481", "abs": "https://arxiv.org/abs/2602.19481", "authors": ["Victor H. de la Pena", "Fangyuan Lin", "Victor K. de la Pena"], "title": "A Selection Premium Decomposition for the Expected Maximum of Random Walks", "comment": null, "summary": "When $K$ models are evaluated on the same validation set of size $n$, the selected winner's apparent performance is biased upward. Suppose $K$ models are evaluated on a shared sequence of i.i.d. observations $X_1,\\dots, X_n$, where model $k$ achieves response $f_k(X_i)$ with mean $μ_k = \\mathbb E[f_k(X)]$. Writing $Y_{i,k} = f_k(X_i)-μ_k$ for the centered increment and $S_{n,k} = \\sum_{i=1}^n Y_{i,k}$ for the centered cumulative score, the expected maximum satisfies $0\\le\\mathbb E\\bigl[\\max_k S_{n,k}\\bigr] = \\sum_{i=1}^n \\mathbb E\\bigl[\\varphi_K(S_{i-1})\\bigr]$ where $\\varphi_K(u) = \\mathbb{E}\\bigl[\\max_k(u_k + Y_k)\\bigr] - \\max_k u_k$, $u\\in \\mathbb R^K$, is the selection premium function. This formula corresponds to the null hypothesis case (all models are equal in the sense that they have the same mean), which clarifies that the bias arises from selection. While this decomposition follows from elementary conditioning and telescoping, we develop the analytical consequences in five directions. (i) structural properties of $\\varphi_K$; (ii) extension to stopping times, recovering Wald's equation at $K=1$; (iii) a winner's curse decomposition for heterogeneous means; (iv) a universal bias concentration law showing that the first $α$-fraction of observations generates a $\\sqrtα$-fraction of total bias."}
{"id": "2602.18546", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18546", "abs": "https://arxiv.org/abs/2602.18546", "authors": ["Lin Chen", "Fengli Xu", "Esteban Moro", "Pan Hui", "Yong Li", "James Evans"], "title": "Urban mobility network centrality predicts social resilience", "comment": null, "summary": "Cities thrive on social interactions that foster well-being, innovation, and prosperity; yet, exogenous shocks such as pandemics, hurricanes, and wildfires can severely disrupt them. Different urban venues exhibit widely divergent response patterns, raising key questions about what factors contribute to these differences and how we can anticipate and respond. Understanding these questions is crucial for safeguarding social resilience, the capacity of urban venues to maintain both visitation and diversity. In this study, we analyze large-scale human mobility data from 15 US cities covering more than 103 million residents across three distinct urban shocks. Despite a general trend of declining visitation and weakened social mixing, 36.28%-53.01% of venues exhibit reduced segregation, and 21.04%-38.55% of venues exhibit increased visitation. By constructing a mobility network interlinking types of urban venues, we reveal that eigenvector network centrality tends to indicate the provision of essential services and robustly predicts social resilience across varied urban shocks. Specifically, centrality elevates the explanatory power by more than 80% in predicting both segregation and mobility change, compared with more intuitive features. Furthermore, compared to peripheral venues, core venues featuring shorter visit distances, broader neighborhood visitation, shorter visitor dwell times, and steadier popularity throughout the day. Such patterns imply a dual social mechanism: core venues sustain social ties through frequent informal interaction, while peripheral ones facilitate deeper engagement around specialized interests and their corresponding social circles. By bridging urban mobility research with economic theories that distinguish staple from discretionary products, we propose a well-and-pool analogy that suggests how people spend their varying urban mobility budgets."}
{"id": "2602.19696", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.19696", "abs": "https://arxiv.org/abs/2602.19696", "authors": ["Sacha Sinet", "Nathalie A. M. Delmeire", "Paul D. L. Ritchie", "Henk A. Dijkstra", "Anna S. von der Heydt"], "title": "A Criterion for Safe Overshoot in Coupled Tipping Systems", "comment": null, "summary": "Abrupt transitions are a central concern in climate and ecological research, and may arise when critical thresholds known as tipping points are crossed. However, previous work has shown that finite-time overshoots of tipping points can be safe, and that such behavior is captured by an inverse-square-law criterion when overshoots are sufficiently small and slow. So far studied in isolated systems with external drivers, (un)safe overshoots may also emerge from interactions between subsystems. Here, we investigate safe-overshoot phenomena in unidirectionally coupled slow-fast systems featuring both nonlinear interactions and coupling through time-derivatives. Specifically, we derive a criterion for the occurrence of safe overshoots analogous to the inverse-square law for isolated systems, but adapted to interactive settings, and expressed explicitly in terms of the timescale separation and coupling strength between subsystems. We illustrate these results using two conceptual models in which the Atlantic Meridional Overturning Circulation interacts with either the Amazon rainforest or the Greenland Ice Sheet."}
{"id": "2602.18892", "categories": ["nlin.AO", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18892", "abs": "https://arxiv.org/abs/2602.18892", "authors": ["Everton R. Constantino", "Alberto Saa"], "title": "Evolving scale-free networks and node-based random edge deletion", "comment": "7 pages, 4 figures", "summary": "We investigate a growing network model that combines preferential and uniform attachment with two distinct mechanisms of edge deletion. In addition to the usual uniform probability edge deletion, we introduce a novel node-based rule in which uniformly chosen non-isolated nodes lose one of their incident edges. This mechanism differs fundamentally from uniform edge deletion and leads to a nonlinear evolution for the stationary degree distribution due to the nonlinear dependence on the fraction of isolated nodes. We solve the general problem in the stationary regime and obtain closed-form expressions for the degree distribution in terms of hypergeometric and confluent hypergeometric functions. Depending on the balance between attachment and deletion rates, three asymptotic regimes for the degree distribution arise: power-law, exponential, and a critical regime characterized by a stretched exponential decay. We show that the node-based edge deletion mechanism is less likely to disrupt the scale-free (power-law) regime than the uniform edge deletion. Moreover, we also demonstrate that the precise balance between preferential attachment and node-based deletion can transform a scale-free network into a critical one with stretched exponential decay. Extensive numerical simulations exhibit excellent agreement with the theoretical predictions."}
{"id": "2602.19767", "categories": ["hep-lat"], "pdf": "https://arxiv.org/pdf/2602.19767", "abs": "https://arxiv.org/abs/2602.19767", "authors": ["Abhijeet Kishore", "Subhasish Basak", "Dipankar Chakrabarti"], "title": "Index theorem with Minimally Doubled Fermions in four space-time dimensions", "comment": "18 pages, 15 figures, 6 tables. Comments and suggestions are welcome", "summary": "We determine the zero eigenmode spectrum of Minimally Doubled Fermions (MDF), namely in Karsten-Wilczek (KW) and Borici-Creutz (BC) formulations on the 4-dimensional space-time lattice. We employ background gauge fields with integer valued topological charges. The Atiyah-Singer index theorem is verified in the presence of two different background gauge fields, namely Smit-Vink [1] and cooled down MILC asqtad ensembles with $N_f=2+1$ dynamical flavors of quarks [2]. Using flavored mass terms [3,4], we find that the spectral flow of the eigenvalues detects the topology of the background gauge field. With the use of the modified chirality operator, we obtain chiralities of the zero eigenmodes and the fermionic topological charge."}
{"id": "2602.18668", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18668", "abs": "https://arxiv.org/abs/2602.18668", "authors": ["Zeinab Salehi", "Elizabeth L. Ratnam", "Yijun Chen", "Ian R. Petersen", "Guodong Shi", "Duncan S. Callaway"], "title": "An Electricity Market with Reactive Power Trading: Incorporating Dynamic Operating Envelopes", "comment": null, "summary": "Electricity market design that accounts for grid constraints such as voltage and thermal limits at the distribution level can increase opportunities for the grid integration of Distributed Energy Resources (DERs). In this paper, we consider rooftop solar backed by battery storage connected to a distribution grid. We design an electricity market to support customers sharing rooftop generation in excess of their energy demand, where customers earn a profit through peer-to-peer (P2P) energy trading. Our proposed electricity market also incorporates P2P reactive power trading to improve the voltage profile across a distribution feeder. We formulate the electricity market as an optimization-based problem, where voltage and thermal limits across a feeder are managed through the assignment of customer-specific dynamic operating envelopes (DOEs). The electricity market equilibrium is referred to as a competitive equilibrium, which is equivalent to a Nash equilibrium in a standard game. Our proposed market design is benchmarked using the IEEE 13-node test feeder."}
{"id": "2602.18442", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.18442", "abs": "https://arxiv.org/abs/2602.18442", "authors": ["Hirofumi Wakimoto"], "title": "Ostrom-Weighted Bootstrap: A Theoretically Optimal and Provably Complete Framework for Hierarchical Imputation in Multi-Agent Systems", "comment": "7 pages, initial submission", "summary": "We study the statistical properties of the \\emph{Ostrom-Weighted Bootstrap} (OWB), a hierarchical, variance-aware resampling scheme for imputing missing values and estimating archetypes in multi-agent voting data. At Level~1, under mild linear model assumptions, the \\emph{ideal} OWB estimator -- with known persona-level (agent-level) variances -- is shown to be the Gauss--Markov best linear unbiased estimator (BLUE) and to strictly dominate uniform weighting whenever persona variances differ. At Level~2, within a canonical hierarchical normal model, the ideal OWB coincides with the conditional Bayesian posterior mean of the archetype. We then analyze the \\emph{feasible} OWB, which replaces unknown variances with hierarchically pooled empirical estimates, and show that it can be interpreted as both a feasible generalized least-squares (FGLS) and an empirical-Bayes shrinkage estimator with asymptotically valid weighted bootstrap confidence intervals under mild regularity conditions. Finally, we establish a Zero-NaN Guarantee: as long as each petal has at least one finite observation, the OWB imputation algorithm produces strictly NaN-free completed data using only explicit, non-uniform bootstrap weights and never resorting to uniform sampling or numerical zero-filling.\n  To our knowledge, OWB is the first resampling-based method that simultaneously achieves exact BLUE optimality, conditional Bayesian posterior mean interpretation, empirical Bayes shrinkage of precision parameters, asymptotic efficiency via FGLS, consistent weighted bootstrap inference, and provable zero-NaN completion under minimal data assumptions."}
{"id": "2602.18668", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18668", "abs": "https://arxiv.org/abs/2602.18668", "authors": ["Zeinab Salehi", "Elizabeth L. Ratnam", "Yijun Chen", "Ian R. Petersen", "Guodong Shi", "Duncan S. Callaway"], "title": "An Electricity Market with Reactive Power Trading: Incorporating Dynamic Operating Envelopes", "comment": null, "summary": "Electricity market design that accounts for grid constraints such as voltage and thermal limits at the distribution level can increase opportunities for the grid integration of Distributed Energy Resources (DERs). In this paper, we consider rooftop solar backed by battery storage connected to a distribution grid. We design an electricity market to support customers sharing rooftop generation in excess of their energy demand, where customers earn a profit through peer-to-peer (P2P) energy trading. Our proposed electricity market also incorporates P2P reactive power trading to improve the voltage profile across a distribution feeder. We formulate the electricity market as an optimization-based problem, where voltage and thermal limits across a feeder are managed through the assignment of customer-specific dynamic operating envelopes (DOEs). The electricity market equilibrium is referred to as a competitive equilibrium, which is equivalent to a Nash equilibrium in a standard game. Our proposed market design is benchmarked using the IEEE 13-node test feeder."}
{"id": "2602.19732", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.19732", "abs": "https://arxiv.org/abs/2602.19732", "authors": ["Fabrizio Cipollini", "Giulia Cruciani", "Giampiero M. Gallo", "Alessandra Insana", "Edoardo Otranto", "Fabio Spagnolo"], "title": "VOLatility Archive for Realized Estimates (VOLARE)", "comment": null, "summary": "VOLARE (VOLatility Archive for Realized Estimates - https://volare.unime.it) is an open research infrastructure providing standardized realized volatility and covariance measures constructed from ultra-high-frequency financial data. The platform processes tick-level observations across equities, exchange rates, and futures using an asset-specific pipeline that addresses heterogeneous trading calendars, microstructure noise, and timestamp precision. For equities, price series are cleaned using a documented outlier detection procedure and sampled at regular intervals.\n  VOLARE delivers a comprehensive set of realized estimators, including realized variance, range-based measures, bipower variation, semivariances, realized quarticity, realized kernels, and multivariate covariance measures, ensuring methodological consistency and cross-asset comparability. In addition to bulk dataset download, the platform supports interactive visualization and real-time estimation of established volatility models such as HAR and MEM specifications."}
{"id": "2602.18485", "categories": ["physics.geo-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2602.18485", "abs": "https://arxiv.org/abs/2602.18485", "authors": ["Jiayang Wang", "I-Tzu Huang", "Bingxu Luo", "Susan L. Beck", "Falk Huettmann", "Skyler DeVaughn", "Benjamin Stilin", "Keith Runge", "Pierre Deymier", "Marat I. Latypov"], "title": "Hearing the forest for the trees: machine learning and topological acoustics for remote sensing with seismic noise", "comment": null, "summary": "Monitoring remote forests is a global challenge central to climate mitigation and biodiversity conservation, yet satellite observations are frequently limited by weather, dense canopies, and solar dependency. Here we show that passive seismic sensing offers a persistent, all-weather alternative for autonomous ecosystem monitoring by capturing characteristic learnable signatures of trees within the ambient wavefield. Using seismic data from Alaska, we demonstrate that cross-correlations between stations provide a physical basis for forest detection by approximating the empirical Green's function of the medium. Supervised machine learning models applied to these data achieve a classification accuracy of 86%, identifying key discriminating frequencies (35 to 60 Hz) consistent with known forest-wave interactions. A topological acoustics analysis of the geometric phase change independently confirms the physical origin of these data-driven classifications. Together, these results provide the first demonstration that subtle forest-wave interactions manifest in ambient seismic noise and can be harnessed as a scalable tool for continuous vegetation monitoring, offering a robust solution for tracking environmental change challenging regions."}
{"id": "2602.18561", "categories": ["cond-mat.str-el", "hep-th", "nucl-th"], "pdf": "https://arxiv.org/pdf/2602.18561", "abs": "https://arxiv.org/abs/2602.18561", "authors": ["Fakher Assaad", "Johanna Erdmenger", "Anika Götz", "René Meyer", "Martin Rackl", "Yanick Thurn"], "title": "Analytic continuation of Green's functions with a neural network", "comment": "15 pages, 4 figures, 1 table", "summary": "An important problem in many-body physics is to reconstruct the spectral density from the imaginary-time domain Green's function. Typically, the imaginary-time Green's function is generated by Monte Carlo methods. As the one-point fermionic kernel diverges exponentially for large frequencies, numerical noise generically causes instabilities. We use a convolutional neural network to obtain the spectral density for a given imaginary time Green's function. The network is trained by data which we generate using random Gaussians. We improve the training data set available by including collision centers for the Gaussians rather than employing uniformly distributed Gaussians. Our network is constructed in such a way that its output fulfills positive semidefiniteness. We compare the results of our network with results of the Maximum Entropy method (MaxEnt), a standard method for the same reconstruction problem for the spectral density. This comparison is performed for three different cases, namely our Gaussian based test data as well as two physical models, the 1d Hubbard model showing spin-charge separation, and the two-dimensional SSH model in the self-consistent Born approximation. We find that the network outperforms MaxEnt when presented data close to the training set. For the physical models considered, MaxEnt recognizes physical features more precisely as compared to our network prediction. While it is hard to improve MaxEnt, the quality of the network depends on the training data set which can be systematically enhanced and improved."}
{"id": "2602.18725", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.18725", "abs": "https://arxiv.org/abs/2602.18725", "authors": ["Zixuan Cang", "Jingfeng Wang", "Xiaoqi Wei", "Yanxiang Zhao"], "title": "Synchronization of Unbalanced Dynamical Optimal Transport across Multiple Spaces", "comment": null, "summary": "Many biological systems are observed through heterogeneous modalities, requiring transport models that couple dynamics across spaces while allowing mass variation. To address this challenge, we introduce Unbalanced Synchronized Optimal Transport (UnSyncOT), a novel dynamical framework that synchronizes transport-reaction flows between spaces via either geometric embeddings (Monge type) or Markov kernels (Kantorovich type). For both cases we prove that UnSyncOT can be reduced to a single-space problem: the Monge model becomes a Benamou-Brenier problem with a metric-modified kinetic energy, and the Kantorovich model yields a nonlocal action induced by the synchronization operator, both of which fit within a dissipation-distance formulation. We also analyze the pure transport (Wasserstein) and pure reaction (Fisher-Rao) limits and derive structural properties. For the Kantorovich case we propose an approximate UnSyncOT by introducing a Hellinger-Kantorovich based trapezoidal time discretization of the secondary action for efficient computation. Finally we present staggered-grid discretizations and primal-dual solvers, validate the convergence, stability, and efficiency, and demonstrate coherent dynamics reconstructions across spaces."}
{"id": "2602.19389", "categories": ["physics.soc-ph", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.19389", "abs": "https://arxiv.org/abs/2602.19389", "authors": ["Simon Woodruff", "Alicia Durham", "Alex Higginbottom", "Chris Raastad"], "title": "Extension of the fusion power plant costing standard", "comment": null, "summary": "This paper documents the work of the Clean Air Task Force (CATF) International Working Group (IWG) on Fusion Cost Analysis in 2024-2025, and the methodological extensions implemented in the CATF-supported branch of the pyFECONs fusion power-plant costing framework. Using the standards-aligned chart-of-accounts and physics-to-economics workflow established by ARPA-E. The IWG development reorganizes and deepens the framework around three architecture-defining cost-driver tracks for Magnetic Fusion Energy (MFE), Inertial Fusion Energy (IFE), and Magneto-Inertial Fusion Energy (MIFE). In particular, the generic driver placeholder in Account 22.1.3 is treated as a controlled swap-point and replaced by a full cost-account development for the dominant driver in each class, enabling auditable traceability from requirements and geometry to rolled-up plant costs. On top of this driver-centric foundation, we introduce a probabilistic costing layer that compounds materials price uncertainty, TRL-based maturity uncertainty, and learning-curve uncertainty into cost distributions. We then describe safety-informed costing that enumerates fusion-relevant hazards and maps mitigating systems, structures, and provisions into standardized accounts, together with scenario-parameterized regulatory and financial adders. Finally, we document expanded macroeconomic and finance parameterization and a value-metrics module that complements LCOE with investment and planning measures (NPV, IRR MIRR, revenue requirements, WACC-based annualization, and residual and follow-on value), all computed from the same COA-mapped outputs. Collectively, these additions convert a deterministic, standards-aligned costing backbone into an extensible analysis environment suitable for transparent sensitivity studies, uncertainty propagation, and safety- and finance-coupled interpretation of fusion pilot-plant and NOAK scenarios."}
{"id": "2602.19031", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19031", "abs": "https://arxiv.org/abs/2602.19031", "authors": ["Meng Zhang", "Ziang Yin", "Nicholas Gangi", "Alexander Chen", "Brett Bamfo", "Tianle Xu", "Jiaqi Gu", "Zhaoran Rena Huang"], "title": "SKYLIGHT: A Scalable Hundred-Channel 3D Photonic In-Memory Tensor Core Architecture for Real-time AI Inference", "comment": null, "summary": "The growing computational demands of artificial intelligence (AI) are challenging conventional electronics, making photonic computing a promising alternative. However, existing photonic architectures face fundamental scalability and reliability barriers. This paper introduces SKYLIGHT, a scalable 3D photonic in-memory tensor core architecture designed for real-time AI inference. By co-designing its topology, wavelength routing, accumulation, and programming in a 3D stack, SKYLIGHT overcomes key limitations. Its innovations include a low-loss 3D Si/SiN crossbar topology, a thermally robust non-micro-ring resonator (MRR)-based wavelength-division multiplexing (WDM) component, a hierarchical signal accumulation using a multi-port photodetector (PD), and optically programmed non-volatile phase-change material (PCM) weights. Importantly, SKYLIGHT enables in-situ weight updates that support label-free, layer-local learning (e.g., forward-forward local updates) in addition to inference. Using SimPhony for system-level modeling, we show that a single 144 x 256 SKYLIGHT core is feasible within a single reticle and delivers 342.1 TOPS at 23.7 TOPS/W, enabling ResNet-50 inference at 1212 FPS with 27 mJ per image, and achieves 84.17 FPS/W end-to-end (1.61 x higher than an NVIDIA RTX PRO 6000 Blackwell GPU) under the same workload in real-time measurements. System-level evaluations on four representative machine learning tasks, including unsupervised local self-learning, demonstrate SKYLIGHT's robustness to realistic hardware non-idealities (low-bit quantization and signal-proportional analog noise capturing modulation, PCM programming, and readout variations). With noise-aware training, SKYLIGHT maintains high task accuracy, validating its potential as a comprehensive solution for energy-efficient, large-scale photonic AI accelerators."}
{"id": "2602.18482", "categories": ["physics.comp-ph", "cond-mat.stat-mech", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18482", "abs": "https://arxiv.org/abs/2602.18482", "authors": ["Emil Hoffmann", "Maximilian Schebek", "Leon Klein", "Frank Noé", "Jutta Rogal"], "title": "Boltzmann Generators for Condensed Matter via Riemannian Flow Matching", "comment": null, "summary": "Sampling equilibrium distributions is fundamental to statistical mechanics. While flow matching has emerged as scalable state-of-the-art paradigm for generative modeling, its potential for equilibrium sampling in condensed-phase systems remains largely unexplored. We address this by incorporating the periodicity inherent to these systems into continuous normalizing flows using Riemannian flow matching. The high computational cost of exact density estimation intrinsic to continuous normalizing flows is mitigated by using Hutchinson's trace estimator, utilizing a crucial bias-correction step based on cumulant expansion to render the stochastic estimates suitable for rigorous thermodynamic reweighting. Our approach is validated on monatomic ice, demonstrating the ability to train on systems of unprecedented size and obtain highly accurate free energy estimates without the need for traditional multistage estimators."}
{"id": "2602.18885", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.18885", "abs": "https://arxiv.org/abs/2602.18885", "authors": ["Yinhua Piao", "Hyomin Kim", "Seonghwan Kim", "Yunhak Oh", "Junhyeok Jeon", "Sang-Yeon Hwang", "Jaechang Lim", "Woo Youn Kim", "Chanyoung Park", "Sungsoo Ahn"], "title": "Learning Adaptive Perturbation-Conditioned Contexts for Robust Transcriptional Response Prediction", "comment": "19 pages, 10 figures, 9 tables", "summary": "Predicting high-dimensional transcriptional responses to genetic perturbations is challenging due to severe experimental noise and sparse gene-level effects. Existing methods often suffer from mean collapse, where high correlation is achieved by predicting global average expression rather than perturbation-specific responses, leading to many false positives and limited biological interpretability. Recent approaches incorporate biological knowledge graphs into perturbation models, but these graphs are typically treated as dense and static, which can propagate noise and obscure true perturbation signals. We propose AdaPert, a perturbation-conditioned framework that addresses mean collapse by explicitly modeling sparsity and biological structure. AdaPert learns perturbation-specific subgraphs from biological knowledge graphs and applies adaptive learning to separate true signals from noise. Across multiple genetic perturbation benchmarks, AdaPert consistently outperforms existing baselines and achieves substantial improvements on DEG-aware evaluation metrics, indicating more accurate recovery of perturbation-specific transcriptional changes."}
{"id": "2602.19233", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.19233", "abs": "https://arxiv.org/abs/2602.19233", "authors": ["Mathieu Mure-Ravaud"], "title": "On Using Medium-Range Ensemble Forecasts for Storm Transposition of Synoptic-Scale Systems in Probable Maximum Precipitation Estimation", "comment": null, "summary": "Most methods for estimating probable maximum precipitation (PMP) -- the greatest depth of precipitation that is physically possible over a given area and duration -- rely on storm transposition (ST), the process of transporting a storm, either historically observed or simulated, from its original location to a target basin. Existing ST approaches, whether classical or physically based, involve assumptions and manipulations that can introduce inconsistencies, leaving the physical validity of the transposed storm uncertain. In this study, the internal variability leveraging (IVL) approach is used to transpose an atmospheric river cluster that affected the U.S. West Coast during 20-29 October 2021. Steering the storm toward the target basin and determining its transposition region are achieved by considering an ensemble of plausible storm evolutions and trajectories obtained from archived ECMWF medium-range forecasts. The Willamette River and Nass River watersheds, located approximately 6 deg N, 2 deg W and 16 deg N, 8 deg W, respectively, from the area most affected by the observed precipitation, were selected as target basins. For each basin, the IVL realization yielding the largest 24-h basin-average precipitation depth was identified, and the initial and boundary condition shifting method was subsequently applied to further enhance its impact, producing 24-h precipitation depths of 119 mm for the Willamette and 98 mm for the Nass."}
{"id": "2602.19709", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.19709", "abs": "https://arxiv.org/abs/2602.19709", "authors": ["Nils Lid Hjort", "Mike Titterington"], "title": "On Expectation Propagation and the Probabilistic Editor in some simple mixture problems", "comment": "22 pages, 0 figures; Mike Titterington passed away in 2023, at the age of 77; this is the October 2010 version of a paper we collaborated on then and (still) planned to extend before submitting to a journal", "summary": "As for other latent-variable problems, exact Bayesian analysis is typically not practicable for mixture problems and approximate methods have been developed. Variational Bayes tends to produce approximate posterior distributions for parameters that are too tightly concentrated in having variances that are too small. The paper identifies a few mixture problems in which Expectation Propagation and variations thereof lead to approximate posterior distributions that asymptotically exhibit `correct' variances and therefore stand to provide reliable interval estimates for the unknown parameter or parameters."}
{"id": "2602.18824", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18824", "abs": "https://arxiv.org/abs/2602.18824", "authors": ["Pedram Riyazimehr", "Seyyed Ehsan Mahmoudi"], "title": "UniRank: A Multi-Agent Calibration Pipeline for Estimating University Rankings from Anonymized Bibliometric Signals", "comment": null, "summary": "We present UniRank, a multi-agent LLM pipeline that estimates university positions across global ranking systems using only publicly available bibliometric data from OpenAlex and Semantic Scholar. The system employs a three-stage architecture: (a) zero-shot estimation from anonymized institutional metrics, (b) per-system tool-augmented calibration against real ranked universities, and (c) final synthesis. Critically, institutions are anonymized -- names, countries, DOIs, paper titles, and collaboration countries are all redacted -- and their actual ranks are hidden from the calibration tools during evaluation, preventing LLM memorization from confounding results. On the Times Higher Education (THE) World University Rankings ($n=352$), the system achieves MAE = 251.5 rank positions, Median AE = 131.5, PNMAE = 12.03%, Spearman $ρ= 0.769$, Kendall $τ= 0.591$, hit rate @50 = 20.7%, hit rate @100 = 39.8%, and a Memorization Index of exactly zero (no exact-match zero-width predictions among all 352 universities). The systematic positive-signed error (+190.1 positions, indicating the system consistently predicts worse ranks than actual) and monotonic performance degradation from elite tier (MAE = 60.5, hit@100 = 90.5%) to tail tier (MAE = 328.2, hit@100 = 20.8%) provide strong evidence that the pipeline performs genuine analytical reasoning rather than recalling memorized rankings. A live demo is available at https://unirank.scinito.ai ."}
{"id": "2602.19029", "categories": ["nlin.AO", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.19029", "abs": "https://arxiv.org/abs/2602.19029", "authors": ["Varun Nevash", "Prakash Kumar", "Chinika Dangi"], "title": "Analytical characterization of self sustained nonlinear oscillators modelling human walking and bouncing", "comment": null, "summary": "Researchers have developed hybrid Van der Pol Rayleigh Duffing type oscillators to model human induced forces; however, their analytical framework has largely relied on the Lindstedt Poincare perturbation method, energy balance approaches, and harmonic balance techniques. This paper aims to apply new mathematical tools to these existing models and address potential research gaps. An analytical proof for the stability of the limit cycle has been formulated by using the Krylov Bogolyubov perturbation method. The multiple scales method has been modified to highlight an iterative algorithm for determining the order of approximation required to capture nonlinear effects. The describing function method is utilised to formulate an alternate amplitude. Comparisons between first order amplitudes obtained from perturbation analysis and the describing function formulations reveal conditions under which the two approaches converge. These conditions are exploited to formulate additional constraints for the estimation of model parameters, offering a systematic alternative to purely optimisation based approaches."}
{"id": "2602.19928", "categories": ["hep-lat"], "pdf": "https://arxiv.org/pdf/2602.19928", "abs": "https://arxiv.org/abs/2602.19928", "authors": ["Javier Suarez Sucunza", "Thomas Luu", "Carsten Urbach"], "title": "The Lambda 1405 at the $SU(3)$ point in lattice QCD", "comment": "8 pages, 4 figures", "summary": "The pole structure of the $Λ(1405)$ has been a topic of debate for a long time. Chiral perturbation theory predicts that its experimental spectrum may be explained by a two pole structure originating in the $SU(3)$ chiral dynamics of the baryon-meson interaction. The $SU(3)$-symmetric flavor point is readily accessible in lattice QCD, in this work we study the baryon-meson states directly at this point. We construct interpolation operators that belong to the irreducible representations of $SU(3)$ that are attractive in the channel with the quantum numbers of the (singlet and two octets). The extracted energy levels can be used as input for chiral perturbation theory to find the poles associated with each representation. The relevant correlation functions are computed on $SU(3)$-symmetric ensembles with $M_π\\approx 714$ MeV using the distillation technique."}
{"id": "2602.18751", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18751", "abs": "https://arxiv.org/abs/2602.18751", "authors": ["Kaichen Jiang", "Yuyue Yan", "Mingda Yue", "Yuhu Wu"], "title": "Seeking Nash Equilibrium in Non-cooperative Quadratic Games Under Delayed Information Exchange", "comment": null, "summary": "In this paper, we investigate the seeking of Nash equilibrium (NE) in a non-cooperative quadratic game where all agents exchange their delayed strategy information with their neighbors. To extend best-response algorithms to the delayed information setting, an estimation mechanism for each agent to estimate the current strategy profile is designed. Based on the best-response strategy to the estimations, the strategy profile dynamics of all agents is established, which is revealed to converge asymptotically to the NE when agents exchange multi-step-delay information via the Lyapunov-Krasovskii functional approach. In the scenario where agents exchange one-step-delay information, the exponential convergence of the strategy profile dynamics to the NE can be guaranteed by restricting the learning rate to less than an upper bound. Moreover, a lower bound on the learning rate for instability of the NE is proposed. Numerical simulations are provided for verifying the developed results."}
{"id": "2602.18570", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.18570", "abs": "https://arxiv.org/abs/2602.18570", "authors": ["Anika Arifin", "Duncan DeProfio", "Layla Lammers", "Benjamin Shapiro", "Brian J Reich", "Henry Uddyback", "Joshua M Gray"], "title": "Spatiotemporal double machine learning to estimate the impact of Cambodian land concessions on deforestation", "comment": null, "summary": "Environmental policy evaluation frequently requires thoughtful consideration of space and time in causal inference. We use novel statistical methods to analyze the causal effect of land concessions on deforestation rates in Cambodia. Standard approaches, such as difference-in-differences regression, effectively address spatiotemporally-correlated treatments under some conditions, but they are limited in their ability to account for unobserved confounders affecting both treatment and outcome. Double Spatial Regression (DSR) is an approach that uses double machine learning to address these scenarios. DSR resolves the confounding variables for both treatment and outcome, comparing the residuals to estimate treatment effectiveness. We improve upon DSR by considering time in our analysis of policy interventions with spatial effects. We conduct a large-scale simulation study using Bayesian Additive Regression Trees (BART) with spatial embeddings and find that, under certain conditions, our DSR model outperforms standard approaches for addressing unobserved spatial confounding. We then apply our method to evaluate the policy impacts of land concessions on deforestation in Cambodia."}
{"id": "2602.18751", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18751", "abs": "https://arxiv.org/abs/2602.18751", "authors": ["Kaichen Jiang", "Yuyue Yan", "Mingda Yue", "Yuhu Wu"], "title": "Seeking Nash Equilibrium in Non-cooperative Quadratic Games Under Delayed Information Exchange", "comment": null, "summary": "In this paper, we investigate the seeking of Nash equilibrium (NE) in a non-cooperative quadratic game where all agents exchange their delayed strategy information with their neighbors. To extend best-response algorithms to the delayed information setting, an estimation mechanism for each agent to estimate the current strategy profile is designed. Based on the best-response strategy to the estimations, the strategy profile dynamics of all agents is established, which is revealed to converge asymptotically to the NE when agents exchange multi-step-delay information via the Lyapunov-Krasovskii functional approach. In the scenario where agents exchange one-step-delay information, the exponential convergence of the strategy profile dynamics to the NE can be guaranteed by restricting the learning rate to less than an upper bound. Moreover, a lower bound on the learning rate for instability of the NE is proposed. Numerical simulations are provided for verifying the developed results."}
{"id": "2602.19841", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.19841", "abs": "https://arxiv.org/abs/2602.19841", "authors": ["Krishna Neupane", "Igor Griva", "Robert Axtell", "William Kennedy", "Jason Kinser"], "title": "Detecting and Explaining Unlawful Insider Trading: A Shapley Value and Causal Forest Approach to Identifying Key Drivers and Causal Relationships", "comment": null, "summary": "Corporate insiders trade for diverse reasons, often possessing Material Non-Public Information (MNPI). Determining whether specific trades leverage MNPI is a significant challenge due to inherent complexity. This study focuses on two critical objectives: accurately detecting Unlawful Insider Trading (UIT) and identifying key features explaining classification. The analysis demonstrates how combining Shapley Values (SHAP) and Causal Forest (CF) reveals these explanatory drivers.\n  The findings underscore the necessity of causality in identifying and interpreting UIT, requiring the consideration of alternative scenarios and potential outcomes. Within a high-dimensional feature space, the proposed architecture integrates state-of-the-art techniques to achieve high classification accuracy. The framework provides robust feature rankings via SHAP and causal significance assessments through CF, facilitating the discovery of unique causal relationships.\n  Statistically significant relationships are documented between the outcome and several key features, including director status, price-to-book ratio, return, and market beta. These features significantly influence the likelihood of UIT, suggesting potential links between insider behavior and factors such as information asymmetry, valuation risk, market volatility, and stock performance. The analysis draws attention to the complexities of financial causality, noting that while initial descriptors offer intuitive insights, deeper examination is required to understand nuanced impacts. These findings reaffirm the architectural flexibility of decision tree models. By incorporating heterogeneity during tree construction, these models effectively uncover latent structures within trade, finance, and governance data, characterizing fraudulent behavior while maintaining reliable results."}
{"id": "2602.18672", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.18672", "abs": "https://arxiv.org/abs/2602.18672", "authors": ["Bishal Thapa", "Po-Hao Chang", "Kirill Belashchenko", "Igor I. Mazin"], "title": "Is altermagnetism in vanadium oxychalcogenides a lost cause?", "comment": "5 pages, 4 figures", "summary": "Vanadium-based oxychalcogenide compounds with the inverse Lieb-lattice (ILL) structural pattern have recently been proposed as candidate altermagnets (AM). However, early studies postulated ferromagnetic interlayer coupling, a critical requirement for preserving the bulk AM state. Here we present a systematic survey of the complete AV2Q2O family (A = K, Rb, Cs; Q = S, Se, Te) in terms of their magnetic ordering and interlayer coupling. While intralayer exchange interaction favors AM ordering in a single ILL layer across the entire family, the relatively weak interlayer coupling in most cases favors Kramers-degenerate antiferromagnetic order with a doubled magnetic unit cell. This means that most stoichiometric bulk materials, including the previously proposed candidate KV2Se2O, are not altermagnetic, with CsV2Te2O being the only exception. Using hole doping to simulate alkali vacancies, we show that realistic deviations from stoichiometry do not change the magnetic ground state in these compounds."}
{"id": "2602.18756", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.18756", "abs": "https://arxiv.org/abs/2602.18756", "authors": ["Jieming Kong", "Karthyek Murthy"], "title": "Multiunit I.I.D. Prophet Inequalities via Extreme Value Asymptotics", "comment": null, "summary": "We study the i.i.d. $k$-selection prophet inequality problem, where a decision-maker sequentially observes $n$ independent nonnegative rewards and may accept at most $k$ of them without knowledge of future realizations. The objective is to maximize the expected total reward relative to that of a prophet who observes all rewards in advance. This problem captures the performance limits achievable in online resource allocation and underlies posted-price mechanisms in online marketplaces. We characterize the optimal welfare achievable relative to the prophet in terms of $k$ and the extreme value index of the reward distribution, in the asymptotic regime where the number of offers $n$ grows large. This optimal performance ratio turns out to be at least $1-\\frac{\\log k}{8k}[1+ε]$ for any $ε> 0$ and sufficiently large $k$, improving upon the respective, tight $1 - \\frac{1}{\\sqrt{2πk}}$ guarantee of static-threshold algorithms. We additionally analyze the certainty-equivalent (CE) heuristic, a widely used online allocation algorithm known to yield optimal regret growth in $n$ when evaluated under the fluid scaling assumption. Even in the absence of the fluid scaling, the CE heuristics's performance improves with $k$ to eventually match the leading order terms of the optimal dynamic program's performance ratio. A finer analysis nevertheless reveals that regret can be divergent and large relative to the optimal dynamic program when $n/k \\to \\infty$. This highlights the sensitivity in viewing the CE heuristic's performance under the commonly adopted, though subjective, fluid scaling assumption."}
{"id": "2602.19453", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.19453", "abs": "https://arxiv.org/abs/2602.19453", "authors": ["Ryan W. Salatti", "André M. Timpanaro"], "title": "Emergence of opinion splits in the Sznajd model with latency", "comment": "7 pages (main text) + 10 pages (appendices), 13 figures", "summary": "In the modelling of social systems, opinion latency is the idea that once an agent changes its opinion, there will be a period of time where it is immune to other changes. When added to the voter model this leads to a situation where no matter how low the latency is or how many opinions are considered, all opinions end up in a coexistence where they are equally represented. In this work, we examine what happens when latency is added to the Sznajd model. What we find is that for low latency, the model behaves roughly like it does in the absence of latency, where one opinion will always eventually dominate. For high latency, the possibility for a symmetric coexistence of 2 opinions arises, but contrary to the voter model, a coexistence of more than 2 opinions is never stable. We provide evidence of this phenomenon with computer simulations of the model in Barabási-Albert networks, together with a mean field treatment that is able to capture the observed behavior. We argue that this could hint at an explanation for the prevalence of two opinion splits in the real world."}
{"id": "2602.19312", "categories": ["cs.ET", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.19312", "abs": "https://arxiv.org/abs/2602.19312", "authors": ["Kyriakos Stylianopoulos", "Mario Edoardo Pandolfo", "Paolo Di Lorenzo", "George C. Alexandropoulos"], "title": "Metasurfaces-Integrated Wireless Neural Networks for Lightweight Over-The-Air Edge Inference", "comment": "9 pages, 6 figures, submitted for magazine publication", "summary": "The upcoming sixth Generation (6G) of wireless networks envisions ultra-low latency and energy efficient Edge Inference (EI) for diverse Internet of Things (IoT) applications. However, traditional digital hardware for machine learning is power intensive, motivating the need for alternative computation paradigms. Over-The-Air (OTA) computation is regarded as an emerging transformative approach assigning the wireless channel to actively perform computational tasks. This article introduces the concept of Metasurfaces-Integrated Neural Networks (MINNs), a physical-layer-enabled deep learning framework that leverages programmable multi-layer metasurface structures and Multiple-Input Multiple-Output (MIMO) channels to realize computational layers in the wave propagation domain. The MINN system is conceptualized as three modules: Encoder, Channel (uncontrollable propagation features and metasurfaces), and Decoder. The first and last modules, realized respectively at the multi-antenna transmitter and receiver, consist of conventional digital or purposely designed analog Deep Neural Network (DNN) layers, and the metasurfaces responses of the Channel module are optimized alongside all modules as trainable weights. This architecture enables computation offloading into the end-to-end physical layer, flexibly among its constituent modules, achieving performance comparable to fully digital DNNs while significantly reducing power consumption. The training of the MINN framework, two representative variations, and performance results for indicative applications are presented, highlighting the potential of MINNs as a lightweight and sustainable solution for future EI-enabled wireless systems. The article is concluded with a list of open challenges and promising research directions."}
{"id": "2602.18626", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.18626", "abs": "https://arxiv.org/abs/2602.18626", "authors": ["Maximiliano Dalinger", "Elia Merzari", "Saya Lee"], "title": "Multiphysics Modelling of the Molten Salt Fast Reactor using NekRS and the Fission Matrix Method", "comment": "ANS Annual Conference 2026", "summary": "The Molten Salt Fast Reactor (MSFR) has the particularity that the coolant is also the fuel, which tightens the coupling between neutronics and thermal hydraulics as the fuel circulates through the primary system. Therefore, developing computational models to analyze the MSFR requires a multiphysics approach. In this paper, we propose developing a neutronic thermal-hydraulic computational model of the MSFR that uses a reduced-order model to solve the neutronics equations. The principal computational tool chosen for this purpose is the high-fidelity code Cardinal, a wrapping within the MOOSE framework that integrates the Computational Fluid Dynamics code NekRS and the Monte Carlo particle transport code OpenMC. However, we use the Fission Matrix (FM) Method to solve the neutronics equations instead of OpenMC. The FM method can perform fast and still accurate neutronics simulations. It relies on precalculated databases obtained through a Monte Carlo simulation."}
{"id": "2602.18888", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.18888", "abs": "https://arxiv.org/abs/2602.18888", "authors": ["Ziquan Zhu", "Hanruo Zhu", "Siyuan Lu", "Xiang Li", "Yanda Meng", "Gaojie Jin", "Lu Yin", "Lijie Hu", "Di Wang", "Lu Liu", "Tianjin Huang"], "title": "Dual-Kernel Adapter: Expanding Spatial Horizons for Data-Constrained Medical Image Analysis", "comment": null, "summary": "Adapters have become a widely adopted strategy for efficient fine-tuning of large pretrained models, particularly in resource-constrained settings. However, their performance under extreme data scarcity, common in medical imaging due to high annotation costs, privacy regulations, and fragmented datasets, remains underexplored. In this work, we present the first comprehensive study of adapter-based fine-tuning for large pretrained models in low-data medical imaging scenarios. We find that, contrary to their promise, conventional adapters can degrade performance under severe data constraints, performing even worse than simple linear probing when trained on less than 1% of the corresponding training data. Through systematic analysis, we identify a sharp reduction in Effective Receptive Field (ERF) as a key factor behind this degradation. Motivated by these findings, we propose the Dual-Kernel Adapter (DKA), a lightweight module that expands spatial context via large-kernel convolutions while preserving local detail with small-kernel counterparts. Extensive experiments across diverse classification and segmentation benchmarks show that DKA significantly outperforms existing adapter methods, establishing new leading results in both data-constrained and data-rich regimes."}
{"id": "2602.19494", "categories": ["physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2602.19494", "abs": "https://arxiv.org/abs/2602.19494", "authors": ["Nozomi Sugiura"], "title": "Koopman Analysis of Sea Surface Temperature with a Signature Kernel", "comment": "21 pages, 6 figures", "summary": "We develop a trajectory-based Koopman method for sea surface temperature (SST) that lifts annual SST segments using a signature kernel -- a reproducing kernel Hilbert space (RKHS) kernel that compares paths via iterated-integral features -- and learns the one-year shift operator. By operating on annual trajectory segments rather than instantaneous fields, the method encodes finite-time history, which helps capture memory effects in SST-only evolution. The resulting operator improves out-of-sample multi-year forecast skill relative to a climatology baseline and reveals coherent spectral modes. We implement the approach via kernel extended dynamic mode decomposition (EDMD) on signature-kernel Gram matrices, yielding a single pipeline for forecasting and spectral diagnostics of high-dimensional SST dynamics."}
{"id": "2602.19803", "categories": ["math.ST", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.19803", "abs": "https://arxiv.org/abs/2602.19803", "authors": ["Gökhan Gül"], "title": "From Asymptotic to Finite-Sample Minimax Robust Hypothesis Testing", "comment": "40 pages, 6 figures. Submitted to IEEE Transactions on Information Theory", "summary": "This paper establishes a formal connection between finite-sample and asymptotically minimax robust hypothesis testing under distributional uncertainty. It is shown that, whenever a finite-sample minimax robust test exists, it coincides with the solution of the corresponding asymptotic minimax problem. This result enables the analytical derivation of finite-sample minimax robust tests using asymptotic theory, bypassing the need for heuristic constructions. The total variation distance and band model are examined as representative uncertainty classes. For each, the least favorable distributions and corresponding robust likelihood ratio functions are derived in parametric form. In the total variation case, the new derivation generalizes earlier results by allowing unequal robustness parameters. The theory also explains and systematizes previously heuristic designs. Simulations are provided to illustrate the theoretical results."}
{"id": "2602.19595", "categories": ["cs.SI", "cs.DM", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.19595", "abs": "https://arxiv.org/abs/2602.19595", "authors": ["Dávid Ferenczi", "Alexander Grigoriev"], "title": "Constrained graph generation: Preserving diameter and clustering coefficient simultaneously", "comment": "15 pages, 5 figures", "summary": "Generating graphs subject to strict structural constraints is a fundamental computational challenge in network science. Simultaneously preserving interacting properties-such as the diameter and the clustering coefficient- is particularly demanding. Simple constructive algorithms often fail to locate vanishingly small sets of feasible graphs, while traditional Markov-chain Monte Carlo (MCMC) samplers suffer from severe ergodicity breaking. In this paper, we propose a two-step hybrid framework combining Ant Colony Optimization (ACO) and MCMC sampling. First, we design a layered ACO heuristic to perform a guided global search, effectively locating valid graphs with prescribed diameter and clustering coefficient. Second, we use these ACO-designed graphs as structurally distinct seed states for an MCMC rewiring algorithm. We evaluate this framework across a wide range of graph edge densities and varying diameter-clustering-coefficient constraint regimes. Using the spectral distance of the normalized Laplacian to quantify structural diversity of the resulting graphs, our experiments reveal a sharp contrast between the methods. Standard MCMC samplers remain rigidly trapped in an isolated subset of feasible graphs around their initial seeds. Conversely, our hybrid ACO-MCMC approach successfully bridges disconnected configuration landscapes, generating a vastly richer and structurally diverse set of valid graphs."}
{"id": "2602.19453", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.19453", "abs": "https://arxiv.org/abs/2602.19453", "authors": ["Ryan W. Salatti", "André M. Timpanaro"], "title": "Emergence of opinion splits in the Sznajd model with latency", "comment": "7 pages (main text) + 10 pages (appendices), 13 figures", "summary": "In the modelling of social systems, opinion latency is the idea that once an agent changes its opinion, there will be a period of time where it is immune to other changes. When added to the voter model this leads to a situation where no matter how low the latency is or how many opinions are considered, all opinions end up in a coexistence where they are equally represented. In this work, we examine what happens when latency is added to the Sznajd model. What we find is that for low latency, the model behaves roughly like it does in the absence of latency, where one opinion will always eventually dominate. For high latency, the possibility for a symmetric coexistence of 2 opinions arises, but contrary to the voter model, a coexistence of more than 2 opinions is never stable. We provide evidence of this phenomenon with computer simulations of the model in Barabási-Albert networks, together with a mean field treatment that is able to capture the observed behavior. We argue that this could hint at an explanation for the prevalence of two opinion splits in the real world."}
{"id": "2602.19387", "categories": ["quant-ph", "cs.ET", "hep-ex", "hep-lat", "hep-ph"], "pdf": "https://arxiv.org/pdf/2602.19387", "abs": "https://arxiv.org/abs/2602.19387", "authors": ["Marco Knipfer", "Alexander Roman", "Konstantin T. Matchev", "Katia Matcheva", "Sergei Gleyzer"], "title": "AI Agents for Variational Quantum Circuit Design", "comment": "43 pages, 12 figures", "summary": "Variational quantum circuits (VQCs) constitute a central building block of near-term quantum machine learning (QML), yet the principled design of expressive and trainable architectures remains a major open challenge. The VQC design space grows combinatorially with the number of qubits, layers, entanglement structures, and gate parameterizations, rendering manual circuit construction inefficient and often suboptimal. We introduce an autonomous agent-based framework for VQC architecture search that integrates high-level reasoning with a quantum simulation environment. The agent proposes candidate circuit architectures, evaluates them through fully automated training and validation pipelines, and iteratively improves its design strategy via performance-driven feedback. Empirically, we show that the agent autonomously evolves circuit architectures from simple initial ansätze toward increasingly expressive designs, progressively trying to improve task performance. This demonstrates that agentic AI can effectively navigate and refine the VQC design landscape with minimal human intervention, providing a scalable methodology for automated quantum model development in the Noisy Intermediate-Scale Quantum (NISQ) regime."}
{"id": "2602.18933", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18933", "abs": "https://arxiv.org/abs/2602.18933", "authors": ["Bowen Song", "Simon Weissmann", "Mathias Staudigl", "Andrea Iannelli"], "title": "A Stochastic Gradient Descent Approach to Design Policy Gradient Methods for LQR", "comment": null, "summary": "In this work, we propose a stochastic gradient descent (SGD) framework to design data-driven policy gradient descent algorithms for the linear quadratic regulator problem. Two alternative schemes are considered to estimate the policy gradient from stochastic trajectory data: (i) an indirect online identification based approach, in which the system matrices are first estimated and subsequently used to construct the gradient, and (ii) a direct zeroth-order approach, which approximates the gradient using empirical cost evaluations. In both cases, the resulting gradient estimates are random due to stochasticity in the data, allowing us to use SGD theory to analyze the convergence of the associated policy gradient methods. A key technical step consists of modeling the gradient estimates as suitable stochastic gradient oracles, which, because of the way they are computed, are inherently based. We derive sufficient conditions under which SGD with a biased gradient oracle converges asymptotically to the optimal policy, and leverage these conditions to design the parameters of the gradient estimation schemes. Moreover, we compare the advantages and limitations of the two data-driven gradient estimators. Numerical experiments validate the effectiveness of the proposed methods."}
{"id": "2602.18577", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.18577", "abs": "https://arxiv.org/abs/2602.18577", "authors": ["Erik Sverdrup", "Trevor Hastie"], "title": "balnet: Pathwise Estimation of Covariate Balancing Propensity Scores", "comment": null, "summary": "We present balnet, an R package for scalable pathwise estimation of covariate balancing propensity scores via logistic covariate balancing loss functions. Regularization paths are computed with Yang and Hastie (2024)'s generic elastic net solver, supporting convex losses with non-smooth penalties, as well as group penalties and feature-specific penalty factors. For lasso penalization, balnet computes a regularized balance path from the largest observed covariate imbalance to a user-specified fraction of this maximum. We illustrate the method with an application to spatial pixel-level balancing for constructing synthetic control weights for the average treatment effect on the treated, using satellite data on wildfires."}
{"id": "2602.18933", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18933", "abs": "https://arxiv.org/abs/2602.18933", "authors": ["Bowen Song", "Simon Weissmann", "Mathias Staudigl", "Andrea Iannelli"], "title": "A Stochastic Gradient Descent Approach to Design Policy Gradient Methods for LQR", "comment": null, "summary": "In this work, we propose a stochastic gradient descent (SGD) framework to design data-driven policy gradient descent algorithms for the linear quadratic regulator problem. Two alternative schemes are considered to estimate the policy gradient from stochastic trajectory data: (i) an indirect online identification based approach, in which the system matrices are first estimated and subsequently used to construct the gradient, and (ii) a direct zeroth-order approach, which approximates the gradient using empirical cost evaluations. In both cases, the resulting gradient estimates are random due to stochasticity in the data, allowing us to use SGD theory to analyze the convergence of the associated policy gradient methods. A key technical step consists of modeling the gradient estimates as suitable stochastic gradient oracles, which, because of the way they are computed, are inherently based. We derive sufficient conditions under which SGD with a biased gradient oracle converges asymptotically to the optimal policy, and leverage these conditions to design the parameters of the gradient estimation schemes. Moreover, we compare the advantages and limitations of the two data-driven gradient estimators. Numerical experiments validate the effectiveness of the proposed methods."}
{"id": "2602.19147", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.19147", "abs": "https://arxiv.org/abs/2602.19147", "authors": ["Qingzhuo Duan", "Hongdao Zhuge", "Ying Liang", "Tianxing Ma"], "title": "Precompression engineering of metal-insulator transition and magnetism in designed breathing kagome systems", "comment": "9 pages and 6 figures", "summary": "Kagome materials featuring dispersive Dirac cones and topological flat bands exhibit unique electronic and magnetic properties. However, kagome compounds with tunable electrical conductivity remain scarce, which severely impedes their device applications. Here, based on density functional theory (DFT) and Boltzmann transport theory, we introduce the breathing effect into kagome materials $\\mathrm{Nb_3XCl_7}$ (X = F, Cl, Br, I) via chemical precompression, thereby inducing a metal-insulator transition and magnetic variation. We determine that the band structures, optical absorption spectra and magnetic ground states agree well with experimental results at the effective correlation strength $U_{\\text{eff}} = 2$ eV. The calculated conductivity and magnetic properties reveal that the monolayer $\\mathrm{Nb_3Cl_8}$ and $\\mathrm{Nb_3XCl_7}$ undergoes transitions from paramagnetic metals to Mott insulators at $U_{\\text{eff}} = 1$ eV and $t_{\\text{out}}/t_{\\text{in}} = 0.6674$, respectively. Our detailed analysis establishes that the stronger breathing effect corresponds to enhanced chemical precompression, which reduces the region of free electron gas between intercell Nb atoms and facilitates the metal-insulator transition. Finally, we propose several viable synthesis routes for $\\mathrm{Nb_3FCl_7}$, $\\mathrm{Nb_3BrCl_7}$, and $\\mathrm{Nb_3ICl_7}$, providing predictive guidance for experimental studies. Our study establishes a practical framework for investigating the breathing effect in correlated kagome systems and yields valuable insights into the mechanisms underlying metal-insulator transition and magnetic properties in real breathing kagome materials."}
{"id": "2602.18796", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.18796", "abs": "https://arxiv.org/abs/2602.18796", "authors": ["Matúš Benko", "R. Tyrrell Rockafellar"], "title": "Variational Sufficiency and Solution Stability in Optimization", "comment": null, "summary": "Variational stability, in the sense of local good behavior of optimal values and solutions in problems of optimization under shifts in parameters, is important not only for validating model robustness in practical applications but also for confidence of outcomes in the design of solution algorithms. Fundamental results are presented here about how such stability relates to a recently developed sufficient condition for local optimality called strong variational sufficiency."}
{"id": "2602.19684", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.19684", "abs": "https://arxiv.org/abs/2602.19684", "authors": ["Cosimo Agostinelli", "Marco Mancastroppa", "Alain Barrat"], "title": "Group adaptation drives opinion dynamics in higher-order networks", "comment": null, "summary": "In modern interconnected societies, opinions and beliefs can quickly spread across large populations, giving rise to collective behaviors such as the adoption of social norms or polarization. These phenomena have motivated many models aimed at reproducing emergent properties from simple interaction mechanisms. In particular, opinion dynamics models describe how individual opinions evolve through interactions and study the conditions for global consensus or polarization. Most models assume that these interactions occur between pairs of agents, typically on a fixed network structure. However, opinion changes can occur in groups, which may also undergo adaptive changes if disagreement arises. Here, we propose a bounded confidence model that incorporates both mechanisms: group discussions can lead to global agreement among members, while strong internal disagreement causes groups to split, with resulting subgroups merging with others. We systematically study the model outcomes as a function of agents' tolerance for agreement. Strikingly, adaptivity suppresses key effects of group interactions, restoring a phenomenology close to that of pairwise interactions. In particular, adaptivity enables the formation of large groups and prevents fragmentation at small tolerance. It also restores a phase transition from polarization to consensus, which would otherwise disappear in a non-adaptive group-based model. Overall, our work shows that both adaptivity and group interactions shape the structure of social ties and global opinion dynamics in a population."}
{"id": "2602.19341", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19341", "abs": "https://arxiv.org/abs/2602.19341", "authors": ["Xinling Li", "Gioele Zardini"], "title": "Where Should Robotaxis Operate? Strategic Network Design for Autonomous Mobility-on-Demand", "comment": null, "summary": "The emergence of Autonomous Mobility-on-Demand (AMoD) services creates new opportunities to improve the efficiency and reliability of on-demand mobility systems. Unlike human-driven Mobility-on-Demand (MoD), AMoD enables fully centralized fleet control, but it also requires appropriate infrastructure, so that vehicles can operate safely only on a suitably instrumented subnetwork of the roads. Most existing AMoD research focuses on fleet control (matching, rebalancing, ridepooling) on a fixed road network and does not address the joint design of the service network and fleet capacity. In this paper, we formalize this strategic design problem as the Autonomous Mobility-on-Demand Network Design Problem (AMoD-NDP), in which an operator selects an operation subnetwork and routes all passengers, subject to infrastructure and fleet constraints and route-level quality-of-service requirements. We propose a path-based mixed-integer formulation of the AMoD-NDP and develop a column-generation-based algorithm that scales to city-sized networks. The master problem optimizes over a restricted set of paths, while the pricing problem reduces to an elementary shortest path with resource constraints, solved exactly by a tailored label-correcting algorithm. The method provides an explicit certificate of the optimality gap and extends naturally to a robust counterpart under box uncertainty in travel times and demand. Using real-world data from Manhattan, New York City, we show that the framework produces stable and interpretable operation subnetworks, quantifies trade-offs between infrastructure investment and fleet time, and accommodates additional path-level constraints, such as limits on left turns as a proxy for operational risk. These results illustrate how the proposed approach can support strategic planning and policy analysis for future AMoD deployments."}
{"id": "2602.18913", "categories": ["quant-ph", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.18913", "abs": "https://arxiv.org/abs/2602.18913", "authors": ["Marvin Kronenberger", "Mihael Erakovic", "Markus Reiher"], "title": "Trotter Error and Orbital Transformations in Quantum Phase Estimation", "comment": "33 pages, 8 figures, 2 tables", "summary": "Quantum computation with Trotter product formulae is straightforward and requires little overhead in terms of logical qubits. The choice of the orbital basis significantly affects circuit depth, with localised orbitals yielding lowest circuit depths. However, literature results point to large Trotter errors incurred by localised orbitals. Here, we therefore investigate the effect of orbital transformations on Trotter error. We consider three strategies to reduce Trotter error by orbital transformation: (i) The a priori selection of an orbital basis that produces low Trotter error. (ii) The derivation of an orbital basis that produces a ground state energy free of Trotter error (as we observed that the Trotter error is a continuous function in the Givens-rotation parameter, from which continuity of this error upon orbital transformation can be deduced). (iii) Application of propagators that change the computational basis between Trotter steps. Our numerical results show that reliably reducing Trotter error by orbital transformations is challenging. General recipes to produce low Trotter errors cannot be easily derived, despite analytical expressions which suggest ways to decrease Trotter error. Importantly, we found that localised orbital bases do not produce large Trotter errors in molecular calculations, which is an important result for efficient QPE set-ups."}
{"id": "2602.19073", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.19073", "abs": "https://arxiv.org/abs/2602.19073", "authors": ["Shengyuan Lin", "Kaiwen He", "Jaisal Patel", "Qinchuan Zhang", "Chris Ding", "James Tang", "Keyi Wang", "Yupeng Cao", "Yan Wang", "Kairong Xiao", "Vincent Caldeira", "Matt White", "Xiao-Yang Liu Yanglet"], "title": "Evaluation and Benchmarking Suite for Financial Large Language Models and Agents", "comment": "13 pages, 13 figures", "summary": "Over the past three years, the financial services industry has witnessed Large Language Models (LLMs) and agents transitioning from the exploration stage to readiness and governance stages. Financial large language models (FinLLMs), such as open FinGPT and proprietary BloombergGPT , have great potential in financial applications, including retrieving real-time data, tutoring, analyzing sentiment of social media, analyzing SEC filings, and agentic trading. However, general-purpose LLMs and agents lack financial expertise and often struggle to handle complex financial reasoning. This paper presents an evaluation and benchmarking suite that covers the lifecycle of FinLLMs and FinAgents. This suite led by SecureFinAI Lab includes an evaluation pipeline and a governance framework collaborating with Linux Foundation and PyTorch Foundation, a FinLLM Leaderboard with HuggingFace, an AgentOps framework with Red Hat, and a documentation website with Rensselear Center of Open Source. Our collaborative development evolves through three stages: FinLLM Exploration (2023), FinLLM Readiness (2024), and FinAI Governance (2025). The proposed suite serves as an open platform that enables researchers and practitioners to perform both quantitative and qualitative analysis of different FinLLMs and FinAgents, fostering a more robust and reliable FinAI ecosystem."}
{"id": "2602.19564", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.19564", "abs": "https://arxiv.org/abs/2602.19564", "authors": ["Xingyue Deng", "Xuechen Liang"], "title": "Deep Learning Based Monthly Temperature Prediction for Jilin Province: A Multi Model Comparative Study 2000 2026", "comment": null, "summary": "Jilin Province, a core commercial grain production base in China with a mid-temperate continental monsoon climate and significant temperature fluctuations, relies heavily on temperature for agricultural production and ecological security. Existing temperature prediction studies focus mostly on national/southeastern coastal regions, with few targeting Jilin's specific climatic characteristics, and most models fail to integrate local temperature's spatiotemporal differentiation and seasonal periodicity, limiting prediction accuracy.\n  Using 1 km $\\times$ 1 km monthly mean temperature raster data (2000--2024) of Jilin Province, we analyzed regional temperature's spatiotemporal variation and constructed a multi-model comparison system including four deep learning models (LSTM, GRU, BiLSTM, Transformer) and five traditional machine learning models (Ridge/Lasso Regression, SVR, Random Forest, Gradient Boosting). Model performance was evaluated via RMSE, MAE, and $R^2$.\n  Results show Jilin's temperature has obvious latitudinal zonal distribution, significant warming trend, strong seasonal periodicity, and high temporal autocorrelation. The LSTM model achieved optimal performance (test set RMSE=2.26 $^\\circ$C, MAE=1.83 $^\\circ$C, $R^2$=0.9655), outperforming traditional models and Transformer. Predictions for 2025--2026 indicate stable seasonal temperature fluctuations with an annual mean of ~4.9 $^\\circ$C.\n  This study enriches mid-latitude cold region temperature prediction research, verifies LSTM's applicability for Jilin's monthly temperature prediction, and provides scientific support for agricultural planning, frost disaster warning, and extreme temperature risk prevention."}
{"id": "2602.19839", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2602.19839", "abs": "https://arxiv.org/abs/2602.19839", "authors": ["Marcio Reverbel"], "title": "Addressing parity blindness of data-driven Sobolev tests on the hypersphere", "comment": "6 pages, 1 figure, submitted to Statistics & Probability Letters", "summary": "We study the asymptotic behavior of the data-driven Sobolev test for testing uniformity on the (hyper)sphere. We show that it can be blind to certain contiguous alternatives and propose a simple modification of the test statistic. This adapted test retains consistency under fixed alternatives and achieves non-trivial asymptotic power against contiguous alternatives for which the original test fails. Simulation results support our theoretical findings."}
{"id": "2602.20009", "categories": ["cs.SI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.20009", "abs": "https://arxiv.org/abs/2602.20009", "authors": ["Giorgia Sampò", "Saverio Giallorenzo", "Zelda Alice Franceschi"], "title": "A Mixed-Method Framework for Evaluating the Social Impact of Community Cooperation Projects in Developing Countries", "comment": null, "summary": "Why do some community-cooperation projects catalyse participation through durable, resilient collaboration networks while others result in negligible impact and leave the local social fabric unchanged? We argue outcomes hinge on participation architecture: simple, visible routines -- onboarding help, templated tasks, lightweight contribution/benefit tracking -- that create easy ``entry portals'' and route work across clusters without heavy hierarchy. We introduce Project Intervention Response Analysis (PIRA), a mixed anthropological-network-analysis framework that compares observed community networks with counterfactual networks absent from project-induced ties. PIRA also adds a new egocentric metric to detect ``architectural alters'' -- latent facilitators and boundary spanners. We begin validating PIRA in a three-month field study in Pomerini, Tanzania, where NGOs coordinated citizens, associations, and specialists. Findings indicate that sociotechnical participation architectures -- not charismatic hubs -- underwrite durable coordination. PIRA offers a reusable method to link organizational design mechanisms to formal network signatures."}
{"id": "2602.19002", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19002", "abs": "https://arxiv.org/abs/2602.19002", "authors": ["Laurenţiu Lucian Anton", "Marija Ilić"], "title": "Unified Diagnostics for Quantifying AC Operating-Point Robustness Under Injection and Topological Uncertainties with Regime Changes", "comment": "20 pages, 9 figures. Under review", "summary": "In the presence of uncertainties in load, generation, and network topology, power system planning must reflect operational conditions, while operations require situational awareness over credible uncertainty sets. Existing methods screen, analyze, embed, and propagate uncertainty in power flow and optimal power flow settings, but provide only partial insight into how physical constraints, controls, and economic interactions shape steady-state operating-point robustness. By formulating operating-point robustness as a post-solution physical response problem around a solved AC optimal power flow (AC-OPF) equilibrium, this paper presents a unified framework for assessing robustness under injection and topological uncertainty without re-optimization. We construct a primal physical response mapping that accounts for connectivity changes, active power redistribution, generator saturation including $PV \\rightarrow PQ$ transitions, and AC network propagation, and introduce quasi-duals that provide a geometric interpretation of shadow prices for off-optimal equilibria. Using these mappings, we develop deterministic screening procedures that generalize $N-k$ contingency analysis to include cost vulnerability $C-k$, and local analogs $N+δ(k)$ and $C+δ(k)$ defined through sensitivity-normalized margins and risk tolerances. The framework is extended to probabilistic screening for distribution- and moment-based uncertainties, with sequentially-pruned mixture modeling and $α$-stressed regime constructions to manage combinatorial branching. A case study on the Puerto Rican bulk power system demonstrates integration with geospatial data to enhance operational and planning awareness."}
{"id": "2602.18651", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.18651", "abs": "https://arxiv.org/abs/2602.18651", "authors": ["Nils Lid Hjort", "Ian W. McKeague", "Ingrid Van Keilegom"], "title": "Hybrid combinations of parametric and empirical likelihoods", "comment": "24 pages, 4 figures. This is the July 2017 authors' manuscript, with Supplementary Material, with final paper published in Statistica Sinica, 2018, their Peter Hall issue, vol. 28, pages 2389-2407, see pmc.ncbi.nlm.nih.gov/articles/PMC6602551/", "summary": "This paper develops a hybrid likelihood (HL) method based on a compromise between parametric and nonparametric likelihoods. Consider the setting of a parametric model for the distribution of an observation $Y$ with parameter $θ$. Suppose there is also an estimating function $m(\\cdot,μ)$ identifying another parameter $μ$ via $E\\,m(Y,μ)=0$, at the outset defined independently of the parametric model. To borrow strength from the parametric model while obtaining a degree of robustness from the empirical likelihood method, we formulate inference about $θ$ in terms of the hybrid likelihood function $H_n(θ)=L_n(θ)^{1-a}R_n(μ(θ))^a$. Here $a\\in[0,1)$ represents the extent of the compromise, $L_n$ is the ordinary parametric likelihood for $θ$, $R_n$ is the empirical likelihood function, and $μ$ is considered through the lens of the parametric model. We establish asymptotic normality of the corresponding HL estimator and a version of the Wilks theorem. We also examine extensions of these results under misspecification of the parametric model, and propose methods for selecting the balance parameter $a$."}
{"id": "2602.19002", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19002", "abs": "https://arxiv.org/abs/2602.19002", "authors": ["Laurenţiu Lucian Anton", "Marija Ilić"], "title": "Unified Diagnostics for Quantifying AC Operating-Point Robustness Under Injection and Topological Uncertainties with Regime Changes", "comment": "20 pages, 9 figures. Under review", "summary": "In the presence of uncertainties in load, generation, and network topology, power system planning must reflect operational conditions, while operations require situational awareness over credible uncertainty sets. Existing methods screen, analyze, embed, and propagate uncertainty in power flow and optimal power flow settings, but provide only partial insight into how physical constraints, controls, and economic interactions shape steady-state operating-point robustness. By formulating operating-point robustness as a post-solution physical response problem around a solved AC optimal power flow (AC-OPF) equilibrium, this paper presents a unified framework for assessing robustness under injection and topological uncertainty without re-optimization. We construct a primal physical response mapping that accounts for connectivity changes, active power redistribution, generator saturation including $PV \\rightarrow PQ$ transitions, and AC network propagation, and introduce quasi-duals that provide a geometric interpretation of shadow prices for off-optimal equilibria. Using these mappings, we develop deterministic screening procedures that generalize $N-k$ contingency analysis to include cost vulnerability $C-k$, and local analogs $N+δ(k)$ and $C+δ(k)$ defined through sensitivity-normalized margins and risk tolerances. The framework is extended to probabilistic screening for distribution- and moment-based uncertainties, with sequentially-pruned mixture modeling and $α$-stressed regime constructions to manage combinatorial branching. A case study on the Puerto Rican bulk power system demonstrates integration with geospatial data to enhance operational and planning awareness."}
{"id": "2602.19221", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.19221", "abs": "https://arxiv.org/abs/2602.19221", "authors": ["Rajesh Tripathi", "Ewan Scott", "D. T. Adroja", "D. Das", "C. Ritter", "Huanzhi Hu", "Michal P. Kwasigroch", "Nicholas Corkill", "Gheorghe Lucian Pascut", "T. Masuda", "S. Asai", "T. Takabatake", "T. Onimaru", "T. Shiroka", "Francis Pratt", "A. M. Strydom", "S. Langridge", "A. Sundaresan", "S. Patil"], "title": "Microscopic origin of hard-plane antiferromagnetism in the Kondo lattice Ce2Rh3Ge5", "comment": "19 pages, 9 figures", "summary": "Hard plane antiferromagnetic order where ordered moments lie perpendicular to the single-ion crystal electric field easy axis is rare in Ce-based Kondo lattices and is a subject of active interest. Here we show that Ce$_2$Rh$_3$Ge$_5$ realizes a hard-plane antiferromagnetic state in which partial delocalization of the local moment gives rise to an RKKY exchange that overturns the single-ion easy-axis preference. Neutron diffraction reveals moments in the $ab$ plane, while inelastic neutron scattering and susceptibility establish a magnetic easy axis along $c$ in the paramagnetic regime, highlighting a clear inversion between single-ion and ordered-state anisotropies. In this work, we establish a unified microscopic framework to consistently account for partial $4f$-moment delocalization, enhanced in-plane RKKY exchange, and the resulting hard-plane antiferromagnetic order. Ce$_2$Rh$_3$Ge$_5$ thus provides a benchmark system in which single-ion anisotropy, Kondo screening, and RKKY exchange compete on comparable energy scales, revealing a cooperative route to hard-axis ordering in strongly hybridized Kondo lattices."}
{"id": "2602.18868", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18868", "abs": "https://arxiv.org/abs/2602.18868", "authors": ["Domenic Rosati", "Xijie Zeng", "Hong Huang", "Sebastian Dionicio", "Subhabrata Majumdar", "Frank Rudzicz", "Hassan Sajjad"], "title": "Limits of Convergence-Rate Control for Open-Weight Safety", "comment": "Submitted to ICML 2026. 13 figures, 30 tables", "summary": "Open-weight foundation models can be fine-tuned for harmful purposes after release, yet no existing training resistance methods provide theoretical guarantees. Treating these interventions as convergence-rate control problems allows us to connect optimization speed to the spectral structure of model weights. We leverage this insight to develop a novel understanding of convergence rate control through spectral reparameterization and derive an algorithm, SpecDef, that can both provably and empirically slow first- and second-order optimization in non-adversarial settings. In adversarial settings, we establish a fundamental limit on a broad class of convergence rate control methods including our own: an attacker with sufficient knowledge can restore fast convergence at a linear increase in model size. In order to overcome this limitation, future works will need to investigate methods that are not equivalent to controlling convergence rate."}
{"id": "2602.19939", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.19939", "abs": "https://arxiv.org/abs/2602.19939", "authors": ["Viktoria Kainz", "Justin Sulik", "Anna Neudert", "Torsten Enßlin"], "title": "Mass Manipulation in Simulated Social Networks: Dominating vs. Diversifying Attention", "comment": "main part: 22 pages, 13 figures, 1 table; appendix: 4 pages, 2 figures, 1 table", "summary": "Modern information environments, especially social media, are highly complex systems that exceed individual processing capacities such as humans' limited attention. This environment/cognition mismatch can increase susceptibility to misinformation, which various actors exploit for anti-social (including anti-democratic or anti-science) aims. This raises the question of how to feasibly sustain societal resilience against misinformation, though the challenge is to find strategies that respect individuals' cognitive limitations. We investigate whether a simple behavioral rule - topic diversification - can enhance collective performance and mitigate vulnerability. In an agent-based model that includes a deceptive mass-influencing agent (MIA), we compare two attention-distribution strategies: (A) acquaintance-based topic selection, where agents return to familiar content, and (B) randomized topics, which diversify attention. We also track dynamics across different network structures. We find that under acquaintance-based topics, a central MIA advances its propaganda effectively, causing volatile and polarized opinions through repeated exposure and echo chambers. Under randomized topics, this leverage disappears: the MIA's influence collapses across all network structures, and opinions become stable and broadly aligned with reality. These results, while deriving from simple simulations, align with realistic theories of bounded rationality and collective cognition, further suggesting a cognitively feasible, easy-to-monitor and robust strategy: distribute attention to combat misinformation."}
{"id": "2602.19694", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.19694", "abs": "https://arxiv.org/abs/2602.19694", "authors": ["Bo Liu", "Tong Li", "Zhu Xiao", "Ruihui Li", "Geyong Min", "Zhuo Tang", "Kenli Li"], "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs", "comment": "under review", "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose UniMob, a unified human mobility generation model across cities. UniMob is composed of three main components: an LLM-powered travel planner that derives high-level, temporally-aware, and semantically meaningful travel plans; a unified spatial embedding module that projects the spatial regions of various cities into a shared representation space; and a diffusion-based mobility generator that captures the joint spatiotemporal characteristics of human movement, guided by the derived travel plans. We evaluate UniMob extensively using two real-world datasets covering five cities. Comprehensive experiments show that UniMob significantly outperforms state-of-the-art baselines, achieving improvements of over 30\\% across multiple evaluation metrics. Further analysis demonstrates UniMob's robustness in both zero- and few-shot scenarios, underlines the importance of LLM guidance, verifies its privacy-preserving nature, and showcases its applicability for downstream tasks."}
{"id": "2602.18475", "categories": ["physics.hist-ph", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2602.18475", "abs": "https://arxiv.org/abs/2602.18475", "authors": ["Simon Beyne", "Christian Marinoni"], "title": "Dark Matter in Zwicky's Cosmology: Towards an Epistemological Reconstruction", "comment": null, "summary": "A new contextualised reading of Fritz Zwicky's 1933 article ''The redshift of extragalactic nebulae'' about the virial analysis of the velocity dispersion of galaxies in the Coma cluster leads to a reconsideration of the traditional discourse on the introduction of dark matter. We argue that this component of matter was not only already on the stage of the scientific debates of the time, but also, in a more concealed form, played a central role in Zwicky's epistemic context. We thus reject the narration that dark matter is the result of a ``na{ï}ve'' astrophysical observation and emphasise the cosmological motivations that prompted Zwicky to presciently search for it. Moreover, with regard to its abundance, we argue that the discrepancy between the observed amount of luminous matter in the Coma Cluster and Zwicky's higher mass estimate derived from virial analysis was not, in fact, astonishing. What Zwicky described as a surprising excess of dark matter was of precisely the order of magnitude he had set out to identify. Consequently, we challenge the widespread view that dark matter was merely an ad hoc hypothesis introduced to rescue Newtonian theory. Instead, we suggest it may represent one of the earliest cosmological indications supporting a new emerging theory of gravitation: General Relativity. This reinterpretation contributes to ongoing debates in the philosophy of science concerning the epistemic status of ad hoc hypotheses."}
{"id": "2602.18593", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18593", "abs": "https://arxiv.org/abs/2602.18593", "authors": ["Aidan Mason-Mackay", "Daniela Calvetti", "Erkki Somersalo", "Antti Aarnio", "Mikko Kettunen", "Ekaterina Paasonen Olli Gröhn", "Ville Kolehmainen"], "title": "Sparse Dictionary-Based Solution of Dynamic Inverse Problems", "comment": null, "summary": "In ill-posed dynamic inverse problems expected spatial features and temporal correlation between frames can be leveraged to improve the quality of the computed solution, in particular when the available data are limited and the dimensionality of the unknown is large. One way to take advantage of the spatial and temporal traits believed to characterize the solution is to encode them into the entries of a dictionary, and to seek the solution as a sparse linear combination of the dictionary atoms. To promote a vector of coefficients with mostly vanishing entries, we consider a stochastic extension of the dictionary coding problem model with a random hierarchical sparsity promoting prior. We compute the Maximum A Posteriori (MAP) estimate of the coefficient vector using the Iterative Alternating Sequential Algorithm (IAS), which has been demonstrated to efficiently solve inverse problems with minimal need for parameter tuning. The proposed methodology is tested on real-world dynamic Computed Tomography and MRI datasets, where it is compared to the popular Alternating Direction Method of Minimizers (ADMM). The computed examples show the that proposed methodology is competitive with the ADMM for compressed sensing, with a significantly lower sensitivity to hyper-parameter selection."}
{"id": "2602.18631", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.18631", "abs": "https://arxiv.org/abs/2602.18631", "authors": ["Jiming Zheng", "Zhiyue Lu"], "title": "Thermodynamic and Kinetic Bounds for Finite-frequency Fluctuation-Response", "comment": null, "summary": "Fluctuation-response relations encode fundamental constraints on nonequilibrium systems. While time-domain static response is bounded by activity and entropy production, finite-frequency extensions for time-dependent perturbations remain largely unexplored. Here, we derive frequency-domain fluctuation-response inequalities for steady-state Markov processes with time-dependent perturbations. For barrier and entropic perturbations, the spectral signal-to-noise ratio (SNR) is universally bounded by dynamical activity. Furthermore, for state-current observables, the SNR is bounded by the entropy production rate (EPR). We illustrate our results using the F1-ATPase model to infer EPR. These finite-frequency inequalities provide a practical route to infer dissipation from power spectra measurements."}
{"id": "2602.19045", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.19045", "abs": "https://arxiv.org/abs/2602.19045", "authors": ["Yan Ru Pei"], "title": "peapods: A Rust-Accelerated Monte Carlo Package for Ising Spin Systems", "comment": null, "summary": "We present peapods (github.com/PeaBrane/peapods), an open-source Python package for Monte Carlo simulation of Ising spin systems with arbitrary coupling constants on arbitrary-dimensional hypercubic lattices with periodic boundary conditions. The computational core is written in Rust and exposed to Python via PyO3, combining the ergonomic interface of Python with the performance of compiled, memory-safe code. The package implements Metropolis and Gibbs single-spin-flip algorithms, Swendsen--Wang and Wolff cluster updates, and parallel tempering. Replica-level parallelism is achieved through the Rayon work-stealing scheduler. We validate the implementation against the exact critical temperature of the two-dimensional Ising model via finite-size scaling of the Binder cumulant."}
{"id": "2602.19475", "categories": ["cs.CE", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.19475", "abs": "https://arxiv.org/abs/2602.19475", "authors": ["Pao-Hsiung Chiu", "Jian Cheng Wong", "Chin Chun Ooi", "Chang Wei", "Yuchen Fan", "Yew-Soon Ong"], "title": "Scale-PINN: Learning Efficient Physics-Informed Neural Networks Through Sequential Correction", "comment": null, "summary": "Physics-informed neural networks (PINNs) have emerged as a promising mesh-free paradigm for solving partial differential equations, yet adoption in science and engineering is limited by slow training and modest accuracy relative to modern numerical solvers. We introduce the Sequential Correction Algorithm for Learning Efficient PINN (Scale-PINN), a learning strategy that bridges modern physics-informed learning with numerical algorithms. Scale-PINN incorporates the iterative residual-correction principle, a cornerstone of numerical solvers, directly into the loss formulation, marking a paradigm shift in how PINN losses can be conceived and constructed. This integration enables Scale-PINN to achieve unprecedented convergence speed across PDE problems from different physics domain, including reducing training time on a challenging fluid-dynamics problem for state-of-the-art PINN from hours to sub-2 minutes while maintaining superior accuracy, and enabling application to representative problems in aerodynamics and urban science. By uniting the rigor of numerical methods with the flexibility of deep learning, Scale-PINN marks a significant leap toward the practical adoption of PINNs in science and engineering through scalable, physics-informed learning. Codes are available at https://github.com/chiuph/SCALE-PINN."}
{"id": "2602.20007", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20007", "abs": "https://arxiv.org/abs/2602.20007", "authors": ["Andrew T. Karl"], "title": "Order Dependence in the Moving-Range Sigma Estimator: A Total-Variance Decomposition", "comment": null, "summary": "In Individuals and Moving Range (I-MR) charts, the process standard deviation is often estimated by the span-2 average moving range, scaled by the usual constant $d_2$. Unlike the sample standard deviation, this estimator depends on the observation order: permuting the values can change the average moving range. We make this dependence explicit by modeling the order as an independent uniformly random permutation. A direct application of the law of total variance then decomposes its variance into a component due to ordering and a component due to the realized values. Averaging over all permutations yields a simple order-invariant baseline for the moving-range estimator: the sample Gini mean difference divided by $d_2$. Simulations quantify the resulting fraction of variance attributable to ordering under i.i.d. Normal sampling, and two NIST examples illustrate a typical ordering and an ordering with strong serial structure relative to random permutations of the same values."}
{"id": "2602.19070", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19070", "abs": "https://arxiv.org/abs/2602.19070", "authors": ["Jie Song", "Yang Bai", "Naoki Wakamiya"], "title": "Cooperative Transportation Without Prior Object Knowledge via Adaptive Self-Allocation and Coordination", "comment": null, "summary": "This work proposes a novel cooperative transportation framework for multi-agent systems that does not require any prior knowledge of cargo locations or sizes. Each agent relies on local sensing to detect cargos, recruit nearby agents, and autonomously form a transportation team with an appropriate size. The core idea is that once an agent detects a cargo within its sensing range, it generates an attraction field represented by a density function, which pulls neighboring agents toward the cargo. When multiple cargos are present, the attraction fields generated by different agents are adaptively weighted and combined with Centroidal Voronoi Tessellation (CVT), enabling agents to self-organize into balanced formations while automatically allocating more agents to larger cargos. To prevent agents from clustering on one side of a large cargo, a Control Barrier Function (CBF)-based mechanism is introduced to enforce safe inter-agent distances and promote a uniform, symmetric distribution of agents around each cargo, which is essential for stable transportation. Simulation results demonstrate that the proposed framework can simultaneously transport multiple cargos of different sizes in a coordinated and collision-free manner."}
{"id": "2602.18656", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.18656", "abs": "https://arxiv.org/abs/2602.18656", "authors": ["Joshua Habiger", "Pratyaydipta Rudra"], "title": "Minimally Discrete and Minimally Randomized p-Values", "comment": null, "summary": "In meta analysis, multiple hypothesis testing and many other methods, p-values are utilized as inputs and assumed to be uniformly distributed over the unit interval under the null hypotheses. If data used to generate p-values have discrete distributions then either natural, mid- or randomized p-values are typically utilized. Natural and mid-p-values can allow for valid, albeit conservative, downstream methods since under the null hypothesis they are dominated by uniform distributions in the stochastic and convex order, respectively. Randomized p-values need not lead to conservative procedures since they permit a uniform distributions under the null hypotheses through the generation of independent auxiliary variates. However, the auxiliary variates necessarily add variation to procedures. This manuscript introduces and studies ``minimally discrete'' (MD) natural p-values, MD mid-p-values and ``minimally randomized'' (MR) p-values. It is shown that MD p-values dominate their non-MD counterparts in the stochastic and convex order, and hence lead to less conservative, yet still valid, downstream methods. Likewise, MR p-values dominate their non-MR counterparts in that they are still uniformly distributed under the null hypotheses, but the added variation attributable to the independently generated auxiliary variate is smaller. It is anticipated that results here will facilitate the construction of new meta-analysis and multiple testing methods via more efficient p-value construction, and facilitate theoretical study of existing and new methods by establishing gold standards for addressing the unavoidable detrimental ``discreteness effect''."}
{"id": "2602.19070", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19070", "abs": "https://arxiv.org/abs/2602.19070", "authors": ["Jie Song", "Yang Bai", "Naoki Wakamiya"], "title": "Cooperative Transportation Without Prior Object Knowledge via Adaptive Self-Allocation and Coordination", "comment": null, "summary": "This work proposes a novel cooperative transportation framework for multi-agent systems that does not require any prior knowledge of cargo locations or sizes. Each agent relies on local sensing to detect cargos, recruit nearby agents, and autonomously form a transportation team with an appropriate size. The core idea is that once an agent detects a cargo within its sensing range, it generates an attraction field represented by a density function, which pulls neighboring agents toward the cargo. When multiple cargos are present, the attraction fields generated by different agents are adaptively weighted and combined with Centroidal Voronoi Tessellation (CVT), enabling agents to self-organize into balanced formations while automatically allocating more agents to larger cargos. To prevent agents from clustering on one side of a large cargo, a Control Barrier Function (CBF)-based mechanism is introduced to enforce safe inter-agent distances and promote a uniform, symmetric distribution of agents around each cargo, which is essential for stable transportation. Simulation results demonstrate that the proposed framework can simultaneously transport multiple cargos of different sizes in a coordinated and collision-free manner."}
{"id": "2602.19484", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19484", "abs": "https://arxiv.org/abs/2602.19484", "authors": ["Miles Waugh", "Chuwei Wang", "Radu Andrei", "Nusair Islam", "Taylor Lee Patti", "Eugene Demler", "Anima Anandkumar"], "title": "Toward the Thermodynamic Limit: Neural Operators for Non-equilibrium Dynamics of Mott Insulators", "comment": null, "summary": "Mott insulators exhibit complex photoexcitation dynamics under intense optical driving, with potential implications for carrier multiplication beyond the Shockley-Queisser limit. Probing these nonequilibrium processes requires access to the thermodynamic limit, where the number of lattice sites becomes arbitrarily large, but conventional solvers are constrained to small systems due to the exponential growth of the Hilbert space. Fourier Neural Operators (FNOs), originally developed for solving partial differential equations, naturally accommodate inputs of varying resolution and are capable of capturing nonlocal effects. Here, we employ FNOs to learn the mapping from noise-perturbed ground-state momentum distributions to their post-pulse counterparts across a range of interaction strengths and driving parameters. Trained only on small lattices, the model generalizes zero-shot to much larger systems, producing physically reasonable momentum distributions well beyond the reach of numerical solvers. Specifically, the model can predict momentum distribution for a 1024x1024 system within a few seconds that matches the theoretical behavior of key observables, whereas direct numerical simulations have so far been restricted to edge sizes of ~30. These results demonstrate the potential of neural operators to directly access large-scale nonequilibrium dynamics, providing a new pathway toward the thermodynamic limit in strongly correlated materials."}
{"id": "2602.18995", "categories": ["math.OC", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.18995", "abs": "https://arxiv.org/abs/2602.18995", "authors": ["Die Xiao", "Yisheng Song"], "title": "Effective Second-Harmonic Generation Coefficient and C-eigenvalue of Nonlinear Susceptibility Tensors", "comment": "15 pages", "summary": "The effective second-harmonic generation (SHG) coefficient is a crucial data that quantifies the efficiency of transforming fundamental frequency light into its second harmonic. With the help of the symmetry of nonlinear optical susceptibility tensors, we mainly discuss the computability of such a effective SHG coefficient in uniaxial crystals. For one thing, the calculation of effective SHG coefficient is converted into the optimization models with some geometric constraints by means of the peculiarity of fundamental frequency light. Secondly, the number of variables of such maximum models are cutted in half to $2$ to calculate it easier, and a comparison between the effective SHG coefficient and C-eigenvalue of susceptibility tensor is given also. Finally, some examples of typical crystal classes are presented to verify the correctness and broader applicabilities of the theoretical results."}
{"id": "2602.20044", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.20044", "abs": "https://arxiv.org/abs/2602.20044", "authors": ["H. C. W. Price", "H. AlMuhanna", "P. M. Bassani", "M. Ho", "T. S. Evans"], "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook", "comment": null, "summary": "Within twelve days of launch, an AI-native social platform exhibits extreme attention concentration, hierarchical role separation, and one-way attention flow, consistent with the hypothesis that stratification in agent ecosystems can emerge rapidly rather than gradually. We analyse publicly observable traces from a 12-day window of Moltbook (28 January -- 8 February 2026), comprising 20,040 posts and 192,410 comments from 15,083 accounts across 759 submolts. We construct co-participation and directed-comment graphs and report reciprocity, community structure, and centrality, alongside descriptive content themes. Under a commenter--post-author tie definition, interaction is strongly asymmetric (reciprocity ~1%), and HITS centrality separates cleanly into hub and authority roles, consistent with broadcast-style attention rather than mutual exchange. Engagement is highly unequal: attention is far more concentrated than production (upvote Gini = 0.992 vs. posting Gini = 0.601), and early-arriving accounts accumulate substantially higher cumulative upvotes prior to exposure-time correction, suggesting rich-get-richer dynamics. Participation is brief and bursty (median observed lifespan 2.48 minutes; 54.8% of posts occur within six peak UTC hours). Embedding-based topic modelling identifies diverse thematic clusters, including technical discussion of memory and identity, onboarding messages, and formulaic token-minting content. These results provide an early structural baseline for large-scale agent--agent social interaction and suggest that familiar forms of hierarchy, amplification, and role differentiation can arise on compressed timescales in agent-facing platforms."}
{"id": "2602.20083", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20083", "abs": "https://arxiv.org/abs/2602.20083", "authors": ["Xinzhao Li", "Alptekin Vardar", "Franz Müller", "Navya Goli", "Umamaheswara Tida", "Kai Ni", "X. Sharon Hu", "Thomas Kämpfe", "Ruiyang Qin"], "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval", "comment": "Accepted by DAC'26", "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimension array constraints. This gap is compounded by the diversity of CiM implementations (e.g., SRAM, ReRAM, FeFET), each with unique designs (e.g., 2-bit cells, 512x512 arrays). Consequently, RAG data must be naively reshaped to fit each target implementation. Current data shaping methods handle dimension and precision disjointly, which degrades data fidelity. This not only negates the advantages of CiM for RAG but also confuses hardware designers, making it unclear if a failure is due to the circuit design or the degraded input data. As a result, CiM adoption remains limited. In this paper, we introduce CQ-CiM, a unified, hardware-aware data shaping framework that jointly learns Compression and Quantization to produce CiM-compatible low-bit embeddings for diverse CiM designs. To the best of our knowledge, this is the first work to shape data for comprehensive CiM usage on RAG."}
{"id": "2602.19364", "categories": ["physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.19364", "abs": "https://arxiv.org/abs/2602.19364", "authors": ["Richard de Grijs"], "title": "Marking Noon: The Time Balls and Time Flaps of the Netherlands", "comment": "24 pages, incl. 15 figures. Accepted for publication in the Journal of Astronomical History and Heritage (December 2026)", "summary": "In the nineteenth century, the Netherlands quickly adopted the time ball -- a British innovation for maritime chronometer calibration -- in its main naval ports (Nieuwediep/Den Helder, Vlissingen, Hellevoetsluis) and commercial centres (Amsterdam, Rotterdam). A large sphere dropped from a mast at a fixed time, the device enabled ships to verify their chronometers against a standard, essential for accurate longitude determination and safe navigation. Its ready acceptance was eased by indigenous Dutch traditions. Rural communities had long used visual time signals like the sjouw on Terschelling island, a wicker ball raised on a mast to mark the lunch hour and milking time for farmers, and the lawei, a basket or sack used in the peat bogs of Friesland to regulate labourers' hours. The Dutch time-signal system was distinguished by its strong institutional backing from the country's Royal Navy, its Hydrographic Service and by professional astronomers. Among the latter, Frederik Kaiser was a pivotal figure, vehemently defending the system's accuracy and pioneering technical improvements. He successfully advocated for replacing the traditional falling ball with a system of rotating flaps, which provided a more instantaneous and reliable visual signal. Beyond their practical role, time signals became civic spectacles and symbols of Dutch scientific modernity and imperial reach. Their decline began with the electric telegraph and was finalised by wireless radio, which allowed ships to calibrate chronometers at sea."}
{"id": "2602.18629", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18629", "abs": "https://arxiv.org/abs/2602.18629", "authors": ["Camille Carvalho", "Stéphanie Chaillat", "Elsie Cortes", "Chrysoula Tsogka"], "title": "Discretization in Multilayered Media with High Contrasts: Is It All About the Boundaries?", "comment": "34 pages, 18 figures", "summary": "Wave propagation in multilayered media with high material contrasts poses significant numerical challenges, as large variations in wavenumbers lead to strong reflections and complex transmission of the incoming wave field. To address these difficulties, we employ a boundary integral formulation thereby avoiding volumetric discretization. In this framework, the accuracy of the numerical solution depends strongly on how the material interfaces are discretized. In this work, we demonstrate that standard meshing strategies based on resolving the maximum wavenumber across the domain become computationally inefficient in multilayered configurations, where high wavenumbers are confined to localized subdomains. Through a systematic study of multilayer transmission problems, we show that no simple discretization rule based on the maximum wavenumber or material contrasts emerges. Instead, the wavenumber of the background (exterior) medium plays a dominant role in determining the optimal boundary resolution. Building on these insights, we propose an adaptive approach that achieves uniform accuracy and efficient computation across multiple layers. Numerical experiments for a range of multilayer configurations demonstrate the scalability and robustness of the proposed approach."}
{"id": "2602.19023", "categories": ["cond-mat.stat-mech", "physics.bio-ph", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.19023", "abs": "https://arxiv.org/abs/2602.19023", "authors": ["Gunn Kim"], "title": "Critical Scaling and Metabolic Regulation in a Ginzburg--Landau Theory of Cognitive Dynamics", "comment": "5 pages, 3 figures. Includes Supplemental Material. Submitted for publication", "summary": "We formulate a phenomenological effective field theory in which biological intelligence emerges as a macroscopic order parameter sustained by continuous metabolic flux. By modeling cognition as a coarse-grained neural activity field governed by a variational free energy, we derive closed-form expressions for information capacity and structural susceptibility using a Gaussian maximum entropy approximation. The theory predicts a universal algebraic divergence of the susceptibility, $χ\\sim K^{-3/2}$, as the structural stiffness $K$ approaches the instability threshold. The exponent $γ= 3/2$ is consistent with the mean-field branching process universality class, thereby providing a theoretical rationale for the observed avalanche size exponent $τ\\approx 3/2$ in cortical dynamics without invoking microscopic equivalence. We identify adult cognition as a metabolically pinned non-equilibrium steady state maintained near the critical regime $Γ\\equiv K/α\\approx 1$ by continuous metabolic regulation, while pathological decline corresponds to a delocalization transition triggered by the violation of structural stability conditions. The framework generates concrete, falsifiable predictions for attention scaling, altered states of consciousness, and transcranial magnetic stimulation responses, each of which can be tested against existing neuroimaging and electrophysiological datasets."}
{"id": "2602.19475", "categories": ["cs.CE", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.19475", "abs": "https://arxiv.org/abs/2602.19475", "authors": ["Pao-Hsiung Chiu", "Jian Cheng Wong", "Chin Chun Ooi", "Chang Wei", "Yuchen Fan", "Yew-Soon Ong"], "title": "Scale-PINN: Learning Efficient Physics-Informed Neural Networks Through Sequential Correction", "comment": null, "summary": "Physics-informed neural networks (PINNs) have emerged as a promising mesh-free paradigm for solving partial differential equations, yet adoption in science and engineering is limited by slow training and modest accuracy relative to modern numerical solvers. We introduce the Sequential Correction Algorithm for Learning Efficient PINN (Scale-PINN), a learning strategy that bridges modern physics-informed learning with numerical algorithms. Scale-PINN incorporates the iterative residual-correction principle, a cornerstone of numerical solvers, directly into the loss formulation, marking a paradigm shift in how PINN losses can be conceived and constructed. This integration enables Scale-PINN to achieve unprecedented convergence speed across PDE problems from different physics domain, including reducing training time on a challenging fluid-dynamics problem for state-of-the-art PINN from hours to sub-2 minutes while maintaining superior accuracy, and enabling application to representative problems in aerodynamics and urban science. By uniting the rigor of numerical methods with the flexibility of deep learning, Scale-PINN marks a significant leap toward the practical adoption of PINNs in science and engineering through scalable, physics-informed learning. Codes are available at https://github.com/chiuph/SCALE-PINN."}
{"id": "2602.19516", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.19516", "abs": "https://arxiv.org/abs/2602.19516", "authors": ["Ruikun Li", "Jun Yao", "Yingfan Hua", "Shixiang Tang", "Biqing Qi", "Bin Liu", "Wanli Ouyang", "Yan Lu"], "title": "Pixel2Phys: Distilling Governing Laws from Visual Dynamics", "comment": "CVPR2026 main track", "summary": "Discovering physical laws directly from high-dimensional visual data is a long-standing human pursuit but remains a formidable challenge for machines, representing a fundamental goal of scientific intelligence. This task is inherently difficult because physical knowledge is low-dimensional and structured, whereas raw video observations are high-dimensional and redundant, with most pixels carrying little or no physical meaning. Extracting concise, physically relevant variables from such noisy data remains a key obstacle. To address this, we propose Pixel2Phys, a collaborative multi-agent framework adaptable to any Multimodal Large Language Model (MLLM). It emulates human scientific reasoning by employing a structured workflow to extract formalized physical knowledge through iterative hypothesis generation, validation, and refinement. By repeatedly formulating, and refining candidate equations on high-dimensional data, it identifies the most concise representations that best capture the underlying physical evolution. This automated exploration mimics the iterative workflow of human scientists, enabling AI to reveal interpretable governing equations directly from raw observations. Across diverse simulated and real-world physics videos, Pixel2Phys discovers accurate, interpretable governing equations and maintaining stable long-term extrapolation where baselines rapidly diverge."}
{"id": "2602.20071", "categories": ["math.ST", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20071", "abs": "https://arxiv.org/abs/2602.20071", "authors": ["A. Martín Andrés", "M. Álvarez Hernández"], "title": "Estimators of different delta coefficients based on the unbiased estimator of the expected proportions of agreements", "comment": null, "summary": "To measure the degree of agreement between two observers that independently classify $n$ subjects within $K$ categories, it is common to use different kappa type coefficients, the most common of which is the $κ_C$ coefficient (Cohen's kappa). As $κ_C$ has some weaknesses -such as its poor performance with highly unbalanced marginal distributions-, the $Δ$ coefficient is sometimes used, based on the $delta$ response model. This model allows us to obtain other parameters like: (a) the $α_i$ contribution of each $i$ category to the value of the global agreement $Δ=\\sum α_i$; and (b) the consistency $\\mathcal{S}_i$ in the category $i$ (degree of agreement in the category $i$), a more appropriate parameter than the kappa value obtained by collapsing the data into the category $i$. It has recently been shown that the classic estimator $\\hatκ_C$ underestimates $κ_C$, having obtained a new estimator $\\hatκ_{CU}$ which is less biased. This article demonstrates that something similar happens to the known estimators $\\hatΔ$, $\\hatα_i$, and $\\hat{\\mathcal{S}}_i$ of $Δ$, $α_i$ and $\\mathcal{S}_i$ (respectively), proposes new and less biased estimators $\\hatΔ_U$, $\\hatα_{iU}$, and $\\hat{\\mathcal{S}}_{iU}$, determines their variances, analyses the behaviour of all estimators, and concludes that the new estimators should be used when $n$ or $K$ are small (at least when $n\\leq 50$ or $K\\leq 3$). Additionally, the case where one of the raters is a gold standard is contemplated, in which situation two new parameters arise: the $conformity$ (the rater's capability to recognize a subject in the category $i$) and the $predictivity$ (the reliability of a response $i$ by the rater)."}
{"id": "2602.19238", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19238", "abs": "https://arxiv.org/abs/2602.19238", "authors": ["Mingliang Xiong", "Zeqian Guo", "Qingqing Zhang", "Qingwen Liu", "Gang Wang", "Gang Li", "Bin He"], "title": "On the Stability of Spatially Distributed Cavity Laser and Boundary of Resonant Beam SLIPT", "comment": null, "summary": "Spatially distributed cavity (SDC) lasers are a promising technology for simultaneous light information and power transfer (SLIPT), offering benefits such as increased mobility and intrinsic safety, which are advantageous for various Internet of Things (IoT) devices. \\mll However, achieving beam transmission over meter-level long working distances presents significant challenges from cavity stability constraints, manufacturing/assembly tolerances, and diffraction losses\\mrr. This paper conducts a theoretical investigation of the fundamental restrictions limiting long-range resonant beam generation. We investigate cavity stability and beam characteristics, and propose a binary-search-based Monte Carlo simulation algorithm as well as a linear approximation algorithm to quantify the maximum acceptable tolerances for stable operation. \\mll Numerical results indicate that the stable region contracts sharply as distance increases. For fixed-component systems, an acceptable tolerance of 0.01 mm restricts the achievable transmission distance to less than 2 m. \\mrr To address this limitation, we also prove the feasibility of long-range beam formation using precision adjustable elements, paving the way for advanced engineering applications. \\mll Experimental results verified this assumption, demonstrating that by tuning the stable region during assembly, the transmission distance could be extended to 2.8 m. \\mrr This work provides essential theoretical insights and practical design guidelines for realizing stable, long-range SDC systems."}
{"id": "2602.18660", "categories": ["stat.ME", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18660", "abs": "https://arxiv.org/abs/2602.18660", "authors": ["Brandon Victor Syiem", "Eduardo Velloso"], "title": "Better Assumptions, Stronger Conclusions: The Case for Ordinal Regression in HCI", "comment": "21 pages, 16 figures, to be published in the Proceedings of the 2026 ACM CHI Conference on Human Factors in Computing Systems", "summary": "Despite the widespread use of ordinal measures in HCI, such as Likert-items, there is little consensus among HCI researchers on the statistical methods used for analysing such data. Both parametric and non-parametric methods have been extensively used within the discipline, with limited reflection on their assumptions and appropriateness for such analyses. In this paper, we examine recent HCI works that report statistical analyses of ordinal measures. We highlight prevalent methods used, discuss their limitations and spotlight key assumptions and oversights that diminish the insights drawn from these methods. Finally, we champion and detail the use of cumulative link (mixed) models (CLM/CLMM) for analysing ordinal data. Further, we provide practical worked examples of applying CLM/CLMMs using R to published open-sourced datasets. This work contributes towards a better understanding of the statistical methods used to analyse ordinal data in HCI and helps to consolidate practices for future work."}
{"id": "2602.19238", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19238", "abs": "https://arxiv.org/abs/2602.19238", "authors": ["Mingliang Xiong", "Zeqian Guo", "Qingqing Zhang", "Qingwen Liu", "Gang Wang", "Gang Li", "Bin He"], "title": "On the Stability of Spatially Distributed Cavity Laser and Boundary of Resonant Beam SLIPT", "comment": null, "summary": "Spatially distributed cavity (SDC) lasers are a promising technology for simultaneous light information and power transfer (SLIPT), offering benefits such as increased mobility and intrinsic safety, which are advantageous for various Internet of Things (IoT) devices. \\mll However, achieving beam transmission over meter-level long working distances presents significant challenges from cavity stability constraints, manufacturing/assembly tolerances, and diffraction losses\\mrr. This paper conducts a theoretical investigation of the fundamental restrictions limiting long-range resonant beam generation. We investigate cavity stability and beam characteristics, and propose a binary-search-based Monte Carlo simulation algorithm as well as a linear approximation algorithm to quantify the maximum acceptable tolerances for stable operation. \\mll Numerical results indicate that the stable region contracts sharply as distance increases. For fixed-component systems, an acceptable tolerance of 0.01 mm restricts the achievable transmission distance to less than 2 m. \\mrr To address this limitation, we also prove the feasibility of long-range beam formation using precision adjustable elements, paving the way for advanced engineering applications. \\mll Experimental results verified this assumption, demonstrating that by tuning the stable region during assembly, the transmission distance could be extended to 2.8 m. \\mrr This work provides essential theoretical insights and practical design guidelines for realizing stable, long-range SDC systems."}
{"id": "2602.19499", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19499", "abs": "https://arxiv.org/abs/2602.19499", "authors": ["Charles Menil", "Andrea Capa Salinas", "Stephen D. Wilson", "Benoît Fauqué", "Kamran Behnia"], "title": "Electron-electron and electron-phonon collision cross sections in CsV3Sb5", "comment": null, "summary": "AV3Sb5 (A=K, Rb, Cs) are kagome metals and superconductors, attracting much recent attention as nexus of multiple quantum states. Here, through a systematic study of electric and thermal transport of CsV3Sb5, we identify iy as a metallic Fermi liquid with moderate electronic correlations ans strong electron-phonon (e-ph) collision cross section. We observe contributions to the inelastic electrical resistivity, each dominating within a distinct temperature window. The prefactor of the T2 is consistent with the Kadowaki-Woods scaling for a Fermi liquid with moderate correlation. By performing thermal conductivity measurements at zero and finite magnetic field, we separate the electronic and the lattice contributions to the thermal conductivity. The Wiedemann-Franz law is is satisfied in the zero-temperature limit, while a downward deviation emerges at finite temperature due to the mismatch between the prefactors of the electrical and thermal quadratic resistivities, as reported in other metals. The Bloch-Grüneisen description of electron-phonon scattering successfully accounts for both electronic thermal and electrical transport, indicating a remarkably large e-ph collision cross section in CsV3Sb5."}
{"id": "2602.19051", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.19051", "abs": "https://arxiv.org/abs/2602.19051", "authors": ["Cheng Lu", "Yu Fei", "Jing Zhou", "Zhibin Deng", "Guangtai Qu"], "title": "New reformulations for 0-1 quadratic programming problem using quadratic nonconvex reformulation techniques and valid inequalities", "comment": null, "summary": "It is well-known that the quadratic convex reformulation (QCR) technique can speed up some general-purpose solvers such as CPLEX and Gurobi. Recently, the method of quadratic nonconvex reformulation (QNR) was proposed, which provides an alternative way for accelerating a solver via reformulation technique. This paper proposes several new reformulations for 0-1 quadratic programming problems using the QNR technique. Such a technique provides more flexibility in adding nonconvex quadratic constraints into the problem formulation, so that some valid inequalities, such as the triangle inequalities, can be incorporated into the formulation to tighten the lower bound of the problem. We analyze the effects of the proposed reformulations on the lower bounds implemented in the solver, and propose some methods to maximize the McCormick relaxation bounds of the reformulations. Our numerical experiments compare the proposed reformulations with the existing quadratic convex reformulations, showing the effectiveness of the proposed reformulations on 0-1 quadratic programming problems."}
{"id": "2602.18892", "categories": ["nlin.AO", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18892", "abs": "https://arxiv.org/abs/2602.18892", "authors": ["Everton R. Constantino", "Alberto Saa"], "title": "Evolving scale-free networks and node-based random edge deletion", "comment": "7 pages, 4 figures", "summary": "We investigate a growing network model that combines preferential and uniform attachment with two distinct mechanisms of edge deletion. In addition to the usual uniform probability edge deletion, we introduce a novel node-based rule in which uniformly chosen non-isolated nodes lose one of their incident edges. This mechanism differs fundamentally from uniform edge deletion and leads to a nonlinear evolution for the stationary degree distribution due to the nonlinear dependence on the fraction of isolated nodes. We solve the general problem in the stationary regime and obtain closed-form expressions for the degree distribution in terms of hypergeometric and confluent hypergeometric functions. Depending on the balance between attachment and deletion rates, three asymptotic regimes for the degree distribution arise: power-law, exponential, and a critical regime characterized by a stretched exponential decay. We show that the node-based edge deletion mechanism is less likely to disrupt the scale-free (power-law) regime than the uniform edge deletion. Moreover, we also demonstrate that the precise balance between preferential attachment and node-based deletion can transform a scale-free network into a critical one with stretched exponential decay. Extensive numerical simulations exhibit excellent agreement with the theoretical predictions."}
{"id": "2602.19387", "categories": ["quant-ph", "cs.ET", "hep-ex", "hep-lat", "hep-ph"], "pdf": "https://arxiv.org/pdf/2602.19387", "abs": "https://arxiv.org/abs/2602.19387", "authors": ["Marco Knipfer", "Alexander Roman", "Konstantin T. Matchev", "Katia Matcheva", "Sergei Gleyzer"], "title": "AI Agents for Variational Quantum Circuit Design", "comment": "43 pages, 12 figures", "summary": "Variational quantum circuits (VQCs) constitute a central building block of near-term quantum machine learning (QML), yet the principled design of expressive and trainable architectures remains a major open challenge. The VQC design space grows combinatorially with the number of qubits, layers, entanglement structures, and gate parameterizations, rendering manual circuit construction inefficient and often suboptimal. We introduce an autonomous agent-based framework for VQC architecture search that integrates high-level reasoning with a quantum simulation environment. The agent proposes candidate circuit architectures, evaluates them through fully automated training and validation pipelines, and iteratively improves its design strategy via performance-driven feedback. Empirically, we show that the agent autonomously evolves circuit architectures from simple initial ansätze toward increasingly expressive designs, progressively trying to improve task performance. This demonstrates that agentic AI can effectively navigate and refine the VQC design landscape with minimal human intervention, providing a scalable methodology for automated quantum model development in the Noisy Intermediate-Scale Quantum (NISQ) regime."}
{"id": "2602.19365", "categories": ["physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.19365", "abs": "https://arxiv.org/abs/2602.19365", "authors": ["Richard de Grijs"], "title": "Dutch Colonial Time: Time Signals in Paramaribo and the Dutch Caribbean", "comment": "14 pages, incl. 5 figures. Accepted for publication in the Journal of Astronomical History and Heritage (December 2026)", "summary": "In the nineteenth century, the Dutch established time signals in their Atlantic colonies to synchronise maritime navigation with European standards. In Paramaribo (Suriname), a sophisticated sequence of apparatus -- including time balls, noon guns, discs and flags -- operated from 1851 until World War I. Naval officers aboard guard ships used sextants equipped with artificial horizons to determine local noon, thus integrating the colony into the global Greenwich-based cartographic system. This infrastructure was not merely technical; it became a civic ritual, with the daily noon gun structuring urban life and becoming a point of political negotiation between naval commanders and the colonial governor. In contrast, the Dutch Caribbean islands employed simpler, pragmatic systems. Curaçao used a daily time flag, a cost-effective solution suited to its climate and harbour scale, while smaller islands like Aruba and St. Eustatius relied on occasional noon guns. This diversity reflected a decentralised colonial administration that adapted technologies to local conditions and budgets. The history of these time signals reveals a process of hybrid adaptation, not simply replication of European models. They were shaped by environmental challenges, fiscal constraints and local politics, functioning simultaneously as navigational aids and civic landmarks. Their eventual decline, owing to budgetary pressures and new technologies like wireless telegraphy, underscores the fragile and negotiated nature of colonial scientific infrastructures."}
{"id": "2602.18634", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18634", "abs": "https://arxiv.org/abs/2602.18634", "authors": ["Tan Bui-Thanh", "Giancarlo Villatoro", "C. G. Krishnanunni"], "title": "From an Elementary Proof of Error Representation for Hermite Quadrature to a Rediscovery of Legendre Polynomials and Rodrigues Formula", "comment": null, "summary": "We generalize two-point interpolatory Hermite quadrature to functions with available values and the first (n-1) derivatives at both end points. Armed with integration by parts in the reverse form we provide an elementary derivation of an exact error represenation of Hermite quadrature rule. This approach possesses several advantages over the classical approaches: i) Only integration by parts is needed for the derivation; ii) the error representation requires much milder regularity, namely the existence of nth-order derivative rather than a (2n)th-order derivative of the function under consideration. As a result, our error formula is valid for less regular functions for which the classical ones are not valid; iii) our approach rediscovers Legendre polynomials and more interestingly it provides a surprisingly elegant relation between Legendre polynomial and Hermite interpolation. In particular, Legendre polynomials are precisely the error kernels for interpolatory Hermite quadrature rules; and iv) We also rediscover the Rodrigues formula for Legendre polynomials as part of our findings. For those who are interested in a different proof of the exact error representation for Hermite quadrature rule, we provide an alternative proof using the Peano kernel theorem. We also provide a composite interpolatory Hermite quadrature rule for practical applications."}
{"id": "2602.19045", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.19045", "abs": "https://arxiv.org/abs/2602.19045", "authors": ["Yan Ru Pei"], "title": "peapods: A Rust-Accelerated Monte Carlo Package for Ising Spin Systems", "comment": null, "summary": "We present peapods (github.com/PeaBrane/peapods), an open-source Python package for Monte Carlo simulation of Ising spin systems with arbitrary coupling constants on arbitrary-dimensional hypercubic lattices with periodic boundary conditions. The computational core is written in Rust and exposed to Python via PyO3, combining the ergonomic interface of Python with the performance of compiled, memory-safe code. The package implements Metropolis and Gibbs single-spin-flip algorithms, Swendsen--Wang and Wolff cluster updates, and parallel tempering. Replica-level parallelism is achieved through the Rayon work-stealing scheduler. We validate the implementation against the exact critical temperature of the two-dimensional Ising model via finite-size scaling of the Binder cumulant."}
{"id": "2602.19637", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.19637", "abs": "https://arxiv.org/abs/2602.19637", "authors": ["Taeung Kim", "Jeongmoo Lee", "Ara Go"], "title": "Data-Driven Bath Fitting for Hamiltonian-Diagonalization Dynamical Mean-Field Theory", "comment": "23 pages, 10 figures", "summary": "We propose a machine-learning-based initialization method to overcome the nonlinear bath-fitting bottleneck in Hamiltonian-diagonalization-based dynamical mean-field theory (HD-DMFT). In HD-DMFT, the continuous hybridization function is approximated by a finite set of bath-site energies and hybridization amplitudes, determined by minimizing a highly non-convex multivariable cost function. As the number of bath sites increases, the optimization becomes more sensitive to the initial guess and more prone to suboptimal local minima, which can slow or destabilize the DMFT self-consistency loop. We reformulate bath fitting as a supervised regression problem and train a kernel ridge regression model to predict near-optimal discrete bath parameters directly from the target hybridization function on the Matsubara axis. To ensure physical relevance and data diversity, we construct the training dataset from tight-binding Hamiltonians of layered-perovskite-like ruthenate models across systematically deformed structures, instead of relying on naive random parameter sampling, and obtain high-quality labels through fully converged conventional bath fitting. Time-reversal symmetry is explicitly incorporated in both feature and target representations to reduce effective dimensionality and enforce physical consistency. Benchmarks in the non-interacting limit show that the learned initialization systematically reduces the initial fitting error, decreases the number of conjugate-gradient iterations, and improves robustness against local minima over a wide range of bath sizes. We further demonstrate transferability to interacting DMFT calculations for $\\mathrm{Sr_{2}RuO_{4}}$ solved with an adaptive-truncation impurity solver, where the ML initialization yields consistently faster convergence than a symmetry-preserving heuristic baseline while preserving the final fitted solution."}
{"id": "2602.19597", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.19597", "abs": "https://arxiv.org/abs/2602.19597", "authors": ["Giacomo Bottacini", "Matteo Torzoni", "Andrea Manzoni"], "title": "Neural Markov chain Monte Carlo: Bayesian inversion via normalizing flows and variational autoencoders", "comment": null, "summary": "This paper introduces a Bayesian framework that combines Markov chain Monte Carlo (MCMC) sampling, dimensionality reduction, and neural density estimation to efficiently handle inverse problems that (i) must be solved multiple times, and (ii) are characterized by intractable or unavailable likelihood functions. The posterior probability distribution over quantities of interest is estimated via differential evolution Metropolis sampling, empowered by learnable mappings. First, a variational autoencoder performs probabilistic feature extraction from observational data. The resulting latent structure inherently quantifies uncertainty, capturing deviations between the actual data-generating process and the training data distribution. At each step of the MCMC random walk, the algorithm jointly samples from the data-informed latent distribution and the space of parameters to be inferred. These samples are fed into a neural likelihood estimator based on normalizing flows, specifically real-valued non-volume preserving transformations. The scaling and translation functions of the affine coupling layers are modeled by neural networks conditioned on the unknown parameters, allowing the representation of arbitrary observation likelihoods. The proposed methodology is validated on two case studies: (i) structural health monitoring of a railway bridge for damage detection, localization, and quantification, and (ii) estimation of the conductivity field in a steady-state Darcy's groundwater flow problem. The results demonstrate the efficiency of the inference strategy, while ensuring that model-reality mismatches do not yield overconfident, yet inaccurate, estimates."}
{"id": "2602.20115", "categories": ["math.ST", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20115", "abs": "https://arxiv.org/abs/2602.20115", "authors": ["Nikolaos Ignatiadis", "Sid Kankanala"], "title": "Compound decisions and empirical Bayes via Bayesian nonparametrics", "comment": "34 pages", "summary": "We study the Gaussian sequence compound decision problem and analyze a Bayesian nonparametric estimator from an empirical Bayes, regret-based perspective. Motivated by sharp results for the classical nonparametric maximum likelihood estimator (NPMLE), we ask whether an analogous guarantee can be obtained using a standard Bayesian nonparametric prior. We show that a Dirichlet-process-based Bayesian procedure achieves near-optimal regret bounds. Our main results are stated in the compound decision framework, where the mean vector is treated as fixed, while we also provide parallel guarantees under a hierarchical model in which the means are drawn from a true unknown prior distribution. The posterior mean Bayes rule is, a fortiori, admissible, whereas we show that the NPMLE plug-in rule is inadmissible."}
{"id": "2602.19310", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19310", "abs": "https://arxiv.org/abs/2602.19310", "authors": ["Yihsu Chen", "Abel Souza", "Fargol Nematkhah", "Andrew L. Liu"], "title": "A Power Market Model with Hypersaclers and Modular Datacenters", "comment": null, "summary": "The rapid adoption of AI has led the growth of computational demand, with large language models (LLMs) at the forefront since ChatGPT's debut in 2022. Meanwhile, large amounts of renewable energy have been deployed but, ultimately, curtailed due to transmission congestion and inadequate demand. This work develops a power market model that allows hyperscalers to spatially migrate LLM inference workloads to geo-distributed modular datacenters (MDCs), which are co-located with near renewable sources of energy at the edge of the network. We introduce the optimization problems faced by the hyperscaler and MDCs in addition to consumers, producers, and the electric grid operator, where the hyerscaler enters an agreement to lease MDCs while ensuring that the required service level objectives (SLOs) are met. The overall market model is formulated as a complementarity problem, where the proof is provided showing the existence and uniqueness of the solutions. When applying the model to an IEEE RTS-24 bus case study, we show that even with a provision that requires MDCs to disclose the CO$_2$ emissions associated with their energy supply sources, renting less polluting MDCs is unlikely to yield meaningful emission reductions due to so-called contract-reshuffling. The situation can be mitigated when conventional loads are supplied by forward contracts through power purchase agreements. This also leads to a decline in system congestion when the hyperscaler becomes increasingly cost-aware."}
{"id": "2602.18677", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.18677", "abs": "https://arxiv.org/abs/2602.18677", "authors": ["Angela M Dahl", "Elizabeth R Brown"], "title": "Bayesian calendar-time survival analysis with epidemic curve priors and variant-specific infection hazards", "comment": "24 pages, 6 figures", "summary": "In this paper, we develop a Bayesian calendar-time survival model motivated by infectious disease prevention studies occurring during an epidemic, when the risk of infection can change rapidly as the epidemic curve shifts. For studies in which a biomarker is the predictor of interest, we include the option to estimate a threshold of protection for the biomarker. If the intervention is hypothesized to have different associations with several circulating viral variants, or if the infectiousness of the dominant variant(s) changes over the course of the study, we treat infection from different variants as competing risks. We also introduce a novel method for incorporating existing epidemic curve estimates into an informative prior for the baseline hazard function, enabling estimation of the intervention's association with infection risk during periods of calendar time with minimal follow-up in one or more comparator groups. We demonstrate the strengths of this method via simulations, and we apply it to data from an observational COVID-19 vaccine study."}
{"id": "2602.19310", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19310", "abs": "https://arxiv.org/abs/2602.19310", "authors": ["Yihsu Chen", "Abel Souza", "Fargol Nematkhah", "Andrew L. Liu"], "title": "A Power Market Model with Hypersaclers and Modular Datacenters", "comment": null, "summary": "The rapid adoption of AI has led the growth of computational demand, with large language models (LLMs) at the forefront since ChatGPT's debut in 2022. Meanwhile, large amounts of renewable energy have been deployed but, ultimately, curtailed due to transmission congestion and inadequate demand. This work develops a power market model that allows hyperscalers to spatially migrate LLM inference workloads to geo-distributed modular datacenters (MDCs), which are co-located with near renewable sources of energy at the edge of the network. We introduce the optimization problems faced by the hyperscaler and MDCs in addition to consumers, producers, and the electric grid operator, where the hyerscaler enters an agreement to lease MDCs while ensuring that the required service level objectives (SLOs) are met. The overall market model is formulated as a complementarity problem, where the proof is provided showing the existence and uniqueness of the solutions. When applying the model to an IEEE RTS-24 bus case study, we show that even with a provision that requires MDCs to disclose the CO$_2$ emissions associated with their energy supply sources, renting less polluting MDCs is unlikely to yield meaningful emission reductions due to so-called contract-reshuffling. The situation can be mitigated when conventional loads are supplied by forward contracts through power purchase agreements. This also leads to a decline in system congestion when the hyperscaler becomes increasingly cost-aware."}
{"id": "2602.19586", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19586", "abs": "https://arxiv.org/abs/2602.19586", "authors": ["Hanieh Najafzadeh", "Abdollah Langari"], "title": "From Quantum Chaos to a Reversed Quantum Disentangled Liquid in a Disorder-Free Spin Ladder", "comment": null, "summary": "The mechanisms by which isolated interacting quantum systems evade thermalization extend beyond disorder-induced many-body localization, encompassing a growing class of interaction-driven phenomena. We investigate a spin-1/2 ladder with asymmetric XY leg couplings and tunable Ising interactions on the rungs, and identify the microscopic origin of many-body localization (MBL) in this setting. Through a suite of diagnostics -including entanglement dynamics, fidelity susceptibility, adiabatic gauge potential norms, level-spacing statistics and entropy of eigenstates- we uncover a reentrant progression of dynamical regimes as the rung coupling Jz is varied: integrable behavior at Jz=0, quantum chaos at intermediate Jz, and a robust nonthermal regime at strong coupling. In the latter regime, we demonstrate the emergence of a reversed quantum disentangled liquid (reversed-QDL), where the light species thermalizes while the heavy species remains localized. The strong-coupling limit further yields emergent local integrals of motion anchored in a fixed-point structure, providing a microscopic origin of the observed quasi-MBL dynamics. These results establish reversed-QDL as a distinct, disorder-free route to nonergodicity and broaden the classification of dynamical phases in quantum matter."}
{"id": "2602.19204", "categories": ["math.OC", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.19204", "abs": "https://arxiv.org/abs/2602.19204", "authors": ["Hyeong-Ohk Bae", "Seung-Yeal Ha", "Chanho Min", "Jane Yoo", "Jaeyoung Yoon"], "title": "CBO algorithm with average drift and applications to portfolio optimization", "comment": null, "summary": "We propose a consensus based optimization algorithm with average drift (in short Ad-CBO) and provide a theoretical framework for it. In the theoretical analysis, we show that particle solutions to Ad-CBO converge to a global minimizer. In numerical simulations, we examine Ad-CBO's performance in optimizing static and dynamic objective functions. As a real-time application, we test the efficiency of Ad-CBO to find the optimal portfolio given stochastically evolving multi-asset prices in a financial market. The proposed Ad-CBO exhibits higher searching speed, lower tracking errors and regret bound than the CBO without stochastic diffusion"}
{"id": "2602.18719", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18719", "abs": "https://arxiv.org/abs/2602.18719", "authors": ["Abdellah Chkifa", "Matthieu Dolbeault", "David Krieg", "Mario Ullrich"], "title": "Constructive discretization and approximation in reproducing kernel Hilbert spaces", "comment": null, "summary": "We generalize the sparsification algorithm of Batson, Spielman and Srivastava, making one part of the result dimension-independent. In particular, we recover discretization inequalities in $L_2$- and sup-norms on general finite-dimensional subspaces, prove a suitable infinite-dimensional variant, and discuss the implications for the error of least-squares approximation based on samples. This gives a more constructive version of several recently established approximation bounds, some of which relied on the stronger and less constructive result of Marcus, Spielman and Srivastava. We also improve the constants and oversampling factors in these results."}
{"id": "2602.19741", "categories": ["cond-mat.stat-mech", "math-ph", "nlin.SI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19741", "abs": "https://arxiv.org/abs/2602.19741", "authors": ["Vsevolod I. Yashin"], "title": "Two-parameter families of MPO integrals of motion in Heisenberg spin chains", "comment": "11 pages", "summary": "Recently, Fendley et al. (2025) [arXiv:2511.04674] revealed a new way to demonstrate the integrability of XYZ Heisenberg model by constructing a one-parameter family of integrals of motion in the matrix product operator (MPO) form. In this short note, I report on the discovery of two-parameter families of MPOs that commute with with the Heisenberg spin chain Hamiltonian in the XXX, XXZ, and XYZ cases. I describe a symbolic algebra approach for finding such integrals of motion and speculate about possible applications."}
{"id": "2602.20108", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20108", "abs": "https://arxiv.org/abs/2602.20108", "authors": ["L. Brodoloni", "G. E. Astrakharchik", "S. Giorgini", "S. Pilati"], "title": "Energy gap of quantum spin glasses: a projection quantum Monte Carlo study", "comment": "6 pages plus additional material", "summary": "The performance of quantum annealing for combinatorial optimization is fundamentally limited by the minimum energy gap $Δ$ encountered at quantum phase transitions. We investigate the scaling of $Δ$ with system size $N$ for two paradigmatic quantum spin-glass models: the two-dimensional Edwards-Anderson (2D-EA) and the all-to-all Sherrington-Kirkpatrick (SK) models. Utilizing a newly proposed unbiased energy-gap estimator for continuous-time projection quantum Monte Carlo simulations, complemented by high-performance sparse eigenvalue solvers, we characterize the gap distributions across disorder realizations. It is found that, in the 2D-EA case, the inverse-gap distribution develops a fat tail with infinite variance as $N$ increases. This indicates that the unfavorable super-algebraic scaling of $Δ$, recently reported for binary couplings [Nature 631, 749 (2024)], persists for the Gaussian disorder considered here, pointing to a universal feature of 2D spin glasses. Conversely, the SK model retains a finite-variance distribution, with the disorder-averaged gap following a rather slow power law, close to $Δ\\propto N^{-1/3}$. This finding provides a promising outlook for the potential efficiency of quantum annealers for optimization problems with dense connectivity."}
{"id": "2602.19662", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.19662", "abs": "https://arxiv.org/abs/2602.19662", "authors": ["Yanda Chen", "Sebastian Rodriguez", "Beatriz Moya", "Francisco Chinesta"], "title": "Stress-constrained Topology Optimization for Metamaterial Microstructure Design", "comment": null, "summary": "Although stress-constrained topology optimization has been extensively studied in structural design, the development of optimization frameworks to enable the creation of metamaterials with optimal mechanical performance is still an open problem. This study incorporates local stress constraints into the topology optimization framework for metamaterial microstructure design, aiming to avoid the stress concentration in the optimized microstructure. For the efficient solution of multi-constraint topology optimization problems, the Augmented Lagrangian formulation is extended to address local minimization problems subjected to the combined action of local and global constraints. Additionally, as an extension of static load conditions, this study further investigates the design of metamaterial microstructures under cyclic loading. Finally, the effectiveness of the proposed approach is demonstrated through a series of two-dimensional and three-dimensional benchmark problems."}
{"id": "2602.18656", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.18656", "abs": "https://arxiv.org/abs/2602.18656", "authors": ["Joshua Habiger", "Pratyaydipta Rudra"], "title": "Minimally Discrete and Minimally Randomized p-Values", "comment": null, "summary": "In meta analysis, multiple hypothesis testing and many other methods, p-values are utilized as inputs and assumed to be uniformly distributed over the unit interval under the null hypotheses. If data used to generate p-values have discrete distributions then either natural, mid- or randomized p-values are typically utilized. Natural and mid-p-values can allow for valid, albeit conservative, downstream methods since under the null hypothesis they are dominated by uniform distributions in the stochastic and convex order, respectively. Randomized p-values need not lead to conservative procedures since they permit a uniform distributions under the null hypotheses through the generation of independent auxiliary variates. However, the auxiliary variates necessarily add variation to procedures. This manuscript introduces and studies ``minimally discrete'' (MD) natural p-values, MD mid-p-values and ``minimally randomized'' (MR) p-values. It is shown that MD p-values dominate their non-MD counterparts in the stochastic and convex order, and hence lead to less conservative, yet still valid, downstream methods. Likewise, MR p-values dominate their non-MR counterparts in that they are still uniformly distributed under the null hypotheses, but the added variation attributable to the independently generated auxiliary variate is smaller. It is anticipated that results here will facilitate the construction of new meta-analysis and multiple testing methods via more efficient p-value construction, and facilitate theoretical study of existing and new methods by establishing gold standards for addressing the unavoidable detrimental ``discreteness effect''."}
{"id": "2602.19366", "categories": ["eess.SY", "cs.MA", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.19366", "abs": "https://arxiv.org/abs/2602.19366", "authors": ["Zirui Xu", "Vasileios Tzoumas"], "title": "Self-Configurable Mesh-Networks for Scalable Distributed Submodular Bandit Optimization", "comment": null, "summary": "We study how to scale distributed bandit submodular coordination under realistic communication constraints in bandwidth, data rate, and connectivity. We are motivated by multi-agent tasks of active situational awareness in unknown, partially-observable, and resource-limited environments, where the agents must coordinate through agent-to-agent communication. Our approach enables scalability by (i) limiting information relays to only one-hop communication and (ii) keeping inter-agent messages small, having each agent transmit only its own action information. Despite these information-access restrictions, our approach enables near-optimal action coordination by optimizing the agents' communication neighborhoods over time, through distributed online bandit optimization, subject to the agents' bandwidth constraints. Particularly, our approach enjoys an anytime suboptimality bound that is also strictly positive for arbitrary network topologies, even disconnected. To prove the bound, we define the Value of Coordination (VoC), an information-theoretic metric that quantifies for each agent the benefit of information access to its neighbors. We validate in simulations the scalability and near-optimality of our approach: it is observed to converge faster, outperform benchmarks for bandit submodular coordination, and can even outperform benchmarks that are privileged with a priori knowledge of the environment."}
{"id": "2602.18865", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.18865", "abs": "https://arxiv.org/abs/2602.18865", "authors": ["Yuanzhi Li", "Shushu Zhang", "Xuming He"], "title": "Expected Shortfall Regression via Optimization", "comment": "Yuanzhi Li and Shushu Zhang contributed equally to this work", "summary": "To provide a comprehensive summary of the tail distribution, the expected shortfall is defined as the average over the tail above (or below) a certain quantile of the distribution. The expected shortfall regression captures the heterogeneous covariate-response relationship and describes the covariate effects on the tail of the response distribution. Based on a critical observation that the superquantile regression from the operations research literature does not coincide with the expected shortfall regression, we propose and validate a novel optimization-based approach for the linear expected shortfall regression, without additional assumptions on the conditional quantile models. While the proposed loss function is implicitly defined, we provide a prototype implementation of the proposed approach with some initial expected shortfall estimators based on binning techniques. With practically feasible initial estimators, we establish the consistency and the asymptotic normality of the proposed estimator. The proposed approach achieves heterogeneity-adaptive weights and therefore often offers efficiency gain over existing linear expected shortfall regression approaches in the literature, as demonstrated through simulation studies."}
{"id": "2602.19366", "categories": ["eess.SY", "cs.MA", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.19366", "abs": "https://arxiv.org/abs/2602.19366", "authors": ["Zirui Xu", "Vasileios Tzoumas"], "title": "Self-Configurable Mesh-Networks for Scalable Distributed Submodular Bandit Optimization", "comment": null, "summary": "We study how to scale distributed bandit submodular coordination under realistic communication constraints in bandwidth, data rate, and connectivity. We are motivated by multi-agent tasks of active situational awareness in unknown, partially-observable, and resource-limited environments, where the agents must coordinate through agent-to-agent communication. Our approach enables scalability by (i) limiting information relays to only one-hop communication and (ii) keeping inter-agent messages small, having each agent transmit only its own action information. Despite these information-access restrictions, our approach enables near-optimal action coordination by optimizing the agents' communication neighborhoods over time, through distributed online bandit optimization, subject to the agents' bandwidth constraints. Particularly, our approach enjoys an anytime suboptimality bound that is also strictly positive for arbitrary network topologies, even disconnected. To prove the bound, we define the Value of Coordination (VoC), an information-theoretic metric that quantifies for each agent the benefit of information access to its neighbors. We validate in simulations the scalability and near-optimality of our approach: it is observed to converge faster, outperform benchmarks for bandit submodular coordination, and can even outperform benchmarks that are privileged with a priori knowledge of the environment."}
{"id": "2602.19637", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.19637", "abs": "https://arxiv.org/abs/2602.19637", "authors": ["Taeung Kim", "Jeongmoo Lee", "Ara Go"], "title": "Data-Driven Bath Fitting for Hamiltonian-Diagonalization Dynamical Mean-Field Theory", "comment": "23 pages, 10 figures", "summary": "We propose a machine-learning-based initialization method to overcome the nonlinear bath-fitting bottleneck in Hamiltonian-diagonalization-based dynamical mean-field theory (HD-DMFT). In HD-DMFT, the continuous hybridization function is approximated by a finite set of bath-site energies and hybridization amplitudes, determined by minimizing a highly non-convex multivariable cost function. As the number of bath sites increases, the optimization becomes more sensitive to the initial guess and more prone to suboptimal local minima, which can slow or destabilize the DMFT self-consistency loop. We reformulate bath fitting as a supervised regression problem and train a kernel ridge regression model to predict near-optimal discrete bath parameters directly from the target hybridization function on the Matsubara axis. To ensure physical relevance and data diversity, we construct the training dataset from tight-binding Hamiltonians of layered-perovskite-like ruthenate models across systematically deformed structures, instead of relying on naive random parameter sampling, and obtain high-quality labels through fully converged conventional bath fitting. Time-reversal symmetry is explicitly incorporated in both feature and target representations to reduce effective dimensionality and enforce physical consistency. Benchmarks in the non-interacting limit show that the learned initialization systematically reduces the initial fitting error, decreases the number of conjugate-gradient iterations, and improves robustness against local minima over a wide range of bath sizes. We further demonstrate transferability to interacting DMFT calculations for $\\mathrm{Sr_{2}RuO_{4}}$ solved with an adaptive-truncation impurity solver, where the ML initialization yields consistently faster convergence than a symmetry-preserving heuristic baseline while preserving the final fitted solution."}
{"id": "2602.19272", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.19272", "abs": "https://arxiv.org/abs/2602.19272", "authors": ["Frédéric Marbach"], "title": "Time-iteration methods for controllability", "comment": null, "summary": "These notes are based on a short course delivered at the Summer School EUR MINT 2025 \"Control, Inverse Problems and Spectral Theory\", held in June 2025 in Toulouse, France. The course presents three important strategies in control theory, formulated as time-iteration methods, where each time step brings the state of the system closer to the desired target.\n  For linear PDEs, we survey the classical Lebeau-Robbiano method and its more recent developments. This method combines spectral inequalities and dissipation estimates to prove null controllability of a dissipative linear system.\n  For nonlinear PDEs, we reinterpret the Liu-Takahashi-Tucsnak method, which establishes local controllability of a nonlinear system by analyzing the control cost of its linearization. We provide an easily applicable black-box formulation of their method.\n  Finally, for nonlinear ODEs, we present the tangent vectors method, which establishes local exact controllability starting from approximately reachable directions."}
{"id": "2602.18722", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18722", "abs": "https://arxiv.org/abs/2602.18722", "authors": ["Guangwei Gao", "Kaibo Hu", "Buyang Li", "Ganghui Zhang"], "title": "Finite element methods for isometric embedding of Riemannian manifolds", "comment": null, "summary": "The isometric embedding problem for Riemannian manifolds, which connects intrinsic and extrinsic geometry, is a central question in differential geometry with deep theoretical significance and wide-ranging applications. Despite extensive analytical progress, the nonlinear and degenerate nature of this problem has hindered the development of rigorous numerical analysis in this area. As the first step toward addressing this gap, we study the numerical approximation of Weyl's problem, i.e., the isometric embedding of two-dimensional Riemannian manifolds with positive Gaussian curvature into $\\mathbb{R}^3$, by establishing a new weak formulation that naturally leads to a numerical scheme well suited for high-order finite element discretization, and conducting a systematic analysis to prove the well-posedness of this weak formulation, the existence and uniqueness of its numerical solution, as well as its convergence with error estimates. This provides a foundational framework for computing isometric embeddings of Riemannian manifolds into Euclidean space, with the goal of extending it to a broader range of cases and applications in the future. Our framework also extends naturally to the isometric embedding of the Ricci flow, with rigorous error estimates, enabling the visualization of geometric evolutions in intrinsic curvature flows. Numerical experiments support the theoretical analysis by demonstrating the convergence of the method and its effectiveness in simulating isometric embeddings of given Riemannian manifolds as well as Ricci flows."}
{"id": "2602.19759", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2602.19759", "abs": "https://arxiv.org/abs/2602.19759", "authors": ["Hosein Mohammadzadeh", "Zahra Ebadi", "Omid Yahyayi Monem", "Mohammad Hossein Naghizadeh Ardabili"], "title": "Thermodynamic Geometry of Classical and Quantum Statistics in the Relativistic Regime", "comment": "9 pages, 5 figures", "summary": "We investigate the thermodynamic geometry of classical and quantum ideal gases in the relativistic regime, with particular emphasis on the effects of particle mass and spatial dimensionality. Relativistic kinematics is incorporated through the full energy-momentum dispersion relation and the corresponding relativistic density of states. Using the Fisher-Rao information metric derived from the partition function, we analyze the thermodynamic curvature for Maxwell-Boltzmann, Bose-Einstein, and Fermi-Dirac statistics. Exact analytical expressions are obtained in two spatial dimensions, while the three-dimensional case is studied numerically. We show that the thermodynamic curvature preserves its characteristic sign-positive for bosons and negative for fermions; even in the relativistic regime, reflecting effective attractive and repulsive statistical interactions, respectively. A distinctive relativistic effect is the shift of curvature singularities from the non-relativistic critical point to a mass-dependent threshold at $μ=mc^{2}$. In addition, the relativistic Bose-Einstein condensation temperature is evaluated, revealing explicit mass-dependent corrections to the non-relativistic result. These findings provide a unified geometric perspective on relativistic statistical systems and clarify the interplay between quantum statistics, relativistic kinematics, and critical behavior."}
{"id": "2602.18516", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18516", "abs": "https://arxiv.org/abs/2602.18516", "authors": ["Ali Abu-Nada", "Russell Ceballos", "Lian-Ao Wu"], "title": "Detecting Initial System-Environment Correlations from a Single Observable", "comment": "The manuscript is 12 pages long and includes 5 figures. Comments are welcome", "summary": "We address the problem of detecting initial system--environment correlations when the environment is not directly accessible. Most existing approaches rely on full state tomography or multiple system preparations, which can be experimentally demanding.\n  We show that, for a known interaction, it can be sufficient to monitor a single expectation value of the system. Focusing on a qubit interacting with an environment via isotropic Heisenberg exchange, we derive exact bounds on the signal $z(t)=\\langleσ_z^S\\rangle(t)$ that hold for all factorized initial states. These bounds define a \\emph{factorized envelope}: if an observed trajectory exits this envelope at any time, initial system--environment correlations are certified.\n  From a reduced-dynamics perspective, the envelope admits a clear operational interpretation as the admissible region generated by the standard product assignment (embedding) map, which serves as a null model for uncorrelated preparations. Envelope violations therefore rule out the entire product-assignment class using only a single calibrated observable.\n  We illustrate the method using three families of correlated initial states and observe clear envelope violations, including cases in which the reduced system state is maximally mixed. We further show that the same single-observable logic extends to an exactly solvable pure-dephasing spin--boson model with an infinite environment, where factorized initial states generate a simple coherence envelope whose violation certifies initial correlations. Overall, our results demonstrate that single-axis measurements, combined with a one-time calibration of $ρ_S(0)$, can certify initial system--environment correlations without tomography or environment access."}
{"id": "2602.19873", "categories": ["cs.CE", "astro-ph.IM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.19873", "abs": "https://arxiv.org/abs/2602.19873", "authors": ["Felix Thaler", "Sebastian Keller"], "title": "GPU-Native Compressed Neighbor Lists with a Space-Filling-Curve Data Layout", "comment": "Accepted at IPDPS 2026", "summary": "We have developed a compressed neighbor list for short-range particle-particle interaction based on a space- filling curve (SFC) memory layout and particle clusters. The neighbor list can be constructed efficiently on GPUs, supporting NVIDIA and AMD hardware, and has a memory footprint of only 4 bytes per particle to store approximately 200 neighbors. Compared to the highly-optimized domain-specific neighbor list implementation of GROMACS, a molecular dynamics code, it has a comparable cluster overhead and delivers similar performance in a neighborhood pass. Thanks to the SFC-based data layout and the support for varying interaction radii per particle, our neighbor list performs well for systems with high density contrasts, such as those encountered in many astrophysical and cosmological applications. Due to the close relation between SFCs and octrees, our neighbor list seamlessly integrates with octree-based domain decomposition and multipole-based methods for long-range gravitational or electrostatic interactions. To demonstrate the coupling between long- and short-range forces, we simulate an Evrard collapse, a standard test case for the coupling between hydrodynamical and gravitational forces, on up to 1024 GPUs, and compare our results to the analytical solution."}
{"id": "2602.18958", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18958", "abs": "https://arxiv.org/abs/2602.18958", "authors": ["Seok-Jin Kim"], "title": "Optimal and Structure-Adaptive CATE Estimation with Kernel Ridge Regression", "comment": null, "summary": "We propose an optimal algorithm for estimating conditional average treatment effects (CATEs) when response functions lie in a reproducing kernel Hilbert space (RKHS). We study settings in which the contrast function is structurally simpler than the nuisance functions: (i) it lies in a lower-complexity RKHS with faster eigenvalue decay, (ii) it satisfies a source condition relative to the nuisance kernel, or (iii) it depends on a known low-dimensional covariate representation. We develop a unified two-stage kernel ridge regression (KRR) method that attains minimax rates governed by the complexity of the contrast function rather than the nuisance class, in terms of both sample size and overlap. We also show that a simple model-selection step over candidate contrast spaces and regularization levels yields an oracle inequality, enabling adaptation to unknown CATE regularity."}
{"id": "2602.19386", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19386", "abs": "https://arxiv.org/abs/2602.19386", "authors": ["Mohamadamin Rajabinezhad", "Muratkhan Abdirash", "Xiaofan Cui", "Shan Zuo"], "title": "Decentralized Attack-Resilient CLF-Based Control of Nonlinear DC Microgrids under FDI Attacks", "comment": "Accepted for presentation at IEEE PES General Meeting 2026. \\c{opyright} IEEE. Personal use permitted. Final version will appear in IEEE Xplore", "summary": "The growing deployment of nonlinear, converter interfaced distributed energy resources (DERs) in DC microgrids demands decentralized controllers that remain stable and resilient under a wide range of cyber-physical attacks and disturbances. Traditional droop or linearized control methods lack resilience and scalability, especially when the system operates in its nonlinear region or faces diverse false-data-injection (FDI) attacks on control inputs. In this work, we develop a Decentralized Attack-Resilient Control Lyapunov Function (AR-CLF) based Quadratic Program (QP) control framework for nonlinear DC microgrids that ensures large-signal stability in a fully decentralized manner. Built upon the port-Hamiltonian representation, the proposed controller dynamically compensates diverse attacks including exponentially unbounded control-input perturbations beyond the bounded-attack regime commonly assumed in existing methods, through an adaptive resilience term, without requiring global information. Simulations validate that the AR-CLF based QP controller achieves superior stability and resilience against unbounded attacks, paving the way for scalable, attack-resilient, and physically consistent control of next-generation DC microgrids."}
{"id": "2602.18958", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18958", "abs": "https://arxiv.org/abs/2602.18958", "authors": ["Seok-Jin Kim"], "title": "Optimal and Structure-Adaptive CATE Estimation with Kernel Ridge Regression", "comment": null, "summary": "We propose an optimal algorithm for estimating conditional average treatment effects (CATEs) when response functions lie in a reproducing kernel Hilbert space (RKHS). We study settings in which the contrast function is structurally simpler than the nuisance functions: (i) it lies in a lower-complexity RKHS with faster eigenvalue decay, (ii) it satisfies a source condition relative to the nuisance kernel, or (iii) it depends on a known low-dimensional covariate representation. We develop a unified two-stage kernel ridge regression (KRR) method that attains minimax rates governed by the complexity of the contrast function rather than the nuisance class, in terms of both sample size and overlap. We also show that a simple model-selection step over candidate contrast spaces and regularization levels yields an oracle inequality, enabling adaptation to unknown CATE regularity."}
{"id": "2602.19386", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19386", "abs": "https://arxiv.org/abs/2602.19386", "authors": ["Mohamadamin Rajabinezhad", "Muratkhan Abdirash", "Xiaofan Cui", "Shan Zuo"], "title": "Decentralized Attack-Resilient CLF-Based Control of Nonlinear DC Microgrids under FDI Attacks", "comment": "Accepted for presentation at IEEE PES General Meeting 2026. \\c{opyright} IEEE. Personal use permitted. Final version will appear in IEEE Xplore", "summary": "The growing deployment of nonlinear, converter interfaced distributed energy resources (DERs) in DC microgrids demands decentralized controllers that remain stable and resilient under a wide range of cyber-physical attacks and disturbances. Traditional droop or linearized control methods lack resilience and scalability, especially when the system operates in its nonlinear region or faces diverse false-data-injection (FDI) attacks on control inputs. In this work, we develop a Decentralized Attack-Resilient Control Lyapunov Function (AR-CLF) based Quadratic Program (QP) control framework for nonlinear DC microgrids that ensures large-signal stability in a fully decentralized manner. Built upon the port-Hamiltonian representation, the proposed controller dynamically compensates diverse attacks including exponentially unbounded control-input perturbations beyond the bounded-attack regime commonly assumed in existing methods, through an adaptive resilience term, without requiring global information. Simulations validate that the AR-CLF based QP controller achieves superior stability and resilience against unbounded attacks, paving the way for scalable, attack-resilient, and physically consistent control of next-generation DC microgrids."}
{"id": "2602.19795", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19795", "abs": "https://arxiv.org/abs/2602.19795", "authors": ["Felix Möckel", "Harald Schmid", "Felix von Oppen"], "title": "Floquet product mode and eigenphase order", "comment": "12 pages, 9 figures", "summary": "We study the robustness of the Floquet quantum Ising model against integrability-breaking perturbations, focusing on the phase hosting both Majorana zero and $π$ modes. A recent work [Phys. Rev. B 110, 075117, (2024)] observed that the Floquet product mode, a composite edge mode constructed from both Majorana operators, is considerably more robust than the individual Majorana edge modes. We analyze these strong modes from the point of view of the eigenphase order present in finite chains with open boundary conditions. As a result of the Majorana modes, all Floquet eigenstates come in quadruplets in the integrable limit. We show that the robustness of the various modes as well as the behavior of the boundary spin correlation functions can be understood in terms of the spectral statistics of these quadruplets in the presence of integrability-breaking perturbations."}
{"id": "2602.19277", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.19277", "abs": "https://arxiv.org/abs/2602.19277", "authors": ["Dongnuan Tian", "Rob Shone"], "title": "Dynamic Repair and Maintenance of Heterogeneous Machines Dispersed on a Network: A Rollout Method for Online Reinforcement Learning", "comment": null, "summary": "We consider a problem in which a single repairer is responsible for the maintenance and repair of a collection of machines, positioned at different locations on a network of nodes and edges. Machines deteriorate according to stochastic processes and incur increasing costs as they approach complete failure. The times needed for repairs to be performed, and the amounts of time needed for the repairer to switch between different machines, are random and machine-dependent. The problem is formulated as a Markov decision process (MDP) in which the objective is to minimize long-run average costs. We prove the equivalence of an alternative formulation based on rewards and use this to develop an index heuristic policy, which is shown to be optimal in certain special cases. We then use rollout-based reinforcement learning techniques to develop a novel online policy improvement (OPI) approach, which uses the index heuristic as a base policy and also as an insurance option at decision epochs where the best action cannot be selected with sufficient confidence. Results from extensive numerical experiments, involving randomly-generated network layouts and parameter values, show that the OPI heuristic is able to achieve close-to-optimal performance in fast-changing systems with state transitions occurring 100 times per second, suggesting that it is suitable for online implementation."}
{"id": "2602.18937", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18937", "abs": "https://arxiv.org/abs/2602.18937", "authors": ["Peter Benner", "Heike Faßbender", "Michel-Niklas Senn"], "title": "Structure-preserving Krylov Subspace Approximations for the Matrix Exponential of Hamiltonian Matrices: A Comparative Study", "comment": null, "summary": "We study structure-preserving Krylov subspace methods for approximating the matrix-vector products f(H)b, where H is a large Hamiltonian matrix and f denotes either the matrix exponential or the related phi-function. Such computations are central to exponential integrators for Hamiltonian systems. Standard Krylov methods generally destroy the Hamiltonian structure under projection, motivating the use of Krylov bases with J-orthogonal columns that yield Hamiltonian projected matrices and symplectic reduced exponentials. We compare several such structure-preserving Krylov methods on representative Hamiltonian test problems, focusing on accuracy, efficiency, and structure preservation, and briefly discuss adaptive strategies for selecting the Krylov subspace dimension."}
{"id": "2602.19865", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19865", "abs": "https://arxiv.org/abs/2602.19865", "authors": ["R. Jafari", "Alireza Akbari"], "title": "Separation of the Kibble-Zurek Mechanism from Quantum Criticality", "comment": null, "summary": "When a system is swept through a quantum critical point (QCP), the Kibble-Zurek mechanism predicts that the average number of topological defects follows a universal power-law scaling with the ramp time scale. This scaling behavior is determined by the equilibrium critical exponents of the underlying phase transition. We show that the correspondence between Kibble-Zurek scaling and quantum criticality does not hold generally. In particular, the defect density can exhibit a suppression faster than the Kibble-Zurek prediction even when the quench crosses a critical point, while conventional Kibble-Zurek scaling may persist for quenches through a non-critical point. Our results, based on models representative of a broad class of quasi-one-dimensional Fermi systems, identify the dynamical conditions under which universal defect scaling emerges and clarify the relation between defect generation and equilibrium criticality."}
{"id": "2602.18517", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18517", "abs": "https://arxiv.org/abs/2602.18517", "authors": ["Arya D. Keni", "Christian M. Lange", "Adhyyan S. Mansukhani", "Emma Daggett", "Ankit Kundu", "Ishita Agarwal", "Patrick Bak", "Benjamin Cerjan", "Jonathan D. Hood"], "title": "Vapor Phase Assembly of Molecular Emitter Crystals for Photonic Integrated Circuits", "comment": "15 pages (7 page main, 7 page supplemental, 1 page references). 5 main figures, 9 supplemental figures", "summary": "Organic molecules embedded in an organic matrix exhibit lifetime-limited optical coherence and bright emission at cryogenic temperatures below 3 K. Here we present a simple vapor-phase growth method for synthesizing optically thin DBT-doped anthracene crystals that are compatible with integrated nanophotonics. The crystals are ~200 nm thick with sub-nm surface roughness and a tunable lateral dimension of up to 200 $μ$m. The molecular transitions remain narrow and spectrally stable, with inhomogeneous broadening below 100 GHz, comparable to DBT in bulk anthracene. The dopant density is tunable up to several hundred molecules per $μ$m$^2$, ensuring emitters within the near-field of nanophotonic structures. We demonstrate that the crystals can be micropositioned onto integrated photonic devices with the molecular dipole aligned to the optical mode. This approach opens a path toward on-chip single-photon sources and collective many-emitter effects."}
{"id": "2602.18441", "categories": ["physics.comp-ph", "cs.CE", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18441", "abs": "https://arxiv.org/abs/2602.18441", "authors": ["Dimitrios Voulanas", "Eduardo Gildin"], "title": "Updating DMD Operators for Changes in Domain Properties", "comment": null, "summary": "Fast and reliable surrogate models are critical for optimization, control and uncertainty analysis in geological carbon-storage projects, yet high-fidelity multiphase simulators remain too expensive. Dynamic Mode Decomposition (DMD) offers an attractive data-driven reduction framework, but its operators are trained for a single set of reservoir properties. When permeability or well location changes, conventional practice is to regenerate snapshots and retrain the surrogate, erasing most of the speed advantage. This work presents a lightweight alternative that updates an existing DMD or DMD-with-control model without incorporating new simulation data or retraining. Two complementary update strategies are introduced. For cases where permeability changes uniformly across the domain, the proposed updates adjust the models internal dynamics and control response to match the new flow timescale. When permeability varies in space, the approach modifies the spatial representation so that high-permeability zones are given greater influence on the models reduced basis. Numerical experiments demonstrate that the proposed updates recover plume migration and pressure build-up within three percent of a freshly trained surrogate yet execute hundreds of times faster than full retraining. These methods therefore enable real-time optimization and rapid what-if studies while preserving the physical fidelity demanded by carbon-storage workflows."}
{"id": "2602.19290", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.19290", "abs": "https://arxiv.org/abs/2602.19290", "authors": ["Kyle Schindl", "Larry Wasserman"], "title": "Distributional Discontinuity Design", "comment": null, "summary": "Regression discontinuity and kink designs are typically analyzed through mean effects, even when treatment changes the shape of the entire outcome distribution. To address this, we introduce distributional discontinuity designs, a framework for estimating causal effects for a scalar outcome at the boundary of a discontinuity in treatment assignment. Our estimand is the Wasserstein distance between limiting conditional outcome distributions; a single scale-interpretable measure of distribution shift. We show that this weakly bounds the average treatment effect, where equality holds if and only if the treatment effect is purely additive; thus, departure from equality measures effect heterogeneity. To further encode effect heterogeneity we show that the Wasserstein distance admits an orthogonal decomposition into squared differences in $L$-moments, thereby quantifying the contribution from location, scale, skewness, and higher-order shape components to the overall distributional distance. Next, we extend this framework to distributional kink designs by evaluating the Wasserstein derivative at a policy kink; this describes the flow of probability mass through the kink. In the case of fuzzy kink designs, we derive new identification results. Finally, we apply our methods on real data by re-analyzing two natural experiments to compare our distributional effects to traditional causal estimands."}
{"id": "2602.19421", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19421", "abs": "https://arxiv.org/abs/2602.19421", "authors": ["Tomonari Kanazawa", "Hikaru Hoshino", "Eiko Furutani"], "title": "A Reinforcement Learning-based Transmission Expansion Framework Considering Strategic Bidding in Electricity Markets", "comment": null, "summary": "Transmission expansion planning in electricity markets is tightly coupled with the strategic bidding behaviors of generation companies. This paper proposes a Reinforcement Learning (RL)-based co-optimization framework that simultaneously learns transmission investment decisions and generator bidding strategies within a unified training process. Based on a multiagent RL framework for market simulation, the proposed method newly introduces a design policy layer that jointly optimizes continuous/discrete transmission expansion decisions together with strategic bidding policies. Through iterative interaction between market clearing and investment design, the framework effectively captures their mutual influence and achieves consistent co-optimization of expansion and bidding decisions. Case studies on the IEEE 30-bus system are provided for proof-of-concept validation of the proposed co-optimization framework."}
{"id": "2602.18988", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.18988", "abs": "https://arxiv.org/abs/2602.18988", "authors": ["Niloofar Ramezani", "Lori P. Selby", "Pascal Nitiema", "Jeffrey R. Wilson"], "title": "Latent Moment Models for Recurrent Binary Outcomes: A Bayesian and Quasi-Distributional Approach", "comment": "16 pages, 1 figure, 4 tables, 1 Supplementary Table", "summary": "Recurrent binary outcomes within individuals, such as hospital readmissions, often reflect latent risk processes that evolve over time. Conventional methods like generalized linear mixed models and generalized estimating equations estimate average risk but fail to capture temporal changes in variability, asymmetry, and tail behavior. We introduce two statistical frameworks that model each binary event as the outcome of a thresholded value drawn from a time-varying latent distribution defined by its location, scale, skewness, and kurtosis. Rather than treating these four quantities as nonparametric moment estimators, we model them as interpretable latent moments within a flexible latent distributional family. The first, BLaS-Recurrent, is a Bayesian model using the sinh-arcsinh distribution (a parametric family that provides explicit control over asymmetry and tail weight) to estimate latent moment trajectories; the second, QuaD-Recurrent, is a quasi-distributional approach that maps simulated moment vectors to event probabilities using a flexible nonparametric surface. Both models support time-dependent covariates, serial correlation, and multiple membership structures. Simulation studies show improved calibration, interpretability, and robustness over standard models. Applied to ICU readmission data from the MIMIC-IV database, both approaches uncover clinically meaningful patterns in latent risk, such as right-skewed escalation and widening dispersion, that are missed by traditional methods. These models provide interpretable, distribution-sensitive tools for longitudinal binary outcomes in healthcare while explicitly acknowledging that latent \"moments\" summarize but do not uniquely determine the underlying distribution."}
{"id": "2602.19421", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19421", "abs": "https://arxiv.org/abs/2602.19421", "authors": ["Tomonari Kanazawa", "Hikaru Hoshino", "Eiko Furutani"], "title": "A Reinforcement Learning-based Transmission Expansion Framework Considering Strategic Bidding in Electricity Markets", "comment": null, "summary": "Transmission expansion planning in electricity markets is tightly coupled with the strategic bidding behaviors of generation companies. This paper proposes a Reinforcement Learning (RL)-based co-optimization framework that simultaneously learns transmission investment decisions and generator bidding strategies within a unified training process. Based on a multiagent RL framework for market simulation, the proposed method newly introduces a design policy layer that jointly optimizes continuous/discrete transmission expansion decisions together with strategic bidding policies. Through iterative interaction between market clearing and investment design, the framework effectively captures their mutual influence and achieves consistent co-optimization of expansion and bidding decisions. Case studies on the IEEE 30-bus system are provided for proof-of-concept validation of the proposed co-optimization framework."}
{"id": "2602.19935", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19935", "abs": "https://arxiv.org/abs/2602.19935", "authors": ["Travis J. Williams", "Douglas L. Abernathy", "Mark D. Lumsden", "Jiaqiang Yan", "Andrew D. Christianson"], "title": "Anisotropic magnons in a layered honeycomb ferromagnet", "comment": null, "summary": "Recent experimental and theoretical studies have suggested a possible Dirac magnon gap in the two-dimensional ferromagnetic semiconductor CrSiTe$_3$. Detailed neutron scattering measurements were performed to shed light on the existence of the magnon gap, and suggest that the gap is very small or non-existent, with previous measurements being complicated by experimental factors. During these measurements, it was found that the out-of-plane couplings could explain the usual property of the increase in the magnetic transition temperature when CrSiTe$_3$ is exfoliated to monolayers. Furthermore, the material was shown to have anisotropic magnons along the out-of-plane direction, through the proposed Dirac point. We speculate that this is due to an exchange anisotropy, though Kitaev-like interactions alone cannot explain the spectra."}
{"id": "2602.19325", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.19325", "abs": "https://arxiv.org/abs/2602.19325", "authors": ["Zhuoyu Xiao"], "title": "Smoothing-Enabled Randomized Stochastic Gradient Schemes for Solving Nonconvex Nonsmooth Potential Games under Uncertainty", "comment": "33 pages, 3 figures", "summary": "The state of the art in solving nonconvex nonsmooth games under uncertainty remains in its infancy. Existing studies primarily rely on stringent growth conditions or local convexity-like properties, making the development of alternative algorithms desirable. In this work, we study a class of stochastic $N$-player noncooperative games characterized by a potential function. We first consider the nonconvex smooth setting and develop a randomized stochastic gradient (RSG) scheme. The RSG scheme achieves the optimal sample complexity of $\\mathcal{O}(N^{2}ε^{-4})$ for reaching a point whose expected residual has norm at most $ε$. Building on this result, we introduce a randomized smoothed RSG (RS-RSG) scheme for solving stochastic potential games afflicted by nonconvexity and nonsmoothness. We show that RS-RSG asymptotically converges to an equilibrium of the smoothed game with sample complexity $\\mathcal{O}(L^{4}_{\\max}n^{3/2}_{\\max}N^{3}η^{-1}ε^{-4})$, where $η>0$ is the smoothing parameter. Under Lipschitz continuity of the Clarke subdifferentials, we show that the expected residual evaluated at the smoothed equilibrium is $\\mathcal{O}(η^{2})$. In addition, we discuss the biased RSG and RS-RSG variants and demonstrate the effectiveness of the biased RS-RSG scheme on a class of stochastic potential hierarchical games where the exact lower-level solution is unavailable in finite time. Collectively, our results provide a new pathway that goes beyond classical conditions for solving stochastic nonconvex nonsmooth games. Some preliminary numerics are also provided."}
{"id": "2602.18950", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.18950", "abs": "https://arxiv.org/abs/2602.18950", "authors": ["Johannes Maly", "Korbinian Neuner", "Samarth Vadia"], "title": "Computing the SVD efficiently with photonic chips", "comment": null, "summary": "In light of today's massive data processing, digital computers are reaching fundamental performance limits due to physical limitations and energy consumption. For specific applications, tailored analog systems offer promising alternatives to digital processors. In this work, we investigate the potential of linear photonic chips for accelerating the computation of the singular value decomposition (SVD) of a matrix. The SVD is a key primitive in linear algebra and forms a crucial component of various modern data processing algorithms. Our main insights are twofold: first, hybrid systems of digital controller and photonic chip asymptotically perform on par with large-scale CPU/GPU systems in terms of runtime. Second, such hybrid systems clearly outperform digital systems in terms of energy consumption."}
{"id": "2602.18482", "categories": ["physics.comp-ph", "cond-mat.stat-mech", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18482", "abs": "https://arxiv.org/abs/2602.18482", "authors": ["Emil Hoffmann", "Maximilian Schebek", "Leon Klein", "Frank Noé", "Jutta Rogal"], "title": "Boltzmann Generators for Condensed Matter via Riemannian Flow Matching", "comment": null, "summary": "Sampling equilibrium distributions is fundamental to statistical mechanics. While flow matching has emerged as scalable state-of-the-art paradigm for generative modeling, its potential for equilibrium sampling in condensed-phase systems remains largely unexplored. We address this by incorporating the periodicity inherent to these systems into continuous normalizing flows using Riemannian flow matching. The high computational cost of exact density estimation intrinsic to continuous normalizing flows is mitigated by using Hutchinson's trace estimator, utilizing a crucial bias-correction step based on cumulant expansion to render the stochastic estimates suitable for rigorous thermodynamic reweighting. Our approach is validated on monatomic ice, demonstrating the ability to train on systems of unprecedented size and obtain highly accurate free energy estimates without the need for traditional multistage estimators."}
{"id": "2602.18524", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18524", "abs": "https://arxiv.org/abs/2602.18524", "authors": ["Salman Sajad Wani", "Saif Al-Kuwari", "Arshid Shabir", "Paolo Vezio", "Francesco Marino", "Mir Faizal"], "title": "Time uncertainty and fundamental sensitivity limits in quantum sensing: application to optomechanical gravimetry", "comment": "26 pages , 2 figures", "summary": "High-sensitivity accelerometers and gravimeters, achieving the ultimate limits of measurement sensitivity are key tools for advancing both fundamental and applied physics. While numerous platforms have been proposed to achieve this goal, from atom interferometers to optomechanical systems, all of these studies neglect the effects of intrinsic quantum uncertainty in time estimation. Starting from the Hamiltonian of a generic linear quantum sensor, we derive the two-parameter quantum Fisher information matrix and establish the corresponding Cram'er-Rao bound, treating time as an uncertain (nuisance) parameter. Our analysis reveals a fundamental coupling between time and signal estimation that inherently degrades measurement sensitivity, with the standard single-parameter quantum limit recovered only at specific interrogation times or under special decoupling conditions. We then apply these results to an optomechanical gravimeter and explicitly derive an optimal decoupling condition under which the effects of time uncertainty are averaged out in a continuous measurement scheme. Our approach is general and can be readily extended to a broad class of quantum sensors."}
{"id": "2602.19462", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.19462", "abs": "https://arxiv.org/abs/2602.19462", "authors": ["Jinyuan Chang", "Yi Ding", "Zhentao Shi", "Bo Zhang"], "title": "Zero Variance Portfolio", "comment": null, "summary": "When the number of assets is larger than the sample size, the minimum variance portfolio interpolates the training data, delivering pathological zero in-sample variance. We show that if the weights of the zero variance portfolio are learned by a novel ``Ridgelet'' estimator, in a new test data this portfolio enjoys out-of-sample generalizability. It exhibits the double descent phenomenon and can achieve optimal risk in the overparametrized regime when the number of assets dominates the sample size. In contrast, a ``Ridgeless'' estimator which invokes the pseudoinverse fails in-sample interpolation and diverges away from out-of-sample optimality. Extensive simulations and empirical studies demonstrate that the Ridgelet method performs competitively in high-dimensional portfolio optimization."}
{"id": "2602.19428", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19428", "abs": "https://arxiv.org/abs/2602.19428", "authors": ["Taiyo Mantani", "Hikaru Hoshino", "Tomonari Kanazawa", "Eiko Furutani"], "title": "Sizing of Battery Considering Renewable Energy Bidding Strategy with Reinforcement Learning", "comment": null, "summary": "This paper proposes a novel computationally efficient algorithm for optimal sizing of Battery Energy Storage Systems (BESS) considering renewable energy bidding strategies. Unlike existing two-stage methods, our algorithm enables the cooptimization of both by updating the BESS size during the training of the bidding policy, leveraging an extended reinforcement learning (RL) framework inspired by advancements in embodied cognition. By integrating the Deep Recurrent Q-Network (DRQN) with a distributed RL framework, the proposed algorithm effectively manages uncertainties in renewable generation and market prices while enabling parallel computation for efficiently handling long-term data."}
{"id": "2602.19012", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.19012", "abs": "https://arxiv.org/abs/2602.19012", "authors": ["Robert Amevor", "Emmanuel Kubuafor", "Dennis Baidoo"], "title": "Adaptive Weighting for Time-to-Event Continual Reassessment Method: Improving Safety in Phase I Dose-Finding Through Data-Driven Delay Distribution Estimation", "comment": null, "summary": "Background: Phase I dose-finding trials increasingly encounter delayed-onset toxicities, especially with immunotherapies and targeted agents. The time-to-event continual reassessment method (TITE-CRM) handles incomplete follow-up using fixed linear weights, but this ad hoc approach doesn't reflect actual delay patterns and may expose patients to excessive risk during dose escalation.\n  Methods: We replace TITE-CRM's fixed weights with adaptive weights, posterior predictive probabilities derived from the evolving toxicity delay distribution. Under a Weibull timing model, we get closed-form weight updates through maximum likelihood estimation, making real-time implementation straightforward. We tested our method (AW-TITE) against TITE-CRM and standard designs (3+3, mTPI, BOIN) across three dose-toxicity scenarios through simulation (N = 30 patients, 2,000 replications). We also examined robustness across varying accrual rates, sample sizes, shape parameters, observation windows, and priors.\n  Results: Our AW-TITE reduced patient overdosing by 40.6% compared to TITE-CRM (mean fraction above MTD: 0.202 vs 0.340; 95% CI: -0.210 to -0.067, p < 0.001) while maintaining comparable MTD selection accuracy (mean difference: +0.023, p = 0.21). Against algorithm-based methods, AW-TITE achieved higher MTD identification: +32.6% vs mTPI, +19.8% vs 3+3, and +5.6% vs BOIN. Performance remained robust across all sensitivity analyses.\n  Conclusions: Adaptive weighting offers a practical way to improve Phase I trial safety while preserving MTD selection accuracy. The method requires minimal computation and is ready for real-time use."}
{"id": "2602.19428", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19428", "abs": "https://arxiv.org/abs/2602.19428", "authors": ["Taiyo Mantani", "Hikaru Hoshino", "Tomonari Kanazawa", "Eiko Furutani"], "title": "Sizing of Battery Considering Renewable Energy Bidding Strategy with Reinforcement Learning", "comment": null, "summary": "This paper proposes a novel computationally efficient algorithm for optimal sizing of Battery Energy Storage Systems (BESS) considering renewable energy bidding strategies. Unlike existing two-stage methods, our algorithm enables the cooptimization of both by updating the BESS size during the training of the bidding policy, leveraging an extended reinforcement learning (RL) framework inspired by advancements in embodied cognition. By integrating the Deep Recurrent Q-Network (DRQN) with a distributed RL framework, the proposed algorithm effectively manages uncertainties in renewable generation and market prices while enabling parallel computation for efficiently handling long-term data."}
{"id": "2602.19972", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.19972", "abs": "https://arxiv.org/abs/2602.19972", "authors": ["Shashikant Singh Kunwar", "Madhuparna Karmakar"], "title": "Non Fermi liquid signatures across strain engineered metal-insulator transition in line-graph lattices", "comment": "13 pages, 11 figures", "summary": "Controlling the properties and thus the functionalities of correlated electron systems via externally tunable perturbations has always remained a cherished goal in quantum condensed matter physics. Recently, straintronics has proved to be one such external control which can dictate the quantum phases and transitions in materials via the reconstruction of their electronic band structure. A particularly intriguing scenario arises in the context of flat band line-graph lattices wherein straintronics is found to bring forth non trivial phase transitions. This paper reports the phase transitions and thermal scales across the Lieb/Kagome interconversion in the electronic interaction-strain-temperature space. Based on the thermodynamic, spectroscopic and transport signatures across the strain tuned interconversion of these line-graph lattices we have mapped out the low temperature phases and thermal transition scales, numerically determined using non perturbative calculations. While at the low temperatures, interaction-strain plane is spanned by magnetically correlated insulators, flat band induced weak transiently localized insulators and non Fermi liquid metallic phases, thermal fluctuations aid in to stabilize coexistent magnetic correlations. Apart from quantifying the magnetic transition scales in this system our results on the spectroscopic and transport signatures distill the strain tuned metal-insulator transition and crossover scales which exhibit variable transport scaling exponents."}
{"id": "2602.19334", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.19334", "abs": "https://arxiv.org/abs/2602.19334", "authors": ["Alexander Zuyev", "Victoria Grushkovskaya", "Sebastian Eisner"], "title": "Implementation of Time-Varying Controllers for a Nonholonomic Mobile Robot: Experimental Studies", "comment": "12 pages, 4 figures", "summary": "We consider a kinematic model of a wheeled mobile robot controlled by translational and angular velocities. For this class of nonholonomic systems, a family of time-varying feedback controllers was proposed in our previous works using gradient flow approximation techniques. In the present study, these controllers are implemented on a TurtleBot3 Burger (TB3) mobile robot to provide experimental validation of the stabilization problem with oscillating input signals. In addition, the admissibility problem of a gradient flow is investigated to justify the construction of a Lyapunov function candidate. The presented experimental results demonstrate the possibility of stabilizing the reference position of the robot using feedback controls with practically acceptable parameters."}
{"id": "2602.19046", "categories": ["math.NA", "math.AP", "math.SP"], "pdf": "https://arxiv.org/pdf/2602.19046", "abs": "https://arxiv.org/abs/2602.19046", "authors": ["Yvonne Alama Bronsard", "Thierry Laurens"], "title": "On the convergence of explicit formulas for $L^2$ solutions to the Benjamin-Ono and continuum Calogero-Moser equations", "comment": null, "summary": "By developing discrete counterparts to recent advances in nonlinear integrability, and in particular to the discovery of explicit formulas, we design and analyze fully-discrete approximations to the Benjamin-Ono (BO) and continuum Calogero-Moser (CCM) equations on the torus. We build on the key observation that discretizing such explicit formulas yields schemes that are exact in time (requiring only spatial discretization) and have a computational cost independent of the final time $T$. In this work, we first generalize the fully-discrete schemes of arXiv:2412.13480 to include numerical approximations with better structure preservation properties, including the conservation of mass and momentum in the case of the (BO) equation. Secondly, building on recent analyses of the corresponding Lax operators, we extend the convergence results to this class of schemes for rough solutions $u(t)$ merely belonging to $L^2(\\mathbb{T})$ for (BO) and $L^2_{+}(\\mathbb{T})$ for (CCM), the latter of which is precisely the scaling-critical regularity. Our main theorem states that the $L^2(\\mathbb{T})$-norm of the error goes to zero as the truncation parameters go to infinity, uniformly on any bounded time interval $[-T,T]$. As an example, we apply our scheme to the (BO) equation with a square-wave initial profile, and obtain the first numerical evidence of the Talbot effect for (BO) supported by a rigorous convergence result."}
{"id": "2602.19280", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.19280", "abs": "https://arxiv.org/abs/2602.19280", "authors": ["Devanshu Shekhar", "Pragya Shukla"], "title": "Entanglement dynamics of many-body quantum states: sensitivity to system conditions and a hidden universality", "comment": "36 pages (double spacing), 8 figures. arXiv admin note: text overlap with arXiv:2503.01989", "summary": "We consider physical Hamiltonians that can be represented by the multiparametric Gaussian ensembles, theoretically derive the state ensembles for its eigenstates and analyze the effect of varying system conditions on its bipartite entanglement entropy. Our approach leads to a single parametric based common mathematical formulation for the evolution of the entanglement statistics of different states of a given Hamiltonian or different Hamiltonians subjected to same symmetry constraints. The parameter turns out to be a single functional of the system parameters and thereby reveals a deep web of connection hidden underneath different quantum states."}
{"id": "2602.18555", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18555", "abs": "https://arxiv.org/abs/2602.18555", "authors": ["Alexandra A. Geim", "Nazli Ugur Koyluoglu", "Simon J. Evered", "Rahul Sahay", "Sophie H. Li", "Muqing Xu", "Dolev Bluvstein", "Nik O. Gjonbalaj", "Nishad Maskara", "Marcin Kalinowski", "Tom Manovitz", "Ruben Verresen", "Susanne F. Yelin", "Johannes Feldmeier", "Markus Greiner", "Vladan Vuletic", "Mikhail D. Lukin"], "title": "Engineering quantum criticality and dynamics on an analog-digital simulator", "comment": "11 pages, 6 figures. Methods: 21 pages, 10 figures", "summary": "Understanding emergent phenomena in out-of-equilibrium interacting many-body systems is an exciting frontier in physical science. While quantum simulators represent a promising approach to this long-standing problem, in practice it can be challenging to directly realize the required interactions, measure arbitrary observables, and mitigate errors. Here we use coherent mapping between the Rydberg and hyperfine qubits in a neutral atom array simulator to engineer and probe complex quantum dynamics. We combine efficient analog dynamics with fully programmable state preparation and measurement, leverage non-destructive readout for loss information and atomic qubit reuse, and use an atom reservoir for replacing lost atoms. With this analog-digital approach, we first demonstrate dynamical engineering of ring-exchange and particle hopping dynamics via Floquet driving and measure the spectral function of single excitations by evolving initial superposition states. Extending these techniques to a 271-site kagome lattice, we employ closed-loop optimization to target an out-of-equilibrium critical quantum spin liquid of the Rokhsar-Kivelson type. We observe the key features of such a state, including the absence of local order, many-body coherences between nearly equal-amplitude dimer configurations over up to 18 sites, and universal correlations consistent with predictions from field theory. Together, these results pave the way for using dynamical control in analog-digital quantum simulators to study complex quantum many-body systems."}
{"id": "2602.19473", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.19473", "abs": "https://arxiv.org/abs/2602.19473", "authors": ["Zhaoxi Zhang", "Vanda Inacio", "Sara Wade"], "title": "The generalized underlap coefficient with an application in clustering", "comment": null, "summary": "Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables. We establish key properties of UNL and provide an explicit connection to the total variation. We further interpret the UNL as a dependence measure between a group label and variables of interest and compare it with mutual information. We propose an importance sampling estimator of the UNL that can be combined with flexible density estimators. The utility of the UNL for assessing partition-covariate dependence in clustering is highlighted in detail, where it is particularly useful for evaluating the single-weights assumption in covariate-dependent mixture models. Finally we illustrate the application of the UNL in clustering using two real world datasets."}
{"id": "2602.19486", "categories": ["eess.SY", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19486", "abs": "https://arxiv.org/abs/2602.19486", "authors": ["Xinyi Yi", "Ioannis Lestas"], "title": "A mixed Hinfty-Passivity approach for Leveraging District Heating Systems as Frequency Ancillary Service in Electric Power Systems", "comment": null, "summary": "This paper introduces a mixed H-infinity-passivity framework that enables district heating systems (DHSs) with heat pumps to support electric-grid frequency regulation. The analysis illustrates how the DHS regulator influences coupled electro-thermal frequency dynamics and provides LMI conditions for efficient controller design. We also present a disturbance-independent temperature regulator that ensures stability and robustness against heat-demand uncertainty. Simulations demonstrate improved frequency-control dynamics in the electrical power grid while maintaining good thermal performance in the DHS."}
{"id": "2602.19129", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19129", "abs": "https://arxiv.org/abs/2602.19129", "authors": ["Zhaozhe Liu", "Gongjun Xu", "Haoran Zhang"], "title": "Estimation and Statistical Inference for Generalized Multilayer Latent Space Model", "comment": null, "summary": "Multilayer networks have become increasingly ubiquitous across diverse scientific fields, ranging from social sciences and biology to economics and international relations. Despite their broad applications, the inferential theory for multilayer networks remains underdeveloped. In this paper, we propose a flexible latent space model for multilayer directed networks with various edge types, where each node is assigned with two latent positions capturing sending and receiving behaviors, and each layer has a connection matrix governing the layer-specific structure. Through nonlinear link functions, the proposed model represents the structure of a multilayer network as a tensor, which admits a Tucker low-rank decomposition. This formulation poses significant challenges on the estimation and statistical inference for the latent positions and connection matrices, where existing techniques are inapplicable. To tackle this issue, a novel unfolding and fusion method is developed to facilitate estimation. We establish both consistency and asymptotic normality for the estimated latent positions and connection matrices, which paves the way for statistical inference tasks in multilayer network applications, such as constructing confidence regions for the latent positions and testing whether two network layers share the same structure. We validate the proposed method through extensive simulation studies and demonstrate its practical utility on real-world data."}
{"id": "2602.19486", "categories": ["eess.SY", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19486", "abs": "https://arxiv.org/abs/2602.19486", "authors": ["Xinyi Yi", "Ioannis Lestas"], "title": "A mixed Hinfty-Passivity approach for Leveraging District Heating Systems as Frequency Ancillary Service in Electric Power Systems", "comment": null, "summary": "This paper introduces a mixed H-infinity-passivity framework that enables district heating systems (DHSs) with heat pumps to support electric-grid frequency regulation. The analysis illustrates how the DHS regulator influences coupled electro-thermal frequency dynamics and provides LMI conditions for efficient controller design. We also present a disturbance-independent temperature regulator that ensures stability and robustness against heat-demand uncertainty. Simulations demonstrate improved frequency-control dynamics in the electrical power grid while maintaining good thermal performance in the DHS."}
{"id": "2602.20073", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.20073", "abs": "https://arxiv.org/abs/2602.20073", "authors": ["Robin Scholle", "Pietro M. Bonetti", "Walter Metzner", "Demetrio Vilardi"], "title": "Coexisting magnetic, charge, and superconducting orders in the two-dimensional Hubbard model", "comment": null, "summary": "We perform a renormalized mean-field study of the two-dimensional repulsive Hubbard model, focusing on the intricate interplay and possible coexistence of magnetic, charge, and superconducting orders. We improve on conventional mean-field theory by utilizing a renormalization group framework that captures high-energy fluctuations. This method generates effective magnetic and $d$-wave pairing interactions, and allows for an unbiased exploration of coexisting phases at weak and moderate interaction strengths. Unrestricted mean-field calculations of the effective Hamiltonian on large finite lattices are combined with analyses in the thermodynamic limit, revealing a rich phase diagram with extensive regions of coexisting orders. We find that $d$-wave superconductivity coexists with Néel order on the electron-doped side. On the hole-doped side, superconductivity is found to coexist with spiral or stripe magnetic orders. Within the stripe ordered region, the superconducting order parameter is spatially modulated, with a period that follows the charge modulation of the stripes. Below van Hove filling, pairing provides the primary energy gain, while the stripe order yields only a small, and hence fragile, additional energy lowering."}
{"id": "2602.19420", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19420", "abs": "https://arxiv.org/abs/2602.19420", "authors": ["Fei Chen", "Jorge Cortés", "Sonia Martínez"], "title": "Enhancing network resilience through topological switching", "comment": "12 pages, 5 figures", "summary": "This work studies how to preemptively increase the resilience of a network by means of time-varying topological actuation. To do this, we focus on linear dynamical systems that are compatible with a given network, and consider policies that switch periodically between the given one and an alternative, topologically-compatible dynamics. In particular, we seek to solve design problems aimed at finding a) the optimal switching schedule between two preselected topologies, and b) an optimal topology and optimal switching schedule. By imposing periodicity, we first provide a metric of resilience in terms of the spectral abscissa of the averaged linear time-invariant dynamics. By restricting our policies to commutative networks, we then show how the optimal scheduling problem reduces to a convex optimization, providing a bound on the net resilience that can be achieved. After this, we find that the optimal, sparse commutative network to switch with is fully disconnected and allocates the spectral sum among the nodes of the network equally. We then impose additional restrictions on topology edge selection, which leads to a biconvex optimization for which certain matrix rank conditions guide the choice of weighting parameters to obtain desirable solutions. Finally, we provide two methods to solve this problem efficiently (based on a McCormick relaxation, and alternating minimization), and illustrate the results in simulations."}
{"id": "2602.19090", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.19090", "abs": "https://arxiv.org/abs/2602.19090", "authors": ["Takeshi Terao", "Katsuhisa Ozaki"], "title": "Forward Error-Oriented Iterative Refinement for Eigenvectors of a Real Symmetric Matrix", "comment": null, "summary": "In this paper, we discuss numerical methods for the eigenvalue decomposition of real symmetric matrices. While many existing methods can compute approximate eigenpairs with sufficiently small backward errors, the magnitude of the resulting forward errors is often unknown. Consequently, when high-precision numerical solutions are required, the computational cost tends to increase significantly because backward errors must be reduced to an excessive degree. To address this issue, we propose an efficient approximation algorithm that aims to achieve a prescribed forward error, together with a high-accuracy numerical algorithm based on the Ozaki scheme -- an emulation technique for matrix multiplication -- adapted to this problem. Since the proposed method is not primarily focused on reducing backward errors, the computational cost can be significantly reduced. Finally, we present numerical experiments to evaluate the efficiency of the proposed method."}
{"id": "2602.19288", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19288", "abs": "https://arxiv.org/abs/2602.19288", "authors": ["Sanjeev Kumar", "Hendrik Weimer"], "title": "Self-correction phase transition in the dissipative toric code", "comment": "6 pages, 5 figures", "summary": "We analyze a time-continuous version of a cellular automaton decoder for the toric code in the form of a Lindblad master equation. In this setting, a self-correcting quantum memory becomes a thermodynamical phase of the steady state, which manifests itself through the steady state being topologically ordered. We compute the steady state phase diagram, finding a competition between the error correction rate and the update rate for the classical field of the cellular automaton. Strikingly, we find that self-correction of errors is possible even in situations where conventional quantum error correction does not have a finite threshold."}
{"id": "2602.18563", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2602.18563", "abs": "https://arxiv.org/abs/2602.18563", "authors": ["Marc Nairn", "Beatriz Olmos", "Parvinder Solanki"], "title": "Controlling emergent dynamical behavior via phase-engineered strong symmetries", "comment": "7+12 pages 3+4 figures", "summary": "Symmetry constraints provide a powerful means to control the dynamics of open quantum systems. However, the set of accessible control parameters is often limited. Here, we show that a tunable phase in the collective light-matter coupling of a cavity QED system induces a phase-dependent strong symmetry of the Liouvillian, enabling dynamical control of the open quantum system evolution. We demonstrate that tuning this phase substantially reduces the critical driving strength for dissipative phase transitions between stationary and non-stationary phases. We illustrate this mechanism in two experimentally relevant cavity QED settings: a two-species ensemble of two-level atoms and a single-species ensemble of three-level atoms. Our results establish phase control as a versatile tool for engineering dissipative phase transitions, with implications for quantum state preparation."}
{"id": "2602.19838", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.19838", "abs": "https://arxiv.org/abs/2602.19838", "authors": ["Kensuke Okada"], "title": "Optimality of the Half-Order Exponent in the Turing-Good Identities for Bayes Factors", "comment": null, "summary": "Bayes factors are widely computed by Monte Carlo, yet heavy-tailed sampling distributions can make numerical validation unreliable. The Turing--Good identities provide exact moment equalities for powers of a Bayes factor (a density ratio). When these identities are used as Good-check diagnostics, the power choice becomes a statistical design parameter. We develop a nonasymptotic variance theory for Monte Carlo evaluation of the identities and show that the half-order (square-root) power is uniquely minimax-stable: it equalizes variability across the two model orientations and is the only choice that guarantees finite second moments in a distribution-free worst-case sense over all mutually absolutely continuous model pairs. This yields a balanced two-sample half-order diagnostic that is symmetric in model labeling and has a uniform variance bound at fixed computational budget; in small-overlap regimes it is guaranteed to be no less efficient than the standard one-sided Turing check. Simulations for binomial Bayes factor workflows illustrate stable finite-sample behavior and sensitivity to simulator--evaluator mismatches. We further connect the half-order overlap viewpoint to stable primitives for normalizing-constant ratios and importance-sampling degeneracy summaries."}
{"id": "2602.19587", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19587", "abs": "https://arxiv.org/abs/2602.19587", "authors": ["Junseon Park", "Hyeongon Park", "Rahul K. Gupta"], "title": "Co-Optimization of Network Topology and Variable Impedance Devices under Dynamic Line Ratings in Power Transmission Systems", "comment": null, "summary": "Power system operators are increasingly deploying Grid Enhancing Technologies (GETs) to mitigate operational challenges such as line and transformer congestion, and voltage violations. These technologies, including Network Topology Optimization (NTO), Variable Impedance Devices (VIDs), and Dynamic Line Rating (DLR), enhance system flexibility and enable better utilization of existing network assets. However, as the deployment of multiple GETs grows, effective coordination among them becomes essential to fully realize their potential benefits. This paper presents a co-optimization framework that models and coordinates NTO, VID, and DLR within a unified optimization scheme to alleviate network congestion and minimize operational costs. The NTO formulation is developed using a node-breaker model, offering finer switching granularity and improved operational flexibility. The inclusion of VIDs introduces nonlinear and non-convex relationships in the optimization problem. DLR takes into account of weather conditions, primarily wind speed and ambient temperature, enabling adaptive utilization of transmission capacity. The proposed framework is validated on standard IEEE benchmark test systems, demonstrating its effectiveness under varying numbers and placements of impedance controllers."}
{"id": "2602.19203", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19203", "abs": "https://arxiv.org/abs/2602.19203", "authors": ["Mst Moushumi Pervin", "Hengfang Wang", "Jae Kwang Kim"], "title": "Generalized entropy calibration for inference with partially observed data: A unified framework", "comment": null, "summary": "Missing data is an universal problem in statistics. We develop a unified framework for estimating parameters defined by general estimating equations under a missing-at-random (MAR) mechanism, based on generalized entropy calibration weighting. We construct weights by minimizing a convex entropy subject to (i) balancing constraints on a data-adaptive calibration function, estimated using flexible machine-learning predictors with cross-fitting, and (ii) a debiasing constraint involving the fitted propensity score (PS) model. The resulting estimator is doubly robust, remaining consistent if either the outcome regression (OR) or the PS model is correctly specified, and attains the semiparametric efficiency bound when both models are correctly specified. Our formulation encompasses classical inverse probability weighting (IPW) and augmented IPW (AIPW) as special cases and accommodates a broad class of entropy functions. We illustrate the versatility of the approach in three important settings: semi-supervised learning with unlabeled outcomes, regression analysis with missing covariates, and causal effect estimation in observational studies. Extensive simulation studies and real-data applications demonstrate that the proposed estimators achieve greater efficiency and numerical stability than existing methods. In particular, the proposed estimator outperforms the classical AIPW estimator under the OR model misspecification."}
{"id": "2602.19587", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19587", "abs": "https://arxiv.org/abs/2602.19587", "authors": ["Junseon Park", "Hyeongon Park", "Rahul K. Gupta"], "title": "Co-Optimization of Network Topology and Variable Impedance Devices under Dynamic Line Ratings in Power Transmission Systems", "comment": null, "summary": "Power system operators are increasingly deploying Grid Enhancing Technologies (GETs) to mitigate operational challenges such as line and transformer congestion, and voltage violations. These technologies, including Network Topology Optimization (NTO), Variable Impedance Devices (VIDs), and Dynamic Line Rating (DLR), enhance system flexibility and enable better utilization of existing network assets. However, as the deployment of multiple GETs grows, effective coordination among them becomes essential to fully realize their potential benefits. This paper presents a co-optimization framework that models and coordinates NTO, VID, and DLR within a unified optimization scheme to alleviate network congestion and minimize operational costs. The NTO formulation is developed using a node-breaker model, offering finer switching granularity and improved operational flexibility. The inclusion of VIDs introduces nonlinear and non-convex relationships in the optimization problem. DLR takes into account of weather conditions, primarily wind speed and ambient temperature, enabling adaptive utilization of transmission capacity. The proposed framework is validated on standard IEEE benchmark test systems, demonstrating its effectiveness under varying numbers and placements of impedance controllers."}
{"id": "2602.18939", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.18939", "abs": "https://arxiv.org/abs/2602.18939", "authors": ["J. M. Varela", "L. L. Keller", "A. de Oliveira Junior", "D. A. Moreira", "R. Chaves", "R. A. Macêdo"], "title": "Predicting Magic from Very Few Measurements", "comment": "20 pages, 3 figures", "summary": "The nonstabilizerness of quantum states is a necessary resource for universal quantum computation, yet its characterization is notoriously demanding. Quantifying nonstabilizerness typically requires an exponential number of measurements and a doubly exponential classical post-processing cost to evaluate its standard monotones. In this work, we show that nonstabilizerness is, to a large extent, in the eyes of the beholder: it can be witnessed and quantified using any set of $m$ $n$-qubit Pauli measurements, provided the set contains anti-commuting pairs. We introduce a general framework that projects the stabilizer polytope onto the subspace defined by these observables and provide an algorithm that estimates magic from Pauli expectation values with runtime exponential in the number of measurements $m$ and polynomial in the number of qubits $n$. By relating the problem to a stabilizer-restricted variant of the quantum marginal problem, we also prove that deciding membership in the corresponding reduced stabilizer polytope is NP-hard. In particular, unless $\\mathrm{P} = \\mathrm{NP}$, no algorithm polynomial in $m$ can solve the problem in full generality, thus establishing fundamental complexity-theoretic limitations. Finally, we employ our framework to compute nonstabilizerness in different Hamiltonian ground states, demonstrating the practical performance of our method in regimes beyond the reach of existing techniques."}
{"id": "2602.19869", "categories": ["math.OC", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.19869", "abs": "https://arxiv.org/abs/2602.19869", "authors": ["Amal Alphonse", "Petar Kunštek", "Marko Vrdoljak"], "title": "Optimal design with uncertainties: a risk-averse approach", "comment": "2 figures", "summary": "We study a class of stochastic optimal design problems for elliptic partial differential equations in divergence form, where the coefficients represent mixtures of two conducting materials. The objective is to minimize a generalized risk measure of the system response, incorporating uncertainty in the loading through probability distributions. We establish existence of relaxed optimal designs via homogenization theory and derive first-order stationarity conditions satisfied by the optima. Based on these conditions, we develop an optimality criteria algorithm for numerical computations. The stochastic component is treated using a truncated Karhunen--Loève expansion, allowing evaluation of the value-at-risk (VaR) and conditional value-at-risk (CVaR) contributions arising from the sensitivity analysis and featured in the algorithm. The method is illustrated for an example involving CVaR-based compliance minimization."}
{"id": "2602.19155", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.19155", "abs": "https://arxiv.org/abs/2602.19155", "authors": ["Sihao Cheng", "Ziming Shao", "Dong Wang"], "title": "A median-filter-based framework for interface optimal design problems", "comment": null, "summary": "We present a robust and efficient numerical framework based on a median filter scheme for solving a broad class of interface-related optimization problems, from image segmentation to topology optimization. A key innovation of our work is the extension of the binary scheme into a continuous level-set scheme via a weighted quantile interpretation. Unlike traditional binary iterative convolution-thresholding method (ICTM), this continuous median filter scheme effectively overcomes the pinning effect caused by spatial discretization, achieving accurate interface evolution even with small time steps. We also provide a rigorous theoretical analysis, proving the unconditional energy stability of the iterative scheme. Furthermore, we prove that for a wide class of data fidelity terms, the convex relaxation inherently enforces a binary solution, justifying the effectiveness of the method without explicit penalization. Numerical experiments on the Chan--Vese model, the local intensity fitting (LIF) model, and topology optimization in Stokes flow demonstrate that the proposed efficient continuous framework effectively eliminates the pinning effect, guarantees unconditional energy stability, and accurately converges to binary solutions."}
{"id": "2602.19453", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.19453", "abs": "https://arxiv.org/abs/2602.19453", "authors": ["Ryan W. Salatti", "André M. Timpanaro"], "title": "Emergence of opinion splits in the Sznajd model with latency", "comment": "7 pages (main text) + 10 pages (appendices), 13 figures", "summary": "In the modelling of social systems, opinion latency is the idea that once an agent changes its opinion, there will be a period of time where it is immune to other changes. When added to the voter model this leads to a situation where no matter how low the latency is or how many opinions are considered, all opinions end up in a coexistence where they are equally represented. In this work, we examine what happens when latency is added to the Sznajd model. What we find is that for low latency, the model behaves roughly like it does in the absence of latency, where one opinion will always eventually dominate. For high latency, the possibility for a symmetric coexistence of 2 opinions arises, but contrary to the voter model, a coexistence of more than 2 opinions is never stable. We provide evidence of this phenomenon with computer simulations of the model in Barabási-Albert networks, together with a mean field treatment that is able to capture the observed behavior. We argue that this could hint at an explanation for the prevalence of two opinion splits in the real world."}
{"id": "2602.18567", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2602.18567", "abs": "https://arxiv.org/abs/2602.18567", "authors": ["Gabriel J. Gregory", "Evan R. Ritchie", "Alex Quinn", "Sean Brudney", "David J. Wineland", "David T. C. Allcock", "Jameson O'Reilly"], "title": "Four- and six-photon stimulated Raman transitions for coherent qubit and qudit operations", "comment": "5 pages, 4 figures, SM 17 pages, SM 15 figures", "summary": "We experimentally demonstrate transitions between electronic angular momentum states with a difference in magnetic quantum numbers $Δ\\mathrm{m_J} = $ 3, 4, and 5 via resonant four- and six-photon stimulated Raman transitions in a single trapped atom. Derivation of the corresponding Rabi frequencies, which are verified experimentally, follows the standard treatment of two-photon transitions including the adiabatic elimination of intermediate states. Finally, we discuss pathways to increase the observed multi-photon transition fidelities to $>99.99\\%$, providing a tool for efficient, high-fidelity control of qu\\textit{d}its and single-atom logical qubits."}
{"id": "2602.20151", "categories": ["stat.ME", "cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20151", "abs": "https://arxiv.org/abs/2602.20151", "authors": ["Anastasios N. Angelopoulos"], "title": "Conformal Risk Control for Non-Monotonic Losses", "comment": null, "summary": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization."}
{"id": "2602.19666", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19666", "abs": "https://arxiv.org/abs/2602.19666", "authors": ["Mario di Bernardo"], "title": "Multicellular Feedback Control Strategies in Synthetic Microbial Consortia: From Embedded to Distributed Control", "comment": null, "summary": "Living organisms rely on endogenous feedback mechanisms to maintain homeostasis in the presence of uncertainty and environmental fluctuations. An emerging challenge at the interface of control systems engineering and synthetic biology is the design of reliable feedback strategies to regulate cellular behavior and collective biological functions. In this article, we review recent advances in multicellular feedback control, where sensing, computation, and actuation are distributed across different cell populations within synthetic microbial consortia, giving rise to biological multiagent control systems governed by molecular communication. From a control-theoretic perspective, these consortia can be interpreted as distributed biomolecular control systems, where coordination among populations replace embedded regulation. We survey theoretical frameworks, control architectures, and modeling approaches, ranging from aggregate population-level dynamics to spatially aware agent-based simulations, and discuss experimental demonstrations in engineered \\textit{Escherichia coli} consortia. We highlight how distributing control functions across populations can reduce metabolic burden, mitigate retroactivity, improve robustness to uncertainty, and enable modular reuse of control components. Beyond regulation of gene expression, we discuss the emerging problem of population composition control, where coordination among growing and competing cell populations becomes an integral part of the control objective. Finally, we outline key open challenges that must be addressed before multicellular control strategies can be deployed in real-world applications such as biomanufacturing, environmental remediation, and therapeutic systems. These challenges span modeling and simulation, experimental platform development, coordination and composition control, and long-term evolutionary stability."}
{"id": "2602.19216", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19216", "abs": "https://arxiv.org/abs/2602.19216", "authors": ["Luisa Stracqualursi", "Patrizia Agati"], "title": "Statistical Measures for Explainable Aspect-Based Sentiment Analysis: A Case Study on Environmental Discourse in Reddit", "comment": "Preprint of an article accepted for publication in Statistics (Taylor & Francis). 14 pages, 2 figures, 4 tables", "summary": "Aspect-Based Sentiment Analysis (ABSA) provides a fine-grained understanding of opinions by linking sentiment to specific aspects in text. While transformer-based models excel at this task, their black-box nature limits their interpretability, posing risks in real-world applications without labeled data. This paper introduces a statistical, model-agnostic framework to assess the behavioral transparency and trustworthiness of ABSA models. Our framework relies on several metrics, such as the entropy of polarity distributions, soft-count-based dominance scores, and sentiment divergence between sources, whose robustness is validated through bootstrap resampling and sensitivity analysis. A case study on environmentally focused Reddit communities illustrates how the proposed indicators provide interpretable diagnostics of model certainty, decisiveness, and cross-source variability. The results show that statistical indicators computed on soft outputs can complement traditional approaches, offering a computationally efficient methodology for validating, monitoring, and interpreting ABSA models in contexts where labeled data are unavailable."}
{"id": "2602.19666", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19666", "abs": "https://arxiv.org/abs/2602.19666", "authors": ["Mario di Bernardo"], "title": "Multicellular Feedback Control Strategies in Synthetic Microbial Consortia: From Embedded to Distributed Control", "comment": null, "summary": "Living organisms rely on endogenous feedback mechanisms to maintain homeostasis in the presence of uncertainty and environmental fluctuations. An emerging challenge at the interface of control systems engineering and synthetic biology is the design of reliable feedback strategies to regulate cellular behavior and collective biological functions. In this article, we review recent advances in multicellular feedback control, where sensing, computation, and actuation are distributed across different cell populations within synthetic microbial consortia, giving rise to biological multiagent control systems governed by molecular communication. From a control-theoretic perspective, these consortia can be interpreted as distributed biomolecular control systems, where coordination among populations replace embedded regulation. We survey theoretical frameworks, control architectures, and modeling approaches, ranging from aggregate population-level dynamics to spatially aware agent-based simulations, and discuss experimental demonstrations in engineered \\textit{Escherichia coli} consortia. We highlight how distributing control functions across populations can reduce metabolic burden, mitigate retroactivity, improve robustness to uncertainty, and enable modular reuse of control components. Beyond regulation of gene expression, we discuss the emerging problem of population composition control, where coordination among growing and competing cell populations becomes an integral part of the control objective. Finally, we outline key open challenges that must be addressed before multicellular control strategies can be deployed in real-world applications such as biomanufacturing, environmental remediation, and therapeutic systems. These challenges span modeling and simulation, experimental platform development, coordination and composition control, and long-term evolutionary stability."}
{"id": "2602.19288", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19288", "abs": "https://arxiv.org/abs/2602.19288", "authors": ["Sanjeev Kumar", "Hendrik Weimer"], "title": "Self-correction phase transition in the dissipative toric code", "comment": "6 pages, 5 figures", "summary": "We analyze a time-continuous version of a cellular automaton decoder for the toric code in the form of a Lindblad master equation. In this setting, a self-correcting quantum memory becomes a thermodynamical phase of the steady state, which manifests itself through the steady state being topologically ordered. We compute the steady state phase diagram, finding a competition between the error correction rate and the update rate for the classical field of the cellular automaton. Strikingly, we find that self-correction of errors is possible even in situations where conventional quantum error correction does not have a finite threshold."}
{"id": "2602.20103", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.20103", "abs": "https://arxiv.org/abs/2602.20103", "authors": ["Shucheng Kang", "Heng Yang"], "title": "Local Second-Order Limit Dynamics of the Alternating Direction Method of Multipliers for Semidefinite Programming", "comment": null, "summary": "The alternating direction method of multipliers (ADMM) is widely used for solving large-scale semidefinite programs (SDPs), yet on instances with multiple primal--dual optimal solution pairs, it often enters prolonged slow-convergence regions where the Karush--Kuhn--Tucker (KKT) residuals nearly stall. To explain and predict the fine-grained dynamical behavior inside these regions, we develop a local second-order limit dynamics framework for ADMM near an arbitrary KKT point -- not necessarily the eventual limit point of the iterates. Assuming the existence of a strictly complementary primal--dual solution pair, we derive a second-order local expansion of the ADMM dynamics by leveraging a refined and simplified variational characterization of the (parabolic) second-order directional derivative of the PSD projection operator. This expansion reveals a closed convex cone of directions along which the local first-order update vanishes, and it induces a second-order limit map that governs the persistent drift after transient effects are filtered out. We characterize fundamental properties of this mapping, including its kernel, range, and continuity. A primal--dual decoupling further yields a clean scaling law for the effect of the penalty parameter in ADMM. We connect these properties to second-order dynamical features of ADMM, including fixed points, almost-invariant sets, and microscopic phases. Three empirical phenomena in slow-convergence regions are then explained or predicted: (i) angles between consecutive iterate differences are small yet nonzero, except for sparse spikes; (ii) primal and dual infeasibilities are insensitive to penalty-parameter updates; and (iii) iterates can be transiently trapped in a low-dimensional subspace for an extended period. Extensive numerical experiments on the Mittelmann dataset corroborate our theoretical predictions."}
{"id": "2602.19375", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.19375", "abs": "https://arxiv.org/abs/2602.19375", "authors": ["Xue Jiang", "Lei Li", "Lingxiao Li"], "title": "Parametric charge-conservative mixed finite element method for 3D incompressible inductionless MHD equations on curved domains", "comment": null, "summary": "This paper develops a charge-conservative mixed finite element method with optimal convergence rates for the stationary incompressible inductionless MHD equations on three-dimensional curved domains. The discretization employs the isoparametric Taylor-Hood elements with grad-div stabilization for the velocity-pressure pair, and parametric Brezzi-Douglas-Marini elements for the current density. Utilizing the Piola's transformation, the discrete current density is exactly divergence-free. By employing suitable extensions and projections, optimal a priori error estimates are derived in both the energy norm and the $L^2$-norm. Numerical experiments are presented to confirm the theoretical results."}
{"id": "2602.19586", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19586", "abs": "https://arxiv.org/abs/2602.19586", "authors": ["Hanieh Najafzadeh", "Abdollah Langari"], "title": "From Quantum Chaos to a Reversed Quantum Disentangled Liquid in a Disorder-Free Spin Ladder", "comment": null, "summary": "The mechanisms by which isolated interacting quantum systems evade thermalization extend beyond disorder-induced many-body localization, encompassing a growing class of interaction-driven phenomena. We investigate a spin-1/2 ladder with asymmetric XY leg couplings and tunable Ising interactions on the rungs, and identify the microscopic origin of many-body localization (MBL) in this setting. Through a suite of diagnostics -including entanglement dynamics, fidelity susceptibility, adiabatic gauge potential norms, level-spacing statistics and entropy of eigenstates- we uncover a reentrant progression of dynamical regimes as the rung coupling Jz is varied: integrable behavior at Jz=0, quantum chaos at intermediate Jz, and a robust nonthermal regime at strong coupling. In the latter regime, we demonstrate the emergence of a reversed quantum disentangled liquid (reversed-QDL), where the light species thermalizes while the heavy species remains localized. The strong-coupling limit further yields emergent local integrals of motion anchored in a fixed-point structure, providing a microscopic origin of the observed quasi-MBL dynamics. These results establish reversed-QDL as a distinct, disorder-free route to nonergodicity and broaden the classification of dynamical phases in quantum matter."}
{"id": "2602.18642", "categories": ["quant-ph", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18642", "abs": "https://arxiv.org/abs/2602.18642", "authors": ["Tomasz Rybotycki", "Sebastian Dziura", "Piotr Gawron"], "title": "Auto Quantum Machine Learning for Multisource Classification", "comment": "15 pages, 4 figures, 3 tables. Submitted to ICCS2026", "summary": "With fault-tolerant quantum computing on the horizon, there is growing interest in applying quantum computational methods to data-intensive scientific fields like remote sensing. Quantum machine learning (QML) has already demonstrated potential for such demanding tasks. One area of particular focus is quantum data fusion -- a complex data analysis problem that has attracted significant recent attention. In this work, we introduce an automated QML (AQML) approach for addressing data fusion challenges. We evaluate how AQML-generated quantum circuits perform compared to classical multilayer perceptrons (MLPs) and manually designed QML models when processing multisource inputs. Furthermore, we apply our method to change detection using the multispectral ONERA dataset, achieving improved accuracy over previously reported QML-based change detection results."}
{"id": "2602.19667", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19667", "abs": "https://arxiv.org/abs/2602.19667", "authors": ["Timon Conrad", "Changhun Kim", "Johann Jäger", "Andreas Maier", "Siming Bayer"], "title": "Impact of Training Dataset Size for ML Load Flow Surrogates", "comment": "Oberlausitzer Energiesymposium 2025 & Zittauer Energieseminar, Zittau, Deutschland, 25./26. November 2025", "summary": "Efficient and accurate load flow calculations are a bedrock of modern power system operation. Classical numerical methods such as the Newton-Raphson algorithm provide highly precise results but are computationally demanding, which limits their applicability in large-scale scenario studies and optimization in time-critical contexts. Research has shown that machine learning approaches can approximate load flow results with high accuracy while substantially reducing computation time.\n  Sample efficiency, i.e., the ability to achieve high accuracy with limited training dataset size, is still insufficiently researched, especially in grids with a fixed topology. This paper presents a systematic investigation of the sample efficiency of a Multilayer Perceptron and two Graph Neural Network variants on a dataset based on a modified IEEE 5-bus system. The results for this grid size show that Graph Neural Networks achieve the lowest losses. However, the availability of large training datasets remains the dominant factor influencing performance compared to architecture choice."}
{"id": "2602.19220", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19220", "abs": "https://arxiv.org/abs/2602.19220", "authors": ["Shanshan Liu", "Guoqing Diao"], "title": "A likelihood approach to proper analysis of secondary outcomes in matched case-control studies", "comment": null, "summary": "Matched case-control studies are commonly employed in epidemiological research for their convenience and efficiency. Analysis of secondary outcomes can yield valuable insights into biological pathways and help identify genetic variants of importance. Naive analysis using standard statistical methods, such as least-squares regression for quantitative traits, can be misleading because they fail to account for unequal sampling induced by the case-control design and matching. In this paper, we propose novel statistical methods that appropriately reflect the study design and sampling scheme in the analysis of secondary outcome data. The new methods provide consistent estimation and accurate coverage probabilities for the confidence interval estimators. We demonstrate the advantages of the new methods through simulation studies and a real application with diabetes patients. R code implementing the proposed methods is publicly available."}
{"id": "2602.19667", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19667", "abs": "https://arxiv.org/abs/2602.19667", "authors": ["Timon Conrad", "Changhun Kim", "Johann Jäger", "Andreas Maier", "Siming Bayer"], "title": "Impact of Training Dataset Size for ML Load Flow Surrogates", "comment": "Oberlausitzer Energiesymposium 2025 & Zittauer Energieseminar, Zittau, Deutschland, 25./26. November 2025", "summary": "Efficient and accurate load flow calculations are a bedrock of modern power system operation. Classical numerical methods such as the Newton-Raphson algorithm provide highly precise results but are computationally demanding, which limits their applicability in large-scale scenario studies and optimization in time-critical contexts. Research has shown that machine learning approaches can approximate load flow results with high accuracy while substantially reducing computation time.\n  Sample efficiency, i.e., the ability to achieve high accuracy with limited training dataset size, is still insufficiently researched, especially in grids with a fixed topology. This paper presents a systematic investigation of the sample efficiency of a Multilayer Perceptron and two Graph Neural Network variants on a dataset based on a modified IEEE 5-bus system. The results for this grid size show that Graph Neural Networks achieve the lowest losses. However, the availability of large training datasets remains the dominant factor influencing performance compared to architecture choice."}
{"id": "2602.19558", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19558", "abs": "https://arxiv.org/abs/2602.19558", "authors": ["Ben T. McDonough", "Jian-Hao Zhang", "Victor V. Albert", "Andrew Lucas"], "title": "Calderbank-Shor-Steane codes on group-valued qudits", "comment": "44 pages, 17 figures", "summary": "Calderbank-Shor-Steane (CSS) codes are a versatile quantum error-correcting family built out of commuting $X$- and $Z$-type checks. We introduce CSS-like codes on $G$-valued qudits for any finite group $G$ that reduce to qubit CSS codes for $G = \\mathbb{Z}_2$ yet generalize the Kitaev quantum double model for general groups. The $X$-checks of our group-CSS codes correspond to left and/or right multiplication by group elements, while $Z$-checks project onto solutions to group word equations. We describe quantum-double models on oriented two-dimensional CW complexes (which need not cellulate a manifold) and prove that, when $G$ is non-Abelian and simple, every $G$-covariant group-CSS code with suitably upper-bounded $Z$-check weight and lower-bounded $Z$-distance reduces to a CW quantum double. We describe the codespace and logical operators of CW quantum doubles via the same intuition used to obtain logical structure of surface codes. We obtain distance bounds for codes on non-Abelian simple groups from the graph underlying the CW complex, and construct intrinsically non-Abelian code families with asymptotically optimal rate and distances. Adding \"ghost vertices\" to the CW complex generalizes quantum double models with defects and rough boundary conditions whose logical structure can be understood without reference to non-Abelian anyons or defects. Several non-invertible symmetry-protected topological states, both with ordinary and higher-form symmetries, are the unique codewords of simply-connected CW quantum doubles with a single ghost vertex."}
{"id": "2602.20136", "categories": ["math.OC", "math.CO", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.20136", "abs": "https://arxiv.org/abs/2602.20136", "authors": ["Sergio Mayorga", "Eugene Stepanov", "Pedro Barrios"], "title": "On a discrete max-plus transportation problem", "comment": null, "summary": "We provide an explicit algorithm to solve the idempotent analogue of the discrete Monge-Kantorovich optimal mass transportation problem with the usual real number field replaced by the tropical (max-plus) semiring, in which addition is defined as the maximum and product is defined as usual addition, with minus infinity and zero playing the roles of additive and multiplicative identities. Such a problem may be naturally called tropical or \"max-plus\" optimal transportation problem. We show that the solutions to the latter, called the optimal tropical plans, may not correspond to perfect matchings even if the data (max-plus probability measures) have all weights equal to zero, in contrast with the classical discrete optimal transportation analogue, where perfect matching optimal plans in similar situations always exist. Nevertheless, in some randomized situation the existence of perfect matching optimal tropical plans may occur rather frequently. At last, we prove that the uniqueness of solutions of the optimal tropical transportation problem is quite rare."}
{"id": "2602.19452", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.19452", "abs": "https://arxiv.org/abs/2602.19452", "authors": ["Longfei Gao", "Frimpong Baidoo"], "title": "Dekker's floating point number system and compensated summation algorithms", "comment": null, "summary": "The recent hardware trend towards reduced precision computing has reignited the interest in numerical techniques that can be used to enhance the accuracy of floating point operations beyond what is natively supported for basic arithmetic operations on the hardware. In this work, we study the behavior of various compensated summation techniques, which can be used to enhance the accuracy for the summation operation, particularly in situations when the addends are not known a priori. Complete descriptions of the error behavior are provided for these techniques. In particular, the relationship between the intermediate results at two consecutive summing steps is provided, which is used to identify the operation that limits accuracy and guide the design of more nuanced techniques. The analysis relies on the work of Dekker [Numerische Mathematik, 1971], which uses a special floating point number system that does not require uniqueness in the number representation. Despite this seemingly strange attribute, Dekker's system is very convenient for the analysis here and allows general statements to be expressed succinctly compared to a number system that requires uniqueness. To prepare the foundation for discussion, we start by giving a thorough exhibition of Dekker's number system and supply the details that were omitted in Dekker [Numerische Mathematik, 1971]. Numerical examples are designed to explain the inner workings of these compensated summation techniques, illustrate their efficacy, and empirically verify the analytical results derived. Discussions are also given on application scenarios where these techniques can be beneficial."}
{"id": "2602.19722", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.19722", "abs": "https://arxiv.org/abs/2602.19722", "authors": ["Hanyan Cao", "Dongyang Feng", "Cheng Ye", "Feng Pan"], "title": "Differentiable Maximum Likelihood Noise Estimation for Quantum Error Correction", "comment": null, "summary": "Accurate noise estimation is essential for fault-tolerant quantum computing, as decoding performance depends critically on the fidelity of the circuit-level noise parameters. In this work, we introduce a differentiable Maximum Likelihood Estimation (dMLE) framework that enables exact, efficient, and fully differentiable computation of syndrome log-likelihoods, allowing circuit-level noise parameters to be optimized directly via gradient descent. Leveraging the exact Planar solver for repetition codes and a novel, simplified Tensor Network (TN) architecture combined with optimized contraction path finding for surface codes, our method achieves tractable and fully differentiable likelihood evaluation even for distance 5 surface codes with up to 25 rounds. Our method recovers the underlying error probabilities with near-exact precision in simulations and reduces logical error rates by up to 30.6(3)% for repetition codes and 8.1(2)% for surface codes on experimental data from Google's processor compared to previous state-of-the-art methods: correlation analysis and Reinforcement Learning (RL) methods. Our approach yields provably optimal, decoder-independent error priors by directly maximizing the syndrome likelihood, offering a powerful noise estimation and control tool for unlocking the full potential of current and future error-corrected quantum processors."}
{"id": "2602.18701", "categories": ["quant-ph", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2602.18701", "abs": "https://arxiv.org/abs/2602.18701", "authors": ["Matt Wilson"], "title": "Higher-order circuits", "comment": null, "summary": "We write down a series of basic laws for (strict) higher-order circuit diagrams. More precisely, we define higher-order circuit theories in terms of: (a) nesting, (b) temporal and spatial composition, and (c) equivalence between lower-order bipartite processes and higher-order bipartite states. In category-theoretic terms, these laws are expressed using enrichment and cotensors in symmetric polycategories, along with a frobenius-like coherence between them. We describe how these laws capture the salient features of higher-order quantum theory, and discover an upper bound for higher-order circuits: any higher-order circuit theory embeds into the theory of strong profunctors."}
{"id": "2602.19784", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19784", "abs": "https://arxiv.org/abs/2602.19784", "authors": ["Bang Huang", "Eddine Youcef Belmekki", "Mohamed-Slim Alouini"], "title": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation", "comment": null, "summary": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth."}
{"id": "2602.19236", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19236", "abs": "https://arxiv.org/abs/2602.19236", "authors": ["Sreya Sarkar", "Kshitij Khare", "Sanvesh Srivastava"], "title": "CoMET: A Compressed Bayesian Mixed-Effects Model for High-Dimensional Tensors", "comment": "50 pages, 11 figures, and 2 tables", "summary": "Mixed-effects models are fundamental tools for analyzing clustered and repeated-measures data, but existing high-dimensional methods largely focus on penalized estimation with vector-valued covariates. Bayesian alternatives in this regime are limited, with no sampling-based mixed-effects framework that supports tensor-valued fixed- and random-effects covariates while remaining computationally tractable. We propose the Compressed Mixed-Effects Tensor (CoMET) model for high-dimensional repeated-measures data with scalar responses and tensor-valued covariates. CoMET performs structured, mode-wise random projection of the random-effects covariance, yielding a low-dimensional covariance parameter that admits simple Gaussian prior specification and enables efficient imputation of compressed random-effects. For the mean structure, CoMET leverages a low-rank tensor decomposition and margin-structured Horseshoe priors to enable fixed-effects selection. These design choices lead to an efficient collapsed Gibbs sampler whose computational complexity grows approximately linearly with the tensor covariate dimensions. We establish high-dimensional theoretical guarantees by identifying regularity conditions under which CoMET's posterior predictive risk decays to zero. Empirically, CoMET outperforms penalized competitors across a range of simulation studies and two benchmark applications involving facial-expression prediction and music emotion modeling."}
{"id": "2602.19784", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19784", "abs": "https://arxiv.org/abs/2602.19784", "authors": ["Bang Huang", "Eddine Youcef Belmekki", "Mohamed-Slim Alouini"], "title": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation", "comment": null, "summary": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth."}
{"id": "2602.19865", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19865", "abs": "https://arxiv.org/abs/2602.19865", "authors": ["R. Jafari", "Alireza Akbari"], "title": "Separation of the Kibble-Zurek Mechanism from Quantum Criticality", "comment": null, "summary": "When a system is swept through a quantum critical point (QCP), the Kibble-Zurek mechanism predicts that the average number of topological defects follows a universal power-law scaling with the ramp time scale. This scaling behavior is determined by the equilibrium critical exponents of the underlying phase transition. We show that the correspondence between Kibble-Zurek scaling and quantum criticality does not hold generally. In particular, the defect density can exhibit a suppression faster than the Kibble-Zurek prediction even when the quench crosses a critical point, while conventional Kibble-Zurek scaling may persist for quenches through a non-critical point. Our results, based on models representative of a broad class of quasi-one-dimensional Fermi systems, identify the dynamical conditions under which universal defect scaling emerges and clarify the relation between defect generation and equilibrium criticality."}
{"id": "2602.18441", "categories": ["physics.comp-ph", "cs.CE", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18441", "abs": "https://arxiv.org/abs/2602.18441", "authors": ["Dimitrios Voulanas", "Eduardo Gildin"], "title": "Updating DMD Operators for Changes in Domain Properties", "comment": null, "summary": "Fast and reliable surrogate models are critical for optimization, control and uncertainty analysis in geological carbon-storage projects, yet high-fidelity multiphase simulators remain too expensive. Dynamic Mode Decomposition (DMD) offers an attractive data-driven reduction framework, but its operators are trained for a single set of reservoir properties. When permeability or well location changes, conventional practice is to regenerate snapshots and retrain the surrogate, erasing most of the speed advantage. This work presents a lightweight alternative that updates an existing DMD or DMD-with-control model without incorporating new simulation data or retraining. Two complementary update strategies are introduced. For cases where permeability changes uniformly across the domain, the proposed updates adjust the models internal dynamics and control response to match the new flow timescale. When permeability varies in space, the approach modifies the spatial representation so that high-permeability zones are given greater influence on the models reduced basis. Numerical experiments demonstrate that the proposed updates recover plume migration and pressure build-up within three percent of a freshly trained surrogate yet execute hundreds of times faster than full retraining. These methods therefore enable real-time optimization and rapid what-if studies while preserving the physical fidelity demanded by carbon-storage workflows."}
{"id": "2602.19457", "categories": ["math.NA", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.19457", "abs": "https://arxiv.org/abs/2602.19457", "authors": ["Yanan He", "Zhihao Ge"], "title": "Optimal Error Estimates of a new Multiphysic Finite Element Method for Nonlinear Poroelasticity model with Hencky-Mises Stress Tensor", "comment": null, "summary": "In this paper, we develop a new multiphysics finite element method for a nonlinear poroelastic model with Hencky-Mises stress tensor. By introducing some new notations, we reformulate the original model into a fluid-fluid coupling problem, which is viewed as a generalized nonlinear Stokes sub-problem combined with a reaction-diffusion sub-problem. Then, we establish the existence and uniqueness of the weak solution for the reformulated problem, and propose a stable, fully discrete multiphysics finite element method which employs Lagrangian finite element pairs for spatial discretization and a backward Euler scheme for temporal discretization. By ensuring the parameters $κ_1$ and $κ_3$ remain bounded and non-zero even as $λ$ tends to infinity, the proposed method maintains stability for a wide range of Lagrangian element pairs. Based on the continuity and monotonicity of the nonlinear term $\\mathcal{N}(\\varepsilon(\\mathbf{u}_h^{n}))$, we give the stability analysis and derive optimal error estimates for the displacement vector $\\mathbf{u}$ and the pressure $p$ in both $L^2$-norm and $H^1$-norm. In particular, the $L^2$-norm error estimate for the displacement $\\mathbf{u}$, which was not present in previous literature, is established here through an auxiliary problem and a Poincar$\\acute{e}$ inequality. Also, we present numerical tests to verify the theoretical analysis, and the results confirm the optimal convergence rates. Finally, we draw conclusions to summarize the work."}
{"id": "2602.19750", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.19750", "abs": "https://arxiv.org/abs/2602.19750", "authors": ["Mohsen Alishahiha", "Fatemeh Tarighi Tabesh", "Mohammad Javad Vasli"], "title": "Krylov Distribution and Universal Convergence of Quantum Fisher Information", "comment": "12 pages, 2 figures", "summary": "We develop a spectral-resolvent framework for computing the quantum Fisher information (QFI) using Krylov subspace methods, extending the notion of the Krylov distribution. By expressing the QFI as a resolvent moment of the superoperator $\\mathcal{K}_ρ$ associated with a density matrix, the Krylov distribution quantifies how the QFI weight is distributed across Krylov levels in operator space and provides a natural measure for controlling the truncation error in Krylov approximations. Leveraging orthogonal polynomial theory, we identify two universal convergence regimes: exponential decay when the Liouville-space spectrum is gapped away from zero, and algebraic decay governed by hard-edge (Bessel) universality when small eigenvalues accumulate near zero. This framework establishes a direct connection between quantum metrology, spectral geometry, and Krylov dynamics, offering both conceptual insight and practical tools for efficient QFI computation in high-dimensional and many-body systems."}
{"id": "2602.18816", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18816", "abs": "https://arxiv.org/abs/2602.18816", "authors": ["Mrinmoy Samanta", "Sudipta Mondal", "Ayan Patra", "Saptarshi Roy", "Aditi Sen De"], "title": "Hierarchies of Gaussian multimode entanglement from thermodynamic quantifiers", "comment": "6+13 pages, 2 figures", "summary": "We develop a thermodynamic characterization of multimode entanglement in pure continuous-variable systems by quantifying the gap between globally and locally extractable work (ergotropy). For arbitrary pure multimode Gaussian states, we prove that the $2$-local ergotropic gap is a faithful entanglement monotone across any bipartition and constitutes a functionally independent upper bound to the Renyi-2 entanglement entropy. We further introduce the $k$-ergotropic score, the minimum $k$-local ergotropic gap, and show that it faithfully quantifies multimode entanglement across $k$ partitions. For pure three-mode Gaussian states, we derive its closed-form relation with the geometric measure for genuine multimode entanglement $(k=2)$, and total Gaussian multimode entanglement $(k=3)$. For systems with more than three modes, the $k$-ergotropic score becomes a functionally independent measure of multimode entanglement to the standard geometric measures. Our results reveal a direct operational hierarchy linking Gaussian multimode entanglement to work extraction under locality constraints, and provide a computable and experimentally accessible thermodynamic framework for characterizing quantum correlations."}
{"id": "2602.19862", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19862", "abs": "https://arxiv.org/abs/2602.19862", "authors": ["Lars Fischer", "Daniel Flögel", "Sören Hohmann"], "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems", "comment": "8 pages, conference paper", "summary": "In-Motion physical coupling of multiple mobile ground robots has the potential to enable new applications like in-motion transfer that improves efficiency in handling and transferring goods, which tackles current challenges in logistics. A key challenge lies in achieving reliable autonomous in-motion physical coupling of two mobile ground robots starting at any initial position. Existing approaches neglect the modeling of the docking interface and the strategy for approaching it, resulting in uncontrolled collisions that make in-motion physical coupling either impossible or inefficient. To address this challenge, we propose a central mpc approach that explicitly models the dynamics and states of two omnidirectional wheeled robots, incorporates constraints related to their docking interface, and implements an approaching strategy for rendezvous and docking. This novel approach enables omnidirectional wheeled robots with a docking interface to physically couple in motion regardless of their initial position. In addition, it makes in-motion transfer possible, which is 19.75% more time- and 21.04% energy-efficient compared to a non-coupling approach in a logistic scenario."}
{"id": "2602.19284", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19284", "abs": "https://arxiv.org/abs/2602.19284", "authors": ["Yuhao Wang", "Tengyao Wang"], "title": "Localized conformal model selection", "comment": "8 pages, 1 figure", "summary": "We propose a localized conformal model selection framework that integrates local adaptivity with post-selection validity for distribution-free prediction. By performing model selection symmetrically across calibration points using upper and lower surrogate intervals, we construct a data-dependent safe index set that contains the oracle model and preserves exchangeability. The resulting ensemble procedure retains exact finite-sample marginal coverage while adapting to spatial heterogeneity and model complexity. Simulations demonstrate substantial reductions in interval length compared to the best fixed model, especially in heterogeneous and low-noise settings."}
{"id": "2602.19862", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19862", "abs": "https://arxiv.org/abs/2602.19862", "authors": ["Lars Fischer", "Daniel Flögel", "Sören Hohmann"], "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems", "comment": "8 pages, conference paper", "summary": "In-Motion physical coupling of multiple mobile ground robots has the potential to enable new applications like in-motion transfer that improves efficiency in handling and transferring goods, which tackles current challenges in logistics. A key challenge lies in achieving reliable autonomous in-motion physical coupling of two mobile ground robots starting at any initial position. Existing approaches neglect the modeling of the docking interface and the strategy for approaching it, resulting in uncontrolled collisions that make in-motion physical coupling either impossible or inefficient. To address this challenge, we propose a central mpc approach that explicitly models the dynamics and states of two omnidirectional wheeled robots, incorporates constraints related to their docking interface, and implements an approaching strategy for rendezvous and docking. This novel approach enables omnidirectional wheeled robots with a docking interface to physically couple in motion regardless of their initial position. In addition, it makes in-motion transfer possible, which is 19.75% more time- and 21.04% energy-efficient compared to a non-coupling approach in a logistic scenario."}
{"id": "2602.20077", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.20077", "abs": "https://arxiv.org/abs/2602.20077", "authors": ["Fabricio Danel Matias", "Facundo Arreyes", "Juan Sebastián Ardenghi"], "title": "Entanglement formation in two-dimensional materials within microcavity", "comment": "10 pages", "summary": "In this work, the entanglement generation between two hexagonal-lattice layers embedded in a microcavity is studied, accounting for both electromagnetic coupling and intrinsic spin-orbit interaction (SOI). Utilizing a short-time dynamical approach, we perform a perturbative Taylor expansion of the reduced density matrix to characterize the bipartite quantum correlations between the hexagonal layers. We demonstrate that the system undergoes a rapid transition from a localized product state in the conduction bands at t = 0 to a coherent superposition of valence and conduction band states. Our results indicate that the degree of entanglement is highly sensitive to the interlayer photon propagator, which contains the geometric ratios of the layer positions and the height cavity, and the specific Fermi energy and SOI signatures of the respective layers. We show the emergence of spacelike-separated quantum correlations in the ultra-short evolution regime, suggesting that heterostructures in cavities may be suitable to develop experiments for a deep understanding of spacelike-separated quantum effects."}
{"id": "2602.19366", "categories": ["eess.SY", "cs.MA", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.19366", "abs": "https://arxiv.org/abs/2602.19366", "authors": ["Zirui Xu", "Vasileios Tzoumas"], "title": "Self-Configurable Mesh-Networks for Scalable Distributed Submodular Bandit Optimization", "comment": null, "summary": "We study how to scale distributed bandit submodular coordination under realistic communication constraints in bandwidth, data rate, and connectivity. We are motivated by multi-agent tasks of active situational awareness in unknown, partially-observable, and resource-limited environments, where the agents must coordinate through agent-to-agent communication. Our approach enables scalability by (i) limiting information relays to only one-hop communication and (ii) keeping inter-agent messages small, having each agent transmit only its own action information. Despite these information-access restrictions, our approach enables near-optimal action coordination by optimizing the agents' communication neighborhoods over time, through distributed online bandit optimization, subject to the agents' bandwidth constraints. Particularly, our approach enjoys an anytime suboptimality bound that is also strictly positive for arbitrary network topologies, even disconnected. To prove the bound, we define the Value of Coordination (VoC), an information-theoretic metric that quantifies for each agent the benefit of information access to its neighbors. We validate in simulations the scalability and near-optimality of our approach: it is observed to converge faster, outperform benchmarks for bandit submodular coordination, and can even outperform benchmarks that are privileged with a priori knowledge of the environment."}
{"id": "2602.19760", "categories": ["math.NA", "math.NT"], "pdf": "https://arxiv.org/pdf/2602.19760", "abs": "https://arxiv.org/abs/2602.19760", "authors": ["Erich Novak", "Friedrich Pillichshammer"], "title": "Extreme $L_p$ discrepancy, numerical integration and the curse of dimensionality", "comment": null, "summary": "The classical notion of extreme $L_p$ discrepancy is a quantitative measure for the irregularity of distribution of finite point sets in the $d$-dimensinal unit cube. In this paper we find a dual integration problem whose worst-case error is exactly the extreme $L_p$ discrepancy of the underlying integration nodes. Studying this integration problem we show that the extreme $L_p$ discrepancy suffers from the curse of dimensionality for all $p \\in (1,\\infty)$. It is known that the problem is tractable for $p=\\infty$; the case $p=1$ stays open."}
{"id": "2602.20108", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20108", "abs": "https://arxiv.org/abs/2602.20108", "authors": ["L. Brodoloni", "G. E. Astrakharchik", "S. Giorgini", "S. Pilati"], "title": "Energy gap of quantum spin glasses: a projection quantum Monte Carlo study", "comment": "6 pages plus additional material", "summary": "The performance of quantum annealing for combinatorial optimization is fundamentally limited by the minimum energy gap $Δ$ encountered at quantum phase transitions. We investigate the scaling of $Δ$ with system size $N$ for two paradigmatic quantum spin-glass models: the two-dimensional Edwards-Anderson (2D-EA) and the all-to-all Sherrington-Kirkpatrick (SK) models. Utilizing a newly proposed unbiased energy-gap estimator for continuous-time projection quantum Monte Carlo simulations, complemented by high-performance sparse eigenvalue solvers, we characterize the gap distributions across disorder realizations. It is found that, in the 2D-EA case, the inverse-gap distribution develops a fat tail with infinite variance as $N$ increases. This indicates that the unfavorable super-algebraic scaling of $Δ$, recently reported for binary couplings [Nature 631, 749 (2024)], persists for the Gaussian disorder considered here, pointing to a universal feature of 2D spin glasses. Conversely, the SK model retains a finite-variance distribution, with the disorder-averaged gap following a rather slow power law, close to $Δ\\propto N^{-1/3}$. This finding provides a promising outlook for the potential efficiency of quantum annealers for optimization problems with dense connectivity."}
{"id": "2602.18860", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18860", "abs": "https://arxiv.org/abs/2602.18860", "authors": ["Mohammad B. Arjmandi"], "title": "Frozen and Growing Quantum Work under Noise: Coherence and Correlations as Key Resources", "comment": null, "summary": "We investigate the decomposition of ergotropy into incoherent and coherent contributions for quantum systems subject to typical Markovian noise channels. The incoherent part originates from population inversion in the energy eigenbasis after dephasing, while the coherent part captures the role of quantum coherence in work extraction. For single-qubit systems, we derive explicit conditions for freezing and enhancement of coherent ergotropy and obtain an analytical upper bound, showing that it cannot exceed one half of the state's quantum coherence. We then study two classes of separable two-qubit states under local noise. For Bell-diagonal states, which are locally completely passive and possess no local coherence, we prove that the total extractable work equals the average of geometric quantum and classical correlations. In this case, coherent ergotropy cannot be enhanced, although freezing occurs under specific noise conditions. By contrast, for separable states with local coherence, coherent ergotropy can increase under all considered noise channels, including phase-flip and depolarizing noise. Extending the analysis to multipartite systems, we show that both the magnitude and range of noise-induced enhancement grow with the number of qubits, indicating collective reinforcement. Finally, we demonstrate through an explicit example that entanglement does not prevent this enhancement: coherent ergotropy may increase under noise even for entangled states. Our results reveal that noise can assist energy storage, challenging the conventional view of noise as purely detrimental and suggesting compatibility between noise-assisted enhancement and fast entanglement-based charging mechanisms in quantum batteries."}
{"id": "2602.19867", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19867", "abs": "https://arxiv.org/abs/2602.19867", "authors": ["Carlo Karam", "Matteo Tacchi", "Mirko Fiacchini"], "title": "A Stochastic Tube-Based MPC Framework with Hard Input Constraints", "comment": null, "summary": "This work presents a stochastic tube-based model predictive control framework that guarantees hard input constraint satisfaction for linear systems subject to unbounded additive disturbances. The approach relies on a structured design of probabilistic reachable sets that explicitly incorporates actuator saturation into the error dynamics and bounds the resulting nonlinearity within a convex embedding. The proposed controller retains the computational efficiency and structural advantages of stochastic tube-based approaches while ensuring state chance constraint satisfaction alongside hard input limits. Recursive feasibility and mean-square stability are established for our scheme, and a numerical example illustrates its effectiveness."}
{"id": "2602.19290", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.19290", "abs": "https://arxiv.org/abs/2602.19290", "authors": ["Kyle Schindl", "Larry Wasserman"], "title": "Distributional Discontinuity Design", "comment": null, "summary": "Regression discontinuity and kink designs are typically analyzed through mean effects, even when treatment changes the shape of the entire outcome distribution. To address this, we introduce distributional discontinuity designs, a framework for estimating causal effects for a scalar outcome at the boundary of a discontinuity in treatment assignment. Our estimand is the Wasserstein distance between limiting conditional outcome distributions; a single scale-interpretable measure of distribution shift. We show that this weakly bounds the average treatment effect, where equality holds if and only if the treatment effect is purely additive; thus, departure from equality measures effect heterogeneity. To further encode effect heterogeneity we show that the Wasserstein distance admits an orthogonal decomposition into squared differences in $L$-moments, thereby quantifying the contribution from location, scale, skewness, and higher-order shape components to the overall distributional distance. Next, we extend this framework to distributional kink designs by evaluating the Wasserstein derivative at a policy kink; this describes the flow of probability mass through the kink. In the case of fuzzy kink designs, we derive new identification results. Finally, we apply our methods on real data by re-analyzing two natural experiments to compare our distributional effects to traditional causal estimands."}
{"id": "2602.19867", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19867", "abs": "https://arxiv.org/abs/2602.19867", "authors": ["Carlo Karam", "Matteo Tacchi", "Mirko Fiacchini"], "title": "A Stochastic Tube-Based MPC Framework with Hard Input Constraints", "comment": null, "summary": "This work presents a stochastic tube-based model predictive control framework that guarantees hard input constraint satisfaction for linear systems subject to unbounded additive disturbances. The approach relies on a structured design of probabilistic reachable sets that explicitly incorporates actuator saturation into the error dynamics and bounds the resulting nonlinearity within a convex embedding. The proposed controller retains the computational efficiency and structural advantages of stochastic tube-based approaches while ensuring state chance constraint satisfaction alongside hard input limits. Recursive feasibility and mean-square stability are established for our scheme, and a numerical example illustrates its effectiveness."}
{"id": "2602.20158", "categories": ["quant-ph", "cond-mat.str-el", "math-ph", "math.QA"], "pdf": "https://arxiv.org/pdf/2602.20158", "abs": "https://arxiv.org/abs/2602.20158", "authors": ["Zijian Liang", "Yu-An Chen"], "title": "Generalized $\\mathbb{Z}_p$ toric codes as qudit low-density parity-check codes", "comment": "9+1 pages, 4 figures", "summary": "We study two-dimensional translation-invariant CSS stabilizer codes over prime-dimensional qudits on the square lattice under twisted boundary conditions, generalizing the Kitaev $\\mathbb{Z}_p$ toric code by augmenting each stabilizer with two additional qudits. Using the Laurent-polynomial formalism, we adapt the Gröbner basis to compute the logical dimension $k$ efficiently, without explicitly constructing large parity-check matrices. We then perform a systematic search over various stabilizer realizations and lattice geometries for $p\\in\\{3,5,7,11\\}$, identifying qudit low-density parity-check codes with the optimal finite-size performance. Representative examples include $[[242,10,22]]_3$ and $[[120,6,20]]_{11}$, both achieving $k d^{2}/n=20$. Across the searched regime, the best observed $k d^{2}$ at fixed $n$ increases with $p$, with an empirical relation $k d^{2} = 0.0541 \\, n^{2}\\ln p + 3.84 \\, n$, compatible with a Bravyi--Poulin--Terhal-type tradeoff when the interaction range grows with system size."}
{"id": "2602.19854", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.19854", "abs": "https://arxiv.org/abs/2602.19854", "authors": ["Zhihao Ge", "Yanan He", "Yajie Yang"], "title": "Optimal $L^2$-norm error estimate of multiphysics finite element method for poroelasticity model and simulating brain edema", "comment": null, "summary": "In this paper, we derive an optimal $L^2$-norm error estimate of the multiphysics finite element method for the poroelasticity model by introducing an auxiliary problem. We show some numerical tests to verify the theoretical result and apply the multiphysics finite element method to simulate the brain edema which caused by abnormal accumulation of cerebrospinal fluid in injured areas. And we investigate the effects of the key physical parameters on brain edema and observed that the permeability $K$ has the biggest influence on intracranial pressure and tissue deformation, Young's modulus $E$ and Poisson ratio $ν$ have little effect on the maximum value of intracranial pressure, but have great effect on the tissue deformation and the developing speed of brain edema."}
{"id": "2602.18898", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18898", "abs": "https://arxiv.org/abs/2602.18898", "authors": ["Tobias Fritz"], "title": "Why measurements are made of effects", "comment": "21 pages, comments welcome", "summary": "Both in quantum theory and in general probabilistic theories, measurements with $n$ outcomes are modelled as $n$-tuples of \\emph{effects} summing up to the unit effect. Why is this the case, and can this assumption be meaningfully relaxed? Here we develop \\emph{generalized measurement theories (GMTs)} as a mathematical framework for physical theories that is complementary to general probabilistic theories, and where this kind of question can be made precise and answered. We then give a definition of \\emph{probabilistic state} on a GMT, prove that measurements are made of effects in every GMT in which the probabilistic states separate the measurements, and also argue that this separation condition is physically well-motivated. Finally, we also discuss when a GMT should be considered classical and characterize GMTs corresponding to Boolean algebras as those that are strongly classical and projective."}
{"id": "2602.19933", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19933", "abs": "https://arxiv.org/abs/2602.19933", "authors": ["Pelin Sekercioglu", "Angela Fontan", "Dimos V. Dimarogonas"], "title": "Edge-based Synchronization over Signed Digraphs with Multiple Leaders", "comment": null, "summary": "We address the edge-based synchronization problem in first-order multi-agent systems containing both cooperative and antagonistic interactions with one or multiple leader groups. The presence of multiple leaders and antagonistic interactions means that the multi-agent typically does not achieve consensus, unless specific conditions (on the number of leaders and on the signed graph) are met, in which case the agents reach a trivial form of consensus. In general, we show that the multi-agent system exhibits a more general form of synchronization, including bipartite consensus and containment. Our approach uses the signed edge-based agreement protocol for signed networks described by signed edge-Laplacian matrices. In particular, in this work, we present new spectral properties of signed edge-Laplacian matrices containing multiple zero eigenvalues and establish global exponential stability of the synchronization errors. Moreover, we compute the equilibrium to which all edge states converge. Numerical simulations validate our theoretical results."}
{"id": "2602.19378", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19378", "abs": "https://arxiv.org/abs/2602.19378", "authors": ["Shuozhi Zuo", "Yixin Wang", "Fan Yang"], "title": "Identification and estimation of the conditional average treatment effect with nonignorable missing covariates, treatment, and outcome", "comment": null, "summary": "Treatment effect heterogeneity is central to policy evaluation, social science, and precision medicine, where interventions can affect individuals differently. In observational studies, covariates, treatment, and outcomes are often only partially observed. When missingness depends on unobserved values (missing not at random; MNAR), standard methods can yield biased estimates of the conditional average treatment effect (CATE). This paper establishes nonparametric identification of the CATE under multivariate MNAR mechanisms that allow covariates, treatment, and outcomes to be MNAR. It also develops nonparametric and parametric estimators and proposes a sensitivity analysis framework for assessing robustness to violations of the missingness assumptions."}
{"id": "2602.19933", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19933", "abs": "https://arxiv.org/abs/2602.19933", "authors": ["Pelin Sekercioglu", "Angela Fontan", "Dimos V. Dimarogonas"], "title": "Edge-based Synchronization over Signed Digraphs with Multiple Leaders", "comment": null, "summary": "We address the edge-based synchronization problem in first-order multi-agent systems containing both cooperative and antagonistic interactions with one or multiple leader groups. The presence of multiple leaders and antagonistic interactions means that the multi-agent typically does not achieve consensus, unless specific conditions (on the number of leaders and on the signed graph) are met, in which case the agents reach a trivial form of consensus. In general, we show that the multi-agent system exhibits a more general form of synchronization, including bipartite consensus and containment. Our approach uses the signed edge-based agreement protocol for signed networks described by signed edge-Laplacian matrices. In particular, in this work, we present new spectral properties of signed edge-Laplacian matrices containing multiple zero eigenvalues and establish global exponential stability of the synchronization errors. Moreover, we compute the equilibrium to which all edge states converge. Numerical simulations validate our theoretical results."}
{"id": "2602.19923", "categories": ["math.NA", "math.DG"], "pdf": "https://arxiv.org/pdf/2602.19923", "abs": "https://arxiv.org/abs/2602.19923", "authors": ["Rasmus Jensen", "Ralf Zimmermann"], "title": "An new polar factor retraction on the Stiefel manifold with closed-form inverse", "comment": "11 pages, 1 figure", "summary": "Retractions are the workhorse in Riemannian computing applications, where computational efficiency is of the essence. This work introduces a new retraction on the compact Stiefel manifold of orthogonal frames. The retraction is second-order accurate under the Euclidean metric and features a closed-form inverse that can be efficiently computed. To the best of our knowledge, this is the first Stiefel retraction with both these properties. A variety of retractions is known on the Stiefel manifold, including the Riemannian exponential map, the polar factor retraction, the QR-retraction and the Cayley retraction, but none of them features a closed-form inverse. The only Stiefel retraction with closed-form inverse that we are aware of is based on quasi-geodesics, but this one is of first order."}
{"id": "2602.18913", "categories": ["quant-ph", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.18913", "abs": "https://arxiv.org/abs/2602.18913", "authors": ["Marvin Kronenberger", "Mihael Erakovic", "Markus Reiher"], "title": "Trotter Error and Orbital Transformations in Quantum Phase Estimation", "comment": "33 pages, 8 figures, 2 tables", "summary": "Quantum computation with Trotter product formulae is straightforward and requires little overhead in terms of logical qubits. The choice of the orbital basis significantly affects circuit depth, with localised orbitals yielding lowest circuit depths. However, literature results point to large Trotter errors incurred by localised orbitals. Here, we therefore investigate the effect of orbital transformations on Trotter error. We consider three strategies to reduce Trotter error by orbital transformation: (i) The a priori selection of an orbital basis that produces low Trotter error. (ii) The derivation of an orbital basis that produces a ground state energy free of Trotter error (as we observed that the Trotter error is a continuous function in the Givens-rotation parameter, from which continuity of this error upon orbital transformation can be deduced). (iii) Application of propagators that change the computational basis between Trotter steps. Our numerical results show that reliably reducing Trotter error by orbital transformations is challenging. General recipes to produce low Trotter errors cannot be easily derived, despite analytical expressions which suggest ways to decrease Trotter error. Importantly, we found that localised orbital bases do not produce large Trotter errors in molecular calculations, which is an important result for efficient QPE set-ups."}
{"id": "2602.20076", "categories": ["eess.SY", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20076", "abs": "https://arxiv.org/abs/2602.20076", "authors": ["Wei Xiao", "Christos Cassandras", "Anni Li"], "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems", "comment": "7 pages", "summary": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods."}
{"id": "2602.19398", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19398", "abs": "https://arxiv.org/abs/2602.19398", "authors": ["Silvia Bacci", "Leonardo Grilli", "Carla Rampichini"], "title": "Variable selection via knockoffs for clustered data", "comment": "11 pages, under submission", "summary": "We extend the knockoffs method for selecting predictors to clustered data (cross-sectional or repeated measures). In the setting of clustered data, variable selection is complex because some predictors are measured at the observation level (level 1), whereas others are measured at the cluster level (level 2), so their values are constant within clusters. The solution we propose is to conduct variable selection separately at the two levels. To this end, we suggest a two-step approach: (i) decompose each level 1 predictor into level 2 and level 1 components by replacing it with the cluster mean and the deviation from the cluster mean; (ii) perform variable selection separately at the two levels, where the level 1 data matrix includes the deviations from the cluster means and the level 2 data matrix includes the cluster means of level 1 predictors and the level 2 predictors. To evaluate the performance of the proposed approach, we conduct a simulation study comparing the sequential knockoff, the derandomized knockoff, and the Lasso. The study shows satisfactory results in terms of false discovery rate and power. All methods fail when applied to the complete data matrix, including both level 1 and level 2 predictors. In contrast, all methods perform better when applied to the level 1 and level 2 data matrices separately. Moreover, the sequential knockoffs method performs substantially better than the Lasso and the derandomized knockoffs. Our proposal to implement the knockoffs method in a clustered data framework is feasible, flexible, and effective."}
{"id": "2602.20076", "categories": ["eess.SY", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20076", "abs": "https://arxiv.org/abs/2602.20076", "authors": ["Wei Xiao", "Christos Cassandras", "Anni Li"], "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems", "comment": "7 pages", "summary": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods."}
{"id": "2602.19963", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.19963", "abs": "https://arxiv.org/abs/2602.19963", "authors": ["Zhengrong Xie", "Zheng Li"], "title": "On the Spectral Properties of Van Leer and AUSM Flux-Vector Splitting Schemes", "comment": null, "summary": "The flux-vector splitting scheme of Van Leer is a cornerstone of computational fluid dynamics, yet its original proof of the eigenvalue sign condition was presented in a condensed form. In this work, we provide a detailed and rigorous analysis of the eigenvalues of the Jacobian matrices associated with the Van Leer splitting for the one-dimensional Euler equations. By constructing the Sturm sequence of the discriminant, we prove that for the admissible parameter range $1 \\le γ\\le 3$, $|M|<1$, and $a>0$, the Jacobian $\\partial F^+/\\partial U$ has one zero eigenvalue and two positive real eigenvalues, confirming Van Leer's original claim. Furthermore, we extend our analysis to two variants of the original AUSM scheme (Advection Upstream Splitting Method) proposed by Liou and Steffen, considering both linear and second-order pressure splittings. For the linear pressure splitting we show that the eigenvalues are not all of the same sign, while for the second-order pressure splitting we prove that all coefficients of the characteristic equation are positive. Numerical experiments reported in the appendix confirm the non-negativity of the discriminant for the AUSM with the second-order pressure splitting, implying that its eigenvalues are real and positive."}
{"id": "2602.18930", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.18930", "abs": "https://arxiv.org/abs/2602.18930", "authors": ["J. L. Montenegro Ferreira"], "title": "Integrable cascaded frequency conversion using the time rescaling shortcut to adiabaticity", "comment": "5 pages, 4 figures", "summary": "In this letter we explore how full frequency conversion can be performed in shorter, integrable devices by using a STIRAP-like protocol modified by the time rescaling shortcut to adiabaticity. We show how the coupled equations for two simultaneous three-wave mixing processes can be written in terms of a STIRAP-like system, which creates robust conversion, albeit requiring long propagation distances inside a bulk crystal or waveguide. We then discuss how the time rescaling (TR) method can be modified to be applied in optical systems, then apply it in the conversion process to create a TR-STIRAP protocol, showing that full conversion is also obtained, but at a fraction of the propagation distance. We also show how the original shaping of the coupling coefficients required by the TR-STIRAP can be approximated by gaussian functions with high conversion fidelity, thus simplifying the experimental implementation. This protocol has the potential to be used in several areas, including the integration of photon sources and efficient detectors for quantum key distribution."}
{"id": "2602.20107", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.20107", "abs": "https://arxiv.org/abs/2602.20107", "authors": ["Anders Hansson", "João Victor Galvão da Mata", "Martin S. Andersen"], "title": "Informativity and Identifiability for Identification of Networks of Dynamical Systems", "comment": "Submitted to IEEE TAC", "summary": "In this paper, we show how informativity and identifiability for networks of dynamical systems can be investigated using Gröbner bases. We provide a sufficient condition for informativity in terms of positive definiteness of the spectrum of external signals and full generic rank of the transfer function relating the external signals to the inputs of the predictor. Moreover, we show how generic local network identifiability can be investigated by computing the dimension of the fiber associated with the closed loop transfer function from external measurable signals to the measured outputs."}
{"id": "2602.19462", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.19462", "abs": "https://arxiv.org/abs/2602.19462", "authors": ["Jinyuan Chang", "Yi Ding", "Zhentao Shi", "Bo Zhang"], "title": "Zero Variance Portfolio", "comment": null, "summary": "When the number of assets is larger than the sample size, the minimum variance portfolio interpolates the training data, delivering pathological zero in-sample variance. We show that if the weights of the zero variance portfolio are learned by a novel ``Ridgelet'' estimator, in a new test data this portfolio enjoys out-of-sample generalizability. It exhibits the double descent phenomenon and can achieve optimal risk in the overparametrized regime when the number of assets dominates the sample size. In contrast, a ``Ridgeless'' estimator which invokes the pseudoinverse fails in-sample interpolation and diverges away from out-of-sample optimality. Extensive simulations and empirical studies demonstrate that the Ridgelet method performs competitively in high-dimensional portfolio optimization."}
{"id": "2602.20107", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.20107", "abs": "https://arxiv.org/abs/2602.20107", "authors": ["Anders Hansson", "João Victor Galvão da Mata", "Martin S. Andersen"], "title": "Informativity and Identifiability for Identification of Networks of Dynamical Systems", "comment": "Submitted to IEEE TAC", "summary": "In this paper, we show how informativity and identifiability for networks of dynamical systems can be investigated using Gröbner bases. We provide a sufficient condition for informativity in terms of positive definiteness of the spectrum of external signals and full generic rank of the transfer function relating the external signals to the inputs of the predictor. Moreover, we show how generic local network identifiability can be investigated by computing the dimension of the fiber associated with the closed loop transfer function from external measurable signals to the measured outputs."}
{"id": "2602.18939", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.18939", "abs": "https://arxiv.org/abs/2602.18939", "authors": ["J. M. Varela", "L. L. Keller", "A. de Oliveira Junior", "D. A. Moreira", "R. Chaves", "R. A. Macêdo"], "title": "Predicting Magic from Very Few Measurements", "comment": "20 pages, 3 figures", "summary": "The nonstabilizerness of quantum states is a necessary resource for universal quantum computation, yet its characterization is notoriously demanding. Quantifying nonstabilizerness typically requires an exponential number of measurements and a doubly exponential classical post-processing cost to evaluate its standard monotones. In this work, we show that nonstabilizerness is, to a large extent, in the eyes of the beholder: it can be witnessed and quantified using any set of $m$ $n$-qubit Pauli measurements, provided the set contains anti-commuting pairs. We introduce a general framework that projects the stabilizer polytope onto the subspace defined by these observables and provide an algorithm that estimates magic from Pauli expectation values with runtime exponential in the number of measurements $m$ and polynomial in the number of qubits $n$. By relating the problem to a stabilizer-restricted variant of the quantum marginal problem, we also prove that deciding membership in the corresponding reduced stabilizer polytope is NP-hard. In particular, unless $\\mathrm{P} = \\mathrm{NP}$, no algorithm polynomial in $m$ can solve the problem in full generality, thus establishing fundamental complexity-theoretic limitations. Finally, we employ our framework to compute nonstabilizerness in different Hamiltonian ground states, demonstrating the practical performance of our method in regimes beyond the reach of existing techniques."}
{"id": "2602.20144", "categories": ["eess.SY", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.20144", "abs": "https://arxiv.org/abs/2602.20144", "authors": ["Zehao Wang", "Mingzhe Han", "Wei Cheng", "Yue-Kai Huang", "Philip Ji", "Denton Wu", "Mahdi Safari", "Flemming Holtorf", "Kenaish AlQubaisi", "Norbert M. Linke", "Danyang Zhuo", "Yiran Chen", "Ting Wang", "Dirk Englund", "Tingjun Chen"], "title": "Agentic AI for Scalable and Robust Optical Systems Control", "comment": null, "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems."}
{"id": "2602.19473", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.19473", "abs": "https://arxiv.org/abs/2602.19473", "authors": ["Zhaoxi Zhang", "Vanda Inacio", "Sara Wade"], "title": "The generalized underlap coefficient with an application in clustering", "comment": null, "summary": "Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables. We establish key properties of UNL and provide an explicit connection to the total variation. We further interpret the UNL as a dependence measure between a group label and variables of interest and compare it with mutual information. We propose an importance sampling estimator of the UNL that can be combined with flexible density estimators. The utility of the UNL for assessing partition-covariate dependence in clustering is highlighted in detail, where it is particularly useful for evaluating the single-weights assumption in covariate-dependent mixture models. Finally we illustrate the application of the UNL in clustering using two real world datasets."}
{"id": "2602.20144", "categories": ["eess.SY", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.20144", "abs": "https://arxiv.org/abs/2602.20144", "authors": ["Zehao Wang", "Mingzhe Han", "Wei Cheng", "Yue-Kai Huang", "Philip Ji", "Denton Wu", "Mahdi Safari", "Flemming Holtorf", "Kenaish AlQubaisi", "Norbert M. Linke", "Danyang Zhuo", "Yiran Chen", "Ting Wang", "Dirk Englund", "Tingjun Chen"], "title": "Agentic AI for Scalable and Robust Optical Systems Control", "comment": null, "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems."}
{"id": "2602.19013", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19013", "abs": "https://arxiv.org/abs/2602.19013", "authors": ["Huibo Hong", "Xiao Xiang", "Runai Quan", "Rongduo Lu", "Qian Zhou", "Dawei Ge", "Liuyan Han", "Bo Liu", "Ru Yuan", "Dechao Zhang", "Yuting Liu", "Bingke Shi", "ZhiGuang Xia", "Xinghua Li", "Mingtao Cao", "Tao Liu", "Ruifang Dong", "Shougang Zhang"], "title": "Co-Propagation of Quantum Time Synchronization and Optical Frequency Transfer over a 122 km Hollow-Core Fiber", "comment": null, "summary": "The co-propagation of quantum and classical signals through shared optical fibers is crucial for scalable quantum networks. However, this coexistence is fundamentally limited by spontaneous Raman scattering (SpRS) from the bright classical light, which generates overwhelming noise that disrupts the single-photon-level quantum signals. Here, we overcome this long-standing challenge by leveraging the inherently ultralow nonlinearity of hollow-core fiber (HCF) to suppress SpRS noise. By operating both the quantum time synchronization (QTS) and classical optical frequency transfer (OFT) signals within the telecom C-band, separated by only ~10 nm, we successfully demonstrate their simultaneous transmission over a 122-km HCF link. With a classical OFT power of 1 mW, the QTS performance shows negligible degradation, maintaining sub-picosecond time stability at 2000 s, while the OFT achieves a fractional frequency instability of 10^-20. Near-sub-picosecond QTS stability is preserved even when the classical power is increased to 3 mW. Furthermore, simulations based on our experimental data indicate that with next-generation low-loss HCF, the platform can tolerate classical powers beyond 10 mW and extend the QTS range to over 500 km. By realizing a unified quantum-classical time-frequency distribution framework, this work establishes HCF as a highly capable and practical platform for future scalable quantum networks."}
{"id": "2602.19341", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19341", "abs": "https://arxiv.org/abs/2602.19341", "authors": ["Xinling Li", "Gioele Zardini"], "title": "Where Should Robotaxis Operate? Strategic Network Design for Autonomous Mobility-on-Demand", "comment": null, "summary": "The emergence of Autonomous Mobility-on-Demand (AMoD) services creates new opportunities to improve the efficiency and reliability of on-demand mobility systems. Unlike human-driven Mobility-on-Demand (MoD), AMoD enables fully centralized fleet control, but it also requires appropriate infrastructure, so that vehicles can operate safely only on a suitably instrumented subnetwork of the roads. Most existing AMoD research focuses on fleet control (matching, rebalancing, ridepooling) on a fixed road network and does not address the joint design of the service network and fleet capacity. In this paper, we formalize this strategic design problem as the Autonomous Mobility-on-Demand Network Design Problem (AMoD-NDP), in which an operator selects an operation subnetwork and routes all passengers, subject to infrastructure and fleet constraints and route-level quality-of-service requirements. We propose a path-based mixed-integer formulation of the AMoD-NDP and develop a column-generation-based algorithm that scales to city-sized networks. The master problem optimizes over a restricted set of paths, while the pricing problem reduces to an elementary shortest path with resource constraints, solved exactly by a tailored label-correcting algorithm. The method provides an explicit certificate of the optimality gap and extends naturally to a robust counterpart under box uncertainty in travel times and demand. Using real-world data from Manhattan, New York City, we show that the framework produces stable and interpretable operation subnetworks, quantifies trade-offs between infrastructure investment and fleet time, and accommodates additional path-level constraints, such as limits on left turns as a proxy for operational risk. These results illustrate how the proposed approach can support strategic planning and policy analysis for future AMoD deployments."}
{"id": "2602.19648", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19648", "abs": "https://arxiv.org/abs/2602.19648", "authors": ["Giuseppe Gismondi", "Rebecca Rivieccio", "Giuseppe Pandolfo"], "title": "Local depth-based classification of directional data", "comment": null, "summary": "Directional data arise in many applications where observations are naturally represented as unit vectors or as observations on the surface of a unit hypersphere. In this context, statistical depth functions provide a center--outward ordering of the data. This work aims at proposing the use of a local notion of data depth function to be applied in the DD-plot (Depth vs. Depth plot) to classify directional data. The proposed method is investigated through an extensive simulation study and two real-data examples."}
{"id": "2602.19341", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19341", "abs": "https://arxiv.org/abs/2602.19341", "authors": ["Xinling Li", "Gioele Zardini"], "title": "Where Should Robotaxis Operate? Strategic Network Design for Autonomous Mobility-on-Demand", "comment": null, "summary": "The emergence of Autonomous Mobility-on-Demand (AMoD) services creates new opportunities to improve the efficiency and reliability of on-demand mobility systems. Unlike human-driven Mobility-on-Demand (MoD), AMoD enables fully centralized fleet control, but it also requires appropriate infrastructure, so that vehicles can operate safely only on a suitably instrumented subnetwork of the roads. Most existing AMoD research focuses on fleet control (matching, rebalancing, ridepooling) on a fixed road network and does not address the joint design of the service network and fleet capacity. In this paper, we formalize this strategic design problem as the Autonomous Mobility-on-Demand Network Design Problem (AMoD-NDP), in which an operator selects an operation subnetwork and routes all passengers, subject to infrastructure and fleet constraints and route-level quality-of-service requirements. We propose a path-based mixed-integer formulation of the AMoD-NDP and develop a column-generation-based algorithm that scales to city-sized networks. The master problem optimizes over a restricted set of paths, while the pricing problem reduces to an elementary shortest path with resource constraints, solved exactly by a tailored label-correcting algorithm. The method provides an explicit certificate of the optimality gap and extends naturally to a robust counterpart under box uncertainty in travel times and demand. Using real-world data from Manhattan, New York City, we show that the framework produces stable and interpretable operation subnetworks, quantifies trade-offs between infrastructure investment and fleet time, and accommodates additional path-level constraints, such as limits on left turns as a proxy for operational risk. These results illustrate how the proposed approach can support strategic planning and policy analysis for future AMoD deployments."}
{"id": "2602.19030", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19030", "abs": "https://arxiv.org/abs/2602.19030", "authors": ["Min Du", "Qian Bin", "Qing-Yang Qiu", "Franco Nori", "Xin-You Lü"], "title": "Exceptional Point Superradiant Lasing with Ultranarrow Linewidth", "comment": "7 pages, 4 figures + Supplemental Material", "summary": "Achieving superradiant lasing with an ultranarrow linewidth is crucial for enhancing atomic clock stability in quantum precision measurement. By employing the exceptional point (EP) property of the system, we demonstrate theoretically superradiant lasing with linewidths in the $μ$Hz range, sustained at the high-power level. This is achieved by incoherently pumping optical lattice clock transitions with ultracold alkaline-earth strontium-87 atoms in the EP of a $\\mathcal{PT}$-symmetric system. Physically, the atomic coherence reaches a maximum in the EP, significantly amplifying the superradiance effect and resulting in superradiant lasing with an ultranarrow linewidth. This linewidth is even three orders of magnitude smaller than that of superradiant lasing in the systems without EP. Our work extends the realm of superradiant lasing by introducing the EP property, and offers promising applications for developing atomic clocks with exceptional stability and accuracy."}
{"id": "2602.19420", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19420", "abs": "https://arxiv.org/abs/2602.19420", "authors": ["Fei Chen", "Jorge Cortés", "Sonia Martínez"], "title": "Enhancing network resilience through topological switching", "comment": "12 pages, 5 figures", "summary": "This work studies how to preemptively increase the resilience of a network by means of time-varying topological actuation. To do this, we focus on linear dynamical systems that are compatible with a given network, and consider policies that switch periodically between the given one and an alternative, topologically-compatible dynamics. In particular, we seek to solve design problems aimed at finding a) the optimal switching schedule between two preselected topologies, and b) an optimal topology and optimal switching schedule. By imposing periodicity, we first provide a metric of resilience in terms of the spectral abscissa of the averaged linear time-invariant dynamics. By restricting our policies to commutative networks, we then show how the optimal scheduling problem reduces to a convex optimization, providing a bound on the net resilience that can be achieved. After this, we find that the optimal, sparse commutative network to switch with is fully disconnected and allocates the spectral sum among the nodes of the network equally. We then impose additional restrictions on topology edge selection, which leads to a biconvex optimization for which certain matrix rank conditions guide the choice of weighting parameters to obtain desirable solutions. Finally, we provide two methods to solve this problem efficiently (based on a McCormick relaxation, and alternating minimization), and illustrate the results in simulations."}
{"id": "2602.19738", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19738", "abs": "https://arxiv.org/abs/2602.19738", "authors": ["Yunping Lu", "Haoang Chi", "Qirui Hu", "Zhiheng Zhang"], "title": "Individualized Causal Effects under Network Interference with Combinatorial Treatments", "comment": null, "summary": "Modern causal decision-making increasingly demands individualized treatment-effect estimation in networks where interventions are high-dimensional, combinatorial vectors. While network interference, effect heterogeneity, and multi-dimensional treatments have been studied separately, their intersection yields an exponentially large intervention space that makes standard identification tools and low-dimensional exposure mappings untenable. We bridge this gap with a unified framework that constructs a \\emph{global potential-outcome emulator} for unit-level inference. Our method combines (1) rooted network configurations to leverage local smoothness, (2) doubly robust orthogonalization to mitigate confounding from network position and covariates, and (3) sparse spectral learning to efficiently estimate response surfaces over the $2^p$-dimensional treatment space. We also decompose networked effects into own-treatment, structural, and interaction components, and provide finite-sample error bounds and asymptotic consistency guarantees. Overall, we show that individualized causal inference remains feasible in high-dimensional networked settings without collapsing the intervention space."}
{"id": "2602.19420", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19420", "abs": "https://arxiv.org/abs/2602.19420", "authors": ["Fei Chen", "Jorge Cortés", "Sonia Martínez"], "title": "Enhancing network resilience through topological switching", "comment": "12 pages, 5 figures", "summary": "This work studies how to preemptively increase the resilience of a network by means of time-varying topological actuation. To do this, we focus on linear dynamical systems that are compatible with a given network, and consider policies that switch periodically between the given one and an alternative, topologically-compatible dynamics. In particular, we seek to solve design problems aimed at finding a) the optimal switching schedule between two preselected topologies, and b) an optimal topology and optimal switching schedule. By imposing periodicity, we first provide a metric of resilience in terms of the spectral abscissa of the averaged linear time-invariant dynamics. By restricting our policies to commutative networks, we then show how the optimal scheduling problem reduces to a convex optimization, providing a bound on the net resilience that can be achieved. After this, we find that the optimal, sparse commutative network to switch with is fully disconnected and allocates the spectral sum among the nodes of the network equally. We then impose additional restrictions on topology edge selection, which leads to a biconvex optimization for which certain matrix rank conditions guide the choice of weighting parameters to obtain desirable solutions. Finally, we provide two methods to solve this problem efficiently (based on a McCormick relaxation, and alternating minimization), and illustrate the results in simulations."}
{"id": "2602.19042", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19042", "abs": "https://arxiv.org/abs/2602.19042", "authors": ["Victor Kasatkin", "Mario Morford-Oberst", "Arian Vezvaee", "Daniel A. Lidar"], "title": "Quantum Error Correction and Dynamical Decoupling: Better Together or Apart?", "comment": "29 pages, 6 figures", "summary": "Quantum error correction (QEC) and dynamical decoupling (DD) are tools for protecting quantum information. A natural goal is to combine them to outperform either approach alone. Such a benefit is not automatic: physical DD can conflict with an encoded subspace, and QEC performance is governed by the errors that survive decoding, not necessarily those DD suppresses. We analyze a hybrid memory cycle where DD is implemented logically (LDD) using normalizer elements of an $[[n,k,d]]$ stabilizer code, followed by a round of syndrome measurement and recovery (or, in the detection setting, postselection on a trivial syndrome). In an effective Pauli model with physical error probability $p$, LDD suppression factor $p_{DD}$, and recovery imperfection rate $p_{QEC}$ (or $p_{QED}$), we derive closed-form entanglement-fidelity expressions for QEC-only, LDD-only, physical DD, and the hybrid LDD+QEC protocol. The formulas are expressed via a small set of code-dependent weight enumerator polynomials, making the role of the decoder and the LDD group explicit. For ideal recovery LDD+QEC outperforms QEC-only iff the conditional fraction of uncorrectable Pauli errors is larger in the LDD-suppressed sector than in the unsuppressed sector. In the low-noise regime, a sufficient design rule guaranteeing hybrid advantage is that LDD suppresses at least one minimum-weight uncorrectable Pauli error for the chosen recovery map. We show how stabilizer-equivalent choices of LDD generators can be used to enforce this condition. We supplement our analysis with numerical results for the $[[7,1,3]]$ Steane code and a $[[13,1,3]]$ code, mapping regions of hybrid-protocol advantage in parameter space beyond the small-$p$ regime. Our work illustrates the need for co-design of the code, decoder, and logical decoupling group, and clarifies the conditions under which the hybrid LDD+QEC protocol is advantageous."}
{"id": "2602.19838", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.19838", "abs": "https://arxiv.org/abs/2602.19838", "authors": ["Kensuke Okada"], "title": "Optimality of the Half-Order Exponent in the Turing-Good Identities for Bayes Factors", "comment": null, "summary": "Bayes factors are widely computed by Monte Carlo, yet heavy-tailed sampling distributions can make numerical validation unreliable. The Turing--Good identities provide exact moment equalities for powers of a Bayes factor (a density ratio). When these identities are used as Good-check diagnostics, the power choice becomes a statistical design parameter. We develop a nonasymptotic variance theory for Monte Carlo evaluation of the identities and show that the half-order (square-root) power is uniquely minimax-stable: it equalizes variability across the two model orientations and is the only choice that guarantees finite second moments in a distribution-free worst-case sense over all mutually absolutely continuous model pairs. This yields a balanced two-sample half-order diagnostic that is symmetric in model labeling and has a uniform variance bound at fixed computational budget; in small-overlap regimes it is guaranteed to be no less efficient than the standard one-sided Turing check. Simulations for binomial Bayes factor workflows illustrate stable finite-sample behavior and sensitivity to simulator--evaluator mismatches. We further connect the half-order overlap viewpoint to stable primitives for normalizing-constant ratios and importance-sampling degeneracy summaries."}
{"id": "2602.19057", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.19057", "abs": "https://arxiv.org/abs/2602.19057", "authors": ["Mohammad Rowshan"], "title": "Structural Analysis of Directional qLDPC Codes", "comment": "22 pages, 13 figures, 4 tables", "summary": "Directional codes, recently introduced by Gehér--Byfield--Ruban \\cite{Geher2025Directional}, constitute a hardware-motivated family of quantum low-density parity-check (qLDPC) codes. These codes are defined by stabilizers measured by ancilla qubits executing a fixed \\emph{direction word} (route) on square- or hex-grid connectivity. In this work, we develop a comprehensive \\emph{word-first} analysis framework for route-generated, translation-invariant CSS codes on rectangular tori. Under this framework, a direction word $W$ deterministically induces a finite support pattern $P(W)$, from which we analytically derive: (i)~a closed-form route-to-support map; (ii)~the odd-multiplicity difference lattice $L(W)$ that classifies commutation-compatible $X/Z$ layouts; and (iii)~conservative finite-torus admissibility criteria. Furthermore, we provide: (iv)~a rigorous word equivalence and canonicalization theory (incorporating dihedral lattice symmetries, reversal/inversion, and cyclic shifts) to enable symmetry-quotiented searches; (v)~an ``inverse problem'' criterion to determine when a translation-invariant support pattern is realizable by a single route, including reconstruction and non-realizability certificates; and (vi)~a quasi-cyclic (group-algebra) reduction for row-periodic layouts that explains the sensitivity of code dimension $k$ to boundary conditions. As a case study, we analyze the word $W=\\texttt{NE$^2$NE$^2$N}$ end-to-end. We provide explicit stabilizer dependencies, commuting-operator motifs, and an exact criterion for dimension collapse on thin rectangles: for $(L_x, L_y) = (2d, d)$ with row alternation, we find $k=4$ if $6 \\mid d$, and $k=0$ otherwise."}
{"id": "2602.19851", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19851", "abs": "https://arxiv.org/abs/2602.19851", "authors": ["Xinyan Su", "Jiacan Gao", "Mingyuan Ma", "Xiao Xu", "Xinrui Wan", "Tianqi Gu", "Enyun Yu", "Jiecheng Guo", "Zhiheng Zhang"], "title": "Orthogonal Uplift Learning with Permutation-Invariant Representations for Combinatorial Treatments", "comment": null, "summary": "We study uplift estimation for combinatorial treatments. Uplift measures the pure incremental causal effect of an intervention (e.g., sending a coupon or a marketing message) on user behavior, modeled as a conditional individual treatment effect. Many real-world interventions are combinatorial: a treatment is a policy that specifies context-dependent action distributions rather than a single atomic label. Although recent work considers structured treatments, most methods rely on categorical or opaque encodings, limiting robustness and generalization to rare or newly deployed policies. We propose an uplift estimation framework that aligns treatment representation with causal semantics. Each policy is represented by the mixture it induces over contextaction components and embedded via a permutation-invariant aggregation. This representation is integrated into an orthogonalized low-rank uplift model, extending Robinson-style decompositions to learned, vector-valued treatments. We show that the resulting estimator is expressive for policy-induced causal effects, orthogonally robust to nuisance estimation errors, and stable under small policy perturbations. Experiments on large-scale randomized platform data demonstrate improved uplift accuracy and stability in long-tailed policy regimes"}
{"id": "2602.19103", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19103", "abs": "https://arxiv.org/abs/2602.19103", "authors": ["Md Manirul Ali", "Sovik Roy", "Dipankar Home"], "title": "Near-perfect Noisy Quantum State Teleportation", "comment": "28 pages, 3 figures", "summary": "Achieving high fidelity of quantum teleportation (QT) in a noisy environment is an essential requirement for its real-world applications. To this end, we devise a distinctive protocol for ensuring teleportation fidelity {\\it close to unity}, hinging essentially on the timing of Alice's Bell-basis measurement (BM) dependent on the choice of Bob's local noise parameters, but is independent of Alice's local noise. Our scheme is enabled by Alice communicating to Bob only two of the BM outcomes corresponding to the states that are decoherence-free under common dephasing at Alice's wing. On the other hand, Bob is asked to discard the states of his qubit for the other two BM outcomes in order to maximize fidelity of the teleported state. This ensures the teleportation fidelity's independence of noise parameters in Alice's wing. We formulate the protocol in terms of a generic two-level quantum system, subjected to non-Markovian dephasing noise, applicable for any pure maximally/non-maximally entangled state as well as a Werner-type mixed state as resource. Notably, we show that high fidelity is achievable even using resource states with small values of the entanglement measure. Remarkably, even within the local regime of Werner states, where Bell-CHSH inequalities are not violated, the teleportation fidelity remains significantly high. Finally, we discuss the empirical feasibility of our scheme using photonic qubits."}
{"id": "2602.19922", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19922", "abs": "https://arxiv.org/abs/2602.19922", "authors": ["Mengyan Li", "Xiaoou Li", "Kenneth D Mandl", "Tianxi Cai"], "title": "Transfer Learning with Network Embeddings under Structured Missingness", "comment": null, "summary": "Modern data-driven applications increasingly rely on large, heterogeneous datasets collected across multiple sites. Differences in data availability, feature representation, and underlying populations often induce structured missingness, complicating efforts to transfer information from data-rich settings to those with limited data. Many transfer learning methods overlook this structure, limiting their ability to capture meaningful relationships across sites. We propose TransNEST (Transfer learning with Network Embeddings under STructured missingness), a framework that integrates graphical data from source and target sites with prior group structure to construct and refine network embeddings. TransNEST accommodates site-specific features, captures within-group heterogeneity and between-site differences adaptively, and improves embedding estimation under partial feature overlap. We establish the convergence rate for the TransNEST estimator and demonstrate strong finite-sample performance in simulations. We apply TransNEST to a multi-site electronic health record study, transferring feature embeddings from a general hospital system to a pediatric hospital system. Using a hierarchical ontology structure, TransNEST improves pediatric embeddings and supports more accurate pediatric knowledge extraction, achieving the best accuracy for identifying pediatric-specific relational feature pairs compared with benchmark methods."}
{"id": "2602.19114", "categories": ["quant-ph", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19114", "abs": "https://arxiv.org/abs/2602.19114", "authors": ["Hongdong Zhu", "Qi Gao", "Yin Ma", "Shaobo Chen", "Haixu Liu", "Fengao Wang", "Tinglan Wang", "Chang Wu", "Kai Wen"], "title": "Kaiwu-PyTorch-Plugin: Bridging Deep Learning and Photonic Quantum Computing for Energy-Based Models and Active Sample Selection", "comment": null, "summary": "This paper introduces the Kaiwu-PyTorch-Plugin (KPP) to bridge Deep Learning and Photonic Quantum Computing across multiple dimensions. KPP integrates the Coherent Ising Machine into the PyTorch ecosystem, addressing classical inefficiencies in Energy-Based Models. The framework facilitates quantum integration in three key aspects: accelerating Boltzmann sampling, optimizing training data via Active Sampling, and constructing hybrid architectures like QBM-VAE and Q-Diffusion. Empirical results on single-cell and OpenWebText datasets demonstrate KPPs ability to achieve SOTA performance, validating a comprehensive quantum-classical paradigm."}
{"id": "2602.19988", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.19988", "abs": "https://arxiv.org/abs/2602.19988", "authors": ["Yi Xu", "Yeonwoo Rho"], "title": "Change point analysis of high-dimensional data using random projections", "comment": null, "summary": "This paper develops a novel change point identification method for high-dimensional data using random projections. By projecting high-dimensional time series into a one-dimensional space, we are able to leverage the rich literature for univariate time series. We propose applying random projections multiple times and then combining the univariate test results using existing multiple comparison methods. Simulation results suggest that the proposed method tends to have better size and power, with more accurate location estimation. At the same time, random projections may introduce variability in the estimated locations. To enhance stability in practice, we recommend repeating the procedure, and using the mode of the estimated locations as a guide for the final change point estimate. An application to an Australian temperature dataset is presented. This study, though limited to the single change point setting, demonstrates the usefulness of random projections in change point analysis."}
{"id": "2602.19222", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19222", "abs": "https://arxiv.org/abs/2602.19222", "authors": ["Subhra Mudli", "Bimalendu Deb"], "title": "Ion-atom two-qubit quantum gate based on phonon blockade", "comment": null, "summary": "In a previous paper [S. Mudli {\\it et al.} Phys. Rev. A 110, 062618 (2024)], it was shown that a trapped ion can mediate interaction between two largely separated Rydberg atoms, and this mediated interaction can be leveraged to perform a universal two-qubit gate operation between neutral atom qubits in optical tweezers. In this paper, we demonstrate the universal two-qubit CNOT gate with high fidelity between an ionic and an atomic qubit relying on Rydberg excitation of the atom and the resulting phonon blockade in the motional states of the harmonically trapped ion. The phonon blockade arises due to strong ion-atom interaction when the atom is excited to a Rydberg state. These demonstrations suggest that an ion-atom hybrid system can serve as a resourceful platform or module for quantum computing and quantum networking as it can utilize the best features of charged as well as neutral atom qubits."}
{"id": "2602.20029", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20029", "abs": "https://arxiv.org/abs/2602.20029", "authors": ["Yueyun Zhu", "Steven Golovkine", "Norma Bargary", "Andrew J. Simpkin"], "title": "Covariance estimation for derivatives of functional data using an additive penalty in P-splines", "comment": null, "summary": "P-splines provide a flexible and computationally efficient smoothing framework and are commonly used for derivative estimation in functional data. Including an additive penalty term in P-splines has been shown to improve estimates of derivatives. We propose a method which incorporates the fast covariance estimation (FACE) algorithm with an additive penalty in P-splines. The proposed method is used to estimate derivatives of covariance for functional data, which play an important role in derivative-based functional principal component analysis (FPCA). Following this, we provide an algorithm for estimating the eigenfunctions and their corresponding scores in derivative-based FPCA. For comparison, we evaluate our algorithm against an existing function \\texttt{FPCAder()} in simulation. In addition, we extend the algorithm to multivariate cases, referred to as derivative multivariate functional principal component analysis (DMFPCA). DMFPCA is applied to joint angles in human movement data, where the derivative-based scores demonstrate strong performance in distinguishing locomotion tasks."}
{"id": "2602.19250", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19250", "abs": "https://arxiv.org/abs/2602.19250", "authors": ["Carlos Navas-Merlo", "Juan Carlos García-Escartín"], "title": "Eigenstate-assisted realization of general quantum controlled unitaries with a fixed cost", "comment": "6 pages. 2 Figures. Comments welcome", "summary": "Controlled unitary gates are a basic element in many quantum algorithms. Converting a general unitary $U$ with a known decomposition into its controlled version, controlled-$U$, can introduce a large overhead in terms of the depth of the circuit. We present a general method to take any unitary $U$ into controlled-$U$ using a fixed circuit with 4 CNOT gates and 2 Toffoli gates per qubit. For $n$-qubit unitaries and one control qubit, we require $2n+1$ qubits and a circuit that can generate an eigenstate of $U$, for which there are many cost-effective known algorithms. The method also works for any black block implementation of $U$, achieving a constant-depth realization independent of its decomposition. We illustrate its use in the Hadamard test and discuss applications to variational and quantum machine-learning algorithms."}
{"id": "2602.20118", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20118", "abs": "https://arxiv.org/abs/2602.20118", "authors": ["Caleb Hiltunen", "Yeonwoo Rho"], "title": "Improving the Power of Bonferroni Adjustments under Joint Normality and Exchangeability", "comment": null, "summary": "Bonferroni's correction is a popular tool to address multiplicity but is notorious for its low power when tests are dependent. This paper proposes a practical modification of Bonferroni's correction when test statistics are jointly normal and exchangeable. This method is intuitive to practitioners and achieves higher power in sparse alternatives, as our simulations suggest. We also prove that this method successfully controls the family-wise error rate at any significance level."}
{"id": "2602.19259", "categories": ["quant-ph", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.19259", "abs": "https://arxiv.org/abs/2602.19259", "authors": ["Sajjad Hashemian"], "title": "Quantum Sketches, Hashing, and Approximate Nearest Neighbors", "comment": "8 pages, 1 figure, submitted to journal of Information and Computation", "summary": "Motivated by Johnson--Lindenstrauss dimension reduction, amplitude encoding, and the view of measurements as hash-like primitives, one might hope to compress an $n$-point approximate nearest neighbor (ANN) data structure into $O(\\log n)$ qubits. We rule out this possibility in a broad quantum sketch model, the dataset $P$ is encoded as an $m$-qubit state $ρ_P$, and each query is answered by an arbitrary query-dependent measurement on a fresh copy of $ρ_P$. For every approximation factor $c\\ge 1$ and constant success probability $p>1/2$, we exhibit $n$-point instances in Hamming space $\\{0,1\\}^d$ with $d=Θ(\\log n)$ for which any such sketch requires $m=Ω(n)$ qubits, via a reduction to quantum random access codes and Nayak's lower bound. These memory lower bounds coexist with potential quantum query-time gains and in candidate-scanning abstractions of hashing-based ANN, amplitude amplification yields a quadratic reduction in candidate checks, which is essentially optimal by Grover/BBBV-type bounds."}
{"id": "2602.20151", "categories": ["stat.ME", "cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20151", "abs": "https://arxiv.org/abs/2602.20151", "authors": ["Anastasios N. Angelopoulos"], "title": "Conformal Risk Control for Non-Monotonic Losses", "comment": null, "summary": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization."}
{"id": "2602.19280", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.19280", "abs": "https://arxiv.org/abs/2602.19280", "authors": ["Devanshu Shekhar", "Pragya Shukla"], "title": "Entanglement dynamics of many-body quantum states: sensitivity to system conditions and a hidden universality", "comment": "36 pages (double spacing), 8 figures. arXiv admin note: text overlap with arXiv:2503.01989", "summary": "We consider physical Hamiltonians that can be represented by the multiparametric Gaussian ensembles, theoretically derive the state ensembles for its eigenstates and analyze the effect of varying system conditions on its bipartite entanglement entropy. Our approach leads to a single parametric based common mathematical formulation for the evolution of the entanglement statistics of different states of a given Hamiltonian or different Hamiltonians subjected to same symmetry constraints. The parameter turns out to be a single functional of the system parameters and thereby reveals a deep web of connection hidden underneath different quantum states."}
{"id": "2602.19486", "categories": ["eess.SY", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.19486", "abs": "https://arxiv.org/abs/2602.19486", "authors": ["Xinyi Yi", "Ioannis Lestas"], "title": "A mixed Hinfty-Passivity approach for Leveraging District Heating Systems as Frequency Ancillary Service in Electric Power Systems", "comment": null, "summary": "This paper introduces a mixed H-infinity-passivity framework that enables district heating systems (DHSs) with heat pumps to support electric-grid frequency regulation. The analysis illustrates how the DHS regulator influences coupled electro-thermal frequency dynamics and provides LMI conditions for efficient controller design. We also present a disturbance-independent temperature regulator that ensures stability and robustness against heat-demand uncertainty. Simulations demonstrate improved frequency-control dynamics in the electrical power grid while maintaining good thermal performance in the DHS."}
{"id": "2602.19288", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19288", "abs": "https://arxiv.org/abs/2602.19288", "authors": ["Sanjeev Kumar", "Hendrik Weimer"], "title": "Self-correction phase transition in the dissipative toric code", "comment": "6 pages, 5 figures", "summary": "We analyze a time-continuous version of a cellular automaton decoder for the toric code in the form of a Lindblad master equation. In this setting, a self-correcting quantum memory becomes a thermodynamical phase of the steady state, which manifests itself through the steady state being topologically ordered. We compute the steady state phase diagram, finding a competition between the error correction rate and the update rate for the classical field of the cellular automaton. Strikingly, we find that self-correction of errors is possible even in situations where conventional quantum error correction does not have a finite threshold."}
{"id": "2602.20007", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20007", "abs": "https://arxiv.org/abs/2602.20007", "authors": ["Andrew T. Karl"], "title": "Order Dependence in the Moving-Range Sigma Estimator: A Total-Variance Decomposition", "comment": null, "summary": "In Individuals and Moving Range (I-MR) charts, the process standard deviation is often estimated by the span-2 average moving range, scaled by the usual constant $d_2$. Unlike the sample standard deviation, this estimator depends on the observation order: permuting the values can change the average moving range. We make this dependence explicit by modeling the order as an independent uniformly random permutation. A direct application of the law of total variance then decomposes its variance into a component due to ordering and a component due to the realized values. Averaging over all permutations yields a simple order-invariant baseline for the moving-range estimator: the sample Gini mean difference divided by $d_2$. Simulations quantify the resulting fraction of variance attributable to ordering under i.i.d. Normal sampling, and two NIST examples illustrate a typical ordering and an ordering with strong serial structure relative to random permutations of the same values."}
{"id": "2602.19306", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19306", "abs": "https://arxiv.org/abs/2602.19306", "authors": ["Lorenzo Braccini", "Alessio Serafini", "Sougato Bose"], "title": "Mass-Independent Gravitationally Induced Entanglement", "comment": "13 pages, 5 figures; Any feedback is welcome", "summary": "We analytically solve the entangling quantum dynamics of two interacting Stern-Gerlach Interferometers~(SGI). Each SGI exploits an operator-valued force applied by a qubit to create and recombine a non-Gaussian state of matter. The entangling phase between the two qubits generated by the leading-order gravitational interaction of the massive degrees of freedom is found to be mass-independent, both for unitary and open dynamics, irrespective of the temperature and squeezing of the initial states. Further, we show that the solution of the four interferometric paths reveals that the mere presence of the interaction does not allow for a perfect recombination of the centre of mass. This second-order effect, alongside higher-order interaction terms, can be used to bound the mass from above and below, thus restricting the experiment's regime to mesoscopic masses. By solving the open dynamics which includes diffusion and dephasing with initial squeezed thermal states, the bounds are tightened by the inclusion of realistic experimental noise. We discuss diamagnetic levitated masses with embedded NV-centres as a specific physical implementation."}
{"id": "2602.20071", "categories": ["math.ST", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20071", "abs": "https://arxiv.org/abs/2602.20071", "authors": ["A. Martín Andrés", "M. Álvarez Hernández"], "title": "Estimators of different delta coefficients based on the unbiased estimator of the expected proportions of agreements", "comment": null, "summary": "To measure the degree of agreement between two observers that independently classify $n$ subjects within $K$ categories, it is common to use different kappa type coefficients, the most common of which is the $κ_C$ coefficient (Cohen's kappa). As $κ_C$ has some weaknesses -such as its poor performance with highly unbalanced marginal distributions-, the $Δ$ coefficient is sometimes used, based on the $delta$ response model. This model allows us to obtain other parameters like: (a) the $α_i$ contribution of each $i$ category to the value of the global agreement $Δ=\\sum α_i$; and (b) the consistency $\\mathcal{S}_i$ in the category $i$ (degree of agreement in the category $i$), a more appropriate parameter than the kappa value obtained by collapsing the data into the category $i$. It has recently been shown that the classic estimator $\\hatκ_C$ underestimates $κ_C$, having obtained a new estimator $\\hatκ_{CU}$ which is less biased. This article demonstrates that something similar happens to the known estimators $\\hatΔ$, $\\hatα_i$, and $\\hat{\\mathcal{S}}_i$ of $Δ$, $α_i$ and $\\mathcal{S}_i$ (respectively), proposes new and less biased estimators $\\hatΔ_U$, $\\hatα_{iU}$, and $\\hat{\\mathcal{S}}_{iU}$, determines their variances, analyses the behaviour of all estimators, and concludes that the new estimators should be used when $n$ or $K$ are small (at least when $n\\leq 50$ or $K\\leq 3$). Additionally, the case where one of the raters is a gold standard is contemplated, in which situation two new parameters arise: the $conformity$ (the rater's capability to recognize a subject in the category $i$) and the $predictivity$ (the reliability of a response $i$ by the rater)."}
{"id": "2602.19307", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19307", "abs": "https://arxiv.org/abs/2602.19307", "authors": ["Christian Candeago", "Paolo Da Rold", "Michele Grossi", "Pawel Horodecki", "Antonio Mandarino"], "title": "Learning partial transpose signatures in qubit ququart states from a few measurements", "comment": null, "summary": "Higher-dimensional quantum systems are attracting interest for improving quantum protocol performance by increasing memory space. Characterizing quantum resources of such systems is fundamental but experimentally costly. We tackle the first non-trivial example: a qubit-ququart system, focusing on partial-transpose spectral classification. Entanglement distillation extracts maximally entangled states from noisy resources, but determining distillability typically requires full state tomography, experimentally prohibitive for high-dimensional systems. We explore a machine learning framework to classify distillable bipartite quantum states using fewer measurements than complete tomography. Our approach employs the PPT criterion, categorizing states by negative eigenvalues in the partial transpose. We use various ML algorithms, including Support Vector Machines, Random Forest, and Artificial Neural Networks, with features from fixed measurements and learnable observables. Results show learnable observables consistently outperform Collective Measurement Witnesses methods. While all models distinguish between non-distillable (PPT) and distillable (NPT) states, differentiating NPT subclasses remains challenging, underscoring the intricate Hilbert space geometry. This work provides an experimentally friendly tool for distillability verification in high-dimensional quantum systems without full state reconstruction"}
{"id": "2602.20115", "categories": ["math.ST", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20115", "abs": "https://arxiv.org/abs/2602.20115", "authors": ["Nikolaos Ignatiadis", "Sid Kankanala"], "title": "Compound decisions and empirical Bayes via Bayesian nonparametrics", "comment": "34 pages", "summary": "We study the Gaussian sequence compound decision problem and analyze a Bayesian nonparametric estimator from an empirical Bayes, regret-based perspective. Motivated by sharp results for the classical nonparametric maximum likelihood estimator (NPMLE), we ask whether an analogous guarantee can be obtained using a standard Bayesian nonparametric prior. We show that a Dirichlet-process-based Bayesian procedure achieves near-optimal regret bounds. Our main results are stated in the compound decision framework, where the mean vector is treated as fixed, while we also provide parallel guarantees under a hierarchical model in which the means are drawn from a true unknown prior distribution. The posterior mean Bayes rule is, a fortiori, admissible, whereas we show that the NPMLE plug-in rule is inadmissible."}
{"id": "2602.19377", "categories": ["quant-ph", "gr-qc"], "pdf": "https://arxiv.org/pdf/2602.19377", "abs": "https://arxiv.org/abs/2602.19377", "authors": ["Nicolò Piccione"], "title": "Gravitational Poissonian Spontaneous Localization Model of Hybrid Quantum-Classical Newtonian Gravity: Energy Increase and Experimental Bounds", "comment": "11 pages plus appendices, 8 figures", "summary": "The Gravitational Poissonian Spontaneous Localization (GPSL) model is a hybrid classical-quantum framework in which Newtonian gravity emerges from stochastic collapses of a smeared mass-density operator. Consistency of the hybrid dynamics entails momentum diffusion and, hence, spontaneous heating. Without smearing, which enters both the collapse (measurement) and gravitational-feedback components of the dynamics, the heating rate would be divergent. Previous work assumed identical smearings for both components. Here, we treat the general case of distinct spatial smearings $g_{r_C} (\\mathbf{x})$ and $g_{r_G} (\\mathbf{x})$, characterized, respectively, by length scales $r_C$ and $r_G$. We characterize the spontaneous heating rate for arbitrary $g_{r_C} (\\mathbf{x})$ and $g_{r_G} (\\mathbf{x})$, and then discuss which smearing profiles minimize the spontaneous heating rate in relevant physical situations. Remarkably, there are situations in which, while the measurement noise remains the same, allowing $g_{r_G} (\\mathbf{x}) \\neq g_{r_C} (\\mathbf{x})$ may reduce the feedback-induced spontaneous heating by more than 60 orders of magnitude already for $r_G = 10 r_C$. Finally, we use our results to estimate the spontaneous heating rate of neutron stars and to set new lower bounds on the model's parameters by comparing the theoretical predictions with astronomical data on temperature, radius, and mass of neutron stars."}
{"id": "2602.19387", "categories": ["quant-ph", "cs.ET", "hep-ex", "hep-lat", "hep-ph"], "pdf": "https://arxiv.org/pdf/2602.19387", "abs": "https://arxiv.org/abs/2602.19387", "authors": ["Marco Knipfer", "Alexander Roman", "Konstantin T. Matchev", "Katia Matcheva", "Sergei Gleyzer"], "title": "AI Agents for Variational Quantum Circuit Design", "comment": "43 pages, 12 figures", "summary": "Variational quantum circuits (VQCs) constitute a central building block of near-term quantum machine learning (QML), yet the principled design of expressive and trainable architectures remains a major open challenge. The VQC design space grows combinatorially with the number of qubits, layers, entanglement structures, and gate parameterizations, rendering manual circuit construction inefficient and often suboptimal. We introduce an autonomous agent-based framework for VQC architecture search that integrates high-level reasoning with a quantum simulation environment. The agent proposes candidate circuit architectures, evaluates them through fully automated training and validation pipelines, and iteratively improves its design strategy via performance-driven feedback. Empirically, we show that the agent autonomously evolves circuit architectures from simple initial ansätze toward increasingly expressive designs, progressively trying to improve task performance. This demonstrates that agentic AI can effectively navigate and refine the VQC design landscape with minimal human intervention, providing a scalable methodology for automated quantum model development in the Noisy Intermediate-Scale Quantum (NISQ) regime."}
{"id": "2602.19397", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19397", "abs": "https://arxiv.org/abs/2602.19397", "authors": ["Min Namkung", "Hyang-Tag Lim"], "title": "Contextuality-enhanced quantum state discrimination under fixed failure probability", "comment": "12 pages, 4 figures", "summary": "Quantum state discrimination enables the accurate identification of quantum states, which are generally nonorthogonal. Among various strategies, minimum-error discrimination and unambiguous state discrimination exhibit contextuality-enhanced success probabilities that surpass classical bounds, offering significant advantages for quantum sensing and communication. However, in practice, both error and failure outcomes can occur, suggesting the need for a unified strategy that incorporates both aspects while exploring the potential for contextuality enhancement. In this work, we theoretically demonstrate contextuality enhancement in quantum state discrimination under a fixed failure probability. We show that this enhancement disappears within a certain intermediate range of failure probabilities--a phenomenon absent in conventional strategies, where both minimum-error and unambiguous discrimination consistently outperform the noncontextual bound for equal priors. Moreover, we analyze how the existence of this non-enhancement region depends on the confusability of the quantum states, which corresponds to their fidelity in a quantum model. We further extend the discussion to the noisy state discrimination, which even encompasses the maximal-confidence discrimination. In this extended discussion, we observe that the non-enhancement region tends to disappear with increasing noise strength."}
{"id": "2602.19405", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19405", "abs": "https://arxiv.org/abs/2602.19405", "authors": ["Jean-Baptiste Waring", "Sébastien Le Beux", "Christophe Pere"], "title": "Robust GHZ State Preparation via Majority-Voted Boundary Measurements", "comment": "6 pages, 2 figures, submitted to NEWCAS 2026", "summary": "Preparing high-fidelity Greenberger-Horne-Zeilinger (GHZ) states on noisy quantum hardware remains challenging due to cumulative gate errors and decoherence. We introduce Group-Majority-Voting (Group-MV), a dynamic-circuit protocol that partitions arbitrary coupling graphs, prepares local GHZ states in parallel, and fuses them via majority-voted mid-circuit measurements. The majority vote over redundant boundary links mitigates measurement errors that would otherwise propagate through classical feedforward. We evaluate Group-MV on simulated Heavy-hex and Grid topologies for 30 through 60 qubits under a realistic noise regime. Group-MV generalizes to arbitrary GHZ sizes on arbitrary coupling topologies, achieving 2.4x higher fidelity than the Line Dynamic method while tracking the unitary baseline within 3%."}
{"id": "2602.19448", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19448", "abs": "https://arxiv.org/abs/2602.19448", "authors": ["Sangchul Oh"], "title": "Subsystem Statistics and Conditional Self-Similarity of Random Quantum States", "comment": "6 pages, 3 figures, submitted to a journal", "summary": "We analytically derive the bit-string probability distributions of subsystems of random pure states and depolarized random states using the Dirichlet distribution. We identify the exact Beta distribution as the universal statistical law of random quantum states, providing a unified finite-size description of full-system, subsystem, and conditional statistics. In the presence of depolarizing noise, these distributions are scaled and shifted by the noise strength, producing a noise-induced gap in their support. Remarkably, we prove that random states exhibit exact conditional self-similarity: the distribution of subsystem bit-string probabilities conditioned on specific outcomes of the complementary subsystem is identical to that of the full system. This hidden scale invariance enables the exact restoration of the full-system statistics from the marginalized Beta distribution via post-selection, and persists under depolarizing noise. Our results uncover a fundamental symmetry of Hilbert space and provide a scalable, rigorous framework for validating random circuit sampling via subsystem or conditional cross-entropy benchmarking."}
{"id": "2602.19496", "categories": ["quant-ph", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.19496", "abs": "https://arxiv.org/abs/2602.19496", "authors": ["Mohammad Aamir Sohail", "Ranga R. Sudharshan", "S. Sandeep Pradhan", "Arvind Rao"], "title": "Quantum Hamiltonian Learning using Time-Resolved Measurement Data and its Application to Gene Regulatory Network Inference", "comment": null, "summary": "We present a new Hamiltonian-learning framework based on time-resolved measurement data from a fixed local IC-POVM and its application to inferring gene regulatory networks. We introduce the quantum Hamiltonian-based gene-expression model (QHGM), in which gene interactions are encoded as a parameterized Hamiltonian that governs gene expression evolution over pseudotime. We derive finite-sample recovery guarantees and establish upper bounds on the number of time and measurement samples required for accurate parameter estimation with high probability, scaling polynomially with system size. To recover the QHGM parameters, we develop a scalable variational learning algorithm based on empirical risk minimization. Our method recovers network structure efficiently on synthetic benchmarks and reveals novel, biologically plausible regulatory connections in Glioblastoma single-cell RNA sequencing data, highlighting its potential in cancer research. This framework opens new directions for applying quantum-like modeling to biological systems beyond the limits of classical inference."}
{"id": "2602.19556", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19556", "abs": "https://arxiv.org/abs/2602.19556", "authors": ["Jeongbin Jo"], "title": "Deterministic Ground State Preparation via Power-Cosine Filtering of Time Evolution Operators", "comment": null, "summary": "The deterministic preparation of quantum many-body ground states is essential for advanced quantum simulation, yet optimal algorithms often require prohibitive hardware resources. Here, we propose a highly efficient, non-variational protocol for ground state preparation using a Power-Cosine quantum signal processing (QSP) filter. By eschewing complex block-encoding techniques, our method directly utilizes coherent time-evolution operators controlled by a single ancillary qubit. The integration of mid-circuit measurement and reset (MCMR) drastically minimizes spatial overhead, translating iterative non-unitary filtering into deep temporal coherence. We analytically demonstrate that this approach achieves exponential suppression of excited states with a circuit depth scaling of $\\mathcal{O}(Δ^{-2}\\log(1/ε))$, prioritizing implementational simplicity over optimal asymptotic complexity. Numerical simulations on the 1D Heisenberg XYZ model validate the theoretical soundness and shot-noise resilience of our method. Furthermore, an advantage analysis reveals that our protocol exponentially outperforms standard Trotterized Adiabatic State Preparation (TASP) at equivalent circuit depths. This single-ancilla framework provides a highly practical and deterministic pathway for many-body ground state preparation on Early Fault-Tolerant (EFT) quantum architectures."}
{"id": "2602.19558", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.19558", "abs": "https://arxiv.org/abs/2602.19558", "authors": ["Ben T. McDonough", "Jian-Hao Zhang", "Victor V. Albert", "Andrew Lucas"], "title": "Calderbank-Shor-Steane codes on group-valued qudits", "comment": "44 pages, 17 figures", "summary": "Calderbank-Shor-Steane (CSS) codes are a versatile quantum error-correcting family built out of commuting $X$- and $Z$-type checks. We introduce CSS-like codes on $G$-valued qudits for any finite group $G$ that reduce to qubit CSS codes for $G = \\mathbb{Z}_2$ yet generalize the Kitaev quantum double model for general groups. The $X$-checks of our group-CSS codes correspond to left and/or right multiplication by group elements, while $Z$-checks project onto solutions to group word equations. We describe quantum-double models on oriented two-dimensional CW complexes (which need not cellulate a manifold) and prove that, when $G$ is non-Abelian and simple, every $G$-covariant group-CSS code with suitably upper-bounded $Z$-check weight and lower-bounded $Z$-distance reduces to a CW quantum double. We describe the codespace and logical operators of CW quantum doubles via the same intuition used to obtain logical structure of surface codes. We obtain distance bounds for codes on non-Abelian simple groups from the graph underlying the CW complex, and construct intrinsically non-Abelian code families with asymptotically optimal rate and distances. Adding \"ghost vertices\" to the CW complex generalizes quantum double models with defects and rough boundary conditions whose logical structure can be understood without reference to non-Abelian anyons or defects. Several non-invertible symmetry-protected topological states, both with ordinary and higher-form symmetries, are the unique codewords of simply-connected CW quantum doubles with a single ghost vertex."}
{"id": "2602.19573", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19573", "abs": "https://arxiv.org/abs/2602.19573", "authors": ["Mickaya A. Razanaparany", "Christian Rakotonirina"], "title": "A Relation Between the Chrestenson Operator, Weyl Operator Basis, and Kronecker-Pauli Operator Basis", "comment": "7 pages", "summary": "Within the framework of quantum theory, we review the Chrestenson operator, the Weyl operator basis, and the Kronecker-Pauli operator basis in $d$-dimensional Hilbert spaces using Dirac notation, where $d$ is a prime integer strictly greater than 2. We establish a new algebraic relation connecting these operators and present the cases $d=3$ and $d=5$ as illustrative examples."}
{"id": "2602.19588", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19588", "abs": "https://arxiv.org/abs/2602.19588", "authors": ["Jaehun You", "Jiyong Kang", "Kyunghye Kim", "Wonhyeong Choi", "Taehyun Kim"], "title": "Characterization and active cancellation of power-line-induced motional-mode frequency noise in a trapped-ion system", "comment": "Main text: 8 pages, 4 figures; Supplementary: 6 pages, 3 figures", "summary": "The stability of motional-mode frequency is essential for realizing high-fidelity quantum gates in trapped-ion quantum computing. While broadband Gaussian noise has been extensively studied and mitigated using pulse shaping techniques, the impact of coherent periodic noise has remained largely unexplored. Here we report a systematic investigation of 60-Hz power-line noise and its effect on the secular frequencies of a single ${}^{171}\\mathrm{Yb}^{+}$ ion. Using spin-echo Ramsey spectroscopy, we characterize the amplitude and phase of the resulting secular-frequency modulation and validate this characterization via passive phase correction of the Ramsey sequence. Building on this, we implement active cancellation by injecting a compensation tone into the set-point of a PI controller that stabilizes the trap RF drive amplitude. A phasor-fitting procedure optimizes the amplitude and phase of the compensation signal, enabling near-complete suppression of the 60-Hz component. With active cancellation engaged, the coherence time of a radial motional mode is extended from approximately 10 ms to 35 ms, consistent with the limit set by motional heating. Our results provide both a clear characterization of periodic motional-mode noise and a practical framework for its suppression in trapped-ion quantum computing platforms."}
{"id": "2602.19671", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19671", "abs": "https://arxiv.org/abs/2602.19671", "authors": ["Yuan-Chao Weng", "Da Xu", "Zhen Chen", "Li-Zhou Tan", "Xu-Ke Gu", "Jie Li", "Hai-Feng Yu", "Shi-Yao Zhu", "Xuedong Hu", "Franco Nori", "J. Q. You"], "title": "Magnon squeezing in the quantum regime", "comment": "9 pages, 4 figures", "summary": "Squeezed states, crucial for quantum metrology and emerging quantum technologies, have been demonstrated in various platforms, but quantum squeezing of magnons in macroscopic spin systems remains elusive. Here we report the experimental observation of quantum-level magnon squeezing in a millimeter-scale yttrium iron garnet (YIG) sphere. By engineering a strong dispersive magnon-superconducting qubit coupling via a microwave cavity, we implement a significant self-Kerr nonlinearity to generate squeezed magnon states with their mean magnon number less than one. Harnessing a magnon-assisted Raman process, we perform Wigner tomography, revealing quadrature variances of $\\sim\\!0.8$ ($\\sim\\!1.0$~dB squeezing) relative to the vacuum. These results lay the groundwork for quantum nonlinear magnonics and promise potential applications in quantum metrology."}
{"id": "2602.19700", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19700", "abs": "https://arxiv.org/abs/2602.19700", "authors": ["Hikaru Wakaura", "Taiki Tanimae"], "title": "Reversible Information Transformation via Quantum Reservoir Computing: Conditions, Protocol, and Noise Resilience", "comment": null, "summary": "Quantum reservoir computing (QRC) exploits fixed quantum dynamics and a trainable linear readout to process temporal data, yet reversing the transformation -- reconstructing the input from the reservoir output -- has been considered intractable owing to the recursive nonlinearity of sequential quantum state evolution. Here we propose a four-equation encode-decode protocol with cross-key pairing and constructively show that quantum reservoir and key combinations satisfying all four equations exist. Using a full XYZ Hamiltonian reservoir with 10 data qubits, we expand the feature dimension to 76 without increasing qubit count and achieve machine-precision reconstruction (mean-squared error $\\mathrm{MSE} \\sim 10^{-17}$) for data lengths up to 30 under ideal conditions; the rank condition $\\mathrm{dim}(V) \\geq N_c$ is identified as a necessary criterion. A comprehensive noise analysis across seven conditions and four baseline methods reveals a clear hierarchy: shot noise dominates, depolarizing noise adds a moderate factor, and asymmetric resource allocation -- 10 shots for encoding, $10^5$ for decoding -- yields approximately two orders of magnitude MSE improvement by exploiting the asymmetric noise roles of the encryption and decryption feature matrices. Under realistic noise the MSE degrades to $10^{-3}$-$10^{-1}$, indicating that error mitigation is needed before practical deployment, but our results establish the feasibility of bidirectional reversible information transformation within QRC."}
{"id": "2602.19701", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19701", "abs": "https://arxiv.org/abs/2602.19701", "authors": ["Mateusz Kuniej", "Katarzyna Roszak"], "title": "Direct access to the initial polarization of ${}^{13}C$ nuclei by measuring coherence evolution of an nitrogen-vacancy center spin qubit", "comment": "9 pages, 6 figures", "summary": "We introduce a method for the measurement of the lower bound on the initial polarization of spinful nuclei in a diamond by following the coherence evolution of an NV center spin qubit after a simple scheme is operated on the qubit to facilitate the transfer of information from the environment into the qubit state. Current polarization measurement techniques are challenging to implement due to the need for direct access to the environment. In our method, information is obtained by measuring the difference of the evolution of the qubit coherence resulting from preparation phase when the environment evolution is conditional on the qubit pointer state. We find that the method does not depend strongly on the applied magnetic field, but rather on the number of spinfull nuclei that lead to decoherence, and gives a reasonable estimate if the environment is polarized. The key advantage of this approach is its simplicity and minimal experimental requirements, allowing the inference of initial nuclear polarizations without direct access to the environment. We demonstrate the efficacy of this method using a simulated environment of up to fifteen randomly placed nuclear spins."}
{"id": "2602.19722", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.19722", "abs": "https://arxiv.org/abs/2602.19722", "authors": ["Hanyan Cao", "Dongyang Feng", "Cheng Ye", "Feng Pan"], "title": "Differentiable Maximum Likelihood Noise Estimation for Quantum Error Correction", "comment": null, "summary": "Accurate noise estimation is essential for fault-tolerant quantum computing, as decoding performance depends critically on the fidelity of the circuit-level noise parameters. In this work, we introduce a differentiable Maximum Likelihood Estimation (dMLE) framework that enables exact, efficient, and fully differentiable computation of syndrome log-likelihoods, allowing circuit-level noise parameters to be optimized directly via gradient descent. Leveraging the exact Planar solver for repetition codes and a novel, simplified Tensor Network (TN) architecture combined with optimized contraction path finding for surface codes, our method achieves tractable and fully differentiable likelihood evaluation even for distance 5 surface codes with up to 25 rounds. Our method recovers the underlying error probabilities with near-exact precision in simulations and reduces logical error rates by up to 30.6(3)% for repetition codes and 8.1(2)% for surface codes on experimental data from Google's processor compared to previous state-of-the-art methods: correlation analysis and Reinforcement Learning (RL) methods. Our approach yields provably optimal, decoder-independent error priors by directly maximizing the syndrome likelihood, offering a powerful noise estimation and control tool for unlocking the full potential of current and future error-corrected quantum processors."}
{"id": "2602.19747", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.19747", "abs": "https://arxiv.org/abs/2602.19747", "authors": ["Yifan Sun", "Lian-Ao Wu"], "title": "Symmetry and Exact Solutions of General Spin-Boson Models", "comment": "12 pages, 3 figures", "summary": "Spin-boson models are the canonical benchmark for quantum dissipation. We show the symmetry structure of general spin-boson Hamiltonians and obtain their spectra explicitly by exploiting the symmetry. As an illustration of the general case, we numerically demonstrate the exact solution for the two-mode case."}
{"id": "2602.19750", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.19750", "abs": "https://arxiv.org/abs/2602.19750", "authors": ["Mohsen Alishahiha", "Fatemeh Tarighi Tabesh", "Mohammad Javad Vasli"], "title": "Krylov Distribution and Universal Convergence of Quantum Fisher Information", "comment": "12 pages, 2 figures", "summary": "We develop a spectral-resolvent framework for computing the quantum Fisher information (QFI) using Krylov subspace methods, extending the notion of the Krylov distribution. By expressing the QFI as a resolvent moment of the superoperator $\\mathcal{K}_ρ$ associated with a density matrix, the Krylov distribution quantifies how the QFI weight is distributed across Krylov levels in operator space and provides a natural measure for controlling the truncation error in Krylov approximations. Leveraging orthogonal polynomial theory, we identify two universal convergence regimes: exponential decay when the Liouville-space spectrum is gapped away from zero, and algebraic decay governed by hard-edge (Bessel) universality when small eigenvalues accumulate near zero. This framework establishes a direct connection between quantum metrology, spectral geometry, and Krylov dynamics, offering both conceptual insight and practical tools for efficient QFI computation in high-dimensional and many-body systems."}
{"id": "2602.19752", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19752", "abs": "https://arxiv.org/abs/2602.19752", "authors": ["Jungyun Lee", "Daniel K. Park"], "title": "Improving Generalization and Trainability of Quantum Eigensolvers via Graph Neural Encoding", "comment": null, "summary": "Determining the ground state of a many-body Hamiltonian is a central problem across physics, chemistry, and combinatorial optimization, yet it is often classically intractable due to the exponential growth of Hilbert space with system size. Even on fault-tolerant quantum computers, quantum algorithms with convergence guarantees -- such as quantum phase estimation and quantum subspace methods -- require an initial state with sufficiently large overlap with the true ground state to be effective. Variational quantum eigensolvers (VQEs) are natural candidates for preparing such states; however, standard VQEs typically exhibit poor generalization, requiring retraining for each Hamiltonian instance, and often suffer from barren plateaus, where gradients can vanish exponentially with circuit depth and system size. To address these limitations, we propose an end-to-end representation learning framework that combines a graph autoencoder with a classical neural network to generate VQE parameters that generalize across Hamiltonian instances. By encoding interaction topology and coupling structure, the proposed model produces high-overlap initial states without instance-specific optimization. Through extensive numerical experiments on families of one- and two-local Hamiltonians, we demonstrate improved generalization and trainability, manifested as reduced test error and a significantly milder decay of gradient variance. We further show that our method substantially accelerates convergence in quantum subspace-based eigensolvers, highlighting its practical impact for downstream quantum algorithms."}
{"id": "2602.19772", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19772", "abs": "https://arxiv.org/abs/2602.19772", "authors": ["Aiman Khan", "Danilo Triggiani", "Vincenzo Tamma"], "title": "Multiphoton Hong-Ou-Mandel Interference Enables Superresolution of Bright Thermal Sources", "comment": "20 pages, 8 figures; comments are welcome", "summary": "We present a quantum optical scheme for imaging transversely displaced thermal sources of arbitrary intensities by employing multiphoton interference with a reference single-photon Fock state at a beamsplitter. Obtaining an analytical form for transverse momenta-resolved $L$-photon probabilities in either output, we show via Fisher information analysis that separation estimators built using interference sampling of multiphoton events exhibit significantly enhanced precision vis-à-vis existing imaging schemes over a wide range of separations and brightness. Even-photon-number coincidences exhibit constant precision in the sub-Rayleigh regime, demonstrating quantum superresolution of our scheme beyond the diffraction limit. For sources emitting on average $N_s\\sim1$ photon per frame (such as in IR emission of thermal sources), precision bounds for our scheme scale linearly in $N_s$, exemplifying an enhanced precision of estimators in relation to weak sources $N_s\\ll1$, and matching the ultimate quantum scaling. Finally, transverse momenta resolution in the Fourier plane produces finite imaging precisions for intermediate and large source separations using coarse pixel sizes of order $δy\\sim100\\,μ\\mathrm{m}$ for exemplary image spot sizes $σ_x \\sim 0.1\\, μ\\mathrm{m}$, in contrast with existing schemes of diffraction-limited direct imaging and superresolved inversion interferometric imaging that are severely degraded by coarse pixel sizes and have limited use. Combining the relatively straightforward sensing operation of Hong-Ou-Mandel interferometers with multiphoton coincidence detection of arbitrarily bright thermal sources and inner variable resolution of transverse photonic momenta, our scheme offers a robust alternative to non-invasive single-particle tracking and imaging of bright sources in nanoscopic chemical and biological systems."}
{"id": "2602.19792", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19792", "abs": "https://arxiv.org/abs/2602.19792", "authors": ["Mateusz Molenda", "Lewis A. Clark", "Marcin Płodzień", "Jan Kolodynski"], "title": "Unlocking photodetection for quantum sensing with Bayesian likelihood-free methods and deep learning", "comment": "16 pages, 11 figures, comments are welcome", "summary": "To operate quantum sensors at their quantum limit in real time, it is crucial to identify efficient data inference tools for rapid parameter estimation. In photodetection, the key challenge is the fast interpretation of click-patterns that exhibit non-classical statistics -- the very features responsible for the quantum enhancement of precision. We achieve this goal by comparing Bayesian likelihood-free methods with ones based on deep learning (DL). While the former are more conceptually intuitive, the latter, once trained, provide significantly faster estimates with comparable precision and yield similar predictions of the associated errors, challenging a common misconception that DL lacks such capabilities. We first verify both approaches for an analytically tractable, yet multiparameter, scenario of a two-level system emitting uncorrelated photons. Our main result, however, is the application to a driven nonlinear optomechanical device emitting non-classical light with complex multiclick correlations; in this case, our methods are essential for fast inference and, hence, unlock the possibility of distinguishing different photon statistics in real time. Our results pave the way for dynamical control of quantum sensors that leverage non-classical effects in photodetection."}
{"id": "2602.19876", "categories": ["quant-ph", "cond-mat.quant-gas", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2602.19876", "abs": "https://arxiv.org/abs/2602.19876", "authors": ["Thies Plassmann", "Leon Schaefer", "Meny Menashes", "Guillaume Salomon"], "title": "Rapid state-resolved single-atom imaging of alkaline-earth fermions", "comment": "9 pages, 5 figures", "summary": "Local Hilbert spaces with large dimension are of key interest for quantum information with applications in quantum computing and memories, quantum simulations and metrology. Thanks to its weak coupling to external perturbations, the large ground-state nuclear spin manifold of fermionic alkaline-earth atoms is an exciting resource to explore for quantum information. Simultaneous single atom and state-resolved detection however remains an outstanding challenge limiting the development of novel quantum computing and simulation schemes beyond qubits. Here, we report on a new imaging technique enabling the simultaneous detection of up to four quantum states encoded in the nuclear spin manifold of a single fermionic strontium atom within 100 microseconds, with state-resolved detection fidelities ranging from 0.936 to 0.997. This technique is further used to track the highly coherent nuclear spin dynamics after a quench highlighting the potential of this system for quantum information. These results offer fascinating perspectives for quantum science with multi-electron atoms ranging from qudit-based quantum computing to quantum simulations of the SU(N) Fermi-Hubbard model."}
{"id": "2602.19908", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.19908", "abs": "https://arxiv.org/abs/2602.19908", "authors": ["Antti Vaaranta", "Marco Cattaneo", "Paolo Muratore-Ginanneschi", "Jukka Pekola"], "title": "Heat flow through the quantum heat valve coupled to ohmic baths via a master equation approach", "comment": null, "summary": "We provide a theoretical model for the non-equilibrium steady state heat flow through a quantum heat valve. The model is based on a master equation approach, where the partial secular approximation has been carefully performed in order to obtain accurate results. Our study assumes an ohmic spectral density for the two thermal baths of the model. This is in contrast with previous treatments of the quantum heat valve, where the baths have been assumed as being structured with a peaked spectral density near the resonance frequency of the resonator. These studies have also taken the resonator to be a part of the open quantum system of interest, which results in double counting of the resonator, as the latter appears both in the spectral density of the bath and as a part of the open system. Although this model accounts for the observations in a satisfactory way, it raises issues regarding its physical interpretation. Our method solves this conceptual problem. We apply it to describe an experiment on a quantum heat valve, showing that it successfully captures the experimental results and improves upon the previous theoretical model, which suffered from the resonator double-counting issue. Our findings confirm that the careful application of the master equation approach, in particular when it comes to the secular approximation, is a useful tool for explaining realistic experimental setups."}
{"id": "2602.19971", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19971", "abs": "https://arxiv.org/abs/2602.19971", "authors": ["Roland Combescot"], "title": "Two components relativistic quantum wave equation for scalar bosons", "comment": "3 pages", "summary": "We show that, in the relativistic regime, scalar bosons satisfy a quantum wave equation which is quite analogous to the Dirac equation. In contrast with the Klein-Gordon equation it is first order with respect to time derivation. It leads in a regular way to the standard Schrödinger equation in the non-relativistic limit. There are two components for the wave function in this representation for the scalar boson, in a way completely analogous to the four components for the spin $1/2$ fermion in the Dirac equation."}
{"id": "2602.19993", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.19993", "abs": "https://arxiv.org/abs/2602.19993", "authors": ["Roderich Tumulka"], "title": "GAP Measures and Wave Function Collapse", "comment": "9 pages LaTeX, no figures", "summary": "GAP measures (also known as Scrooge measures) are a natural class of probability distributions on the unit sphere of a Hilbert space that come up in quantum statistical mechanics; for each density matrix $ρ$ there is a unique measure GAP$_ρ$. We describe and prove a property of these measures that was not recognized so far: If a wave function $Ψ$ is GAP$_ρ$ distributed and a collapse occurs, then the collapsed wave function $Ψ'$ is again GAP distributed (relative to the appropriate $ρ'$). This fact applies to collapses due to a quantum measurement carried out by an observer, as well as to spontaneous collapse theories such as CSL or GRW. More precisely, it is the conditional distribution of $Ψ'$, given the measurement outcome (respectively, the noise in CSL or the collapse history in GRW), that is GAP$_{ρ'}$."}
{"id": "2602.19998", "categories": ["quant-ph", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19998", "abs": "https://arxiv.org/abs/2602.19998", "authors": ["Angela Sara Cacciapuoti", "Marcello Caleffi"], "title": "A Quantum Internet Protocol Suite Beyond Layering", "comment": "This work has been funded by the European Union under Horizon Europe ERC-CoG grant QNattyNet (\"Quantum-Native Communication Networks: from Quantum Message to Quantum Functioning\"), n.101169850. Details at https://qnattynet.quantuminternet.it", "summary": "Layering, the protocol organization principle underpinning the classical Internet, is ill-suited to the Quantum Internet, built around entanglement, which is non-local and stateful. This paper proposes a quantum-native organizational principle based on dynamic composition, which replaces static layering with a distributed orchestration fabric driven by the node local state and in-band control. Each node runs a Dynamic Kernel that i) constructs a local PoA of candidate steps to advance a service intent, and ii) executes the PoA by composing atomic micro-protocols into context-aware procedures (the meta-protocols). Quantum packets carry an in-band control-field (the meta-header) containing the service intent and an append-only list of action-commit records, termed as stamps. Successive nodes exploit this minimal, authoritative history to construct their local PoAs. As quantum packets progress, these local commits collectively induce a network-wide, direct acyclic graph that certifies end-to-end service fulfillment, without requiring global synchronization. In contrast to classical encapsulation, the proposed suite enforces order by certification: dependency-aware local scheduling decides what may run at a certain node, stamps certify what did run and constrain subsequent planning. By embedding procedural control within the quantum packet, the design ensures coherence and consistency between entanglement-state evolution and control-flow, preventing divergence between resource state ad protocol logic, while remaining MP-agnostic and implementation-decoupled. The resulting suite is modular, adaptable to entanglement dynamics, and scalable. It operates correctly with or without optional control-plane hints. Indeed, when present, hints can steer QoS policies, without changing semantics. We argue that dynamic composition is the organizing principle required for a truly quantum-native Internet."}
{"id": "2602.20002", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20002", "abs": "https://arxiv.org/abs/2602.20002", "authors": ["Christian Križan", "Maurizio Toselli", "Irshad Ahmad", "Hadi Khaksaran", "Marcus Rommel", "Nermin Trnjanin", "Janka Biznárová", "Mamta Dahiya", "Emil Hogedal", "Halldór Jakobsson", "Andreas Nylander", "Jonas Bylander", "Per Delsing", "Giovanna Tancredi"], "title": "Electrical post-fabrication tuning of aluminum Josephson junctions at room temperature", "comment": "34 pages, 14 figures", "summary": "Josephson junctions are a key element of superconducting quantum technology, serving as the core building blocks of superconducting qubits. We present an experimental study on room-temperature electrical tuning of aluminum junctions, showing that voltage pulses can controllably increase their resistance and adjust the Josephson energy while maintaining qubit quality factors above 1 million. We find that the rate of resistance increase scales exponentially with pulse amplitude during manipulation, after which the spontaneous resistance increase scales proportionally to the amount of manipulation. We show that this spontaneous increase halts at cryogenic temperatures, and resumes again at room temperature. Using our stepwise protocol, we achieve up to a 270% increase in junction resistance, corresponding to a reduction of nearly 2 GHz of the qubit transition frequency. These results establish the achievable range, relaxation behavior, and practical limits of electrical tuning, enabling post-fabrication mitigation of frequency crowding in quantum processors."}
{"id": "2602.20013", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20013", "abs": "https://arxiv.org/abs/2602.20013", "authors": ["S. Bhuvaneswari", "R. Muthuganesan", "R. Radha"], "title": "Quantum correlation and coherence in a mononuclear nickel-based molecular Magnet", "comment": "10 pages, 4 Figures; Comments are welcome. To appear in Journal of Magnetisam and Magnetic Materials", "summary": "We investigate the behaviors of thermal entanglement, quantum correlation beyond entanglement namely, measurement-induced nonlocality (MIN) and coherence in a nickel radical molecular magnet (Et3NH)[Ni(hfac)2L], whose spin-spin interactions are well described by the Heisenberg model. Using experimentally estimated coupling parameters, we compute the thermal state of the system and analyze the dependence of quantum resources on temperature and magnetic field. The results indicate that the quantum resources of the nickel-radical molecular magnet persist even at room temperature. We show that while negativity (the entanglement measure) rapidly vanishes with increasing temperature and magnetic field, measurement-induced nonlocality and quantum coherence remain comparatively more stable and persist in regions where entanglement is absent. These results highlight the significance of nonclassical correlations beyond entanglement in thermally activated spin systems and suggest that such molecular magnets could serve as viable platforms for quantum information processing in realistic conditions."}
{"id": "2602.20030", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20030", "abs": "https://arxiv.org/abs/2602.20030", "authors": ["J. Munárriz", "F. Domínguez-Adame", "R. P. A. Lima"], "title": "Spectroscopy of the Dirac oscillator perturbed by a surface delta potential", "comment": null, "summary": "We study theoretically the level shift of the Dirac oscillator perturbed by any sharply peaked potential approaching a surface delta potential. A Green function method is used to obtain closed expressions for all partial waves and parities."}
{"id": "2602.20077", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.20077", "abs": "https://arxiv.org/abs/2602.20077", "authors": ["Fabricio Danel Matias", "Facundo Arreyes", "Juan Sebastián Ardenghi"], "title": "Entanglement formation in two-dimensional materials within microcavity", "comment": "10 pages", "summary": "In this work, the entanglement generation between two hexagonal-lattice layers embedded in a microcavity is studied, accounting for both electromagnetic coupling and intrinsic spin-orbit interaction (SOI). Utilizing a short-time dynamical approach, we perform a perturbative Taylor expansion of the reduced density matrix to characterize the bipartite quantum correlations between the hexagonal layers. We demonstrate that the system undergoes a rapid transition from a localized product state in the conduction bands at t = 0 to a coherent superposition of valence and conduction band states. Our results indicate that the degree of entanglement is highly sensitive to the interlayer photon propagator, which contains the geometric ratios of the layer positions and the height cavity, and the specific Fermi energy and SOI signatures of the respective layers. We show the emergence of spacelike-separated quantum correlations in the ultra-short evolution regime, suggesting that heterostructures in cavities may be suitable to develop experiments for a deep understanding of spacelike-separated quantum effects."}
{"id": "2602.20085", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20085", "abs": "https://arxiv.org/abs/2602.20085", "authors": ["Lei Du", "Juliette Monsel", "Witlef Wieczorek", "Janine Splettstoesser"], "title": "Nonlinear quantum optomechanics in a Fano-mirror microcavity system", "comment": "16 pages, 7 figures. Comments are welcome", "summary": "We study a Fano-mirror optomechanical system in the quantum nonlinear regime. In this system, two strongly lossy optical modes hybridize through both coherent and dissipative couplings to form an effective optical mode with a drastically reduced linewidth. This linewidth reduction enables the system to access the single-photon strong-coupling and sideband-resolved regimes simultaneously. We formulate the system dynamics using an effective master-equation approach and benchmark it against quantum Langevin and dressed-state master-equation descriptions. With experimentally realistic parameters, we predict clear quantum signatures, including photon blockade and the generation of mechanical cat states. Our work establishes the Fano-mirror architecture as a promising platform for harnessing single-photon optomechanical nonlinearities for quantum state engineering under achievable experimental conditions."}
{"id": "2602.20106", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20106", "abs": "https://arxiv.org/abs/2602.20106", "authors": ["Ossama Kullie", "Igor A. Ivanov"], "title": "The quantum superluminality in the tunnel-ionization process of H-like atoms", "comment": null, "summary": "The quantum tunneling time remains the subject of heated debate,\n  and one of its most curious features is faster-than-light or\n  superluminal tunneling.\n  Our tunnel-ionization model of the time-delay, presented in previous work, shows good agreement with the attoclock measurement in the adiabatic and nonadiabatic field calibrations, which also enables the determination of the barrier time-delay.\n  In the present work, we show that the tunnel-ionization for H-like atoms with large nuclear charge can be superluminal (quantum superluminality), which in principle can be investigated experimentally using the attoclock scheme.\n  We discuss the quantum superluminality in detail for the different regimes of the tunnel-ionization. Our result shows that quantum tunneling faster-than-light is indeed possible, albeit only under somewhat extreme conditions."}
{"id": "2602.20123", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20123", "abs": "https://arxiv.org/abs/2602.20123", "authors": ["Maxwell Poster", "Sayam Sethi", "Jonathan Baker"], "title": "CQM: Cyclic Qubit Mappings", "comment": null, "summary": "Quantum computers show promise to solve select problems otherwise intractable on classical computers. However, noisy intermediate-scale quantum (NISQ) era devices are currently prone to various sources of error. Quantum error correction (QEC) shows promise as a path towards fault tolerant quantum computing. Surface codes, in particular, have become ubiquitous throughout literature for their efficacy as a quantum error correcting code, and can execute quantum circuits via lattice surgery operations. Lattice surgery also allows for logical qubits to maneuver around the architecture, if there is space for it. Hardware used for near-term demonstrations have both spatially and temporally varying error results in logical qubits. By maneuvering logical qubits around the topology, an average logical error rate (LER) can be enforced. We propose cyclic qubit mappings (CQM), a dynamic remapping technique implemented during compilation to mitigate hardware heterogeneity by expanding and contracting logical qubits. In addition to LER averaging, CQM shows initial promise given it's minimal execution time overhead and effective resource utilization."}
{"id": "2602.20128", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20128", "abs": "https://arxiv.org/abs/2602.20128", "authors": ["Elia Perego", "Andrea Rodriguez-Blanco", "K. Birgitta Whaley", "Bharath Hebbe Madhusudhana"], "title": "Experimental characterization of coherent and non-Markovian errors using tangent space decomposition", "comment": "12 pages, 4 figures", "summary": "Accurate characterization of coherent and non-Markovian errors remains a central challenge in quantum information processing, as conventional benchmarking techniques typically rely on Markovian and time-independent noise assumptions. In practice, however, quantum devices exhibit both systematic coherent miscalibrations and temporally correlated fluctuations, which complicate error diagnosis and mitigation. Here, we apply a technique based on tangent-space decomposition to characterize such error in single-qubit quantum gates implemented on a trapped ion platform. Small imperfections in a quantum operation are treated as perturbations of the target quantum map, represented as tangent vectors in the space of quantum channels. This formulations enables a natural decomposition of the deviation into three components corresponding to coherent, Markovian and non-Markovian processes.The relative weights of these components provide a quantitative measure of the contribution from each type of error mechanism, directly from a single tomographic snapshot. We experimentally validate this method on a single-qubit gates implemented on a trapped $^{40}$Ca$^+$ ion, where control is achieved through laser-driven optical transitions. By analyzing experimentally reconstructed process matrices, expressed in the Pauli Transfer Matrix and Choi representations, we identify and quantify non-Markovian effects arising from controlled injection of slow fluctuations in the experimental environment. We also characterize deterministic coherent miscalibrations using the same technique. This approach provides a physically transparent and experimentally accessible tool for diagnosing complex error sources in quantum control systems."}
{"id": "2602.20149", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.20149", "abs": "https://arxiv.org/abs/2602.20149", "authors": ["Radhakrishnan Balu", "S. James Gates"], "title": "Quantum Information Approach to Bosonization of Supersymmetric Yang-Mills Fields", "comment": null, "summary": "We consider bosonization of supersymmetry in the context of Wess-Zumino quantum mechanics. Our motivation for this investigation is the flexibility the bosonic fock space affords as any classical probability distribution can be realized on it making it a versatile framework to work with for quantum processes. We proceed by constructing a minimal bosonization of a system with one bosonic and two fermionic degrees of freedom. We iterate this process to construct a tower of SUSY systems that is akin to unfolded Adinkras. We then identify an osp(2|2) symmetry of the system constructed. To build an irreducible representation of the system we induce representations across the sectors, a first to our knowledge, as the previous work have focused on induction only within the bosonic sector. First, we start with a fermionic representation using Clifford algebras and then induce a representation to gl(2|2) and restrict it to osp(2|2). In the second method, we induce a representation from that of the bosonic sector. In both cases, our representations are in terms of qubit operators that provide a way to solve SUSY problems using quantum information based approaches. Depending upon the direction of induction the representations are suitable for implementation on a hybrid qubit and fermionic or bosonic quantum computers."}
{"id": "2602.20154", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20154", "abs": "https://arxiv.org/abs/2602.20154", "authors": ["Shao-Hen Chiew", "Armando Angrisani", "Zoë Holmes", "Giuseppe Carleo"], "title": "Quantum simulation in the Heisenberg picture via vectorization", "comment": null, "summary": "We present a general framework for simulating quantum systems in the Heisenberg picture on quantum hardware. Based on the vectorization map, our framework fully exploits the mapping between operators and quantum states, allowing any task defined on Heisenberg operators to be mapped to standard Schrödinger-picture tasks that are naturally accessible via quantum computers and simulators. This yields new or improved protocols for tasks such as operator sampling, the computation of OTOCs/superoperator expectation values and their higher order moments, two-point correlators, and operator stabilizer and entanglement entropies. Our approach is also amenable to implementation, as it inherits the structure and resource requirements of the (forward and time-reversed) Schrödinger-picture quantum simulation problem. We demonstrate this by proposing implementations of our framework for a 2D problem on digital and analog quantum simulators, taking into account device connectivity constraints."}
{"id": "2602.20158", "categories": ["quant-ph", "cond-mat.str-el", "math-ph", "math.QA"], "pdf": "https://arxiv.org/pdf/2602.20158", "abs": "https://arxiv.org/abs/2602.20158", "authors": ["Zijian Liang", "Yu-An Chen"], "title": "Generalized $\\mathbb{Z}_p$ toric codes as qudit low-density parity-check codes", "comment": "9+1 pages, 4 figures", "summary": "We study two-dimensional translation-invariant CSS stabilizer codes over prime-dimensional qudits on the square lattice under twisted boundary conditions, generalizing the Kitaev $\\mathbb{Z}_p$ toric code by augmenting each stabilizer with two additional qudits. Using the Laurent-polynomial formalism, we adapt the Gröbner basis to compute the logical dimension $k$ efficiently, without explicitly constructing large parity-check matrices. We then perform a systematic search over various stabilizer realizations and lattice geometries for $p\\in\\{3,5,7,11\\}$, identifying qudit low-density parity-check codes with the optimal finite-size performance. Representative examples include $[[242,10,22]]_3$ and $[[120,6,20]]_{11}$, both achieving $k d^{2}/n=20$. Across the searched regime, the best observed $k d^{2}$ at fixed $n$ increases with $p$, with an empirical relation $k d^{2} = 0.0541 \\, n^{2}\\ln p + 3.84 \\, n$, compatible with a Bravyi--Poulin--Terhal-type tradeoff when the interaction range grows with system size."}
{"id": "2602.19586", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19586", "abs": "https://arxiv.org/abs/2602.19586", "authors": ["Hanieh Najafzadeh", "Abdollah Langari"], "title": "From Quantum Chaos to a Reversed Quantum Disentangled Liquid in a Disorder-Free Spin Ladder", "comment": null, "summary": "The mechanisms by which isolated interacting quantum systems evade thermalization extend beyond disorder-induced many-body localization, encompassing a growing class of interaction-driven phenomena. We investigate a spin-1/2 ladder with asymmetric XY leg couplings and tunable Ising interactions on the rungs, and identify the microscopic origin of many-body localization (MBL) in this setting. Through a suite of diagnostics -including entanglement dynamics, fidelity susceptibility, adiabatic gauge potential norms, level-spacing statistics and entropy of eigenstates- we uncover a reentrant progression of dynamical regimes as the rung coupling Jz is varied: integrable behavior at Jz=0, quantum chaos at intermediate Jz, and a robust nonthermal regime at strong coupling. In the latter regime, we demonstrate the emergence of a reversed quantum disentangled liquid (reversed-QDL), where the light species thermalizes while the heavy species remains localized. The strong-coupling limit further yields emergent local integrals of motion anchored in a fixed-point structure, providing a microscopic origin of the observed quasi-MBL dynamics. These results establish reversed-QDL as a distinct, disorder-free route to nonergodicity and broaden the classification of dynamical phases in quantum matter."}
{"id": "2602.19741", "categories": ["cond-mat.stat-mech", "math-ph", "nlin.SI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19741", "abs": "https://arxiv.org/abs/2602.19741", "authors": ["Vsevolod I. Yashin"], "title": "Two-parameter families of MPO integrals of motion in Heisenberg spin chains", "comment": "11 pages", "summary": "Recently, Fendley et al. (2025) [arXiv:2511.04674] revealed a new way to demonstrate the integrability of XYZ Heisenberg model by constructing a one-parameter family of integrals of motion in the matrix product operator (MPO) form. In this short note, I report on the discovery of two-parameter families of MPOs that commute with with the Heisenberg spin chain Hamiltonian in the XXX, XXZ, and XYZ cases. I describe a symbolic algebra approach for finding such integrals of motion and speculate about possible applications."}
{"id": "2602.19795", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19795", "abs": "https://arxiv.org/abs/2602.19795", "authors": ["Felix Möckel", "Harald Schmid", "Felix von Oppen"], "title": "Floquet product mode and eigenphase order", "comment": "12 pages, 9 figures", "summary": "We study the robustness of the Floquet quantum Ising model against integrability-breaking perturbations, focusing on the phase hosting both Majorana zero and $π$ modes. A recent work [Phys. Rev. B 110, 075117, (2024)] observed that the Floquet product mode, a composite edge mode constructed from both Majorana operators, is considerably more robust than the individual Majorana edge modes. We analyze these strong modes from the point of view of the eigenphase order present in finite chains with open boundary conditions. As a result of the Majorana modes, all Floquet eigenstates come in quadruplets in the integrable limit. We show that the robustness of the various modes as well as the behavior of the boundary spin correlation functions can be understood in terms of the spectral statistics of these quadruplets in the presence of integrability-breaking perturbations."}
{"id": "2602.19865", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19865", "abs": "https://arxiv.org/abs/2602.19865", "authors": ["R. Jafari", "Alireza Akbari"], "title": "Separation of the Kibble-Zurek Mechanism from Quantum Criticality", "comment": null, "summary": "When a system is swept through a quantum critical point (QCP), the Kibble-Zurek mechanism predicts that the average number of topological defects follows a universal power-law scaling with the ramp time scale. This scaling behavior is determined by the equilibrium critical exponents of the underlying phase transition. We show that the correspondence between Kibble-Zurek scaling and quantum criticality does not hold generally. In particular, the defect density can exhibit a suppression faster than the Kibble-Zurek prediction even when the quench crosses a critical point, while conventional Kibble-Zurek scaling may persist for quenches through a non-critical point. Our results, based on models representative of a broad class of quasi-one-dimensional Fermi systems, identify the dynamical conditions under which universal defect scaling emerges and clarify the relation between defect generation and equilibrium criticality."}
{"id": "2602.20108", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20108", "abs": "https://arxiv.org/abs/2602.20108", "authors": ["L. Brodoloni", "G. E. Astrakharchik", "S. Giorgini", "S. Pilati"], "title": "Energy gap of quantum spin glasses: a projection quantum Monte Carlo study", "comment": "6 pages plus additional material", "summary": "The performance of quantum annealing for combinatorial optimization is fundamentally limited by the minimum energy gap $Δ$ encountered at quantum phase transitions. We investigate the scaling of $Δ$ with system size $N$ for two paradigmatic quantum spin-glass models: the two-dimensional Edwards-Anderson (2D-EA) and the all-to-all Sherrington-Kirkpatrick (SK) models. Utilizing a newly proposed unbiased energy-gap estimator for continuous-time projection quantum Monte Carlo simulations, complemented by high-performance sparse eigenvalue solvers, we characterize the gap distributions across disorder realizations. It is found that, in the 2D-EA case, the inverse-gap distribution develops a fat tail with infinite variance as $N$ increases. This indicates that the unfavorable super-algebraic scaling of $Δ$, recently reported for binary couplings [Nature 631, 749 (2024)], persists for the Gaussian disorder considered here, pointing to a universal feature of 2D spin glasses. Conversely, the SK model retains a finite-variance distribution, with the disorder-averaged gap following a rather slow power law, close to $Δ\\propto N^{-1/3}$. This finding provides a promising outlook for the potential efficiency of quantum annealers for optimization problems with dense connectivity."}
