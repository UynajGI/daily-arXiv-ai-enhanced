{"id": "2601.20005", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20005", "abs": "https://arxiv.org/abs/2601.20005", "authors": ["Zixin Jiang", "Weili Xu", "Bing Dong"], "title": "OptAgent: an Agentic AI framework for Intelligent Building Operations", "comment": null, "summary": "The urgent need for building decarbonization calls for a paradigm shift in future autonomous building energy operation, from human-intensive engineering workflows toward intelligent agents that interact with physics-grounded digital environments. This study proposes an end-to-end agentic AI-enabled Physics-Informed Machine Learning (PIML) environment for scalable building energy modeling, simulation, control, and automation. The framework consists of (1) a modular and physics-consistent PIML digital environment spanning building thermal dynamics, Heating, Ventilation, and Air Conditioning (HVAC), and distributed energy resources (DER) for grid-interactive energy management; and (2) an agentic AI layer with 11 specialist agents and 72 Model Context Protocol (MCP) tools that enable end-to-end execution of multi-step energy analytics. A representative case study demonstrates multi-domain, multi-agent coordination for assessing how system and control upgrades affect energy use, operating cost, thermal comfort, and flexibility. In addition, a large-scale benchmark (about 4000 runs) systematically evaluates workflow performance in terms of accuracy, token consumption, execution time, and inference cost. The results quantify the impacts of intelligence mode design, model size, task complexity, and orchestrator-specialist coordination, and provide key lessons for building future agentic AI systems in real-world building energy applications. This work establishes a scalable, physics-grounded foundation for deploying agentic AI in decarbonized and grid-interactive building operations."}
{"id": "2601.20135", "categories": ["eess.SY", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2601.20135", "abs": "https://arxiv.org/abs/2601.20135", "authors": ["Domitilla Del Vecchio"], "title": "Control systems for synthetic biology and a case-study in cell fate reprogramming", "comment": null, "summary": "This paper gives an overview of the use of control systems engineering in synthetic biology, motivated by applications such as cell therapy and cell fate reprogramming for regenerative medicine. A ubiquitous problem in these and other applications is the ability to control the concentration of specific regulatory factors in the cell accurately despite environmental uncertainty and perturbations. The paper describes the origin of these perturbations and how they affect the dynamics of the biomolecular ``plant'' to be controlled. A variety of biomolecular control implementations are then introduced to achieve robustness of the plant's output to perturbations and are grouped into feedback and feedforward control architectures. Although sophisticated control laws can be implemented in a computer today, they cannot be necessarily implemented inside the cell via biomolecular processes. This fact constraints the set of feasible control laws to those realizable through biomolecular processes that can be engineered with synthetic biology. After reviewing biomolecular feedback and feedforward control implementations, mostly focusing on the author's own work, the paper illustrates the application of such control strategies to cell fate reprogramming. Within this context, a master regulatory factor needs to be controlled at a specific level inside the cell in order to reprogram skin cells to pluripotent stem cells. The article closes by highlighting on-going challenges and directions of future research for biomolecular control design."}
{"id": "2601.20183", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20183", "abs": "https://arxiv.org/abs/2601.20183", "authors": ["Yuhua Zhao", "Tiejun Lv", "Ke Wang"], "title": "C-AoEI-Aware Cross-Layer Optimization in Satellite IoT Systems: Balancing Data Freshness and Transmission Efficiency", "comment": "18 pages, 13 figures, IEEE Internet of Things Journal, Accepted", "summary": "Satellite-based Internet of Things (S-IoT) faces a fundamental trilemma: propagation delay, dynamic fading, and bandwidth scarcity. While Layer-coded Hybrid ARQ (L-HARQ) enhances reliability, its backtracking decoding introduces age ambiguity, undermining the standard Age of Information (AoI) metric and obscuring the critical trade-off between data freshness and transmission efficiency. To bridge this gap, we propose a novel cross-layer optimization framework centered on a new metric, the Cross-layer Age of Error Information (C-AoEI). We derive a closed-form expression for C-AoEI, explicitly linking freshness to system parameters, establishing an explicit analytical connection between freshness degradation and channel dynamics. Building on this, we develop a packet-level encoded L-HARQ scheme for multi-GBS scenarios and an adaptive algorithm that jointly optimizes coding and decision thresholds. Extensive simulations demonstrate the effectiveness of our proposed framework: it achieves 31.8% higher transmission efficiency and 17.2% lower C-AoEI than conventional schemes. The framework also proves robust against inter-cell interference and varying channel conditions, providing a foundation for designing efficient, latency-aware next-generation S-IoT protocols."}
{"id": "2601.20298", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20298", "abs": "https://arxiv.org/abs/2601.20298", "authors": ["Omid Akbarzadeh", "MohammadHossein Ashoori", "Amy Nejati", "Abolfazl Lavaei"], "title": "A Data-Driven Krasovskii-Based Approach for Safety Controller Design of Time-Delayed Uncertain Polynomial Systems", "comment": null, "summary": "We develop a data-driven framework for the synthesis of robust Krasovskii control barrier certificates (RK-CBC) and corresponding robust safety controllers (R-SC) for discrete-time input-affine uncertain polynomial systems with unknown dynamics, while explicitly accounting for unknown-but-bounded disturbances and time-invariant delays using only observed input-state data. Although control barrier certificates have been extensively studied for safety analysis of control systems, existing work on unknown systems with time delays, particularly in the presence of disturbances, remains limited. The challenge of safety synthesis for such systems stems from two main factors: first, the system's mathematical model is unavailable; and second, the safety conditions should explicitly incorporate the effects of time delays on system evolution during the synthesis process, while remaining robust to unknown disturbances. To address these challenges, we develop a data-driven framework based on Krasovskii control barrier certificates, extending the classical CBC formulation for delay-free systems to explicitly account for time delays by aggregating delayed components within the barrier construction. The proposed framework relies solely on input-state data collected over a finite time horizon, enabling the direct synthesis of RK-CBC and R-SC from observed trajectories without requiring an explicit system model. The synthesis is cast as a data-driven sum-of-squares (SOS) optimization program, yielding a structured design methodology. As a result, robust safety is guaranteed in the presence of unknown disturbances and time delays over an infinite time horizon. The effectiveness of the proposed method is demonstrated through three case studies, including two physical systems."}
{"id": "2601.20413", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.20413", "abs": "https://arxiv.org/abs/2601.20413", "authors": ["Nouar Aldahoul", "Hazem Ibrahim", "Majd Mahmutoglu", "Hajra Tarar", "Muhammad Fareed Zaffar", "Talal Rahwan", "Yasir Zaki"], "title": "Schadenfreude in the Digital Public Sphere: A cross-national and decade-long analysis of Facebook news engagement", "comment": null, "summary": "Schadenfreude, or the pleasure derived from others' misfortunes, has become a visible and performative feature of online news engagement, yet little is known about its prevalence, dynamics, or social patterning. We examine schadenfreude on Facebook over a ten-year period across nine major news publishers in the United States, the United Kingdom, and India (one left-leaning, one right-leaning, and one centrist per country). Using a combination of human annotation and machine-learning classification, we identify posts describing misfortune and detect schadenfreude in nearly one million associated comments. We find that while sadness and anger dominate reactions to misfortune posts, laughter and amusement form a substantial and patterned minority. Schadenfreude is most frequent in moralized and political contexts, higher among right-leaning audiences, and more pronounced in India than in the United States or United Kingdom. Temporal and regression analyses further reveal that schadenfreude generally increases when groups are politically out of power, but these patterns differ across party lines. Together, our findings move beyond anecdotal accounts to map schadenfreude as a dynamic, context-dependent feature of digital discourse, revealing how it evolves over time and across ideological and cultural divides."}
{"id": "2601.20211", "categories": ["cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20211", "abs": "https://arxiv.org/abs/2601.20211", "authors": ["Moritz Hütten"], "title": "High-Resolution Mapping of Port Dynamics from Open-Access AIS Data in Tokyo Bay", "comment": "29 pages, 18 figures, and 7 tables, matching the version published in Geomatics. Accompanying research data are available at https://dx.doi.org/10.6084/m9.figshare.29037401", "summary": "Knowledge about vessel activity in port areas and around major industrial zones provides insights into economic trends, supports decision-making for shipping and port operators, and contributes to maritime safety. Vessel data from terrestrial receivers of the Automatic Identification System (AIS) have become increasingly openly available, and we demonstrate that such data can be used to infer port activities at high resolution and with precision comparable to official statistics. We analyze open-access AIS data from a three-month period in 2024 for Tokyo Bay, located in Japan's most densely populated urban region. Accounting for uneven data coverage, we reconstruct vessel activity in Tokyo Bay at $\\sim\\,$30~m resolution and identify 161 active berths across seven major port areas in the bay. During the analysis period, we find an average of $35\\pm17_{\\text{stat}}$ vessels moving within the bay at any given time, and $293\\pm22_{\\text{stat}}+65_{\\text{syst}}-10_{\\text{syst}}$ vessels entering or leaving the bay daily, with an average gross tonnage of $11{,}860^{+280}_{-\\;\\,50}$. These figures indicate an accelerating long-term trend toward fewer but larger vessels in Tokyo Bay's commercial traffic. Furthermore, we find that in dense urban environments, radio shadows in vessel AIS data can reveal the precise locations of inherently passive receiver stations."}
{"id": "2601.20018", "categories": ["math.ST", "econ.EM", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.20018", "abs": "https://arxiv.org/abs/2601.20018", "authors": ["Mingxuan Zou", "Jingfan Xu", "Peng Ding", "Fang Han"], "title": "Decoupling and randomization for double-indexed permutation statistics", "comment": "42 pages", "summary": "This paper introduces a version of decoupling and randomization to establish concentration inequalities for double-indexed permutation statistics. The results yield, among other applications, a new combinatorial Hanson-Wright inequality and a new combinatorial Bennett inequality. Several illustrative examples from rank-based statistics, graph-based statistics, and causal inference are also provided."}
{"id": "2601.20024", "categories": ["physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.20024", "abs": "https://arxiv.org/abs/2601.20024", "authors": ["Zhenda Shen", "Zhongjian Wang", "Jack Xin", "Zhiwen Zhang"], "title": "Two-Step Diffusion: Fast Sampling and Reliable Prediction for 3D Keller--Segel and KPP Equations in Fluid Flows", "comment": "40 pages, 17 figures, 4 tables. Preprint", "summary": "We study fast and reliable generative transport for the 3D KS (Keller-Segel) and KPP (Kolmogorov-Petrovsky-Piskunov) equations in the presence of fluid flows with the goal to approximate the map between initial and terminal distributions for a range of physical parameters $σ$ under the Wasserstein metric. To minimize the inaccuracy of direct Wasserstein solver, we propose a two-stage pipeline that retains one-step efficiency while reinstating an explicit $W_2$ objective where it is tractable. In Stage I, a Meanflow-style regressor yields a deterministic, one-step global transport that moves particles close to their terminal states. In Stage II, we freeze this initializer and train a near-identity corrector (Deep Particle, DP) that directly minimizes a mini-batch $W_2$ objective using warm-started optimal transport couplings computed on the Meanflow outputs. Crucially, after the one-step transport (from Stage I) concentrating mass on the approximated correct support, the induced geometry stabilizes high-dimensional $W_2$ computation of the direct Wasserstein solver. We validate our construction in the 3D KS and KPP equations subject to fluid flows with ordered and chaotic streamlines."}
{"id": "2601.20532", "categories": ["cond-mat.dis-nn", "cond-mat.quant-gas", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20532", "abs": "https://arxiv.org/abs/2601.20532", "authors": ["Yucheng Wang"], "title": "A Unified Symmetry Classification of Many-Body Localized Phases", "comment": null, "summary": "Anderson localization admits a complete symmetry classification given by the Altland-Zirnbauer (AZ) tenfold scheme, whereas an analogous framework for interacting many-body localization (MBL) has remained elusive. Here we develop a symmetry-based classification of static MBL phases formulated at the level of local integrals of motion (LIOMs). We show that a symmetry is compatible with stable MBL if and only if its action can be consistently represented within a quasi-local LIOM algebra, without enforcing extensive degeneracies or nonlocal operator mixing. This criterion sharply distinguishes symmetry classes: onsite Abelian symmetries are compatible with stable MBL and can host distinct symmetry-protected topological MBL phases, whereas continuous non-Abelian symmetries generically preclude stable MBL. By systematically combining AZ symmetries with additional onsite symmetries, we construct a complete classification table of MBL phases, identify stable, fragile, and unstable classes, and provide representative lattice realizations. Our results establish a unified and physically transparent framework for understanding symmetry constraints on MBL."}
{"id": "2601.20076", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20076", "abs": "https://arxiv.org/abs/2601.20076", "authors": ["Abhishek Chakraborty", "Angelia Nedić"], "title": "Randomized Feasibility Methods for Constrained Optimization with Adaptive Step Sizes", "comment": null, "summary": "We consider minimizing an objective function subject to constraints defined by the intersection of lower-level sets of convex functions. We study two cases: (i) strongly convex and Lipschitz-smooth objective function and (ii) convex but possibly nonsmooth objective function. To deal with the constraints that are not easy to project on, we use a randomized feasibility algorithm with Polyak steps and a random number of sampled constraints per iteration, while taking (sub)gradient steps to minimize the objective function. For case (i), we prove linear convergence in expectation of the objective function values to any prescribed tolerance using an adaptive stepsize. For case (ii), we develop a fully problem parameter-free and adaptive stepsize scheme that yields an $O(1/\\sqrt{T})$ worst-case rate in expectation. The infeasibility of the iterates decreases geometrically with the number of feasibility updates almost surely, while for the averaged iterates, we establish an expected lower bound on the function values relative to the optimal value that depends on the distribution for the random number of sampled constraints. For certain choices of sample-size growth, optimal rates are achieved. Finally, simulations on a Quadratically Constrained Quadratic Programming (QCQP) problem and Support Vector Machines (SVM) demonstrate the computational efficiency of our algorithm compared to other state-of-the-art methods."}
{"id": "2601.20000", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20000", "abs": "https://arxiv.org/abs/2601.20000", "authors": ["Alina Chertock", "Qingcheng Fu", "Alexander Kurganov", "Lorenzo Micalizzi"], "title": "New Adaptive Numerical Methods Based on Dual Formulation of Hyperbolic Conservation Laws", "comment": null, "summary": "In this paper, we propose an adaptive high-order method for hyperbolic systems of conservation laws. The proposed method is based on a dual formulation approach: Two numerical solutions, corresponding to conservative and nonconservative formulations of the same system, are evolved simultaneously. Since nonconservative schemes are known to produce nonphysical weak solutions near discontinuities, we exploit the difference between these two solutions to construct a smoothness indicator (SI). In smooth regions, the difference between the conservative and nonconservative solutions is of the same order as the truncation error of the underlying discretization, whereas in nonsmooth regions, it is ${\\cal O}(1)$. We apply this idea to the Euler equations of gas dynamics and define the SI using differences in the momentum and pressure variables. This choice allows us to further distinguish neighborhoods of contact discontinuities from other nonsmooth parts of the computed solution. The resulting classification is used to adaptively select numerical discretizations. In the vicinities of contact discontinuities, we employ the low-dissipation central-upwind numerical flux and a second-order piecewise linear reconstruction with the slopes computed using an overcompressive SBM limiter. Elsewhere, we use an alternative weighted essentially non-oscillatory (A-WENO) framework with the central-upwind finite-volume numerical fluxes and either unlimited (in smooth regions) or Ai-WENO-Z (in the nonsmooth regions away from contact discontinuities) fifth-order interpolation. Numerical results for the one- and two-dimensional compressible Euler equations show that the proposed adaptive method improves both the computational efficiency and resolution of complex flow features compared with the non-adaptive fifth-order A-WENO scheme."}
{"id": "2601.20150", "categories": ["hep-lat", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.20150", "abs": "https://arxiv.org/abs/2601.20150", "authors": ["Laurence G. Yaffe"], "title": "Resurrecting the coherent state variational algorithm for large $N$ gauge theories", "comment": "45 pages", "summary": "The feasibility of studying, numerically, properties of infinite volume QCD-like theories in the large $N$ limit using coherent state variational methods is reassessed. An entirely new implementation of this approach is described, applicable to SU($N$) lattice gauge theories, with or without fundamental representation fermions, on cubic lattices of up to four dimensions. In addition to various test cases, initial results are presented for Hamiltonian Yang-Mills theory on an infinite two-dimensional spatial lattice."}
{"id": "2601.20049", "categories": ["physics.geo-ph", "physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.20049", "abs": "https://arxiv.org/abs/2601.20049", "authors": ["Kristoffer Aalstad", "Esteban Alonso-González", "Norbert Pirk", "Sebastian Westermann", "Clarissa Willmes", "Ruitang Yang"], "title": "Evolving beyond collapse: An adaptive particle batch smoother for cryospheric data assimilation", "comment": "Kristoffer Aalstad and Esteban Alonso-González contributed equally as joint first authors to this work", "summary": "We present a new adaptive particle-based data assimilation scheme for cryospheric applications that leverages promising developments in importance sampling. The proposed approach seeks to combine some of the advantages of two widely used classes of schemes: particle methods and iterative ensemble Kalman methods. Specifically, it extends the PBS that is commonly used in cryospheric data assimilation, with the AMIS algorithm. This adaptive formulation transforms the PBS into an iterative scheme with improved resilience against ensemble collapse and the ability to implement early-stopping strategies. As such, computational cost is automatically adapted to the complexity of the problem at hand, even down to the grid-cell and water year level in distributed multiyear simulations. In homage to the schemes that it builds on, we coin this new algorithm the Adaptive Particle Batch Smoother (AdaPBS) and we test it across a range of scenarios. First, we conducted an intercomparison of some of the most commonly used cryospheric data assimilation algorithms using MCMC simulation as a costly gold-standard benchmark in a simplified temperature index model assimilating snow depth observations. We further evaluated AdaPBS by assimilating snow depth observations from the ESMSnowMIP project at 6 different sites spanning 3 continents, using an ensemble of simulations generated with the more complex FSM2. Our results demonstrate that AdaPBS is a robust and reliable tool, outperforming or at least matching the performance of other commonly used algorithms and successfully handling complex cases with dense observational datasets. All experiments were carried out using the open-source MuSA toolbox, which now includes AdaPBS and MCMC among the growing list of available cryospheric data assimilation methods."}
{"id": "2601.20132", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20132", "abs": "https://arxiv.org/abs/2601.20132", "authors": ["Drew Yarger"], "title": "Connecting reflective asymmetries in multivariate spatial and spatio-temporal covariances", "comment": null, "summary": "In the analysis of multivariate spatial and univariate spatio-temporal data, it is commonly recognized that asymmetric dependence may exist, which can be addressed using an asymmetric (matrix or space-time, respectively) covariance function within a Gaussian process framework. This paper introduces a new paradigm for constructing asymmetric space-time covariances, which we refer to as \"reflective asymmetric,\" by leveraging recently-introduced models for multivariate spatial data. We first provide new results for reflective asymmetric multivariate spatial models that extends their applicability. We then propose their asymmetric space-time extension, which come from a substantially different perspective than Lagrangian asymmetric space-time covariances. There are fewer parameters in the new models, one controls both the spatial and temporal marginal covariances, and the standard separable model is a special case. In simulation studies and analysis of the frequently-studied Irish wind data, these new models also improve model fit and prediction performance, and they can be easier to estimate. These features indicate broad applicability for improved analysis in environmental and other space-time data."}
{"id": "2601.20005", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20005", "abs": "https://arxiv.org/abs/2601.20005", "authors": ["Zixin Jiang", "Weili Xu", "Bing Dong"], "title": "OptAgent: an Agentic AI framework for Intelligent Building Operations", "comment": null, "summary": "The urgent need for building decarbonization calls for a paradigm shift in future autonomous building energy operation, from human-intensive engineering workflows toward intelligent agents that interact with physics-grounded digital environments. This study proposes an end-to-end agentic AI-enabled Physics-Informed Machine Learning (PIML) environment for scalable building energy modeling, simulation, control, and automation. The framework consists of (1) a modular and physics-consistent PIML digital environment spanning building thermal dynamics, Heating, Ventilation, and Air Conditioning (HVAC), and distributed energy resources (DER) for grid-interactive energy management; and (2) an agentic AI layer with 11 specialist agents and 72 Model Context Protocol (MCP) tools that enable end-to-end execution of multi-step energy analytics. A representative case study demonstrates multi-domain, multi-agent coordination for assessing how system and control upgrades affect energy use, operating cost, thermal comfort, and flexibility. In addition, a large-scale benchmark (about 4000 runs) systematically evaluates workflow performance in terms of accuracy, token consumption, execution time, and inference cost. The results quantify the impacts of intelligence mode design, model size, task complexity, and orchestrator-specialist coordination, and provide key lessons for building future agentic AI systems in real-world building energy applications. This work establishes a scalable, physics-grounded foundation for deploying agentic AI in decarbonized and grid-interactive building operations."}
{"id": "2601.19937", "categories": ["physics.soc-ph", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.19937", "abs": "https://arxiv.org/abs/2601.19937", "authors": ["Sean Elliott", "Sohini Roy"], "title": "Critical Transit Infrastructure in Smart Cities and Urban Air Quality: A Multi-City Seasonal Comparison of Ridership and PM2.5", "comment": null, "summary": "Public transit is a critical component of urban mobility and equity, yet mobility and air-quality linkages are rarely operationalized in reproducible smart-city analytics workflows. This study develops a transparent, multi-source monitoring dataset that integrates agency-reported transit ridership with ambient fine particulate matter PM2.5 from the U.S. EPA Air Quality System (AQS) for four U.S. metropolitan areas - New York City, Chicago, Las Vegas, and Phoenix, using two seasonal snapshots (March and October 2024). We harmonize heterogeneous ridership feeds (daily and stop-level) to monthly system totals and pair them with monthly mean PM2.5 , reporting both absolute and per-capita metrics to enable cross-city comparability. Results show pronounced structural differences in transit scale and intensity, with consistent seasonal shifts in both ridership and PM2.5 that vary by urban context. A set of lightweight regression specifications is used as a descriptive sensitivity analysis, indicating that apparent mobility-PM2.5 relationships are not uniform across cities or seasons and are strongly shaped by baseline city effects. Overall, the paper positions integrated mobility and environment monitoring as a practical smart-city capability, offering a scalable framework for tracking infrastructure utilization alongside exposure-relevant air-quality indicators to support sustainable communities and public-health-aware urban resilience."}
{"id": "2601.19959", "categories": ["cond-mat.str-el", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2601.19959", "abs": "https://arxiv.org/abs/2601.19959", "authors": ["D. Belitz", "T. R. Kirkpatrick"], "title": "Comment on \"Instability of the ferromagnetic quantum critical point and symmetry of the ferromagnetic ground state in two-dimensional and three-dimensional electron gases with arbitrary spin-orbit splitting\"", "comment": "4pp; comment on arXiv:2201.10995", "summary": "Metallic quantum ferromagnets in the absence of quenched disorder are known to generically undergo a first-order quantum phase transition, avoiding the quantum critical point that had originally been expected. This is due to soft modes in the underlying Fermi liquid that lead to long-ranged correlations. These correlations in turn yield a nonanalytic dependence of the free energy on the magnetization even at a mean-field level that results in a fluctuation-induced first-order transition. Kirkpatrick and Belitz [Phys. Rev. Lett. {\\bf 124}, 147201 (2020)] have pointed out that one notable exception are non-centrosymmetric metals with a strong spin-orbit interaction. In such materials the spin-orbit interaction gives the relevant soft modes a mass, which inhibits the mechanism leading to a first-order transition. Miserev, Loss, and Klinovaja [Phys. Rev. B {\\bf 106}, 134417 (2022)] have claimed that this conclusion does not hold if electron-electron interactions in the particle-particle channel, or 2$\\kF$ scattering processes, are considered. They concluded that this interaction channel leads to soft modes that are not rendered massive by the spin-orbit interaction and again lead to a first-order quantum phase transition. In this Comment we show that this conclusion is not correct in three-dimensional magnets if the screening of the interaction is properly taken into account."}
{"id": "2601.20342", "categories": ["physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.20342", "abs": "https://arxiv.org/abs/2601.20342", "authors": ["Haofei Sun", "Yunfan Yang", "Wei Han", "Wei Huang", "Huaguan Chen", "Zhiqiu Gao", "Zeting Li", "Zhaoyang Huo", "Zeyi Niu"], "title": "StormDiT: A generative AI model bridges the 2-6 hour 'gray zone' in precipitation nowcasting", "comment": null, "summary": "Accurate short-term warnings for extreme precipitation are critical for global disaster mitigation but are hindered by a persistent predictability barrier at the 2-6 hour horizon -- the \"nowcasting gray zone.\" In this window, traditional observation-based extrapolation fails due to error accumulation, while numerical weather prediction is computationally too slow to resolve storm-scale dynamics. Recent generative AI approaches attempt to bridge this gap by decomposing precipitation into separate deterministic advection and stochastic diffusion components. However, this decomposition can sever fundamental causal links between entangled atmospheric processes, such as the dynamic initiation of convection triggered by boundary advection. Here we present StormDiT, a unified generative model that treats weather evolution as a holistic spatiotemporal problem, learning the coupled physics of the gray zone without human-imposed structural priors. Trained on a massive dataset of 7,720 precipitation events from China, our model achieves a breakthrough in long-horizon stability. On a heavy-rainfall test set, it maintains skillful prediction for strong convection ($\\ge$ 35 dBZ) with a Critical Success Index (CSI) near 0.2 across the full 6-hour forecast at 6-minute resolution. Crucially, the model exhibits superior probabilistic calibration, accurately quantifying operational risks. On the public SEVIR benchmark, our unified paradigm more than doubles the state-of-the-art 1-hour performance for heavy rain and establishes the first robust baseline for 3-hour forecasting. Furthermore, interpretability analysis reveals that the model attends to non-local physical precursors, such as outflow boundaries, explicitly validating its emergent understanding of convective organization."}
{"id": "2601.20336", "categories": ["q-fin.CP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20336", "abs": "https://arxiv.org/abs/2601.20336", "authors": ["Murad Farzulla"], "title": "Do Whitepaper Claims Predict Market Behavior? Evidence from Cryptocurrency Factor Analysis", "comment": "35 pages, 8 figures, 10 tables. Code available at https://github.com/studiofarzulla/tensor-defi", "summary": "Cryptocurrency projects articulate value propositions through whitepapers, making claims about functionality and technical capabilities. This study investigates whether these narratives align with observed market behavior. We construct a pipeline combining zero-shot NLP classification (BART-MNLI) with CP tensor decomposition to compare three spaces: (1) a claims matrix from 24 whitepapers across 10 semantic categories, (2) market statistics for 49 assets over two years of hourly data, and (3) latent factors from tensor decomposition (rank 2, 92.45% variance explained). Using Procrustes rotation and Tucker's congruence coefficient, we test alignment across 23 common entities.\n  Results show weak alignment: claims-statistics (phi=0.341, p=0.332), claims-factors (phi=0.077, p=0.747), and statistics-factors (phi=0.197, p<0.001). The statistics-factors significance validates our methodology, confirming the pipeline detects relationships when present. Inter-model validation with DeBERTa-v3 yields 32% exact agreement but 67% top-3 agreement. Cross-sectional analysis reveals heterogeneous contributions: NEAR, MKR, ATOM show positive alignment while ENS, UNI, Bitcoin diverge most. Excluding Bitcoin confirms results are not driven by market dominance.\n  We interpret findings as weak alignment between whitepaper narratives and market factor structure. Limited power (n=23) precludes distinguishing weak from no alignment, but strong alignment (phi>=0.70) can be confidently rejected. Implications for narrative economics and investment analysis are discussed."}
{"id": "2601.19976", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.19976", "abs": "https://arxiv.org/abs/2601.19976", "authors": ["Tian-Xing Zheng", "M. Iqbal Bakti Utama", "Xingyu Gao", "Moumita Kar", "Xiaofei Yu", "Sungsu Kang", "Hanyan Cai", "Tengyang Ruan", "David Ovetsky", "Uri Zvi", "Guanming Lao", "Yu-Xin Wang", "Omri Raz", "Sanskriti Chitransh", "Grant T. Smith", "Leah R. Weiss", "Magdalena H. Czyz", "Shengsong Yang", "Alex J. Fairhall", "Kenji Watanabe", "Takashi Taniguchi", "David D. Awschalom", "A. Paul Alivisatos", "Randall H. Goldsmith", "George C. Schatz", "Mark C. Hersam", "Peter C. Maurer"], "title": "A Surface-Scaffolded Molecular Qubit", "comment": "Main text, 10 pages, 4 figures", "summary": "Fluorescent spin qubits are central building blocks of quantum technologies. Placing these qubits at surfaces maximizes coupling to nearby spins and fields, enabling nanoscale sensing and facilitating integration with photonic and superconducting devices. However, reducing the dimensions or size of established qubit systems without sacrificing the qubit performance or degrading the coherence lifetime remains challenging. Here, we introduce a surface molecular qubit formed by pentacene molecules scaffolded on a two-dimensional (2D) material, hexagonal boron nitride (hBN). The qubit exhibits stable fluorescence and optically detected magnetic resonance (ODMR) from cryogenic to ambient conditions. With fully deuterated pentacene, the Hahn-echo coherence reaches 22 $μ$s and further extends to 214 $μ$s under dynamical decoupling, outperforming state-of-the-art shallow NV centers in diamond, despite being positioned directly on the surface. We map the local spin environment, resolving couplings to nearby nuclear and electron spins that can serve as auxiliary quantum resources. This platform combines true surface integration, long qubit coherence, and scalable fabrication, opening routes to quantum sensing, quantum simulation, and hybrid quantum devices. It also paves the way for a broader family of 2D material-supported molecular qubits."}
{"id": "2601.20077", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.20077", "abs": "https://arxiv.org/abs/2601.20077", "authors": ["Gabriele de Mauro", "Satya N. Majumdar", "Gregory Schehr"], "title": "Tuning the strength of emergent correlations in a Brownian gas via batch resetting", "comment": "8 + 20 pages, 3 + 6 figures", "summary": "We study a gas of $N$ diffusing particles on the line subject to batch resetting: at rate $r$, a uniformly random subset of $m$ particles is reset to the origin. Despite the absence of interactions, the dynamics generates a nonequilibrium stationary state (NESS) with long-range correlations. We obtain exact results, both for the NESS and for the time dependence of the correlations, which are valid for arbitrary $m$ and $N$. By varying $m$, the system interpolates between an uncorrelated regime ($m=1$) and the fully synchronous resetting case ($m=N$). For all $1<m<N$, correlations exhibit a non-monotonic time dependence due to the emergence of an intrinsic decorrelation mechanism. In the stationary state, the correlation strength can be tuned by varying $m$, and it displays a transition at a critical value $N_c=6$. Our predictions extend straightforwardly to any spatial dimension $d$ and the critical value $N_c=6$ remains the same in all dimensions. Our predictions are testable in existing experimental setups on optically trapped colloidal particles."}
{"id": "2601.20314", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20314", "abs": "https://arxiv.org/abs/2601.20314", "authors": ["Xinran Wang", "Peng Wu", "Xiaopeng Yuan", "Yulin Hu", "Anke Schmeink"], "title": "Efficient Trajectory Design and Communication Scheduling for Dual-UAV Jamming-Aided Secure Communication Networks", "comment": null, "summary": "We study dual-unmanned aerial vehicle (UAV) jamming-aided secure communication networks, in which one UAV delivers confidential data to multiple ground users (GUs), while a cooperative UAV provides protective interference against a ground eavesdropper. To enforce fairness, we maximize the minimum secrecy throughput across GUs by jointly designing trajectories and communication scheduling. The key difficulty lies in the continuous-time nature of UAV trajectories and the tight space-time coupling between the transmitter and the jammer, which jointly render the problem infinite-dimensional and nonconvex. To address these challenges, we characterize, for the first time, the structure of the optimal trajectories and rigorously prove that they follow a collaborative successive hover-and-fly (co-SHF) structure, where the two UAVs visit a limited number of synchronized co-hovering point pairs, and during each flight segment at least one UAV moves at maximum speed. Leveraging this structure, we reformulate the problem into a finite-dimensional form, without loss of optimality, over hovering and turning points, hovering durations, and scheduling. For tractability, we adopt a minimum-distance approximation of continuous anti-collision constraints and employ concave lower bounds on secrecy throughput within a successive convex approximation (SCA) method, which converges and, thanks to the co-SHF reduction in optimization variables and constraints, achieves low computational complexity. Numerical results show that, compared with time-discretization and no-jamming benchmarks, the proposed co-SHF design improves the min-secrecy and user fairness while requiring significantly less runtime."}
{"id": "2601.20646", "categories": ["cs.SI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20646", "abs": "https://arxiv.org/abs/2601.20646", "authors": ["Zhejian Yang", "Songwei Zhao", "Zilin Zhao", "Hechang Chen"], "title": "TGSBM: Transformer-Guided Stochastic Block Model for Link Prediction", "comment": "12 pages, 4 figures", "summary": "Link prediction is a cornerstone of the Web ecosystem, powering applications from recommendation and search to knowledge graph completion and collaboration forecasting. However, large-scale networks present unique challenges: they contain hundreds of thousands of nodes and edges with heterogeneous and overlapping community structures that evolve over time. Existing approaches face notable limitations: traditional graph neural networks struggle to capture global structural dependencies, while recent graph transformers achieve strong performance but incur quadratic complexity and lack interpretable latent structure. We propose \\textbf{TGSBM} (Transformer-Guided Stochastic Block Model), a framework that integrates the principled generative structure of Overlapping Stochastic Block Models with the representational power of sparse Graph Transformers. TGSBM comprises three main components: (i) \\emph{expander-augmented sparse attention} that enables near-linear complexity and efficient global mixing, (ii) a \\emph{neural variational encoder} that infers structured posteriors over community memberships and strengths, and (iii) a \\emph{neural edge decoder} that reconstructs links via OSBM's generative process, preserving interpretability. Experiments across diverse benchmarks demonstrate competitive performance (mean rank 1.6 under HeaRT protocol), superior scalability (up to $6\\times$ faster training), and interpretable community structures. These results position TGSBM as a practical approach that strikes a balance between accuracy, efficiency, and transparency for large-scale link prediction."}
{"id": "2601.20462", "categories": ["cs.CE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.20462", "abs": "https://arxiv.org/abs/2601.20462", "authors": ["Shan Tang", "Ziwei Cao", "Zhenling Yang", "Jiachen Guo", "Yicheng Lu", "Wing Kam Liu", "Xu Guo"], "title": "CM-GAI: Continuum Mechanistic Generative Artificial Intelligence Theory for Data Dynamics", "comment": null, "summary": "Generative artificial intelligence (GAI) plays a fundamental role in high-impact AI-based systems such as SORA and AlphaFold. Currently, GAI shows limited capability in the specialized domains due to data scarcity. In this paper, we develop a continuum mechanics-based theoretical framework to generalize the optimal transport theory from pure mathematics, which can be used to describe the dynamics of data, realizing the generative tasks with a small amount of data. The developed theory is used to solve three typical problem involved in many mechanical designs and engineering applications: at material level, how to generate the stress-strain response outside the range of experimental conditions based on experimentally measured stress-strain data; at structure level, how to generate the temperature-dependent stress fields under the thermal loading; at system level, how to generate the plastic strain fields under transient dynamic loading. Our results show the proposed theory can complete the generation successfully, showing its potential to solve many difficult problems involved in engineering applications, not limited to mechanics problems, such as image generation. The present work shows that mechanics can provide new tools for computer science. The limitation of the proposed theory is also discussed."}
{"id": "2601.20020", "categories": ["math.ST", "math.PR", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20020", "abs": "https://arxiv.org/abs/2601.20020", "authors": ["Zhirui Li", "Keith D. Levin", "Zhiang Zhao", "Vince Lyzinski"], "title": "Matching and mixing: Matchability of graphs under Markovian error", "comment": "48 pages, 12 figures", "summary": "We consider the problem of graph matching for a sequence of graphs generated under a time-dependent Markov chain noise model. Our edgelighter error model, a variant of the classical lamplighter random walk, iteratively corrupts the graph $G_0$ with edge-dependent noise, creating a sequence of noisy graph copies $(G_t)$. Much of the graph matching literature is focused on anonymization thresholds in edge-independent noise settings, and we establish novel anonymization thresholds in this edge-dependent noise setting when matching $G_0$ and $G_t$. Moreover, we also compare this anonymization threshold with the mixing properties of the Markov chain noise model. We show that when $G_0$ is drawn from an Erdős-Rényi model, the graph matching anonymization threshold and the mixing time of the edgelighter walk are both of order $Θ(n^2\\log n)$. We further demonstrate that for more structured model for $G_0$ (e.g., the Stochastic Block Model), graph matching anonymization can occur in $O(n^α\\log n)$ time for some $α<2$, indicating that anonymization can occur before the Markov chain noise model globally mixes. Through extensive simulations, we verify our theoretical bounds in the settings of Erdős-Rényi random graphs and stochastic block model random graphs, and explore our findings on real-world datasets derived from a Facebook friendship network and a European research institution email communication network."}
{"id": "2601.20462", "categories": ["cs.CE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.20462", "abs": "https://arxiv.org/abs/2601.20462", "authors": ["Shan Tang", "Ziwei Cao", "Zhenling Yang", "Jiachen Guo", "Yicheng Lu", "Wing Kam Liu", "Xu Guo"], "title": "CM-GAI: Continuum Mechanistic Generative Artificial Intelligence Theory for Data Dynamics", "comment": null, "summary": "Generative artificial intelligence (GAI) plays a fundamental role in high-impact AI-based systems such as SORA and AlphaFold. Currently, GAI shows limited capability in the specialized domains due to data scarcity. In this paper, we develop a continuum mechanics-based theoretical framework to generalize the optimal transport theory from pure mathematics, which can be used to describe the dynamics of data, realizing the generative tasks with a small amount of data. The developed theory is used to solve three typical problem involved in many mechanical designs and engineering applications: at material level, how to generate the stress-strain response outside the range of experimental conditions based on experimentally measured stress-strain data; at structure level, how to generate the temperature-dependent stress fields under the thermal loading; at system level, how to generate the plastic strain fields under transient dynamic loading. Our results show the proposed theory can complete the generation successfully, showing its potential to solve many difficult problems involved in engineering applications, not limited to mechanics problems, such as image generation. The present work shows that mechanics can provide new tools for computer science. The limitation of the proposed theory is also discussed."}
{"id": "2601.20608", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2601.20608", "abs": "https://arxiv.org/abs/2601.20608", "authors": ["Tao Chen", "Jing Liu", "Yantao Wu", "Pan Zhang", "Youjin Deng"], "title": "Variational Monte Carlo (VMC) with row-update Projected Entangled-Pair States (PEPS) and its applications in quantum spin glasses", "comment": "6 pages, 4 figures", "summary": "Solving the quantum many-body ground state problem remains a central challenge in computational physics. In this context, the Variational Monte Carlo (VMC) framework based on Projected Entangled Pair States (PEPS) has witnessed rapid development, establishing itself as a vital approach for investigating strongly correlated two-dimensional systems. However, standard PEPS-VMC algorithms predominantly rely on sequential local updates. This conventional approach often suffers from slow convergence and critical slowing down, particularly in the vicinity of phase transitions or within frustrated landscapes. To address these limitations, we propose an efficient autoregressive row-wise sampling algorithm for PEPS that enables direct, rejection-free sampling via single-layer contractions. By utilizing autoregressive single-layer row updates to generate collective, non-local configuration proposals, our method significantly reduces temporal correlations compared to local Metropolis moves. We benchmark the algorithm on the two-dimensional transverse-field Ising model and the quantum spin glass. Our results demonstrate that the row-wise scheme effectively mitigates critical slowing down near the Ising critical point. Furthermore, in the rugged landscape of the quantum spin glass, it yields improved optimization stability and lower ground-state energies. These findings indicate that single-layer autoregressive row updates provide a flexible and robust improvement to local PEPS-VMC sampling and may serve as a basis for more advanced sampling schemes."}
{"id": "2601.20181", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20181", "abs": "https://arxiv.org/abs/2601.20181", "authors": ["Christian Parkinson", "Souvik Roy"], "title": "A Fokker-Planck Framework for Control of Epidemics", "comment": "20 pages, 5 figures", "summary": "We present a control framework for stochastic compartmental models in epidemiology. In this framework, rather than directly controlling the stochastic system, we perform optimal control of an associated Fokker-Planck equation, with the goal of steering the distribution of possible solutions of the stochastic system to some desirable state. In particular, this allows for robust control mechanism with uncertainty not only in the dynamics, but also in the initial data. We formulate and fully analyze a partial differential equation constrained optimization problem, including a proof of existence of optimal controls via analysis of the control-to-state map, and a characterization of optimal controls via the Pontryagin minimum principle. We describe the application of the sequential quadratic Hamiltonian method to our problem, which provides numerical approximations of optimal control maps. We demonstrate our method using a minimal stochastic susceptible-infected-recovered model with different choices of cost functionals that represent different policy-maker concerns."}
{"id": "2601.20013", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20013", "abs": "https://arxiv.org/abs/2601.20013", "authors": ["Min Liu", "Zhiqiang Cai"], "title": "Least-Squares Neural Network (LSNN) Method for Scalar Hyperbolic Partial Differential Equations", "comment": "25 pages, 5 figures", "summary": "This chapter offers a comprehensive introduction to the least-squares neural network (LSNN) method introduced in [14,16], for solving scalar first-order hyperbolic partial differential equations, specifically linear advection-reaction equations and nonlinear hyperbolic conservation laws. The LSNN method is built on an equivalent least-squares formulation of the underlying problem on an admissible solution set that accommodates discontinuous solutions. It employs ReLU neural networks (in place of finite elements) as the approximating functions, uses a carefully designed physics-preserved numerical differentiation, and avoids penalization techniques such as artificial viscosity, entropy condition, and/or total variation. This approach captures shock features in the solution without oscillations or overshooting. Efficiently and reliably solving the resulting non-convex optimization problem posed by the LSNN method remains an open challenge. This chapter concludes with a brief discussion on application of the structure-guided Gauss-Newton (SgGN) method developed recently in [21] for solving shallow NN approximation."}
{"id": "2601.20159", "categories": ["hep-lat"], "pdf": "https://arxiv.org/pdf/2601.20159", "abs": "https://arxiv.org/abs/2601.20159", "authors": ["Okuto Morikawa", "Hiroshi Suzuki"], "title": "Direct numerical simulation of the 't Hooft partition function and (de)confining phases", "comment": "8 pages, 4 figures, talk presented at the 42nd International Symposium on Lattice Field Theory (Lattice2025), November 2nd - 8th, 2025, Tata Institute of Fundamental Research (TIFR), Mumbai", "summary": "The 't Hooft partition function $Z_{\\mathrm{tH}}[E_i;B_{ij}]$ is a discrete Fourier transform of Yang--Mills partition functions in background $\\mathbb{Z}_N$ 2-form gauge fields and encodes information on confinement, Higgs, Coulomb and oblique-confining phases. We report a direct Monte Carlo strategy to measure $Z_{\\mathrm{tH}}$ without reweighting, by extending hybrid Monte Carlo to include dynamical updates of the background flux variables. As a first application we measure all flux sectors of four-dimensional $SU(2)$ lattice Yang--Mills on $T^4$ and observe the characteristic ``light/heavy'' behavior expected in the confining phase, together with the shift implied by the Witten effect at $θ=2π$. We also present a preliminary finite-temperature study and discuss outstanding issues on thermalization and separability between different flux sectors."}
{"id": "2601.20136", "categories": ["physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.20136", "abs": "https://arxiv.org/abs/2601.20136", "authors": ["Rikuto Fukushima", "Masayuki Kano", "Kazuro Hirahara", "Makiko Ohtani"], "title": "Physics-informed deep learning links geodetic data and fault friction", "comment": "38 pages, 14 figures", "summary": "Fault slip modeling, based on laboratory-derived friction laws, has significantly enhanced our understanding of fault mechanics. Agreement between model predictions and observations supports the hypothesis that observed slip diversity, including fast earthquakes and slow transient slips (Slow Slip Events; SSEs), originates from frictional heterogeneity. However, quantitative assessments of frictional heterogeneity from geodetic observations while fully incorporating fault mechanics are lacking due to the difficulties of high-dimensional optimization. In this study, we aim to address this gap using Physics-Informed Neural Networks (PINNs) to link frictional heterogeneity with geodetic observations. PINNs employ a neural network to represent the spatially variable frictional properties, making their estimation feasible. Targeting the 2010 Bungo SSE in southwest Japan, our estimation reveals heterogeneous friction coinciding with localized SSE nucleation in southwest Shikoku, and subsequent westward propagation. The calculated fault slip of SSE successfully reproduces the spatio-temporal pattern of observed surface displacements. This PINN-based inversion provides a mechanically consistent fault slip model validated through quantitative comparison with observations. Furthermore, we predict the future fault slip evolution, demonstrating the importance of assimilating observations spanning multiple SSE cycles. Our results demonstrate the potential of PINN for advancing understanding of fault mechanics and enabling physics-based fault slip forecasting."}
{"id": "2601.20192", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20192", "abs": "https://arxiv.org/abs/2601.20192", "authors": ["Xiaokai Luo", "Haotian Xu", "Carlos Misael Madrid Padilla", "Oscar Hernan Madrid Padilla"], "title": "Online Change Point Detection for Multivariate Inhomogeneous Poisson Processes Time Series", "comment": null, "summary": "We study online change point detection for multivariate inhomogeneous Poisson point process time series. This setting arises commonly in applications such as earthquake seismology, climate monitoring, and epidemic surveillance, yet remains underexplored in the machine learning and statistics literature. We propose a method that uses low-rank matrices to represent the multivariate Poisson intensity functions, resulting in an adaptive nonparametric detection procedure. Our algorithm is single-pass and requires only constant computational cost per new observation, independent of the elapsed length of the time series. We provide theoretical guarantees to control the overall false alarm probability and characterize the detection delay under temporal dependence. We also develop a new Matrix Bernstein inequality for temporally dependent Poisson point process time series, which may be of independent interest. Numerical experiments demonstrate that our method is both statistically robust and computationally efficient."}
{"id": "2601.20135", "categories": ["eess.SY", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2601.20135", "abs": "https://arxiv.org/abs/2601.20135", "authors": ["Domitilla Del Vecchio"], "title": "Control systems for synthetic biology and a case-study in cell fate reprogramming", "comment": null, "summary": "This paper gives an overview of the use of control systems engineering in synthetic biology, motivated by applications such as cell therapy and cell fate reprogramming for regenerative medicine. A ubiquitous problem in these and other applications is the ability to control the concentration of specific regulatory factors in the cell accurately despite environmental uncertainty and perturbations. The paper describes the origin of these perturbations and how they affect the dynamics of the biomolecular ``plant'' to be controlled. A variety of biomolecular control implementations are then introduced to achieve robustness of the plant's output to perturbations and are grouped into feedback and feedforward control architectures. Although sophisticated control laws can be implemented in a computer today, they cannot be necessarily implemented inside the cell via biomolecular processes. This fact constraints the set of feasible control laws to those realizable through biomolecular processes that can be engineered with synthetic biology. After reviewing biomolecular feedback and feedforward control implementations, mostly focusing on the author's own work, the paper illustrates the application of such control strategies to cell fate reprogramming. Within this context, a master regulatory factor needs to be controlled at a specific level inside the cell in order to reprogram skin cells to pluripotent stem cells. The article closes by highlighting on-going challenges and directions of future research for biomolecular control design."}
{"id": "2601.20450", "categories": ["physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.20450", "abs": "https://arxiv.org/abs/2601.20450", "authors": ["David Martin", "José Moran", "Debabrata Panja", "Jean-Philippe Bouchaud"], "title": "Resilient-to-Fragile Transition and Excess Volatility in Supply Chain Networks", "comment": "37 pages, 6 figures", "summary": "We study the disequilibrium dynamics of a stylised model of production networks in which firms use perishable and non-substitutable intermediate inputs, so that adverse idiosyncratic productivity shocks can trigger downstream shortages and output losses. To protect against such disruptions, firms hold precautionary inventories that act as buffer stocks. We show that, for a given dispersion of firm-level productivity shocks, there exists a critical level of inventories above which the economy remains in a stable stochastic steady state. Below this critical level, the system becomes fragile, i.e., it becomes prone to system-wide crises. As this resilience-fragility boundary is approached from above, aggregate output volatility rises sharply and diverges, even though shocks are purely idiosyncratic. Because inventories are costly, competitive pressures induce firms to economize on buffers. Although we do not explicitly model such costs, we argue that the resulting behaviour of individual firms drives the system close to criticality, generating persistent excess macroeconomic volatility -- in other words, ``small shocks, large cycles'' -- in line with other settings where efficiency and resilience are in tension with each other. In the language of phase transitions, the resilient-to-fragile transition is continuous (supercritical): the economy exhibits a well-defined stochastic equilibrium with finite volatility on one side of the boundary, while beyond it the probability of a collapse in finite time tends to one. We characterize this transition primarily through numerical simulations and derive an analytical description in a high-perishability, high-connectivity limit."}
{"id": "2601.19980", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.19980", "abs": "https://arxiv.org/abs/2601.19980", "authors": ["Andreas Feuerpfeil", "Leyna Shackleton", "Atanu Maity", "Ronny Thomale", "Subir Sachdev", "Yasir Iqbal"], "title": "Unifying Dirac Spin Liquids on Square and Shastry-Sutherland Lattices via Fermionic Deconfined Criticality", "comment": "36 pages, 11 figures, 4 tables", "summary": "We present a fermionic gauge theory for deconfined quantum criticality on the Shastry-Sutherland lattice and reveal its shared low-energy field-theoretic structure with the square lattice. Starting from an SU(2) $π$-flux parent state, we construct a continuum theory of Dirac spinons coupled to an SU(2) gauge field and adjoint Higgs fields whose condensates drive transitions to a staggered-flux U(1) spin liquid and a gapless $\\mathbb{Z}_{2}$ Dirac spin liquid. While the Shastry-Sutherland lattice permits additional symmetry-allowed fermion bilinears compared to the square lattice, the quantum field theories are identical up to additional irrelevant terms. Consequently, the Higgs potential structure and the leading low-energy theory coincide with the square-lattice case at the quantum critical point. The SO(5) critical point is expected to realize conformal deconfined criticality: we analyze it in a large flavor expansion, calculate its critical exponents, and identify the Yukawa coupling between the fermions and Higgs fields as the relevant perturbation that destabilizes it, consistent with pseudocritical behavior observed in recent Monte Carlo studies. We show that the emergent SO(5) order parameter acquires a large anomalous dimension at the critical point, leading to strongly enhanced Néel and VBS susceptibilities-a hallmark of fermionic deconfined quantum criticality consistent with numerical studies. Our results place recent numerical evidence for a gapless $\\mathbb{Z}_{2}$ Dirac spin liquid on the Shastry-Sutherland lattice within a controlled field-theoretic framework and demonstrate that fermionic deconfined criticality on the square lattice-including critical exponents and stability-extends to frustrated lattices with reduced symmetry."}
{"id": "2601.20049", "categories": ["physics.geo-ph", "physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.20049", "abs": "https://arxiv.org/abs/2601.20049", "authors": ["Kristoffer Aalstad", "Esteban Alonso-González", "Norbert Pirk", "Sebastian Westermann", "Clarissa Willmes", "Ruitang Yang"], "title": "Evolving beyond collapse: An adaptive particle batch smoother for cryospheric data assimilation", "comment": "Kristoffer Aalstad and Esteban Alonso-González contributed equally as joint first authors to this work", "summary": "We present a new adaptive particle-based data assimilation scheme for cryospheric applications that leverages promising developments in importance sampling. The proposed approach seeks to combine some of the advantages of two widely used classes of schemes: particle methods and iterative ensemble Kalman methods. Specifically, it extends the PBS that is commonly used in cryospheric data assimilation, with the AMIS algorithm. This adaptive formulation transforms the PBS into an iterative scheme with improved resilience against ensemble collapse and the ability to implement early-stopping strategies. As such, computational cost is automatically adapted to the complexity of the problem at hand, even down to the grid-cell and water year level in distributed multiyear simulations. In homage to the schemes that it builds on, we coin this new algorithm the Adaptive Particle Batch Smoother (AdaPBS) and we test it across a range of scenarios. First, we conducted an intercomparison of some of the most commonly used cryospheric data assimilation algorithms using MCMC simulation as a costly gold-standard benchmark in a simplified temperature index model assimilating snow depth observations. We further evaluated AdaPBS by assimilating snow depth observations from the ESMSnowMIP project at 6 different sites spanning 3 continents, using an ensemble of simulations generated with the more complex FSM2. Our results demonstrate that AdaPBS is a robust and reliable tool, outperforming or at least matching the performance of other commonly used algorithms and successfully handling complex cases with dense observational datasets. All experiments were carried out using the open-source MuSA toolbox, which now includes AdaPBS and MCMC among the growing list of available cryospheric data assimilation methods."}
{"id": "2601.19986", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.19986", "abs": "https://arxiv.org/abs/2601.19986", "authors": ["Fabrizio Ramírez", "David Villaseñor", "Nahum Vázquez", "Jorge G. Hirsch"], "title": "The superradiant phase is a finite size effect in two-photon processes", "comment": "7 pages, 3 figures, 1 supplemental material", "summary": "Two-photon light-matter interactions exhibit distinctive features such as spectral collapse. The two-photon Dicke model has been reported to exhibit a superradiant phase which could be useful in quantum applications. Here we show that this superradiant phase is not a genuine thermodynamic phase but a finite-size effect. Combining analytical and numerical analyses, we demonstrate that the superradiant region shrinks with increasing system size and disappears in the thermodynamic limit, while spectral collapse remains. Our results clarify the nature of superradiant conditions in two-photon systems and constrain its realization in quantum platforms."}
{"id": "2601.20095", "categories": ["cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.20095", "abs": "https://arxiv.org/abs/2601.20095", "authors": ["Yen-Chi Lee"], "title": "First-Hitting Location Laws as Boundary Observables of Drift--Diffusion Processes", "comment": "12 pages, 4 figures, 2 tables", "summary": "We investigate first-hitting location (FHL) statistics induced by drift--diffusion processes in domains with absorbing boundaries, and examine how such boundary laws give rise to intrinsic information observables. Rather than introducing explicit encoding or decoding mechanisms, information is viewed as emerging directly from the geometry and dynamics of stochastic transport through first-passage events. Treating the FHL as the primary observable, we characterize how geometry and drift jointly shape the induced boundary measure. In diffusion-dominated regimes, the exit law exhibits scale-free, heavy-tailed spatial fluctuations along the boundary, whereas a nonzero drift component introduces an intrinsic length scale that suppresses these tails and reorganizes the exit statistics. Within a generator-based formulation, the FHL arises naturally as a boundary measure induced by an elliptic operator, allowing explicit boundary kernels to be derived analytically in canonical geometries. Planar absorbing boundaries in different ambient dimensions are examined as benchmark cases, illustrating how directed transport regularizes diffusion-driven fluctuations and induces qualitative transitions in boundary statistics. Overall, the present work provides a unified structural framework for first-hitting location laws and highlights FHL statistics as natural probes of geometry, drift, and irreversibility in stochastic transport."}
{"id": "2601.20324", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20324", "abs": "https://arxiv.org/abs/2601.20324", "authors": ["Jingyuan Zhou", "Haoze Wu", "Kaidi Yang"], "title": "Neural Cooperative Reach-While-Avoid Certificates for Interconnected Systems", "comment": null, "summary": "Providing formal guarantees for neural network-based controllers in large-scale interconnected systems remains a fundamental challenge. In particular, using neural certificates to capture cooperative interactions and verifying these certificates at scale is crucial for the safe deployment of such controllers. However, existing approaches fall short on both fronts. To address these limitations, we propose neural cooperative reach-while-avoid certificates with Dynamic-Localized Vector Control Lyapunov and Barrier Functions, which capture cooperative dynamics through state-dependent neighborhood structures and provide decentralized certificates for global exponential stability and safety. Based on the certificates, we further develop a scalable training and verification framework that jointly synthesizes controllers and neural certificates via a constrained optimization objective, and leverages a sufficient condition to ensure formal guarantees considering modeling error. To improve scalability, we introduce a structural reuse mechanism to transfer controllers and certificates between substructure-isomorphic systems. The proposed methodology is validated with extensive experiments on multi-robot coordination and vehicle platoons. Results demonstrate that our framework ensures certified cooperative reach-while-avoid while maintaining strong control performance."}
{"id": "2601.20723", "categories": ["eess.SY", "cs.GT", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.20723", "abs": "https://arxiv.org/abs/2601.20723", "authors": ["Emrah Akyol", "Marcos Vasconcelos"], "title": "Distributed Learning over Noisy Communication Networks", "comment": "draft, submitted to IEEE JSAC Special Issue on Distributed Optimization, Learning, and Inference over Communication-Constrained Networks", "summary": "We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality."}
{"id": "2601.20833", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2601.20833", "abs": "https://arxiv.org/abs/2601.20833", "authors": ["Tengyue Xu", "Zhuoyang Qian", "Gaoge Liu", "Li Ling", "Zhentao Zhang", "Biao Wu", "Shuo Zhang", "Ke Lu", "Wei Shi", "Ziqi Wang", "Zheng Feng", "Yan Luo", "Shu Xu", "Yongjin Chen", "Zhibo Feng", "Zhuo Chen", "Bruce Yuan", "Harry Wang", "Kris Chen"], "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "comment": "11 pages, 3 figures", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery."}
{"id": "2601.20152", "categories": ["math.ST", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20152", "abs": "https://arxiv.org/abs/2601.20152", "authors": ["Chen Cheng", "Rina Foygel Barber"], "title": "Concentration Inequalities for Exchangeable Tensors and Matrix-valued Data", "comment": "45 pages, 3 figures", "summary": "We study concentration inequalities for structured weighted sums of random data, including (i) tensor inner products and (ii) sequential matrix sums. We are interested in tail bounds and concentration inequalities for those structured weighted sums under exchangeability, extending beyond the classical framework of independent terms.\n  We develop Hoeffding and Bernstein bounds provided with structure-dependent exchangeability. Along the way, we recover known results in weighted sum of exchangeable random variables and i.i.d. sums of random matrices to the optimal constants. Notably, we develop a sharper concentration bound for combinatorial sum of matrix arrays than the results previously derived from Chatterjee's method of exchangeable pairs.\n  For applications, the richer structures provide us with novel analytical tools for estimating the average effect of multi-factor response models and studying fixed-design sketching methods in federated averaging. We apply our results to these problems, and find that our theoretical predictions are corroborated by numerical evidence."}
{"id": "2601.20726", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.20726", "abs": "https://arxiv.org/abs/2601.20726", "authors": ["Yunxue Sun", "Xueming Liu", "Ginestra Bianconi"], "title": "Directionality and node heterogeneity reshape criticality in hypergraph percolation", "comment": "(25 pages, 6 figures, plus SM)", "summary": "Directed and heterogeneous hypergraphs capture directional higher-order interactions with intrinsically asymmetric functional dependencies among nodes. As a result, damage to certain nodes can suppress entire hyperedges, whereas failure of others only weakens interactions. Metabolic reaction networks offer an intuitive example of such asymmetric dependencies. Here we develop a message-passing and statistical mechanics framework for percolation in directed hypergraphs that explicitly incorporates directionality and node heterogeneity. Remarkably, we show that these hypergraph features have a fundamental effect on the critical properties of hypergraph percolation, reshaping criticality in a way that depends on network structure. Specifically, we derive anomalous critical exponents that depend on whether node or hyperedge percolation is considered in maximally correlated, heavy-tailed regimes. These theoretical predictions are validated on synthetic hypergraph models and on a real directed metabolic network, opening new perspectives for the characterization of the robustness and resilience of real-world directed, heterogeneous higher-order networks."}
{"id": "2601.20292", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20292", "abs": "https://arxiv.org/abs/2601.20292", "authors": ["Shuyang Ling"], "title": "Improved Global Landscape Guarantees for Low-rank Factorization in Synchronization", "comment": null, "summary": "The orthogonal group synchronization problem, which aims to recover a set of $d \\times d$ orthogonal matrices from their pairwise noisy products, plays a fundamental role in signal processing, computer vision, and network analysis. In recent years, numerous optimization techniques, such as semidefinite relaxation (SDR) and low-rank (Burer-Monteiro) factorization, have been proposed to address this problem and their theoretical guarantees have been extensively studied. While SDR is provably powerful and exact in recovering the least-squares estimator under certain mild conditions, it is not scalable. In contrast, the low-rank factorization is highly efficient but nonconvex, meaning its iterates may get trapped in local minima. To close the gap, we analyze the low-rank approach and focus on understanding when the associated nonconvex optimization landscape is benign, i.e., free of spurious local minima. Recent works suggest that the benignness depends on the condition number of the Hessian at the global minimizer, but it remains unclear whether sharp guarantees can be achieved. In this work, we consider the low-rank approach which corresponds to an optimization problem over the Stiefel manifold ${\\rm St}(p,d)^{\\otimes n}$. By formulating the landscape analysis into another convex optimization problem, we provide a unified characterization of the optimization landscape for all parameter pairs $(p,d)$ with $p \\geq d+2$ for $d\\geq 1$ and $p = d+1$ for $1\\leq d\\leq 3$ which gives a much improved dependence on the condition number of the Hessian. Our results recover the known sharp state-of-the-art bound for $d=1$ which is extremely useful for characterizing the Kuramoto synchronization, and significantly improved the guarantees for the general case $d \\geq 2$ with $p \\geq d+2$ over the existing results. The theoretical results are versatile and applicable to a wide range of examples."}
{"id": "2601.20050", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20050", "abs": "https://arxiv.org/abs/2601.20050", "authors": ["Felipe Lepe", "Gonzalo Rivera"], "title": "A mixed virtual element discretization for the generalized Oseen problem", "comment": null, "summary": "In this paper we introduce a mixed virtual element method to approximate the solution for the two dimensional generalized Oseen problem. We introduce the pseudostress as an additional unknown, which allows to eliminate the pressure from the system; the pressure can be recovered via a post-process of the pseudostress tensor. We prove existence and uniqueness of the continuous solution via a fixed point argument. Under standard mesh assumptions, we develop a virtual element method to approximate both the tensor and the velocity field, and we show that it is stable. Furthermore, we provide a priori error estimates for the method and validate them through a series of numerical tests using different polygonal meshes."}
{"id": "2601.20527", "categories": ["hep-lat", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.20527", "abs": "https://arxiv.org/abs/2601.20527", "authors": ["Anosh Joseph", "Arpith Kumar"], "title": "Thermodynamic Consistency as a Reliability Test for Complex Langevin Simulations", "comment": "10 pages, 4 figures, 4 tables, contribution to the 42nd International Symposium on Lattice Field Theory (LATTICE2025), 2-8 November 2025, Tata Institute of Fundamental Research, Mumbai, India", "summary": "The complex Langevin method (CLM) is a promising tool to address the sign problem in quantum field theories with complex actions. However, it can converge to incorrect results even when simulations appear stable, highlighting the need for robust diagnostics. Existing checks, such as monitoring drift distributions, are useful but indirect. We propose a complementary test based on the configurational temperature, constructed from the gradient and Hessian of the complex action. Unlike drift-based criteria, this estimator directly probes thermodynamic consistency and provides a physically interpretable cross-check of CLM dynamics. Using one-dimensional PT-symmetric models, we show that it reproduces the input temperature with high precision and sensitively detects algorithmic errors, step-size artifacts, and incomplete thermalization. While demonstrated in simple systems, the method extends naturally to higher-dimensional scalar and gauge theories. Since temperature is tied to the bare coupling in many lattice theories, configurational monitoring can also provide an independent check on coupling-dependent observables. Our results indicate that configurational temperature can enhance CLM reliability across a broad range of applications, including lattice QCD at finite density."}
{"id": "2601.20197", "categories": ["stat.ME", "cs.LG", "econ.EM", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.20197", "abs": "https://arxiv.org/abs/2601.20197", "authors": ["Raphaël Langevin"], "title": "Bias-Reduced Estimation of Finite Mixtures: An Application to Latent Group Structures in Panel Data", "comment": null, "summary": "Finite mixture models are widely used in econometric analyses to capture unobserved heterogeneity. This paper shows that maximum likelihood estimation of finite mixtures of parametric densities can suffer from substantial finite-sample bias in all parameters under mild regularity conditions. The bias arises from the influence of outliers in component densities with unbounded or large support and increases with the degree of overlap among mixture components. I show that maximizing the classification-mixture likelihood function, equipped with a consistent classifier, yields parameter estimates that are less biased than those obtained by standard maximum likelihood estimation (MLE). I then derive the asymptotic distribution of the resulting estimator and provide conditions under which oracle efficiency is achieved. Monte Carlo simulations show that conventional mixture MLE exhibits pronounced finite-sample bias, which diminishes as the sample size or the statistical distance between component densities tends to infinity. The simulations further show that the proposed estimation strategy generally outperforms standard MLE in finite samples in terms of both bias and mean squared errors under relatively weak assumptions. An empirical application to latent group panel structures using health administrative data shows that the proposed approach reduces out-of-sample prediction error by approximately 17.6% relative to the best results obtained from standard MLE procedures."}
{"id": "2601.20183", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20183", "abs": "https://arxiv.org/abs/2601.20183", "authors": ["Yuhua Zhao", "Tiejun Lv", "Ke Wang"], "title": "C-AoEI-Aware Cross-Layer Optimization in Satellite IoT Systems: Balancing Data Freshness and Transmission Efficiency", "comment": "18 pages, 13 figures, IEEE Internet of Things Journal, Accepted", "summary": "Satellite-based Internet of Things (S-IoT) faces a fundamental trilemma: propagation delay, dynamic fading, and bandwidth scarcity. While Layer-coded Hybrid ARQ (L-HARQ) enhances reliability, its backtracking decoding introduces age ambiguity, undermining the standard Age of Information (AoI) metric and obscuring the critical trade-off between data freshness and transmission efficiency. To bridge this gap, we propose a novel cross-layer optimization framework centered on a new metric, the Cross-layer Age of Error Information (C-AoEI). We derive a closed-form expression for C-AoEI, explicitly linking freshness to system parameters, establishing an explicit analytical connection between freshness degradation and channel dynamics. Building on this, we develop a packet-level encoded L-HARQ scheme for multi-GBS scenarios and an adaptive algorithm that jointly optimizes coding and decision thresholds. Extensive simulations demonstrate the effectiveness of our proposed framework: it achieves 31.8% higher transmission efficiency and 17.2% lower C-AoEI than conventional schemes. The framework also proves robust against inter-cell interference and varying channel conditions, providing a foundation for designing efficient, latency-aware next-generation S-IoT protocols."}
{"id": "2601.20726", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.20726", "abs": "https://arxiv.org/abs/2601.20726", "authors": ["Yunxue Sun", "Xueming Liu", "Ginestra Bianconi"], "title": "Directionality and node heterogeneity reshape criticality in hypergraph percolation", "comment": "(25 pages, 6 figures, plus SM)", "summary": "Directed and heterogeneous hypergraphs capture directional higher-order interactions with intrinsically asymmetric functional dependencies among nodes. As a result, damage to certain nodes can suppress entire hyperedges, whereas failure of others only weakens interactions. Metabolic reaction networks offer an intuitive example of such asymmetric dependencies. Here we develop a message-passing and statistical mechanics framework for percolation in directed hypergraphs that explicitly incorporates directionality and node heterogeneity. Remarkably, we show that these hypergraph features have a fundamental effect on the critical properties of hypergraph percolation, reshaping criticality in a way that depends on network structure. Specifically, we derive anomalous critical exponents that depend on whether node or hyperedge percolation is considered in maximally correlated, heavy-tailed regimes. These theoretical predictions are validated on synthetic hypergraph models and on a real directed metabolic network, opening new perspectives for the characterization of the robustness and resilience of real-world directed, heterogeneous higher-order networks."}
{"id": "2601.20189", "categories": ["cond-mat.str-el", "hep-lat"], "pdf": "https://arxiv.org/pdf/2601.20189", "abs": "https://arxiv.org/abs/2601.20189", "authors": ["Anders W. Sandvik"], "title": "High-precision ground state parameters of the two-dimensional spin-1/2 Heisenberg model on the square lattice", "comment": "17 pages, 10 figures", "summary": "Several ground state properties of the square-lattice $S=1/2$ Heisenberg antiferromagnet are computed (the energy, order parameter, spin stiffness, spinwave velocity, long-wavelength susceptibility, and staggered susceptibility) using extensive quantum Monte Carlo simulations with the stochastic series expansion method. Moderately sized lattices are studied at temperatures $T$ sufficiently low to realize the $T \\to 0$ limit. Results for periodic $L\\times L$ lattices with $L \\in [6,96]$ are tabulated versus $L$ and extrapolations to infinite system size are carried out. The extrapolated ground state energy density is $e_0=-0.669441857(7)$, which represents an improvement in precision of three orders of magnitude over the previously best result. The leading and subleading finite-size corrections to $e_0$ are in full quantitative agreement with predictions from chiral perturbation theory, thus further supporting the soundness of both the extrapolations and the theory. The extrapolated sublattice magnetization is $m_s=0.307447(2)$, which agrees well with previous estimates but with a much smaller statistical error. The coefficient of the linear in $L^{-1}$ correction to $m^2_s$ agrees with the value from chiral perturbation theory and the presence of a factor $\\ln^γ(L)$ in the second-order correction is also confirmed, with the previously not known value of the exponent being $γ= 0.82(4)$. The finite-size corrections to the staggered susceptibility point to logarithmic corrections also in this quantity. To facilitate benchmarking of methods for which periodic boundary conditions are challenging, results for systems with open and cylindrical boundaries are also listed and their spatially inhomogeneous order parameters are analyzed."}
{"id": "2601.19988", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.19988", "abs": "https://arxiv.org/abs/2601.19988", "authors": ["Xuankai Zhou", "Yan-Tung Kong", "Cheuk Kit Cheung", "Guodong Bian", "Reda Moukaouine", "King Cho Wong", "Yumeng Sun", "Cheng-I Ho", "Vladislav Bushmakin", "Nils Gross", "Chun-Chieh Yen", "Tim Priessnitz", "Malik Lenger", "Sreehari Jayaram", "Takashi Taniguchi", "Kenji Watanabe", "Anton Pershin", "Ruoming Peng", "Ádám Gali", "Jurgen Smet", "Jörg Wrachtrup"], "title": "Optically Addressable Molecular Spins at 2D Surfaces", "comment": null, "summary": "Optically addressable spins at material surfaces have represented a long-standing ambition in quantum sensing, providing atomic resolution and quantum-limited sensitivity. However, they are constrained by a finite depth at which the quantum spins can be stabilized. Here, we demonstrate a hybrid molecular-2D architecture that realizes quantum spin sensors directly on top of the surface. By anchoring spin-active molecules onto hexagonal boron nitride (hBN), we eliminate the depth of the quantum sensor while also exhibiting robust spin properties from 4~K to room temperature (RT). The Hahn-echo spin coherence time exceeds \\(T_2 = 3.4~\\upmu\\text{s}\\) at 4~K, outperforming values in bulk organic crystals and overturning the prevailing expectation that spin inevitably deteriorates upon approaching the surface. By chemically tuning the molecule through deuteration, \\(T_2\\) improves by more than 10-fold, and under dynamic decoupling, coherence is prolonged to the intrinsic lifetime limit, exceeding 300~\\(\\upmu\\text{s}\\). Proximal proton spins and the magnetic response of two-dimensional magnets beneath the hBN layer have been detected at RT. These molecular spins form surface quantum sensors with long coherence, optical addressability, and interfacial versatility, enabling a scalable, adaptable architecture beyond what conventional solid-state platforms offer."}
{"id": "2601.20166", "categories": ["cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20166", "abs": "https://arxiv.org/abs/2601.20166", "authors": ["Kazuki Yamamoto", "Kohei Kawabata"], "title": "Complex nonlinear sigma model", "comment": "14 pages, 10 figures", "summary": "Motivated by the recent interest in the criticality of open quantum many-body systems, we study nonlinear sigma models with complexified couplings as a general framework for nonunitary field theory. Applying the perturbative renormalization-group analysis to the tenfold symmetric spaces, we demonstrate that fixed points with complex scaling dimensions and critical exponents arise generically, without counterparts in conventional nonlinear sigma models with real couplings. We further clarify the global phase diagrams in the complex-coupling plane and identify both continuous and discontinuous phase transitions. Our work elucidates universal aspects of critical phenomena in complexified field theory."}
{"id": "2601.20427", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20427", "abs": "https://arxiv.org/abs/2601.20427", "authors": ["Yixuan Zhu", "Yinkang Gao", "Bo Zhang", "Xiaohang Gong", "Binze Jiang", "Lei Gong", "Wenqi Lou", "Teng Wang", "Chao Wang", "Xi Li", "Xuehai Zhou"], "title": "Reducing End-to-End Latency of Cause-Effect Chains with Shared Cache Analysis", "comment": null, "summary": "Cause-effect chains, as a widely used modeling method in real-time embedded systems, are extensively applied in various safety-critical domains. End-to-end latency, as a key real-time attribute of cause-effect chains, is crucial in many applications. But the analysis of end-to-end latency for cause-effect chains on multicore platforms with shared caches still presents an unresolved issue. Traditional methods typically assume that the worst-case execution time (WCET) of each task in the cause-effect chain is known. However, in the absence of scheduling information, these methods often assume that all shared cache accesses result in misses, leading to an overestimation of WCET and, consequently, affecting the accuracy of end-to-end latency. However, effectively integrating scheduling information into the WCET analysis process of the chains may introduce two challenges: first, how to leverage the structural characteristics of the chains to optimize shared cache analysis, and second, how to improve analysis accuracy while avoiding state space explosion.\n  To address these issues, this paper proposes a novel end-to-end latency analysis framework designed for multi-chain systems on multicore platforms with shared caches. This framework extracts scheduling information and structural characteristics of cause-effect chains, constructing fine-grained and scalable inter-core memory access contexts at the basic block level for time-sensitive shared cache analysis. This results in more accurate WCET (TSC-WCET) estimates, which are then used to derive the end-to-end latency. Finally, we conduct experiments on dual-core and quad-core systems with various cache configurations, which show that under certain settings, the average maximum end-to-end latency of cause-effect chains is reduced by up to 34% and 26%."}
{"id": "2601.20341", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.20341", "abs": "https://arxiv.org/abs/2601.20341", "authors": ["Baba Thiam"], "title": "Partial heteroscedastic deconvolution estimation in nonparametric regression", "comment": null, "summary": "In this paper, we consider a partial deconvolution kernel estimator for nonparametric regression when some covariates are measured with error while others are observed without error. We focus on a general and realistic setting in which the measurement errors are heteroscedastic. We propose a kernel-based estimator of the regression function in this framework and show that it achieves the optimal convergence rate under suitable regularity conditions. The finite-sample performance of the proposed estimator is illustrated through simulation studies."}
{"id": "2601.20338", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20338", "abs": "https://arxiv.org/abs/2601.20338", "authors": ["Nam Van Tran"], "title": "A novel neural network with predefined-time stability for solving generalized monotone inclusion problems with applications", "comment": null, "summary": "We propose a novel dynamical framework for solving inclusion\n  problems of the form \\(0 \\in F(x) + G(x)\\) in Hilbert spaces, where \\(F\\) is a\n  maximal set-valued operator and \\(G\\) is a single-valued mapping. The analysis is\n  conducted under a generalized monotonicity assumption, which relaxes the\n  classical monotonicity conditions commonly imposed in the literature and thereby\n  extends the applicability of the proposed approach.\n  Under mild conditions on the system parameters, we establish both fixed-time and\n  predefined-time stability of the resulting dynamical system. The fixed-time\n  stability guarantees a uniform upper bound on the settling time that is\n  independent of the initial condition, whereas the predefined-time stability\n  framework allows the system parameters to be selected \\emph{a priori} in order\n  to ensure convergence within a user-specified time horizon.\n  Moreover, we investigate an explicit forward Euler discretization of the\n  continuous-time dynamics, leading to a novel forward--backward iterative\n  algorithm. A rigorous convergence analysis of the resulting discrete scheme is\n  provided. Finally, the effectiveness and versatility of the proposed method are\n  illustrated through several classes of problems, including constrained\n  optimization problems, mixed variational inequalities, and variational\n  inequalities, together with numerical experiments that corroborate the\n  theoretical results."}
{"id": "2601.20119", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20119", "abs": "https://arxiv.org/abs/2601.20119", "authors": ["Chris Siefert", "Raymond Tuminaro", "Daniel Sunderland"], "title": "Improving Smoothed Aggregation AMG Robustness on Stretched Mesh Applications", "comment": null, "summary": "Strength-of-connection algorithms play a key role in algebraic multigrid (AMG). Specifically, they determine which matrix nonzeros are classified as weak and so ignored when coarsening matrix graphs and defining interpolation sparsity patterns. The general goal is to encourage coarsening only in directions where error can be smoothed and to avoid coarsening across sharp problem variations. Unfortunately, developing robust and inexpensive strength-of-connection schemes is challenging.\n  The classification of matrix nonzeros involves four aspects: (a) choosing a strength-of-connection matrix, (b) scaling its values, (c) choosing a criterion to classify scaled values as strong or weak, and (d) dropping weak entries which includes adjusting matrix values to account for dropped terms. Typically, smoothed aggregation AMG uses the linear system being solved as a strength-of-connection matrix. It scales values symmetrically using square-roots of the matrix diagonal. It classifies based on whether scaled values are above or below a threshold. Finally, it adjusts matrix values by modifying the diagonal so that the sum of entries within each row of the dropped matrix matches that of the original. While these procedures can work well, we illustrate failure cases that motivate alternatives. The first alternative uses a distance Laplacian strength-of-connection matrix. The second centers on non-symmetric scaling. We then investigate alternative classification criteria based on identifying gaps in the values of the scaled entries. Finally, an alternative lumping procedure is proposed where row sums are preserved by modifying all retained matrix entries (as opposed to just diagonal entries). A series of numerical results illustrates trade-offs demonstrating in some cases notably more robust convergence on matrices coming from linear finite elements on stretched meshes."}
{"id": "2601.20690", "categories": ["hep-lat"], "pdf": "https://arxiv.org/pdf/2601.20690", "abs": "https://arxiv.org/abs/2601.20690", "authors": ["Yuto Sugimoto", "Shinichiro Akiyama", "Yoshinobu Kuramashi"], "title": "Tensor renormalization group study of cold and dense QCD in the strong coupling limit", "comment": "17 pages, 9 figures, 1 table", "summary": "We study the phase structure of the (3+1)-dimensional cold and dense QCD with the Kogut--Susskind quark in the strong coupling limit using the tensor renormalization group method. The chiral and nuclear transitions are investigated by calculating the chiral condensate and the quark number density as a function of the chemical potential. For a fixed temporal extent $N_τ=8$, we determine the critical quark masses $m_c^χ$ and $m_c^{n}$ for the chiral condensate and the quark number density, respectively, at which the first-order phase transition terminates with the vanishing discontinuity in thermodynamic quantities. We find that both quantities at the same quark mass exhibit a discontinuity at the same chemical potential, and the resulting critical quark masses are consistent with each other. We also compare our results for the critical quark masses with those obtained from the Monte Carlo simulation in the dual formulation and from the mean-field analysis. We further confirm the first-order phase transition at finite quark mass on a $1024^4$ lattice, which is essentially in the thermodynamic limit at zero temperature, as expected from the mean-field analysis."}
{"id": "2601.20219", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20219", "abs": "https://arxiv.org/abs/2601.20219", "authors": ["Yong He", "Zizhou Huang", "Bingyi Jing", "Diqing Li"], "title": "Joint Estimation of Edge Probabilities for Multi-layer Networks via Neighborhood Smoothing", "comment": null, "summary": "In this paper we focus on jointly estimating the edge probabilities for multi-layer networks. We define a novel multi-layer graphon, a ternary function in contrast to the bivariate graphon function in the literature by introducing an additional latent layer position parameter, which is model-free and covers a wide range of multi-layer networks. We develop a computationally efficient two-step neighborhood smoothing algorithm to estimate the edge probabilities of multi-layer networks, which requires little tuning and fully utilize the similarity across both network layers and nodes. Numerical experiments demonstrate the advantages of our method over the existing state-of-the-art ones. A real Worldwide Food Import/Export Network dataset example is analyzed to illustrate the better performance of the proposed method over benchmark methods in terms of link prediction."}
{"id": "2601.20298", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20298", "abs": "https://arxiv.org/abs/2601.20298", "authors": ["Omid Akbarzadeh", "MohammadHossein Ashoori", "Amy Nejati", "Abolfazl Lavaei"], "title": "A Data-Driven Krasovskii-Based Approach for Safety Controller Design of Time-Delayed Uncertain Polynomial Systems", "comment": null, "summary": "We develop a data-driven framework for the synthesis of robust Krasovskii control barrier certificates (RK-CBC) and corresponding robust safety controllers (R-SC) for discrete-time input-affine uncertain polynomial systems with unknown dynamics, while explicitly accounting for unknown-but-bounded disturbances and time-invariant delays using only observed input-state data. Although control barrier certificates have been extensively studied for safety analysis of control systems, existing work on unknown systems with time delays, particularly in the presence of disturbances, remains limited. The challenge of safety synthesis for such systems stems from two main factors: first, the system's mathematical model is unavailable; and second, the safety conditions should explicitly incorporate the effects of time delays on system evolution during the synthesis process, while remaining robust to unknown disturbances. To address these challenges, we develop a data-driven framework based on Krasovskii control barrier certificates, extending the classical CBC formulation for delay-free systems to explicitly account for time delays by aggregating delayed components within the barrier construction. The proposed framework relies solely on input-state data collected over a finite time horizon, enabling the direct synthesis of RK-CBC and R-SC from observed trajectories without requiring an explicit system model. The synthesis is cast as a data-driven sum-of-squares (SOS) optimization program, yielding a structured design methodology. As a result, robust safety is guaranteed in the presence of unknown disturbances and time delays over an infinite time horizon. The effectiveness of the proposed method is demonstrated through three case studies, including two physical systems."}
{"id": "2601.20281", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.20281", "abs": "https://arxiv.org/abs/2601.20281", "authors": ["Yusuke Sakai", "Fumiya Hori", "Hiroki Matsumura", "Shumpei Oguchi", "Shunsaku Kitagawa", "Kenji Ishida", "Hiroshi Tanida"], "title": "Microscopic Determination of the c-axis-Oriented Antiferromagnetic Structure in LaMnSi by $^{55}$Mn and $^{139}$La NMR", "comment": null, "summary": "We report a microscopic investigation of the magnetic structure and electronic properties of LaMnSi in its antiferromagnetic (AFM) state using nuclear magnetic resonance (NMR). Field-swept $^{55}$Mn- and $^{139}$La-NMR spectra, as well as zero-field 55Mn-NMR (ZFNMR) spectra, reveal that the Mn ordered moments are parallel to the tetragonal c axis, consistent with the C-type AFM structure and the realization of an odd-parity multipole order. The internal field at the Mn site is determined to be 19.64 T at 4.2 K, corresponding to a hyperfine coupling constant of Ahf = 6.0 T/uB. Nuclear spin-lattice relaxation rate 1/T1 exhibits a characteristic behavior of itinerant antiferromagnetism, showing metallic behavior at low temperatures and magnon-induced enhancement upon approaching the Neel temperature (TN = 295 K). These results show LaMnSi as an ideal compound to study 3d electron magnetism and odd-parity multipole order in the RT Si (R = rare-earth, T = transition metal) system, free of the complexities of 4f electrons."}
{"id": "2601.20007", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20007", "abs": "https://arxiv.org/abs/2601.20007", "authors": ["Ludwig Schmid", "Korbinian Staudacher", "Robert Wille"], "title": "Alternating ZX Circuit Extraction for Hardware-Adaptive Compilation", "comment": "DATE 2026", "summary": "We present a novel quantum circuit extraction scheme that tightly integrates graph-like ZX diagrams with hardware-adaptive routing. The method utilizes the degrees of freedom during the conversion from a ZX diagram to a quantum circuit (extraction). It alternates between generating multiple extraction options and evaluating them based on hardware constraints, allowing the routing algorithm to inform and guide the extraction process. This feedback loop extends existing graph-like ZX extraction and supports modular integration of different extraction algorithms, routing strategies, and target hardware, making it a versatile building block during compilation. To perform numerical evaluations, a reference instance of the scheme is implemented with SWAP-based routing for neutral atom hardware and evaluated using various benchmark collections on small-to mid-scale circuits. The reference code is available as open-source, allowing fast integration of other extraction and/or routing tools to stimulate further research and foster improvements of the proposed scheme."}
{"id": "2601.19986", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.19986", "abs": "https://arxiv.org/abs/2601.19986", "authors": ["Fabrizio Ramírez", "David Villaseñor", "Nahum Vázquez", "Jorge G. Hirsch"], "title": "The superradiant phase is a finite size effect in two-photon processes", "comment": "7 pages, 3 figures, 1 supplemental material", "summary": "Two-photon light-matter interactions exhibit distinctive features such as spectral collapse. The two-photon Dicke model has been reported to exhibit a superradiant phase which could be useful in quantum applications. Here we show that this superradiant phase is not a genuine thermodynamic phase but a finite-size effect. Combining analytical and numerical analyses, we demonstrate that the superradiant region shrinks with increasing system size and disappears in the thermodynamic limit, while spectral collapse remains. Our results clarify the nature of superradiant conditions in two-photon systems and constrain its realization in quantum platforms."}
{"id": "2601.20445", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20445", "abs": "https://arxiv.org/abs/2601.20445", "authors": ["Yixuan Zhu", "Yinkang Gao", "Lei Gong", "Binze Jiang", "Xiaohang Gong", "Zihan Wang", "Cheng Tang", "Wenqi Lou", "Teng Wang", "Chao Wang", "Xi Li", "Xuehai Zhou"], "title": "A Timing-Anomaly Free Dynamic Scheduling on Heterogeneous Systems", "comment": null, "summary": "Heterogeneous systems commonly adopt dynamic scheduling algorithms to improve resource utilization and enhance scheduling flexibility. However, such flexibility may introduce timing anomalies, wherein locally reduced execution times can lead to an increase in the overall system execution time. This phenomenon significantly complicates the analysis of Worst-Case Response Time (WCRT), rendering conventional analysis either overly pessimistic or unsafe, and often necessitating exhaustive state-space exploration to ensure correctness.\n  To address this challenge, this paper presents the first timing-anomaly-free dynamic scheduling algorithm for heterogeneous systems, referred to as Deterministic Dynamic Execution. It achieves a safe and tight WCRT estimate through a single offline simulation execution. The core idea is to apply deterministic execution constraints, which partially restrict the resource allocation and execution order of tasks at runtime. Based on a formally defined execution progress model for heterogeneous system scheduling, we prove the correctness of the proposed execution constraints and their ability to eliminate timing anomalies. Furthermore, we propose two methods to generate execution constraints. The first method derives execution constraints directly from the execution traces produced by existing scheduling algorithms. The second method is a heuristic-based approach that constructs execution constraints, enabling further reduction of the WCRT. Experimental results on synthetically generated DAG task sets under various system configurations demonstrate that, compared to traditional dynamic scheduling algorithms, our approach not only eliminates timing anomalies but also effectively reduces both the WCRT and response time jitter."}
{"id": "2601.20442", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20442", "abs": "https://arxiv.org/abs/2601.20442", "authors": ["José E. Chacón", "Eduardo García-Portugués", "Andrea Meilán-Vila"], "title": "Blessing of dimensionality in cross-validated bandwidth selection on the sphere", "comment": "25 pages, 6 figures, 2 tables. Supplementary material: 43 pages, 4 figures", "summary": "We study the asymptotic behavior of least-squares cross-validation bandwidth selection in kernel density estimation on the $d$-dimensional hypersphere, $d\\geq 1$. We show that the exact rate of convergence with respect to the optimal bandwidth minimizing the mean integrated squared error, shown to exist under mild non-uniformity conditions, is $n^{-d/(2d+8)}$, thus approaching the $n^{-1/2}$ parametric rate as $d$ grows. This ``blessing of dimensionality'' in bandwidth selection offers theoretical support for utilizing the conceptually simpler cross-validation selector over plug-in techniques for larger dimensions $d$. We compare this result for bandwidth estimation on the $d$-dimensional Euclidean space through explicit expressions for the asymptotic variance functionals. Numerical experiments corroborate the speed of this convergence in an array of scenarios and dimensions, precisely illustrating the tipping dimension where cross-validation outperforms plug-in approaches."}
{"id": "2601.20340", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20340", "abs": "https://arxiv.org/abs/2601.20340", "authors": ["Zhaohua Yang", "Yuxing Zhong", "Ling Shi"], "title": "Data-Driven Structured Control for Continuous-Time LTI Systems", "comment": null, "summary": "This paper addresses the data-driven structured controller design problem for continuous-time linear time-invariant (LTI) systems. We consider three control objectives, including stabilization, $H_2$ performance, and $H_\\infty$ performance. Using the collected data, we construct a minimal matrix ellipsoid that contains all admissible system matrices. We propose some linearization techniques that enable us to incorporate the structural constraint on the controller, which motivates an iterative algorithm for each control objective. Finally, we provide some numerical examples to demonstrate the effectiveness of the proposed methods."}
{"id": "2601.20145", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.20145", "abs": "https://arxiv.org/abs/2601.20145", "authors": ["Xingyuan Lin", "Xiuxiu Lin", "Xuesong Chen"], "title": "Error estimates of $hp$-finite element method for elliptic optimal control problems with robin boundary", "comment": null, "summary": "A priori and a posteriori error analysis of $hp$ finite element method for elliptic control problem with Robin boundary condition and boundary observation are presented. are presented. Through the Clément-type approach and the construction of an auxiliary system, we derived a priori error estimates for the elliptic optimal control problem. Residual-based a posteriori error estimates are derived based on the well-known Scott-Zhang-type quasi-interpolation and coupled state-control approximations, thus establishing an a posteriori error estimator for the $hp$ finite element method. The numerical example demonstrates the accuracy of error estimation for the elliptic optimal control problems with Robin boundary."}
{"id": "2601.20708", "categories": ["hep-lat", "cond-mat.stat-mech", "cs.LG", "hep-ph"], "pdf": "https://arxiv.org/pdf/2601.20708", "abs": "https://arxiv.org/abs/2601.20708", "authors": ["Claudio Bonanno", "Andrea Bulgarelli", "Elia Cellini", "Alessandro Nada", "Dario Panfalone", "Davide Vadacchino", "Lorenzo Verzichelli"], "title": "A scalable flow-based approach to mitigate topological freezing", "comment": "1+9 pages, 3 figures, contribution to the 42nd International Symposium on Lattice Field Theory (Lattice 2025), 2-8 November 2025, Mumbai, India", "summary": "As lattice gauge theories with non-trivial topological features are driven towards the continuum limit, standard Markov Chain Monte Carlo simulations suffer for topological freezing, i.e., a dramatic growth of autocorrelations in topological observables. A widely used strategy is the adoption of Open Boundary Conditions (OBC), which restores ergodic sampling of topology but at the price of breaking translation invariance and introducing unphysical boundary artifacts. In this contribution we summarize a scalable, exact flow-based strategy to remove them by transporting configurations from a prior with a OBC defect to a fully periodic ensemble, and apply it to 4d SU(3) Yang--Mills theory. The method is based on a Stochastic Normalizing Flow (SNF) that alternates non-equilibrium Monte Carlo updates with localized, gauge-equivariant defect coupling layers implemented via masked parametric stout smearing. Training is performed by minimizing the average dissipated work, equivalent to a Kullback--Leibler divergence between forward and reverse non-equilibrium path measures, to achieve more reversible trajectories and improved efficiency. We discuss the scaling with the number of degrees of freedom affected by the defect and show that defect SNFs achieve better performances than purely stochastic non-equilibrium methods at comparable cost. Finally, we validate the approach by reproducing reference results for the topological susceptibility."}
{"id": "2601.20254", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20254", "abs": "https://arxiv.org/abs/2601.20254", "authors": ["Hengrui Luo", "Akira Horiguchi", "Li Ma"], "title": "Wavelet Tree Ensembles for Triangulable Manifolds", "comment": "56 pages, 16 figures", "summary": "We develop unbalanced Haar (UH) wavelet tree ensembles for regression on triangulable manifolds. Given data sampled on a triangulated manifold, we construct UH wavelet trees whose atoms are supported on geodesic triangles and form an orthonormal system in $L^2(μ_n)$, where $μ_n$ is the empirical measure on the sample, which allows us to use UH trees as weak learners in additive ensembles. Our construction extends classical UH wavelet trees from regular Euclidean grids to generic triangulable manifolds while preserving three key properties: (i) orthogonality and exact reconstruction at the sampled locations, (ii) recursive, data-driven partitions adapted to the geometry of the manifold via geodesic triangulations, and (iii) compatibility with optimization-based and Bayesian ensemble building. In Euclidean settings, the framework reduces to standard UH wavelet tree regression and provides a baseline for comparison. We illustrate the method on synthetic regression on the sphere and on climate anomaly fields on a spherical mesh, where UH ensembles on triangulated manifolds substantially outperform classical tree ensembles and non-adaptive mesh-based wavelets. For completeness, we also report results on image denoising on regular grids. A Bayesian variant (RUHWT) provides posterior uncertainty quantification for function estimates on manifolds. Our implementation is available at http://www.github.com/hrluo/WaveletTrees."}
{"id": "2601.20314", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20314", "abs": "https://arxiv.org/abs/2601.20314", "authors": ["Xinran Wang", "Peng Wu", "Xiaopeng Yuan", "Yulin Hu", "Anke Schmeink"], "title": "Efficient Trajectory Design and Communication Scheduling for Dual-UAV Jamming-Aided Secure Communication Networks", "comment": null, "summary": "We study dual-unmanned aerial vehicle (UAV) jamming-aided secure communication networks, in which one UAV delivers confidential data to multiple ground users (GUs), while a cooperative UAV provides protective interference against a ground eavesdropper. To enforce fairness, we maximize the minimum secrecy throughput across GUs by jointly designing trajectories and communication scheduling. The key difficulty lies in the continuous-time nature of UAV trajectories and the tight space-time coupling between the transmitter and the jammer, which jointly render the problem infinite-dimensional and nonconvex. To address these challenges, we characterize, for the first time, the structure of the optimal trajectories and rigorously prove that they follow a collaborative successive hover-and-fly (co-SHF) structure, where the two UAVs visit a limited number of synchronized co-hovering point pairs, and during each flight segment at least one UAV moves at maximum speed. Leveraging this structure, we reformulate the problem into a finite-dimensional form, without loss of optimality, over hovering and turning points, hovering durations, and scheduling. For tractability, we adopt a minimum-distance approximation of continuous anti-collision constraints and employ concave lower bounds on secrecy throughput within a successive convex approximation (SCA) method, which converges and, thanks to the co-SHF reduction in optimization variables and constraints, achieves low computational complexity. Numerical results show that, compared with time-discretization and no-jamming benchmarks, the proposed co-SHF design improves the min-secrecy and user fairness while requiring significantly less runtime."}
{"id": "2601.20328", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.20328", "abs": "https://arxiv.org/abs/2601.20328", "authors": ["Kazuo Hida"], "title": "Ground-State Phase Diagram of (1/2,1/2,1) Mixed Diamond Chains with Single-Site Anisotropy", "comment": "5 pages, 5 figures", "summary": "The ground-state phases of mixed diamond chains with ($S, τ^{(1)}, τ^{(2)})=(1/2,1/2,1)$, where $S$ is the magnitude of vertex spins, and $τ^{(1)}$ and $τ^{(2)}$ are those of apical spins, are investigated with the single-site anisotropy $D$ on the $τ^{(2)}$-site. The two apical spins in each unit cell are coupled by an exchange coupling $λ$. The vertex spins are coupled with the top and bottom apical spins by exchange couplings $1+δ$ and $1-δ$, respectively. The ground-state phase diagram is determined using the numerical exact diagonalization and DMRG method in addition to the analytical approximations in various limiting cases. The phase diagram consists of a Néel ordered phase, a nonmagnetic Tomonaga-Luttinger liquid phase, quantized and partial ferrimagnetic phases. A region with anisotropy inversion is found where the Ising-like Néel phase is realized for the easy-plane anisotropy $D >0$ and the XY-like Tomonaga-Luttinger liquid phase is realized for the easy-axis anisotropy $D <0$ on the $S=1$ sites."}
{"id": "2601.20025", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20025", "abs": "https://arxiv.org/abs/2601.20025", "authors": ["Jawaher Almutlaq", "Alessandro Buzzi", "Anders Khaykin", "Linsen Li", "William Yzaguirre", "Maxim Sirotin", "Gerald Gilbert", "Genevieve Clark", "Dirk Englund"], "title": "Foundry-Enabled Patterning of Diamond Quantum Microchiplets for Scalable Quantum Photonics", "comment": null, "summary": "Quantum technologies promise secure communication networks and powerful new forms of information processing, but building these systems at scale remains a major challenge. Diamond is an especially attractive material for quantum devices because it can host atomic-scale defects that emit single photons and store quantum information with exceptional stability. However, fabricating the optical structures needed to control light in diamond typically relies on slow, bespoke processes that are difficult to scale. In this work, we introduce a manufacturing approach that brings diamond quantum photonics closer to industrial production. Instead of sequentially defining each device by lithography written directly on diamond, we fabricate high-precision silicon masks using commercial semiconductor foundries and transfer them onto diamond via microtransfer printing. These masks define large arrays of nanoscale optical structures, shifting the most demanding pattern-definition steps away from the diamond substrate, improving uniformity, yield, and throughput. Using this method, we demonstrate hundreds of diamond \"quantum microchiplets\" with improved optical performance and controlled interaction with quantum emitters. The chiplet format allows defective devices to be replaced and enables integration with existing photonic and electronic circuits. Our results show that high-quality diamond quantum devices can be produced using scalable, foundry-compatible techniques. This approach provides a practical pathway toward large-scale quantum photonic systems and hybrid quantum-classical technologies built on established semiconductor manufacturing infrastructure."}
{"id": "2601.20474", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20474", "abs": "https://arxiv.org/abs/2601.20474", "authors": ["José F. B. Afonso", "Stefan Kirchner", "Pedro Ribeiro"], "title": "Critical Charge and Current Fluctuations across a Voltage-Driven Phase Transition", "comment": "13 pages, 6 figures", "summary": "We investigate bias-driven non-equilibrium quantum phase transitions in a paradigmatic quantum-transport setup: an interacting quantum dot coupled to non-interacting metallic leads. Using the Random Phase Approximation, which is exact in the limit of a large number of dot levels, we map out the zero-temperature non-equilibrium phase diagram as a function of interaction strength and applied bias. We focus our analysis on the behavior of the charge susceptibility and the current noise in the vicinity of the transition. Remarkably, despite the intrinsically non-equilibrium nature of the steady state, critical charge fluctuations admit an effective-temperature description, $T_{\\text{eff}}(T,V)$, that collapses the steady-state behavior onto its equilibrium form. In sharp contrast, current fluctuations exhibit genuinely non-equilibrium features: the fluctuation-dissipation ratio becomes negative in the ordered phase, corresponding to a negative effective temperature for the current degrees of freedom. These results establish current noise as a sensitive probe of critical fluctuations at non-equilibrium quantum phase transitions and open new directions for exploring voltage-driven critical phenomena in quantum transport systems."}
{"id": "2601.20561", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20561", "abs": "https://arxiv.org/abs/2601.20561", "authors": ["Jilles S. van Hulst", "Erik M. Franken", "Bart J. Janssen", "W. P. M. H.", "Heemels", "Duarte J. Antunes"], "title": "Tilt-based Aberration Estimation in Transmission Electron Microscopy", "comment": "Submitted version (pre-print)", "summary": "Transmission electron microscopes (TEMs) enable atomic-scale imaging but suffer from aberrations caused by lens imperfections and environmental conditions, reducing image quality. These aberrations can be compensated by adjusting electromagnetic lenses, but this requires accurate estimates of the aberration coefficients, which can drift over time. This paper introduces a method for the estimation of aberrations in TEM by leveraging the relationship between an induced electron beam tilt and the resulting image shift. The method uses a Kalman filter (KF) to estimate the aberration coefficients from a sequence of image shifts, while accounting for the drift of the aberrations over time. The applied tilt sequence is optimized by minimizing the trace of the predicted error covariance in the KF, which corresponds to the A-optimality criterion in experimental design. We show that this optimization can be performed offline, as the cost criterion is independent of the actual measurements. The resulting non-convex optimization problem is solved using a gradient-based, receding-horizon approach with multi-starts. Additionally, we develop an approach to estimate specimen-dependent noise properties using expectation maximization (EM), which are then used to tailor the tilt pattern optimization to the specific specimen being imaged. The proposed method is validated on a real TEM set-up with several optimized tilt patterns. The results show that optimized patterns significantly outperform naive approaches and that the aberration and drift model accurately captures the underlying physical phenomena. In total, the alignment time is reduced from typically several minutes to less than a minute compared to the state-of-the-art."}
{"id": "2601.20522", "categories": ["math.ST", "cs.DS", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.20522", "abs": "https://arxiv.org/abs/2601.20522", "authors": ["Zhangsong Li"], "title": "Improved Computational Lower Bound of Estimation for Multi-Frequency Group Synchronization", "comment": "22 pages", "summary": "We study the computational phase transition in a multi-frequency group synchronization problem, where pairwise relative measurements of group elements are observed across multiple frequency channels and corrupted by Gaussian noise. Using the framework of \\emph{low-degree polynomial algorithms}, we analyze the task of estimating the structured signal in such observations. We show that, assuming the low-degree heuristic, in synchronization models over the circle group $\\mathsf{SO}(2)$, a simple spectral method is computationally optimal among all polynomial-time estimators when the number of frequencies satisfies $L=n^{o(1)}$. This significantly extends prior work \\cite{KBK24+}, which only applied to a fixed constant number of frequencies. Together with known upper bounds on the statistical threshold \\cite{PWBM18a}, our results establish the existence of a \\emph{statistical-to-computational gap} in this model when the number of frequencies is sufficiently large."}
{"id": "2601.20345", "categories": ["math.OC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20345", "abs": "https://arxiv.org/abs/2601.20345", "authors": ["Shivangi Dubey Sharma", "Basil M. Idrees", "Lavish Arora", "Ketan Rajawat"], "title": "Decentralized Stochastic Constrained Optimization via Prox-Linearization", "comment": "18 pages, 6 figures", "summary": "This paper studies consensus-based decentralized stochastic optimization for minimizing possibly non-convex expected objectives with convex non-smooth regularizers and nonlinear functional inequality constraints. We reformulate the constrained problem using the exact-penalty model and develop two algorithms that require only local stochastic gradients and first-order constraint information. The first method, Decentralized Stochastic Momentum-based Prox-Linear Algorithm (D-SMPL), combines constraint linearization with a prox-linear step, resulting in a linearly constrained quadratic subproblem per iteration. Building on this approach, we propose a successive convex approximation (SCA) variant, Decentralized SCA Momentum-based Prox-Linear (D-SCAMPL), which handles additional objective structure through strongly convex surrogate subproblems while still allowing infeasible initialization. Both methods incorporate recursive momentum-based gradient estimators and a consensus mechanism requiring only two communication rounds per iteration. Under standard smoothness and regularity assumptions, both algorithms achieve an oracle complexity of $\\mathcal{O}(ε^{-3/2})$, matching the optimal rate known for unconstrained centralized stochastic non-convex optimization. Numerical experiments on energy-optimal ocean trajectory planning corroborate the theory and demonstrate improved performance over existing decentralized baselines."}
{"id": "2601.20191", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20191", "abs": "https://arxiv.org/abs/2601.20191", "authors": ["Junqing Chen", "Chengzhe Jiang"], "title": "A direct sampling method for magnetic induction tomography", "comment": null, "summary": "This paper proposes a direct sampling method for the inverse problem of magnetic induction tomography (MIT). Our approach defines a class of point spread functions with explicit expressions, which are computed via inner products, leading to a simple and fast imaging process. We then prove that these point spread functions decay with distance, establishing the theoretical basis of the algorithm. Specific expressions for special cases are also derived to visually demonstrate their attenuation pattern. Numerical experimental results further confirm the efficiency and accuracy of the proposed algorithm."}
{"id": "2601.20189", "categories": ["cond-mat.str-el", "hep-lat"], "pdf": "https://arxiv.org/pdf/2601.20189", "abs": "https://arxiv.org/abs/2601.20189", "authors": ["Anders W. Sandvik"], "title": "High-precision ground state parameters of the two-dimensional spin-1/2 Heisenberg model on the square lattice", "comment": "17 pages, 10 figures", "summary": "Several ground state properties of the square-lattice $S=1/2$ Heisenberg antiferromagnet are computed (the energy, order parameter, spin stiffness, spinwave velocity, long-wavelength susceptibility, and staggered susceptibility) using extensive quantum Monte Carlo simulations with the stochastic series expansion method. Moderately sized lattices are studied at temperatures $T$ sufficiently low to realize the $T \\to 0$ limit. Results for periodic $L\\times L$ lattices with $L \\in [6,96]$ are tabulated versus $L$ and extrapolations to infinite system size are carried out. The extrapolated ground state energy density is $e_0=-0.669441857(7)$, which represents an improvement in precision of three orders of magnitude over the previously best result. The leading and subleading finite-size corrections to $e_0$ are in full quantitative agreement with predictions from chiral perturbation theory, thus further supporting the soundness of both the extrapolations and the theory. The extrapolated sublattice magnetization is $m_s=0.307447(2)$, which agrees well with previous estimates but with a much smaller statistical error. The coefficient of the linear in $L^{-1}$ correction to $m^2_s$ agrees with the value from chiral perturbation theory and the presence of a factor $\\ln^γ(L)$ in the second-order correction is also confirmed, with the previously not known value of the exponent being $γ= 0.82(4)$. The finite-size corrections to the staggered susceptibility point to logarithmic corrections also in this quantity. To facilitate benchmarking of methods for which periodic boundary conditions are challenging, results for systems with open and cylindrical boundaries are also listed and their spatially inhomogeneous order parameters are analyzed."}
{"id": "2601.20320", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20320", "abs": "https://arxiv.org/abs/2601.20320", "authors": ["Alessandro Colombi", "Mario Beraha", "Amichai Painsky", "Stefano Favaro"], "title": "Confidence intervals for maximum unseen probabilities, with application to sequential sampling design", "comment": null, "summary": "Discovery problems often require deciding whether additional sampling is needed to detect all categories whose prevalence exceeds a prespecified threshold. We study this question under a Bernoulli product (incidence) model, where categories are observed only through presence--absence across sampling units. Our inferential target is the \\emph{maximum unseen probability}, the largest prevalence among categories not yet observed. We develop nonasymptotic, distribution-free upper confidence bounds for this quantity in two regimes: bounded alphabets (finite and known number of categories) and unbounded alphabets (countably infinite under a mild summability condition). We characterise the limits of data-independent worst-case bounds, showing that in the unbounded regime no nontrivial data-independent procedure can be uniformly valid. We then propose data-dependent bounds in both regimes and establish matching lower bounds demonstrating their near-optimality. We compare empirically the resulting procedures in both simulated and real datasets. Finally, we use these bounds to construct sequential stopping rules with finite-sample guarantees, and demonstrate robustness to contamination that introduces spurious low-prevalence categories."}
{"id": "2601.20324", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20324", "abs": "https://arxiv.org/abs/2601.20324", "authors": ["Jingyuan Zhou", "Haoze Wu", "Kaidi Yang"], "title": "Neural Cooperative Reach-While-Avoid Certificates for Interconnected Systems", "comment": null, "summary": "Providing formal guarantees for neural network-based controllers in large-scale interconnected systems remains a fundamental challenge. In particular, using neural certificates to capture cooperative interactions and verifying these certificates at scale is crucial for the safe deployment of such controllers. However, existing approaches fall short on both fronts. To address these limitations, we propose neural cooperative reach-while-avoid certificates with Dynamic-Localized Vector Control Lyapunov and Barrier Functions, which capture cooperative dynamics through state-dependent neighborhood structures and provide decentralized certificates for global exponential stability and safety. Based on the certificates, we further develop a scalable training and verification framework that jointly synthesizes controllers and neural certificates via a constrained optimization objective, and leverages a sufficient condition to ensure formal guarantees considering modeling error. To improve scalability, we introduce a structural reuse mechanism to transfer controllers and certificates between substructure-isomorphic systems. The proposed methodology is validated with extensive experiments on multi-robot coordination and vehicle platoons. Results demonstrate that our framework ensures certified cooperative reach-while-avoid while maintaining strong control performance."}
{"id": "2601.20474", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20474", "abs": "https://arxiv.org/abs/2601.20474", "authors": ["José F. B. Afonso", "Stefan Kirchner", "Pedro Ribeiro"], "title": "Critical Charge and Current Fluctuations across a Voltage-Driven Phase Transition", "comment": "13 pages, 6 figures", "summary": "We investigate bias-driven non-equilibrium quantum phase transitions in a paradigmatic quantum-transport setup: an interacting quantum dot coupled to non-interacting metallic leads. Using the Random Phase Approximation, which is exact in the limit of a large number of dot levels, we map out the zero-temperature non-equilibrium phase diagram as a function of interaction strength and applied bias. We focus our analysis on the behavior of the charge susceptibility and the current noise in the vicinity of the transition. Remarkably, despite the intrinsically non-equilibrium nature of the steady state, critical charge fluctuations admit an effective-temperature description, $T_{\\text{eff}}(T,V)$, that collapses the steady-state behavior onto its equilibrium form. In sharp contrast, current fluctuations exhibit genuinely non-equilibrium features: the fluctuation-dissipation ratio becomes negative in the ordered phase, corresponding to a negative effective temperature for the current degrees of freedom. These results establish current noise as a sensitive probe of critical fluctuations at non-equilibrium quantum phase transitions and open new directions for exploring voltage-driven critical phenomena in quantum transport systems."}
{"id": "2601.20029", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20029", "abs": "https://arxiv.org/abs/2601.20029", "authors": ["Enhyeok Jang", "Zihan Chen", "Dongho Ha", "Seungwoo Choi", "Yongju Lee", "Jaewon Kwon", "Eddy Z. Zhang", "Yipeng Huang", "Won Woo Ro"], "title": "A Cyclic Layerwise QAOA Training", "comment": null, "summary": "The quantum approximate optimization algorithm (QAOA) is a hybrid quantum-classical algorithm for solving combinatorial optimization problems. Multi-angle QAOA (MA-QAOA), which assigns independent parameters to each Hamiltonian operator term, achieves superior approximation performance even with fewer layers than standard QAOA. Unfortunately, this increased expressibility can raise the classical computational cost due to a greater number of parameters. The recently proposed Layerwise MA-QAOA (LMA-QAOA) reduces this overhead by training one layer at a time, but it may suffer from obtaining the precise solution due to the previously fixed parameters. This work addresses two questions for efficient MA-QAOA training: (i) What is the optimal granularity for parameter updates per epoch, and (ii) How can we get precise final cost function results while only partially updating the parameters per epoch? Despite the benefit of reducing the parameters that update per epoch can reduce the classical computation overhead, too fine or coarse a granularity of Hamiltonian update can degrade the MA-QAOA training efficiency. We find that optimizing one complete layer per epoch is an efficient granularity. Moreover, selectively retraining each layer by tracking gradient variations can achieve a final cost function equivalent to the standard MA-QAOA while lowering the parameter update overhead. Based on these insights, we propose Orbit-QAOA, which cyclically revisits layers and selectively freezes stabilized parameters. Across diverse graph benchmarks, Orbit-QAOA reduces training steps by up to 81.8%, reduces approximation ratio error by up to 72x compared to the unified stop condition-applied enhanced LMA-QAOA, and achieves equivalent approximation performance compared to the standard MA-QAOA."}
{"id": "2601.20532", "categories": ["cond-mat.dis-nn", "cond-mat.quant-gas", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20532", "abs": "https://arxiv.org/abs/2601.20532", "authors": ["Yucheng Wang"], "title": "A Unified Symmetry Classification of Many-Body Localized Phases", "comment": null, "summary": "Anderson localization admits a complete symmetry classification given by the Altland-Zirnbauer (AZ) tenfold scheme, whereas an analogous framework for interacting many-body localization (MBL) has remained elusive. Here we develop a symmetry-based classification of static MBL phases formulated at the level of local integrals of motion (LIOMs). We show that a symmetry is compatible with stable MBL if and only if its action can be consistently represented within a quasi-local LIOM algebra, without enforcing extensive degeneracies or nonlocal operator mixing. This criterion sharply distinguishes symmetry classes: onsite Abelian symmetries are compatible with stable MBL and can host distinct symmetry-protected topological MBL phases, whereas continuous non-Abelian symmetries generically preclude stable MBL. By systematically combining AZ symmetries with additional onsite symmetries, we construct a complete classification table of MBL phases, identify stable, fragile, and unstable classes, and provide representative lattice realizations. Our results establish a unified and physically transparent framework for understanding symmetry constraints on MBL."}
{"id": "2601.20723", "categories": ["eess.SY", "cs.GT", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.20723", "abs": "https://arxiv.org/abs/2601.20723", "authors": ["Emrah Akyol", "Marcos Vasconcelos"], "title": "Distributed Learning over Noisy Communication Networks", "comment": "draft, submitted to IEEE JSAC Special Issue on Distributed Optimization, Learning, and Inference over Communication-Constrained Networks", "summary": "We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality."}
{"id": "2601.20528", "categories": ["math.ST", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20528", "abs": "https://arxiv.org/abs/2601.20528", "authors": ["Claudio Durastanti"], "title": "Spectral Bayesian Regression on the Sphere", "comment": "42 pages, 3 figures, 2 tables", "summary": "We develop a fully intrinsic Bayesian framework for nonparametric regression on the unit sphere based on isotropic Gaussian field priors and the harmonic structure induced by the Laplace-Beltrami operator. Under uniform random design, the regression model admits an exact diagonalization in the spherical harmonic basis, yielding a Gaussian sequence representation with frequency-dependent multiplicities.\n  Exploiting this structure, we derive closed-form posterior distributions, optimal spectral truncation schemes, and sharp posterior contraction rates under integrated squared loss. For Gaussian priors with polynomially decaying angular power spectra, including spherical Matérn priors, we establish posterior contraction rates over Sobolev classes, which are minimax-optimal under correct prior calibration.\n  We further show that the posterior mean admits an exact variational characterization as a geometrically intrinsic penalized least-squares estimator, equivalent to a Laplace-Beltrami smoothing spline."}
{"id": "2601.20387", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.20387", "abs": "https://arxiv.org/abs/2601.20387", "authors": ["Zhongqin Gao", "Yan Lv", "Jingmin He"], "title": "Reinforcement Learning for Dividend Optimization in Partially Observed Regime-Switching Diffusion Model", "comment": "41 pages, 4 figures, 3 tables", "summary": "This paper studies the optimal dividend problem with a bounded payout rate in a partially observed regime-switching diffusion model, where, in practice, the market regime is unobserved and key model parameters are unknown. To address this partial-information setting, we propose a continuous-time reinforcement learning (RL) approach within an exploratory (entropy-regularized) stochastic control framework for discounted dividends under regime switching. The associated exploratory Hamilton-Jacobi-Bellman (HJB) system admits semi-analytical characterizations of the value function and the optimal exploratory dividend policy, determined by two unknown functions solving two ordinary differential equations (ODEs) together with positive real roots of the induced quadratic equations. Exploiting this structure, we introduce parametric families for both the value function and the policy, using low-degree polynomial approximations to the ODE solutions. We then develop an actor-critic RL algorithm to learn the optimal exploratory policy through interactions with the market environment: it performs belief-state filtering from observed data and iterates policy evaluation and policy improvement online to refine the policy. Numerical experiments demonstrate strong out-of-sample performance of the learned dividend policies."}
{"id": "2601.20207", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20207", "abs": "https://arxiv.org/abs/2601.20207", "authors": ["Xiaobin Li", "Leevan Ling", "Yizhong Sun"], "title": "Local Regularity Estimation through Sobolev-Scale Norm Profile", "comment": null, "summary": "We develop a kernel-based approach for estimating the spatially varying Sobolev regularity~$s$ of an unknown $d$-variate function~$f$ from scattered sampling data, which quantifies the degree of local differentiability supported by the data. Relying only on neighborhood data near the point of interest $z\\in Ω_z$, our method constructs a sequence of Sobolev-space reproducing kernel interpolants whose kernel smoothness order is specified by an index~$m > d/2$. The native-space norms of these interpolants are evaluated over a bounded range of~$m$, producing a \\emph{Sobolev-scale norm profile}. The elbow of this profile serves as a quantitative probe of the underlying local regularity~$s(Ω_z)$. In particular, when $m > s(Ω_z)$, the profile exhibits rapid, near-worst-case growth governed by the classical upper bound associated with the conditioning of the kernel matrix. A band-limited surrogate analysis explains this transition and establishes a lower-bound relation linking native-norm growth to the Sobolev regularity of~$f$. Two complementary strategies are incorporated for further enhancement: (i)~a \\emph{stencil-shift} subroutine, which repositions local neighborhoods to avoid crossing discontinuities whenever possible, thereby suppressing artifacts in the norm estimates; and (ii)~a local--global \\emph{norm-sweep comparison} strategy that combines short two-point local tails with an optional one-point global screen to detect outlier $Ω_z$ of low Sobolev regularity and accelerate evaluation on large datasets. Numerical experiments on synthetic test functions and turbulent-flow data demonstrate accurate recovery of spatially varying regularity and confirm the robustness of the proposed characterization for kernel-based approximation and differentiation."}
{"id": "2601.20386", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20386", "abs": "https://arxiv.org/abs/2601.20386", "authors": ["Qi Kuang", "Bowen Gang", "Yin Xia"], "title": "SCORE: A Unified Framework for Overshoot Refund in Online FDR Control", "comment": null, "summary": "We propose a unified framework to enhance the power of online multiple hypothesis testing procedures based on $e$-values. While $e$-value-based methods offer robust online False Discovery Rate (FDR) control under minimal assumptions, they often suffer from power loss by discarding evidence that exceeds the rejection threshold. We address this inefficiency via the \\textbf{S}equential \\textbf{C}ontrol with \\textbf{O}vershoot \\textbf{R}efund for \\textbf{E}-values (SCORE) framework, which leverages the inequality $\\mathbb{I}(y \\ge 1) \\le y - (y-1)_+$ to reclaim this otherwise ``wasted'' evidence. This simple yet powerful insight yields a unified principle for improving a broad class of online testing algorithms. Building on this framework, we develop SCORE-enhanced versions of several state-of-the-art procedures, including SCORE-LOND, SCORE-LORD, and SCORE-SAFFRON, all of which strictly dominate their original counterparts while preserving valid finite-sample FDR control. Furthermore, under mild assumptions, SCORE permits retroactive updates of alpha-wealth by using the latest decision twice: first to determine its reward or loss, and then to refresh past wealth. Such a mechanism enables more aggressive testing strategies while maintaining valid FDR control, thereby further improving statistical power. The effectiveness of the proposed methods is validated through extensive simulation and real-data experiments."}
{"id": "2601.20427", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20427", "abs": "https://arxiv.org/abs/2601.20427", "authors": ["Yixuan Zhu", "Yinkang Gao", "Bo Zhang", "Xiaohang Gong", "Binze Jiang", "Lei Gong", "Wenqi Lou", "Teng Wang", "Chao Wang", "Xi Li", "Xuehai Zhou"], "title": "Reducing End-to-End Latency of Cause-Effect Chains with Shared Cache Analysis", "comment": null, "summary": "Cause-effect chains, as a widely used modeling method in real-time embedded systems, are extensively applied in various safety-critical domains. End-to-end latency, as a key real-time attribute of cause-effect chains, is crucial in many applications. But the analysis of end-to-end latency for cause-effect chains on multicore platforms with shared caches still presents an unresolved issue. Traditional methods typically assume that the worst-case execution time (WCET) of each task in the cause-effect chain is known. However, in the absence of scheduling information, these methods often assume that all shared cache accesses result in misses, leading to an overestimation of WCET and, consequently, affecting the accuracy of end-to-end latency. However, effectively integrating scheduling information into the WCET analysis process of the chains may introduce two challenges: first, how to leverage the structural characteristics of the chains to optimize shared cache analysis, and second, how to improve analysis accuracy while avoiding state space explosion.\n  To address these issues, this paper proposes a novel end-to-end latency analysis framework designed for multi-chain systems on multicore platforms with shared caches. This framework extracts scheduling information and structural characteristics of cause-effect chains, constructing fine-grained and scalable inter-core memory access contexts at the basic block level for time-sensitive shared cache analysis. This results in more accurate WCET (TSC-WCET) estimates, which are then used to derive the end-to-end latency. Finally, we conduct experiments on dual-core and quad-core systems with various cache configurations, which show that under certain settings, the average maximum end-to-end latency of cause-effect chains is reduced by up to 34% and 26%."}
{"id": "2601.20517", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.20517", "abs": "https://arxiv.org/abs/2601.20517", "authors": ["Yanan Zhang", "Zhaoyang Shan", "Jiawen Zhang", "Kaixin Ye", "Yongjian Li", "Dajun Su", "Pascal Manuel", "Dmitry Khalyavin", "Devashibhai Adroja", "Daniel Mayoh", "Geetha Balakrishnan", "Yu Liu", "Michael Smidman", "Huiqiu Yuan"], "title": "Magnetic states of the Kondo lattice Ce$_2$PdSi$_3$ and their pressure evolution", "comment": "6 pages, 5 figures", "summary": "Frustrated Kondo lattices are ideal platforms for exploring unconventional forms of quantum criticality, as well as magnetism and other emergent phases. Here we report the magnetic properties of the candidate frustrated heavy fermion compound Ce$_2$PdSi$_3$, and map their evolution upon applying magnetic fields and hydrostatic pressure. We find that at ambient pressure Ce$_2$PdSi$_3$ exhibits two distinct magnetic phase transitions, a ferromagnetic-like transition at $T_{\\mathrm{M1}}=3.8$ K and an incommensurate antiferromagnetic transition at $T_{\\mathrm{M2}}=2.9$ K. Upon applying pressure, $T_{\\mathrm{M1}}$ is continuously suppressed and becomes undetectable above 4.2 GPa, whereas $T_{\\mathrm{M2}}$ increases and remains robust up to at least 7.5 GPa. The observed pressure evolution of magnetic order in Ce$_2$PdSi$_3$ suggests the presence of competing magnetic orders, and cannot be simply encapsulated by the Doniach phase diagram, motivating further investigations for its origin, including discerning the role of geometric frustration."}
{"id": "2601.20044", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20044", "abs": "https://arxiv.org/abs/2601.20044", "authors": ["Giuseppe Catalano", "Farzad Kianvash", "Vittorio Giovannetti"], "title": "Quantum Channels on Graphs: a Resonant Tunneling Perspective", "comment": "5 + 13 pages, 2 + 8 figures", "summary": "Quantum transport on structured networks is strongly influenced by interference effects, which can dramatically modify how information propagates through a system. We develop a quantum-information-theoretic framework for scattering on graphs in which a full network of connected scattering sites is treated as a quantum channel linking designated input and output ports. Using the Redheffer star product to construct global scattering matrices from local ones, we identify resonant concatenation, a nonlinear composition rule generated by internal back-reflections. In contrast to ordinary channel concatenation, resonant concatenation can suppress noise and even produce super-activation of the quantum capacity, yielding positive capacity in configurations where each constituent channel individually has zero capacity. We illustrate these effects through models exhibiting resonant-tunneling-enhanced transport. Our approach provides a general methodology for analyzing coherent information flow in quantum graphs, with relevance for quantum communication, control, and simulation in structured environments."}
{"id": "2601.20560", "categories": ["quant-ph", "cond-mat.other", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.20560", "abs": "https://arxiv.org/abs/2601.20560", "authors": ["Emma C. King", "Sayan Roy", "Francesco Mattiotti", "Maximilian Kiefer-Emmanouilidis", "Markus Bläser", "Giovanna Morigi"], "title": "Time complexity of a monitored quantum search with resetting", "comment": "22 pages (7 main text + 15 supplemental material), 13 figures (3 figures + 10 supplemental figures)", "summary": "Searching a database is a central task in computer science and is paradigmatic of transport and optimization problems in physics. For an unstructured search, Grover's algorithm predicts a quadratic speedup, with the search time $τ(N)=Θ(\\sqrt{N})$ and $N$ the database size. Numerical studies suggest that the time complexity can change in the presence of feedback, injecting information during the search. Here, we determine the time complexity of the quantum analog of a randomized algorithm, which implements feedback in a simple form. The search is a continuous-time quantum walk on a complete graph, where the target is continuously monitored by a detector. Additionally, the quantum state is reset if the detector does not click within a specified time interval. This yields a non-unitary, non-Markovian dynamics. We optimize the search time as a function of the hopping amplitude, detection rate, and resetting rate, and identify the conditions under which time complexity could outperform Grover's scaling. The overall search time does not violate Grover's optimality bound when including the time budget of the physical implementation of the measurement. For databases of finite sizes monitoring can warrant rapid convergence and provides a promising avenue for fault-tolerant quantum searches."}
{"id": "2601.20610", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.20610", "abs": "https://arxiv.org/abs/2601.20610", "authors": ["Ting Li", "Ethan Fan", "Tengfei Li", "Hongtu Zhu"], "title": "Causal Inference in Biomedical Imaging via Functional Linear Structural Equation Models", "comment": null, "summary": "Understanding the causal effects of organ-specific features from medical imaging on clinical outcomes is essential for biomedical research and patient care. We propose a novel Functional Linear Structural Equation Model (FLSEM) to capture the relationships among clinical outcomes, functional imaging exposures, and scalar covariates like genetics, sex, and age. Traditional methods struggle with the infinite-dimensional nature of exposures and complex covariates. Our FLSEM overcomes these challenges by establishing identifiable conditions using scalar instrumental variables. We develop the Functional Group Support Detection and Root Finding (FGS-DAR) algorithm for efficient variable selection, supported by rigorous theoretical guarantees, including selection consistency and accurate parameter estimation. We further propose a test statistic to test the nullity of the functional coefficient, establishing its null limit distribution. Our approach is validated through extensive simulations and applied to UK Biobank data, demonstrating robust performance in detecting causal relationships from medical imaging."}
{"id": "2601.20399", "categories": ["math.OC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20399", "abs": "https://arxiv.org/abs/2601.20399", "authors": ["Gaku Omiya", "Pierre-Louis Poirion", "Akiko Takeda"], "title": "Convergence Analysis of Randomized Subspace Normalized SGD under Heavy-Tailed Noise", "comment": "41 pages", "summary": "Randomized subspace methods reduce per-iteration cost; however, in nonconvex optimization, most analyses are expectation-based, and high-probability bounds remain scarce even under sub-Gaussian noise. We first prove that randomized subspace SGD (RS-SGD) admits a high-probability convergence bound under sub-Gaussian noise, achieving the same order of oracle complexity as prior in-expectation results. Motivated by the prevalence of heavy-tailed gradients in modern machine learning, we then propose randomized subspace normalized SGD (RS-NSGD), which integrates direction normalization into subspace updates. Assuming the noise has bounded $p$-th moments, we establish both in-expectation and high-probability convergence guarantees, and show that RS-NSGD can achieve better oracle complexity than full-dimensional normalized SGD."}
{"id": "2601.20212", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20212", "abs": "https://arxiv.org/abs/2601.20212", "authors": ["Lun Ji", "Hang Li", "Alexander Ostermann"], "title": "A low regularity exponential-type integrator for the derivative nonlinear Schrödinger equation", "comment": "20 pages, 6 figures", "summary": "In this work, we present a first-order unfiltered exponential integrator for the one-dimensional derivative nonlinear Schrödinger equation with low regularity. Our analysis shows that for any $s>\\frac12$, the method converges with first-order in $H^s(\\mathbb{T})$ for initial data $u_0\\in H^{s+1}(\\mathbb{T})$. Moreover, we constructed a symmetrized version of this method that performs better in terms of both global error and conservation behavior. To the best of our knowledge, these are the first low regularity integrators for the derivative nonlinear Schrödinger equation. Numerical experiments illustrate our theoretical findings."}
{"id": "2601.20589", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20589", "abs": "https://arxiv.org/abs/2601.20589", "authors": ["Lucas Kook", "Søren Wengel Mogensen"], "title": "Exact Graph Learning via Integer Programming", "comment": null, "summary": "Learning the dependence structure among variables in complex systems is a central problem across medical, natural, and social sciences. These structures can be naturally represented by graphs, and the task of inferring such graphs from data is known as graph learning or as causal discovery if the graphs are given a causal interpretation. Existing approaches typically rely on restrictive assumptions about the data-generating process, employ greedy oracle algorithms, or solve approximate formulations of the graph learning problem. As a result, they are either sensitive to violations of central assumptions or fail to guarantee globally optimal solutions. We address these limitations by introducing a nonparametric graph learning framework based on nonparametric conditional independence testing and integer programming. We reformulate the graph learning problem as an integer-programming problem and prove that solving the integer-programming problem provides a globally optimal solution to the original graph learning problem. Our method leverages efficient encodings of graphical separation criteria, enabling the exact recovery of larger graphs than was previously feasible. We provide an implementation in the openly available R package 'glip' which supports learning (acyclic) directed (mixed) graphs and chain graphs. From the resulting output one can compute representations of the corresponding Markov equivalence classes or weak equivalence classes. Empirically, we demonstrate that our approach is faster than other existing exact graph learning procedures for a large fraction of instances and graphs of various sizes. GLIP also achieves state-of-the-art performance on simulated data and benchmark datasets across all aforementioned classes of graphs."}
{"id": "2601.20445", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20445", "abs": "https://arxiv.org/abs/2601.20445", "authors": ["Yixuan Zhu", "Yinkang Gao", "Lei Gong", "Binze Jiang", "Xiaohang Gong", "Zihan Wang", "Cheng Tang", "Wenqi Lou", "Teng Wang", "Chao Wang", "Xi Li", "Xuehai Zhou"], "title": "A Timing-Anomaly Free Dynamic Scheduling on Heterogeneous Systems", "comment": null, "summary": "Heterogeneous systems commonly adopt dynamic scheduling algorithms to improve resource utilization and enhance scheduling flexibility. However, such flexibility may introduce timing anomalies, wherein locally reduced execution times can lead to an increase in the overall system execution time. This phenomenon significantly complicates the analysis of Worst-Case Response Time (WCRT), rendering conventional analysis either overly pessimistic or unsafe, and often necessitating exhaustive state-space exploration to ensure correctness.\n  To address this challenge, this paper presents the first timing-anomaly-free dynamic scheduling algorithm for heterogeneous systems, referred to as Deterministic Dynamic Execution. It achieves a safe and tight WCRT estimate through a single offline simulation execution. The core idea is to apply deterministic execution constraints, which partially restrict the resource allocation and execution order of tasks at runtime. Based on a formally defined execution progress model for heterogeneous system scheduling, we prove the correctness of the proposed execution constraints and their ability to eliminate timing anomalies. Furthermore, we propose two methods to generate execution constraints. The first method derives execution constraints directly from the execution traces produced by existing scheduling algorithms. The second method is a heuristic-based approach that constructs execution constraints, enabling further reduction of the WCRT. Experimental results on synthetically generated DAG task sets under various system configurations demonstrate that, compared to traditional dynamic scheduling algorithms, our approach not only eliminates timing anomalies but also effectively reduces both the WCRT and response time jitter."}
{"id": "2601.20695", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.20695", "abs": "https://arxiv.org/abs/2601.20695", "authors": ["D. R. Baykusheva", "D. P. Carmichael", "C. S. Weber", "I-T. Lu", "F. Glerean", "T. Meng", "P. B. M. De Oliveira", "C. C. Homes", "I. A. Zaliznyak", "G. D. Gu", "M. P. M. Dean", "A. Rubio", "D. M. Kennes", "M. Claassen", "M. Mitrano"], "title": "Quantum control of Hubbard excitons", "comment": "main+supplementary, 43 pages, 12 figures", "summary": "Quantum control of the many-body wavefunction is a central challenge in quantum materials research, as it could yield a precise control knob to manipulate emergent phenomena. Floquet engineering, the coherent dressing of quantum states with periodic non-resonant optical fields, has become an important strategy for quantum control. Most applications to solid-state systems have targeted weakly interacting or single-ion states, leaving the manipulation of many-body wavefunctions largely unexplored. Here, we use Floquet engineering to achieve quantum control of a strongly correlated Hubbard exciton in the one-dimensional Mott insulator Sr$_2$CuO$_3$. A nonresonant midinfrared optical field coherently dresses the exciton wavefunction, driving its rotation between bright and dark states. We use resonant third-harmonic generation to quantify ultrafast $π/2$ rotations on the Bloch sphere spanned by these exciton states. Our work advances the quest towards programmable control of correlated states and exciton-based quantum sensing."}
{"id": "2601.20062", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2601.20062", "abs": "https://arxiv.org/abs/2601.20062", "authors": ["M. Chilcott", "N. Kjærgaard"], "title": "Comment on \"Determining angle of arrival of radio-frequency fields using subwavelength, amplitude-only measurements of standing waves in a Rydberg atom sensor\"", "comment": "Comment on arXiv:2502.09835 published in J. Appl. Phys. 138, 114402 (2025). The following article has been submitted to Journal of Applied Physics", "summary": "We discuss the consequence of excluding allowed RF-transition between substates of a field-dressed Rydberg manifold when predicting the spectrum that will be observed if the dressed system is probed in an optical EIT scheme."}
{"id": "2601.20708", "categories": ["hep-lat", "cond-mat.stat-mech", "cs.LG", "hep-ph"], "pdf": "https://arxiv.org/pdf/2601.20708", "abs": "https://arxiv.org/abs/2601.20708", "authors": ["Claudio Bonanno", "Andrea Bulgarelli", "Elia Cellini", "Alessandro Nada", "Dario Panfalone", "Davide Vadacchino", "Lorenzo Verzichelli"], "title": "A scalable flow-based approach to mitigate topological freezing", "comment": "1+9 pages, 3 figures, contribution to the 42nd International Symposium on Lattice Field Theory (Lattice 2025), 2-8 November 2025, Mumbai, India", "summary": "As lattice gauge theories with non-trivial topological features are driven towards the continuum limit, standard Markov Chain Monte Carlo simulations suffer for topological freezing, i.e., a dramatic growth of autocorrelations in topological observables. A widely used strategy is the adoption of Open Boundary Conditions (OBC), which restores ergodic sampling of topology but at the price of breaking translation invariance and introducing unphysical boundary artifacts. In this contribution we summarize a scalable, exact flow-based strategy to remove them by transporting configurations from a prior with a OBC defect to a fully periodic ensemble, and apply it to 4d SU(3) Yang--Mills theory. The method is based on a Stochastic Normalizing Flow (SNF) that alternates non-equilibrium Monte Carlo updates with localized, gauge-equivariant defect coupling layers implemented via masked parametric stout smearing. Training is performed by minimizing the average dissipated work, equivalent to a Kullback--Leibler divergence between forward and reverse non-equilibrium path measures, to achieve more reversible trajectories and improved efficiency. We discuss the scaling with the number of degrees of freedom affected by the defect and show that defect SNFs achieve better performances than purely stochastic non-equilibrium methods at comparable cost. Finally, we validate the approach by reproducing reference results for the topological susceptibility."}
{"id": "2601.20812", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.20812", "abs": "https://arxiv.org/abs/2601.20812", "authors": ["Alfredo Alegría", "John Gómez", "Jorge Mateu", "Ronny Vallejos"], "title": "Effective Sample Size for Functional Spatial Data", "comment": null, "summary": "The effective sample size quantifies the amount of independent information contained in a dataset, accounting for redundancy due to correlation between observations. While widely used in geostatistics for scalar data, its extension to functional spatial data has remained largely unexplored. In this work, we introduce a novel definition of the effective sample size for functional geostatistical data, employing the trace-covariogram as a measure of correlation, and show that it retains the intuitive properties of the classical scalar ESS. We illustrate the behavior of this measure using a functional autoregressive process, demonstrating how serial dependence and the allocation of variability across eigen-directions influence the resulting functional ESS. Finally, the approach is applied to a real meteorological dataset of geometric vertical velocities over a portion of the Earth, showing how the method can quantify redundancy and determine the effective number of independent curves in functional spatial datasets."}
{"id": "2601.20418", "categories": ["math.OC", "math.AG"], "pdf": "https://arxiv.org/pdf/2601.20418", "abs": "https://arxiv.org/abs/2601.20418", "authors": ["Wenqi Zhu", "Coralia Cartis"], "title": "Sufficiently Regularized Nonnegative Quartic Polynomials are Sum-of-Squares", "comment": null, "summary": "Hilbert's 17th problem famously established that not all nonnegative polynomials admit a sum-of-squares (SoS) representation. Hilbert also identified a few special classes in which nonnegativity and SoS are equivalent, such as univariate polynomials, quadratic polynomials, and bivariate quartic polynomials. In this paper, we extend this equivalence to several new subclasses of multivariate quartically regularized polynomials and characterize the NP-hardness boundary of these special-structure polynomials. Specifically, we consider the global optimization of multivariate symmetric cubic polynomials regularized by weighted quartic powers of the Euclidean norm. These special-structure polynomials arise as iterative subproblems in high-order tensor methods for nonconvex optimization problems. We consider shifting these polynomials by their global optimum so as to make them nonnegative, and show that for sufficiently large regularization parameters and under mild assumptions, these polynomials admit a sum-of-squares representation. We also identify several structured subclasses of quartically regularized cubic polynomials for which global optimality of the model implies that nonnegativity is certified by a sum-of-squares decomposition for all values of the regularization parameter, including quadratic-quartic polynomials and quartic polynomials containing a special cubic term that can be decomposed as the product of a quadratic norm and a linear form. We provide counterexamples based on quartic separable norms that demonstrate the crucial role of the Euclidean norm in these representations. Finally, we illustrate how these SoS-based certificates can be used for Taylor subproblems arising in high-order tensor methods for nonconvex optimization, with encouraging numerical results."}
{"id": "2601.20235", "categories": ["math.NA", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.20235", "abs": "https://arxiv.org/abs/2601.20235", "authors": ["Wenbin Wang", "Yunqing Huang", "Huayi Wei"], "title": "A Unified Variational Functional for Equidistribution and Alignment in Moving Mesh Adaptation", "comment": "28 pages,41 figures", "summary": "Existing variational mesh functionals often suffer from strong nonlinearity or dependence on empirical parameters.We propose a new variational functional for adaptive moving mesh generation that enforces equidistribution and alignment through an $\\boldsymbol A$-pullback formulation, where $\\boldsymbol A=\\boldsymbol J^{-1}\\boldsymbol M^{-1}\\boldsymbol J^{-T}$. The functional combines a trace-based term with a logarithmic determinant term, achieving balanced control of mesh size and anisotropy without empirical parameters. We establish coercivity, polyconvexity, existence of minimizers, and geodesic convexity with respect to the inverse Jacobian, and derive a simplified geometric discretization leading to an efficient moving mesh algorithm. Numerical experiments confirm the theoretical properties and demonstrate robust adaptive behavior for function-induced meshes and Rayleigh-Taylor instability simulations."}
{"id": "2601.20610", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.20610", "abs": "https://arxiv.org/abs/2601.20610", "authors": ["Ting Li", "Ethan Fan", "Tengfei Li", "Hongtu Zhu"], "title": "Causal Inference in Biomedical Imaging via Functional Linear Structural Equation Models", "comment": null, "summary": "Understanding the causal effects of organ-specific features from medical imaging on clinical outcomes is essential for biomedical research and patient care. We propose a novel Functional Linear Structural Equation Model (FLSEM) to capture the relationships among clinical outcomes, functional imaging exposures, and scalar covariates like genetics, sex, and age. Traditional methods struggle with the infinite-dimensional nature of exposures and complex covariates. Our FLSEM overcomes these challenges by establishing identifiable conditions using scalar instrumental variables. We develop the Functional Group Support Detection and Root Finding (FGS-DAR) algorithm for efficient variable selection, supported by rigorous theoretical guarantees, including selection consistency and accurate parameter estimation. We further propose a test statistic to test the nullity of the functional coefficient, establishing its null limit distribution. Our approach is validated through extensive simulations and applied to UK Biobank data, demonstrating robust performance in detecting causal relationships from medical imaging."}
{"id": "2601.20561", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20561", "abs": "https://arxiv.org/abs/2601.20561", "authors": ["Jilles S. van Hulst", "Erik M. Franken", "Bart J. Janssen", "W. P. M. H.", "Heemels", "Duarte J. Antunes"], "title": "Tilt-based Aberration Estimation in Transmission Electron Microscopy", "comment": "Submitted version (pre-print)", "summary": "Transmission electron microscopes (TEMs) enable atomic-scale imaging but suffer from aberrations caused by lens imperfections and environmental conditions, reducing image quality. These aberrations can be compensated by adjusting electromagnetic lenses, but this requires accurate estimates of the aberration coefficients, which can drift over time. This paper introduces a method for the estimation of aberrations in TEM by leveraging the relationship between an induced electron beam tilt and the resulting image shift. The method uses a Kalman filter (KF) to estimate the aberration coefficients from a sequence of image shifts, while accounting for the drift of the aberrations over time. The applied tilt sequence is optimized by minimizing the trace of the predicted error covariance in the KF, which corresponds to the A-optimality criterion in experimental design. We show that this optimization can be performed offline, as the cost criterion is independent of the actual measurements. The resulting non-convex optimization problem is solved using a gradient-based, receding-horizon approach with multi-starts. Additionally, we develop an approach to estimate specimen-dependent noise properties using expectation maximization (EM), which are then used to tailor the tilt pattern optimization to the specific specimen being imaged. The proposed method is validated on a real TEM set-up with several optimized tilt patterns. The results show that optimized patterns significantly outperform naive approaches and that the aberration and drift model accurately captures the underlying physical phenomena. In total, the alignment time is reduced from typically several minutes to less than a minute compared to the state-of-the-art."}
{"id": "2601.20702", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.20702", "abs": "https://arxiv.org/abs/2601.20702", "authors": ["Hongyu Lu", "Wei Zhu", "Wang Yao"], "title": "Collective excitations in chiral spin liquid: chiral roton and long-wavelength nematic mode", "comment": "5+5 pages, 5+8 figures", "summary": "Chiral spin liquid (CSL) is a magnetic analogue of the fractional quantum Hall (FQH) liquid. Collective excitations play a vital role in shaping our understanding of these exotic quantum phases of matter and their quantum phase transitions. While the magneto-roton and long-wavelength chiral graviton modes in the FQH liquids have been extensively explored, the collective excitations of CSLs remain elusive. Here we explore the collective excitations in the SU(2) symmetric CSL phase of the spin-1/2 square-lattice $J_1-J_2-J_χ$ model, where an intriguing quantum phase diagram was recently revealed. Combining exact diagonalization and time-dependent variational principle calculations, we observe two spin-singlet collective modes: a chiral p-wave (low-energy) roton mode at finite momentum and a d-wave (higher-energy) nematic mode at zero momentum, both of which are prominent across the CSL phase. Such exotic modes exhibit fingerprints distinct from those of FQH liquids, and to the best of our knowledge, are reported for the first time. By tuning $J_2$, we find the nematic mode to be pronouncedly soft, together with the spin-triplet two-spinon bound states, potentially promoting strong nematic and spin stripe instabilities. Our work paves the way for further understanding CSL from the dynamical perspective and provides new spectroscopic signatures for future experiments of CSL candidates."}
{"id": "2601.20073", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20073", "abs": "https://arxiv.org/abs/2601.20073", "authors": ["Suying Liu", "Yulong Dong", "Dong An", "Murphy Yuezhen Niu"], "title": "Ensemble-Based Quantum Signal Processing for Error Mitigation", "comment": "8+14 pages", "summary": "Despite rapid advances in quantum hardware, noise remains a central obstacle to deploying quantum algorithms on near-term devices. In particular, random coherent errors that accumulate during circuit execution constitute a dominant and fundamentally challenging noise source. We introduce a noise-resilient framework for Quantum Signal Processing (QSP) that mitigates such coherent errors without increasing circuit depth or ancillary qubit requirements. Our approach uses ensembles of noisy QSP circuits combined with measurement-level averaging to suppress random phase errors in Z rotations. Building on this framework, we develop robust QSP algorithms for implementing polynomial functions of Hermitian matrices and for estimating observables, with applications to Hamiltonian simulation, quantum linear systems, and ground-state preparation. We analyze the trade-off between approximation error and hardware noise, which is essential for practical implementation under the stringent depth and coherence constraints of current quantum hardware. Our results establish a practical pathway for integrating error mitigation seamlessly into algorithmic design, advancing the development of robust quantum computing, and enabling the discovery of scientific applications with near- and mid-term quantum devices."}
{"id": "2601.20726", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.20726", "abs": "https://arxiv.org/abs/2601.20726", "authors": ["Yunxue Sun", "Xueming Liu", "Ginestra Bianconi"], "title": "Directionality and node heterogeneity reshape criticality in hypergraph percolation", "comment": "(25 pages, 6 figures, plus SM)", "summary": "Directed and heterogeneous hypergraphs capture directional higher-order interactions with intrinsically asymmetric functional dependencies among nodes. As a result, damage to certain nodes can suppress entire hyperedges, whereas failure of others only weakens interactions. Metabolic reaction networks offer an intuitive example of such asymmetric dependencies. Here we develop a message-passing and statistical mechanics framework for percolation in directed hypergraphs that explicitly incorporates directionality and node heterogeneity. Remarkably, we show that these hypergraph features have a fundamental effect on the critical properties of hypergraph percolation, reshaping criticality in a way that depends on network structure. Specifically, we derive anomalous critical exponents that depend on whether node or hyperedge percolation is considered in maximally correlated, heavy-tailed regimes. These theoretical predictions are validated on synthetic hypergraph models and on a real directed metabolic network, opening new perspectives for the characterization of the robustness and resilience of real-world directed, heterogeneous higher-order networks."}
{"id": "2601.20443", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20443", "abs": "https://arxiv.org/abs/2601.20443", "authors": ["Shota Takahashi"], "title": "Adaptive Conditional Gradient Sliding: Projection-Free and Line-Search-Free Acceleration", "comment": "24 pages, 10 figures", "summary": "We study convex optimization problems over a compact convex set where projections are expensive but a linear minimization oracle (LMO) is available. We propose the adaptive conditional gradient sliding method (AdCGS), a projection-free and line-search-free method that retains Nesterov's acceleration with adaptive stepsizes based on local Lipschitz estimates. AdCGS combines an accelerated outer scheme with an LMO-based inner routine. It reuses gradients across multiple LMO calls to reduce gradient evaluations, while controlling the subproblem inexactness via a prescribed accuracy level coupled with adaptive stepsizes. We prove accelerated convergence rates for convex objective functions matching those of projection-based accelerated methods, while requiring no projection oracle. For strongly convex objective functions, we further establish linear convergence without additional geometric assumptions on the constraint set, such as polytopes or strongly convex sets. Experiments on constrained $\\ell_p$ regression, logistic regression with real-world datasets, and least-squares problems demonstrate improvements over both projection-free and projection-based baselines."}
{"id": "2601.20243", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20243", "abs": "https://arxiv.org/abs/2601.20243", "authors": ["Peng Yang", "Maodong Pan", "Falai Chen", "Zhimin Zhang"], "title": "Element-based B-spline basis function spaces: construction and application in isogeometric analysis", "comment": "28 pages, 8 figures, 4 tables", "summary": "This paper develops a unified theoretical framework for constructing B-spline basis function spaces with structural equivalence to finite element spaces. The theory rigorously establishes that these bases emerge as explicit linear combinations of B-spline element bases. For any prescribed smoothness requirements, this element-wise formulation enables the Hermite interpolation at nodes, which directly utilizes function values and derivatives without solving global linear systems. By focusing on explicit interpolation properties, element-wise analysis establishes optimal approximation errors, even when the space smoothness attains its theoretical maximum for the space degree. In isogeometric analysis (IgA), the construction naturally decomposes geometric mappings into element-level representations, allowing efficient computations across elements regardless of node distribution. Notably, the same Hermite interpolation framework simultaneously handles domain parameterization and IgA solutions, allowing direct imposition of boundary conditions through function and derivative matching. Numerical tests demonstrate optimal convergence rates and superconvergence properties in 2D IgA under uniform knot configurations, and improved computational efficiency in 3D IgA with non-uniform knot distributions."}
{"id": "2601.20710", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20710", "abs": "https://arxiv.org/abs/2601.20710", "authors": ["Linda Sun", "Yixin Ren", "Cong Chen"], "title": "Two-dose vs. Three-Dose Optimization Under Sample Size Constraint", "comment": "14 pages; 4 figures", "summary": "Dose optimization is a hallmark of Project Optimus for oncology drug development. The number of doses to include in a dose optimization study depends on the totality of evidence, which is often unclear in early-phase development. With equal sample sizes per dose, carrying three doses is clearly more advantageous than two for optimization. In this paper, we show that, even when the total sample size is fixed, it is still preferable to carry three unless there is very strong evidence that one can be dropped. A mathematical approximation is applied to guide the investigation, followed by a simulation study to complement the theoretical findings. Semi-quantitative guidance is provided for practitioners, addressing both randomized and non-randomized dose optimization while considering population homogeneity."}
{"id": "2601.20723", "categories": ["eess.SY", "cs.GT", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.20723", "abs": "https://arxiv.org/abs/2601.20723", "authors": ["Emrah Akyol", "Marcos Vasconcelos"], "title": "Distributed Learning over Noisy Communication Networks", "comment": "draft, submitted to IEEE JSAC Special Issue on Distributed Optimization, Learning, and Inference over Communication-Constrained Networks", "summary": "We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality."}
{"id": "2601.20766", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.20766", "abs": "https://arxiv.org/abs/2601.20766", "authors": ["Bo Yuan", "M. Powell", "X. Liu", "J. Ni", "E. M. Smith", "F. Ye", "J. Dudemaine", "A. D. Bianchi", "J. W. Kolis", "B. D. Gaulin"], "title": "Observation of Dipolar Spin-ice--like Correlations in the Quantum Spin Ice Candidate Ce$_2$Sn$_2$O$_7$", "comment": "Supplemental materials including experimental details and additional neutron scattering data are available upon request", "summary": "The Ce$_2$X$_2$O$_7$ (X=Sn, Hf, Zr) family of cubic pyrochlores has emerged as one of the most promising classes of Quantum Spin Ice candidates. However, understanding their microscopic exchange Hamiltonian and spin correlations has been hampered by varying sample quality, and poor signal-to-noise in the existing neutron data due to a small Ce$^{3+}$ magnetic dipole moment. In this work, we overcome these challenges and report single-crystal diffuse neutron scattering from hydrothermally grown Ce$_2$Sn$_2$O$_7$ -- the highest quality crystals obtained to date for the Ce$_2$X$_2$O$_7$ family. In contrast to the broad diffuse scattering observed in Ce$_2$Hf$_2$O$_7$ and Ce$_2$Zr$_2$O$_7$, we find highly structured diffuse scattering from Ce$_2$Sn$_2$O$_7$ featuring strong intensities along the Brillouin zone boundaries. The observed $\\mathbf{Q}$-dependence disagrees with predictions of the nearest neighbour XYZ model commonly used for Ce$_2$X$_2$O$_7$, but is remarkably similar to the diffuse scattering observed in \\textit{classical} Dipolar Spin Ice. Our study highlights the importance of further neighbour interactions in determining the low energy physics of the Ce-pyrochlores, and calls for a revision of the current theoretical framework to incorporate their effects."}
{"id": "2601.20074", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20074", "abs": "https://arxiv.org/abs/2601.20074", "authors": ["Ian George", "Mohammad A. Alhejji"], "title": "Local Distinguishability of Multipartite Orthogonal Quantum States: Generalized and Simplified", "comment": "8 pages, 2 figures, comments welcome", "summary": "In a seminal work [PRL85.4972], Walgate, Short, Hardy, and Vedral prove in finite dimensions that for every pair of pure multipartite orthogonal quantum states, there exists a one-way local operations and classical communication (LOCC) protocol that perfectly distinguishes the pair. We extend this result to infinite dimensions with a simpler proof. For states on $\\mathbb{C}^{d_A \\times d_A} \\otimes \\mathbb{C}^{d_B \\times d_B}$, we strengthen this existence result by constructing an $O(d_A^2 d_B^2)$-time algorithm that specifies such a perfect one-way LOCC protocol. Finally, we establish the equivalence between Walgate et al.'s result and the fact that the one-shot environment-assisted classical capacity of every quantum channel is at least 1 bit per channel use, thereby clarifying the literature on these notions. At the core of all of these results is the fact that every operator with vanishing trace admits a basis where its diagonal entries are all zero."}
{"id": "2601.20594", "categories": ["math.OC", "math.FA"], "pdf": "https://arxiv.org/pdf/2601.20594", "abs": "https://arxiv.org/abs/2601.20594", "authors": ["Florentin Münch", "Christian Seifert", "Peter Stollmann", "Martin Tautenhahn"], "title": "On controllability, observability and stabilizability of the heat equation on discrete graphs", "comment": null, "summary": "We consider linear control problems for the heat equation of the form $\\dot f (t) = -Hf (t) + \\mathbf{1}_D u (t)$, $f (0) \\in \\ell_2 (X,m)$, where $H$ is the weighted Laplacian on a discrete graph $(X,b,m)$, and where $D \\subseteq X$ is relatively dense. We show cost-uniform $α$-controllability by means of a weak observability estimate for the corresponding dual observation problem. We discuss optimality of our result as well as consequences on stabilizability properties."}
{"id": "2601.20290", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20290", "abs": "https://arxiv.org/abs/2601.20290", "authors": ["Mou Cai", "Takashi Goda"], "title": "A note on approximation in weighted Korobov spaces via multiple rank-1 lattices", "comment": "25 pages, no figure", "summary": "This paper studies the multivariate approximation of functions in weighted Korobov spaces using multiple rank-1 lattice rules. It has been shown by Kämmerer and Volkmer (2019) that algorithms based on multiple rank-1 lattices achieve the optimal convergence rate for the $L_{\\infty}$ error in Wiener-type spaces, up to logarithmic factors. While this result was translated to weighted Korobov spaces in the recent monograph by Dick, Kritzer, and Pillichshammer (2022), the analysis requires the smoothness parameter $α$ to be greater than $1$ and is restricted to product weights. In this paper, we extend this result for multiple rank-1 lattice-based algorithms to the case where $1/2<α\\le 1$ and for general weights, covering a broader range of periodic functions with low smoothness and general relative importance of variables. We also provide a summability condition on the weights to ensure strong polynomial tractability for any $α>1/2$. Furthermore, by incorporating random shifts into multiple rank-1 lattice-based algorithms, we prove that the resulting randomized algorithm achieves a nearly optimal convergence rate in terms of the worst-case root mean squared $L_2$ error, while retaining the same tractability property."}
{"id": "2601.20788", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.20788", "abs": "https://arxiv.org/abs/2601.20788", "authors": ["Tatiana Krikella", "Joel A. Dubin"], "title": "A General Mixture Loss Function to Optimize a Personalized PredictiveModel", "comment": null, "summary": "Advances in precision medicine increasingly drive methodological innovation in health research. A key development is the use of personalized prediction models (PPMs), which are fit using a similar subpopulation tailored to a specific index patient, and have been shown to outperform one-size-fits-all models, particularly in terms of model discrimination performance. We propose a generalized loss function that enables tuning of the subpopulation size used to fit a PPM. This loss function allows joint optimization of discrimination and calibration, allowing both the performance measures and their relative weights to be specified by the user. To reduce computational burden, we conducted extensive simulation studies to identify practical bounds for the grid of subpopulation sizes. Based on these results, we recommend using a lower bound of 20\\% and an upper bound of 70\\% of the entire training dataset. We apply the proposed method to both simulated and real-world datasets and demonstrate that previously observed relationships between subpopulation size and model performance are robust. Furthermore, we show that the choice of performance measures in the loss function influences the optimal subpopulation size selected. These findings support the flexible and computationally efficient implementation of PPMs in precision health research."}
{"id": "2601.20836", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2601.20836", "abs": "https://arxiv.org/abs/2601.20836", "authors": ["Erekle Jmukhadze", "Sam Olin", "Allan H. MacDonald", "Wei-Cheng Lee"], "title": "Stripe antiferromagnetism and chiral superconductivity in tWSe$_2$", "comment": "10 pages, 5 figures", "summary": "The layer-dependent Hamiltonians of parallel-stacked MoTe$_2$ and WSe$_2$ homobilayer moiré materials are topologically non-trivial, both in real space and in momentum space, and have been shown to support integer and fractional quantum anomalous Hall states, as well as antiferromagnetic and superconducting states. Here, we address the interplay between the antiferromagnetic and superconducting states observed in tWSe$_2$ when the Fermi level is close to its $M$-point van Hove singularity and the displacement field is small. We combine DFT with path-integrals to construct a minimal moiré band model that accounts for lattice relaxation along the $c$-axis and perform Hartree-Fock calculations to identify competing charge and spin ordered states. For tWSe$_2$ at $θ=2.7^\\circ$ and $θ=3.65^\\circ$, we find that a layer antiferromagnet (AFM), a stripe spin-density-wave (SDW), and the ferromagnetic Chern insulator (FM) are the primary candidates for the ground state at zero displacement field, and argue that antiferromagnetic spin interactions on the next neighbor bond $J_2$ can induce a time-reversal symmetry breaking chiral superconducting state."}
{"id": "2601.20081", "categories": ["quant-ph", "math.SP"], "pdf": "https://arxiv.org/pdf/2601.20081", "abs": "https://arxiv.org/abs/2601.20081", "authors": ["Xinyu Yang", "Long Li", "Qi Zhou"], "title": "Spectral Transitions and Singular Continuous Spectrum in A New Family of Quasi-periodic Quantum Walks", "comment": "29 pages, 1 figure", "summary": "This paper introduces and rigorously analyzes a new class of one-dimensional discrete-time quantum walks whose dynamics are governed by a parametrized family of extended CMV matrices. The model generalizes the unitary almost Mathieu operator (UAMO) and exhibits a richer spectral phase diagram, closely resembling the extended Harper's model. It provides the first example of a solvable quasi-periodic quantum walk that exhibits a stable region of purely singular continuous spectrum."}
{"id": "2601.20693", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20693", "abs": "https://arxiv.org/abs/2601.20693", "authors": ["Amirhossein Abbaszadeh", "Hossein Hashemi Doulabi"], "title": "Drone-Aided Blood Collection Routing Problem: A Column Generation Approach", "comment": null, "summary": "Platelet extraction requires whole blood to be processed within six hours of donation. To meet this deadline, blood collection organizations must optimally route a fleet of vehicles to pick up blood units from donation sites and deliver them to a processing center. This paper introduces a drone-aided blood collection routing problem in which a fleet of trucks, each equipped with a drone, operates in a synchronized manner to collect blood units before their processing time limit expires. Each truck-drone tandem can perform multiple trips throughout the planning horizon, allowing donation sites to be visited repeatedly as new blood units become available over time. We formulate this problem as a mixed-integer linear program that jointly optimizes the routing of trucks and drones, pickup schedules, and timing decisions to maximize the total number of viable blood units collected. We also develop a column generation approach that decomposes the problem into a master problem to select the optimal set of truck-drone tours and a pricing subproblem, which is solved using a tailored memetic algorithm to generate promising new columns. Through a comprehensive computational study, we show the operational benefits of integrating drones into the blood collection system. In addition, we demonstrate the superior performance of the proposed algorithm over Gurobi and two metaheuristics from the literature, namely the hybrid genetic algorithm and the invasive weed optimization, in both the drone-aided and truck-only settings."}
{"id": "2601.20407", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20407", "abs": "https://arxiv.org/abs/2601.20407", "authors": ["Bernard Haasdonk", "Gabriele Santin", "Tizian Wenzel", "Daniel Winkle"], "title": "Refined rates of convergence for target-data dependent greedy generalized interpolation with Sobolev kernels", "comment": null, "summary": "Greedy methods have recently been successfully applied to generalized kernel interpolation, or the recovery of a function from data stemming from the evaluation of linear functionals, including the approximation of solutions of linear PDEs by symmetric collocation. When applied to kernels generating Sobolev spaces as their native Hilbert spaces, some of these greedy methods can provide the same error guarantee of generalized interpolation on quasi-uniform points. More importantly, certain target-data-adaptive methods even give a dimension- and smoothness-independent improvement in the speed of convergence over quasi-uniform points, thus offering advantages for high-dimensional problems. These convergence rates however contain a spurious logarithmic term that limits this beneficial effect. The goal of this note is to remove this factor, and this is possible by using estimates on metric entropy numbers."}
{"id": "2601.20805", "categories": ["stat.ME", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.20805", "abs": "https://arxiv.org/abs/2601.20805", "authors": ["Lukas Koch"], "title": "Plotting correlated data", "comment": "9 pages, 9 figures", "summary": "A very common task in data visualization is to plot many data points with some measured y-value as a function of fixed x-values. Uncertainties on the y-values are typically presented as vertical error bars that represent either a Frequentist confidence interval or Bayesian credible interval for each data point. Most of the time, these error bars represent a 68\\% confidence/credibility level, which leads to the intuition that a model fits the data reasonably well if its prediction lies within the error bars of roughly two thirds of the data points. Unfortunately, this and other intuitions no longer work when the uncertainties of the data points are correlated. If the error bars only show the square root of diagonal elements of some covariance matrix with non-negligible off-diagonal elements, we simply do not have enough information in the plot to judge whether a drawn model line agrees well with the data or not. In this paper we will demonstrate this problem and discuss ways to add more information to the plots to make it easier to judge the agreement between the data and some model prediction in the plot, as well as glean some insight where the model might be deficient. This is done by explicitly showing the contribution of the first principal component of the uncertainties, and by displaying the conditional uncertainties of all data points."}
{"id": "2601.20782", "categories": ["quant-ph", "cond-mat.str-el", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20782", "abs": "https://arxiv.org/abs/2601.20782", "authors": ["Massimo Solinas", "Agnes Valenti", "Nawaf Bou-Rabee", "Roeland Wiersema"], "title": "Neural Quantum States in Mixed Precision", "comment": "22 pages, 12 figures", "summary": "Scientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency. In this work, we investigate the role of mixed-precision arithmetic in neural-network based Variational Monte Carlo (VMC), a widely used method for solving computationally otherwise intractable quantum many-body systems. We first derive general analytical bounds on the error introduced by reduced precision on Metropolis-Hastings MCMC, and then empirically validate these bounds on the use-case of VMC. We demonstrate that significant portions of the algorithm, in particular, sampling the quantum state, can be executed in half precision without loss of accuracy. More broadly, this work provides a theoretical framework to assess the applicability of mixed-precision arithmetic in machine-learning approaches that rely on MCMC sampling. In the context of VMC, we additionally demonstrate the practical effectiveness of mixed-precision strategies, enabling more scalable and energy-efficient simulations of quantum many-body systems."}
{"id": "2601.20091", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.20091", "abs": "https://arxiv.org/abs/2601.20091", "authors": ["Maciej W. Olszewski", "Lingda Kong", "Simon Reinhardt", "Daniel Tong", "Xinyi Du", "Gabriele Di Gianluca", "Haoran Lu", "Saswata Roy", "Luojia Zhang", "Aleksandra B. Biedron", "David A. Muller", "Valla Fatemi"], "title": "Krypton-sputtered tantalum films for scalable high-performance quantum devices", "comment": null, "summary": "Superconducting qubits based on tantalum (Ta) thin films have demonstrated the highest-performing microwave resonators and qubits. This makes Ta an attractive material for superconducting quantum computing applications, but, so far, direct deposition has largely relied on high substrate temperatures exceeding \\SI{400}{\\celsius} to achieve the body-centered cubic phase, BCC (\\textalpha-Ta). This leads to compatibility issues for scalable fabrication leveraging standard semiconductor fabrication lines. Here, we show that changing the sputter gas from argon (Ar) to krypton (Kr) promotes BCC Ta synthesis on silicon (Si) at temperatures as low as \\SI{200}{\\celsius}, providing a wide process window compatible with back-end-of-the-line fabrication standards. Furthermore, we find these films to have substantially higher electronic conductivity, consistent with clean-limit superconductivity. We validated the microwave performance through coplanar waveguide resonator measurements, finding that films deposited at \\SI{250}{\\celsius} and \\SI{350}{\\celsius} exhibit a tight performance distribution at the state of the art. Higher temperature-grown films exhibit higher losses, in correlation with the degree of Ta/Si intermixing revealed by cross-sectional transmission electron microscopy. Finally, with these films, we demonstrate transmon qubits with a relatively compact, \\SI{20}{\\micro\\meter} capacitor gap, achieving a median quality factor up to 14 million."}
{"id": "2601.20697", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20697", "abs": "https://arxiv.org/abs/2601.20697", "authors": ["Yifan Bai", "Clarice Poon", "Jingwei Liang"], "title": "Adaptive Dimension Reduction for Overlapping Group Sparsity", "comment": null, "summary": "Typical dimension reduction techniques for nonoverlapping sparse optimization involve screening or sieving strategies based on a dual certificate derived from the first-order optimality condition, approximating the gradients or exploiting certain inherent low-dimensional structure of the sparse solution. In comparison, dimension reduction rules for overlapping group sparsity are generally less developed because the subgradient structure is more complex, making the link between sparsity pattern and the dual variable indirect due to the non-separability. In this work, we propose new dual certificates for overlapping group sparsity and a novel adaptive scheme for identifying the support of the overlapping group LASSO. We demonstrate how this scheme can be integrated into and significantly accelerate existing algorithms, including Primal-Dual splitting method, alternating direction method of multipliers and a recently developed variable projection scheme based on over-parameterization. We provide convergence analysis of the method and verify its practical effectiveness through experiments on standard datasets."}
{"id": "2601.20456", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20456", "abs": "https://arxiv.org/abs/2601.20456", "authors": ["Ritu Kumari", "Cyrille Kenne", "Landry Djomegne", "Mani Mehra"], "title": "Fokker--Planck Dynamics on Star Graphs with Variable Drift: Well-Posedness, Adjoint Analysis, and Numerical Approximation", "comment": null, "summary": "Stochastic transport processes on networked domains (modelled on metric graphs) arise in a variety of applications where diffusion and drift mechanisms interact with an underlying graph structure. The Fokker--Planck equation provides a natural framework for describing the evolution of probability densities associated with such dynamics. While Fokker--Planck equations on metric graphs have been studied from an analytical viewpoint, their optimal control remains largely unexplored, particularly in settings where the control acts through the drift term. In this paper, we investigate an optimal control problem governed by the Fokker--Planck equation on a star graph, with a bilinear control appearing in the drift. We establish the well-posedness of the state equation and prove the existence of at least one optimal control. The associated adjoint system is derived, and first-order necessary optimality conditions are formulated. A wavelet-based numerical scheme is proposed to approximate the optimal solution, and its performance is illustrated through representative numerical experiments. These results contribute to the analytical and computational understanding of controlled stochastic dynamics on network-like domains."}
{"id": "2601.20809", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20809", "abs": "https://arxiv.org/abs/2601.20809", "authors": ["Tatiana Krikella", "Jane M. Heffernan", "Hanna Jankowski"], "title": "Joint estimation of the basic reproduction number and serial interval using Sequential Bayes", "comment": null, "summary": "Early in an infectious disease outbreak, timely and accurate estimation of the basic reproduction number ($R_0$) and the serial interval (SI) is critical for understanding transmission dynamics and informing public health responses. While many methods estimate these quantities separately, and a small number jointly estimate them from incidence data, existing joint approaches are largely likelihood-based and do not fully exploit prior information. We propose a novel Bayesian framework for the joint estimation of $R_0$ and the serial interval using only case count data, implemented through a sequential Bayes approach. Our method assumes an SIR model and employs a mildly informative joint prior constructed by linking log-Gamma marginal distributions for $R_0$ and the SI via a Gaussian copula, explicitly accounting for their dependence. The prior is updated sequentially as new incidence data become available, allowing for real-time inference. We assess the performance of the proposed estimator through extensive simulation studies under correct model specification as well as under model misspecification, including when the true data come from an SEIR or SEAIR model, and under varying degrees of prior misspecification. Comparisons with the widely used White and Pagano likelihood-based joint estimator show that our approach yields substantially more precise and stable estimates of $R_0$, with comparable or improved bias, particularly in the early stages of an outbreak. Estimation of the SI is more sensitive to prior misspecification; however, when prior information is reasonably accurate, our method provides reliable SI estimates and remains more stable than the competing approach. We illustrate the practical utility of the proposed method using Canadian COVID-19 incidence data at both national and provincial levels."}
{"id": "2601.20114", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20114", "abs": "https://arxiv.org/abs/2601.20114", "authors": ["J. N. Bai", "F. Yang", "D. Yan", "Weibin Li", "X. Q. Shao"], "title": "Engineering the non-Hermitian SSH model with skin effects in Rydberg atom arrays", "comment": "13 pages, 9 figures", "summary": "We propose and systematically analyze a practical scheme for implementing a one-dimensional non-Hermitian Su-Schrieffer-Heeger model using individually addressable Rydberg atom arrays. Our setup consists of an atomic chain with three-atom unit cells, in which a synthetic gauge field is generated by applying multi-color laser fields. By engineering fast dissipative channels for one auxiliary atom in each unit cell, the adiabatic elimination effectively gives rise to a non-Hermitian skin effect. We examine how fluctuations in the experimental parameters influence both the skin effect and the topological invariant under open and periodic boundary conditions in real space and find that both features remain highly robust. This work establishes a versatile, controllable, and programmable open-system quantum simulator with neutral atoms, providing a clear route for exploring rich non-Hermitian topological phenomena."}
{"id": "2601.20811", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.20811", "abs": "https://arxiv.org/abs/2601.20811", "authors": ["Charles Audet", "Andrea Brilli", "Youssef Diouane", "Sébastien Le Digabel", "Everton J. Silva", "Christophe Tribes"], "title": "A penalty-interior point method combined with MADS for equality and inequality constrained optimization", "comment": null, "summary": "This work introduces MADS-PIP, an efficient framework that integrates a penalty-interior point strategy into the mesh adaptive direct search (MADS) algorithm for solving nonsmooth blackbox optimization problems with general inequality and equality constraints. Inequality constraints are partitioned into two subsets: one treated via a logarithmic barrier applied to an aggregated interior constraint violation, and the other handled through an exterior quadratic penalty. All equality constraints are treated by the exterior penalty. A merit function defines a sequence of unconstrained subproblems, which are solved approximately using MADS, while a carefully designed update rule drives the penalty-barrier parameter to zero. In the nonsmooth setting, we establish convergence results ensuring feasibility for general constraints as well as Clarke stationarity for inequality-constrained problems. Computational experiments on both analytical test sets and challenging blackbox problems demonstrate that the proposed MADS-PIP algorithm is competitive with, and often outperforms, MADS with the progressive barrier strategy, particularly in the presence of equality constraints."}
{"id": "2601.20494", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20494", "abs": "https://arxiv.org/abs/2601.20494", "authors": ["Anika Beckers", "Jan Friedrich"], "title": "Monotone-based Numerical Schemes for Two-Dimensional Systems of Nonlocal Conservation Laws", "comment": null, "summary": "We present a class of numerical schemes for two-dimensional systems of nonlocal conservation laws, which are based on utilizing well-known monotone numerical flux functions after suitably approximating the nonlocal terms. The considered systems are weakly coupled by the nonlocal terms and the underlying flux function is rather general to guarantee that our results are applicable to a wide range of common nonlocal models. We state sufficient conditions to ensure the convergence of the monotone-based numerical schemes to the unique weak entropy solution. Moreover, we provide an error estimate that yields the convergence rate of $\\mathcal{O}(\\sqrt{Δt})$ for the numerical approximations of the solution. Our results include an existence and uniqueness proof of the nonlocal system, too. Numerical results illustrate our theoretical findings."}
{"id": "2601.20812", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.20812", "abs": "https://arxiv.org/abs/2601.20812", "authors": ["Alfredo Alegría", "John Gómez", "Jorge Mateu", "Ronny Vallejos"], "title": "Effective Sample Size for Functional Spatial Data", "comment": null, "summary": "The effective sample size quantifies the amount of independent information contained in a dataset, accounting for redundancy due to correlation between observations. While widely used in geostatistics for scalar data, its extension to functional spatial data has remained largely unexplored. In this work, we introduce a novel definition of the effective sample size for functional geostatistical data, employing the trace-covariogram as a measure of correlation, and show that it retains the intuitive properties of the classical scalar ESS. We illustrate the behavior of this measure using a functional autoregressive process, demonstrating how serial dependence and the allocation of variability across eigen-directions influence the resulting functional ESS. Finally, the approach is applied to a real meteorological dataset of geometric vertical velocities over a portion of the Earth, showing how the method can quantify redundancy and determine the effective number of independent curves in functional spatial datasets."}
{"id": "2601.20155", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20155", "abs": "https://arxiv.org/abs/2601.20155", "authors": ["Philippe Faist"], "title": "Universal thermodynamic implementation of a process with a variable work cost", "comment": "11+13 pages, 5 figures", "summary": "The minimum amount of thermodynamic work required in order to implement a quantum computation or a quantum state transformation can be quantified using frameworks based on the resource theory of thermodynamics, deeply rooted in the works of Landauer and Bennett. For instance, the work we need to invest in order to implement $n$ independent and identically distributed (i.i.d.) copies of a quantum channel is quantified by the thermodynamic capacity of the channel when we require the implementation's accuracy to be guaranteed in diamond norm over the $n$-system input. Recent work showed that work extraction can be implemented universally, meaning the same implementation works for a large class of input states, while achieving a variable work cost that is optimal for each individual i.i.d. input state. Here, we revisit some techniques leading to derivation of the thermodynamic capacity, and leverage them to construct a thermodynamic implementation of $n$ i.i.d. copies of any time-covariant quantum channel, up to some process decoherence that is necessary because the implementation reveals the amount of consumed work. The protocol uses so-called thermal operations and achieves the optimal per-input work cost for any i.i.d. input state; it relies on the conditional erasure protocol in our earlier work, adjusted to yield variable work. We discuss the effect of the work-cost decoherence. While it can significantly corrupt the correlations between the output state and any reference system, we show that for any time-covariant i.i.d. input state, the state on the output system faithfully reproduces that of the desired process to be implemented. As an immediate consequence of our results, we recover recent results for optimal work extraction from i.i.d. states up to the error scaling and implementation specifics, and propose an optimal preparation protocol for time-covariant i.i.d. states."}
{"id": "2601.20145", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.20145", "abs": "https://arxiv.org/abs/2601.20145", "authors": ["Xingyuan Lin", "Xiuxiu Lin", "Xuesong Chen"], "title": "Error estimates of $hp$-finite element method for elliptic optimal control problems with robin boundary", "comment": null, "summary": "A priori and a posteriori error analysis of $hp$ finite element method for elliptic control problem with Robin boundary condition and boundary observation are presented. are presented. Through the Clément-type approach and the construction of an auxiliary system, we derived a priori error estimates for the elliptic optimal control problem. Residual-based a posteriori error estimates are derived based on the well-known Scott-Zhang-type quasi-interpolation and coupled state-control approximations, thus establishing an a posteriori error estimator for the $hp$ finite element method. The numerical example demonstrates the accuracy of error estimation for the elliptic optimal control problems with Robin boundary."}
{"id": "2601.20566", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20566", "abs": "https://arxiv.org/abs/2601.20566", "authors": ["Chang Hou", "Hu Chen"], "title": "Local convergence analysis of a linearized Alikhanov scheme for the time fractional sine-Gordon equation", "comment": null, "summary": "This paper investigates the time fractional sine-Gordon equation whose solution exhibits a weak singularity of type t^α. By means of the Alikhanov formula we derive a fully discrete, linearized scheme. Using the more general regularity assumption, we derive a sharp truncation-error bound for the fractional derivative. Furthermore, we prove a key inequality and a less restrictive stability result that is valid on general graded temporal meshes. Consequently, the temporal local convergence order is shown to be min{2, r} in H^1-seminorm, where r is the degree of grading; numerical experiments confirm that the optimal rate is already attained as soon as r = 2."}
{"id": "2601.20442", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.20442", "abs": "https://arxiv.org/abs/2601.20442", "authors": ["José E. Chacón", "Eduardo García-Portugués", "Andrea Meilán-Vila"], "title": "Blessing of dimensionality in cross-validated bandwidth selection on the sphere", "comment": "25 pages, 6 figures, 2 tables. Supplementary material: 43 pages, 4 figures", "summary": "We study the asymptotic behavior of least-squares cross-validation bandwidth selection in kernel density estimation on the $d$-dimensional hypersphere, $d\\geq 1$. We show that the exact rate of convergence with respect to the optimal bandwidth minimizing the mean integrated squared error, shown to exist under mild non-uniformity conditions, is $n^{-d/(2d+8)}$, thus approaching the $n^{-1/2}$ parametric rate as $d$ grows. This ``blessing of dimensionality'' in bandwidth selection offers theoretical support for utilizing the conceptually simpler cross-validation selector over plug-in techniques for larger dimensions $d$. We compare this result for bandwidth estimation on the $d$-dimensional Euclidean space through explicit expressions for the asymptotic variance functionals. Numerical experiments corroborate the speed of this convergence in an array of scenarios and dimensions, precisely illustrating the tipping dimension where cross-validation outperforms plug-in approaches."}
{"id": "2601.20167", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20167", "abs": "https://arxiv.org/abs/2601.20167", "authors": ["Song-Ju Kim"], "title": "Contextuality as an Information-Theoretic Obstruction to Classical Probability", "comment": "6 pages, 0 figure", "summary": "Contextuality is a central feature distinguishing quantum from classical probability theories, yet its operational meaning remains subject to interpretation. We reconsider contextuality from an information-theoretic perspective, focusing on operational models constrained to maintain a single internal state with fixed semantics across multiple contexts. Under this constraint, we show that contextual statistics certify an unavoidable obstruction to classical probabilistic descriptions. Specifically, any classical model that reproduces such statistics must either embed contextual dependence into the internal state or introduce additional external labels carrying nonzero information. This result identifies contextuality as a witness of irreducible information cost in classical representations, rather than as a purely nonclassical anomaly. From this viewpoint, quantum probability emerges as a canonical framework that accommodates contextual operations without requiring explicit contextual encoding."}
{"id": "2601.20583", "categories": ["math.NA", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20583", "abs": "https://arxiv.org/abs/2601.20583", "authors": ["Liliana Borcea", "Josselin Garnier", "Alexander V. Mamonov", "Jörn Zimmerling"], "title": "Quantitative synthetic aperture radar inversion", "comment": "Submitted to IEEE Transactions on Antennas and Propagation, keywords: SAR, imaging, multiple scattering, inversion", "summary": "We study an inverse scattering problem for monostatic synthetic aperture radar (SAR): Estimate the wave speed in a heterogeneous, isotropic and nonmagnetic medium probed by waves emitted and measured by a moving antenna. The forward map, from the wave speed to the measurements, is derived from Maxwell's equations. It is a nonlinear map that accounts for multiple scattering and it is very oscillatory at high frequencies. This makes the standard, nonlinear least squares data fitting formulation of the inverse problem difficult to solve. We introduce an alternative, two-step approach: The first step computes the nonlinear map from the measurements to an approximation of the electric field inside the unknown medium aka, the internal wave. This is done for each antenna location in a non-iterative manner.\n  The internal wave fits the data by construction, but it does not solve Maxwell's equations. The second step uses optimization to minimize the discrepancy between the internal wave and the solution of Maxwell's equations, for all antenna locations. The optimization is iterative. The first step defines an imaging function whose computational cost is comparable to that of standard SAR imaging, but it gives a better estimate of the support of targets. Further iterations improve the quantitative estimation of the wave speed. We assess the performance of the method with numerical simulations and compare the results with those of standard inversion."}
{"id": "2601.20186", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20186", "abs": "https://arxiv.org/abs/2601.20186", "authors": ["Song-hai Li", "Najmeh Es'haqi-Sani", "Xingli Li", "Wenlin Li"], "title": "A general interpretation of nonlinear connected time crystals: quantum self-sustaining combined with quantum synchronization", "comment": null, "summary": "Although classical nonlinear dynamics suggests that sufficiently strong nonlinearity can sustain oscillations, quantization of such model typically yields a time-independent steady state that respects time-translation symmetry and thus precludes time-crystal behavior. We identify dephasing as the primary mechanism enforcing this symmetry, which can be suppressed by intercomponent phase correlations. Consequently, a sufficient condition for realizing a continuous time crystal is a nonlinear quantum self-sustaining system exhibiting quantum synchronization among its constituents. As a concrete example, we demonstrate spontaneous oscillations in a synchronized array of van der Pol oscillators, corroborated by both semiclassical dynamics and the quantum Liouville spectrum. These results reduce the identification of time crystals in many-body systems to the evaluation of only two-body correlations and provide a framework for classifying uncorrelated time crystals as trivial."}
{"id": "2601.20677", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20677", "abs": "https://arxiv.org/abs/2601.20677", "authors": ["Philipp Bringmann", "Christoph Lietz", "Dirk Praetorius"], "title": "Unconditional full linear convergence and quasi-optimal complexity of smoothed adaptive finite element methods", "comment": null, "summary": "We present the first rigorous convergence analysis of the smoothed adaptive finite element method (S-AFEM) proposed in [Mulita, Giani, Heltai: SIAM J. Sci. Comput. 43, 2021]. S-AFEM modifies the classical adaptive finite element method (AFEM) by performing accurate discrete solves only on periodically determined mesh levels, while the intermediate levels employ a fixed number of cheap smoothing iterations. Numerical experiments in that work showed that this strategy generates adapted meshes comparable to those of AFEM at substantially lower computational cost. In this paper, we prove unconditional full R-linear convergence of a suitable quasi-error quantity and, for sufficiently small adaptivity parameters, optimal convergence rates with respect to the overall computational cost. The analysis requires only a mild uniform stability assumption on the employed smoother, satisfied by standard methods such as Richardson, Gauss-Seidel, conjugate gradient, and multigrid schemes. Our results apply to general second-order linear elliptic PDEs and show that S-AFEM retains all desired abstract convergence guarantees of AFEM while reducing the cumulative computational time. Numerical experiments validate the theory, analyze runtime performance, and underline the potential of S-AFEM for speed-up in AFEM computations."}
{"id": "2601.20237", "categories": ["quant-ph", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.20237", "abs": "https://arxiv.org/abs/2601.20237", "authors": ["Gabor Lippner", "Yujia Shi"], "title": "Fast state transfer via loop weights", "comment": null, "summary": "We prove that almost-linear-time high-fidelity state transfer is achievable in a quantum spin chain using loop weights at the second and second-to-last nodes. We provide specific parameter values, and using a careful analysis of the eigenvectors we make precise quantitative estimates of the transfer time and strength."}
{"id": "2601.20750", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20750", "abs": "https://arxiv.org/abs/2601.20750", "authors": ["Vit Dolejsi", "Jakub Sistek"], "title": "Adaptive domain decomposition method for time-dependent problems with applications in fluid dynamics", "comment": null, "summary": "We deal with the numerical solution of the time-dependent partial differential equations using the adaptive space-time discontinuous Galerkin (DG) method. The discretization leads to a nonlinear algebraic system at each time level, the size of the system is varying due to mesh adaptation. A Newton-like iterative solver leads to a sequence of linear algebraic systems which are solved by GMRES solver with a domain decomposition preconditioner. Particularly, we consider additive and hybrid two-level Schwarz preconditioners which are efficient and easy to implement for DG discretization. We study the convergence of the linear solver in dependence on the number of subdomains and the number of element of the coarse grid. We propose a simplified cost model measuring the computational costs in terms of floating-point operations, the speed of computation, and the wall-clock time for communications among computer cores. Moreover, the cost model serves as a base of the presented adaptive domain decomposition method which chooses the number of subdomains and the number of element of the coarse grid in order to minimize the computational costs. The efficiency of the proposed technique is demonstrated by two benchmark problems of compressible flow simulations."}
{"id": "2601.20247", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20247", "abs": "https://arxiv.org/abs/2601.20247", "authors": ["Jens Palsberg", "Jason Cong", "Yufei Ding", "Bill Fefferman", "Moinuddin Qureshi", "Gokul Subramanian Ravi", "Kaitlin N. Smith", "Hanrui Wang", "Xiaodi Wu", "Henry Yuen"], "title": "Computer Science Challenges in Quantum Computing: Early Fault-Tolerance and Beyond", "comment": "43 pages", "summary": "Quantum computing is entering a period in which progress will be shaped as much by advances in computer science as by improvements in hardware. The central thesis of this report is that early fault-tolerant quantum computing shifts many of the primary bottlenecks from device physics alone to computer-science-driven system design, integration, and evaluation. While large-scale, fully fault-tolerant quantum computers remain a long-term objective, near- and medium-term systems will support early fault-tolerant computation with small numbers of logical qubits and tight constraints on error rates, connectivity, latency, and classical control. How effectively such systems can be used will depend on advances across algorithms, error correction, software, and architecture. This report identifies key research challenges for computer scientists and organizes them around these four areas, each centered on a fundamental question."}
{"id": "2601.20781", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20781", "abs": "https://arxiv.org/abs/2601.20781", "authors": ["Jessie Chen", "Hangjie Ji", "Arvind K. Saibaba"], "title": "Optimal Sensor Placement in Gaussian Processes via Column Subset Selection", "comment": null, "summary": "Gaussian process regression uses data measured at sensor locations to reconstruct a spatially dependent function with quantified uncertainty. However, if only a limited number of sensors can be deployed, it is important to determine how to optimally place the sensors to minimize a measure of the uncertainty in the reconstruction. We consider the Bayesian D-optimal criterion to determine the optimal sensor locations by choosing sensors from a candidate set of sensors. Since this is an NP-hard problem, our approach models sensor placement as a column subset selection problem (CSSP) on the covariance matrix, computed using the kernel function on the candidate sensor points. We propose an algorithm that uses the Golub-Klema-Stewart framework (GKS) to select sensors and provide an analysis of lower bounds on the D-optimality of these sensor placements. To reduce the computational cost in the GKS step, we propose and analyze algorithms for the D-optimal sensor placements using Nyström approximations on the covariance matrix. Moreover, we propose several algorithms that select sensors via Nyström approximation of the covariance matrix, utilizing the randomized Nyström approximation, random pivoted Cholesky and greedy pivoted Cholesky. We demonstrate the performance of our method on two applications: thin liquid film dynamics and sea surface temperature."}
{"id": "2601.20263", "categories": ["quant-ph", "cs.DM"], "pdf": "https://arxiv.org/pdf/2601.20263", "abs": "https://arxiv.org/abs/2601.20263", "authors": ["Jesua Epequin", "Pascale Bendotti", "Joseph Mikael"], "title": "A Quantum Photonic Approach to Graph Coloring", "comment": "15 pages, 2 figures", "summary": "Gaussian Boson Sampling (GBS) is a quantum computational model that leverages linear optics to solve sampling problems believed to be classically intractable. Recent experimental breakthroughs have demonstrated quantum advantage using GBS, motivating its application to real-world combinatorial optimization problems.\n  In this work, we reformulate the graph coloring problem as an integer programming problem using the independent set formulation. This enables the use of GBS to identify cliques in the complement graph, which correspond to independent sets in the original graph. Our method is benchmarked against classical heuristics and exact algorithms on two sets of instances: Erdős-Rényi random graphs and graphs derived from a smart-charging use case. The results demonstrate that GBS can provide competitive solutions, highlighting its potential as a quantum-enhanced heuristic for graph-based optimization."}
{"id": "2601.20799", "categories": ["math.NA", "math-ph", "math.DG", "math.SG"], "pdf": "https://arxiv.org/pdf/2601.20799", "abs": "https://arxiv.org/abs/2601.20799", "authors": ["Adérito Araújo", "Gonçalo Inocêncio Oliveira", "João Nuno Mestre"], "title": "Jacobi Hamiltonian Integrators: construction and applications", "comment": "33 pages, 18 figures", "summary": "We propose a systematic framework for constructing geometric integrators for Hamiltonian systems on Jacobi manifolds. By combining Poissonization of Jacobi structures with homogeneous symplectic bi-realizations, Jacobi dynamics are lifted to homogeneous Poisson Hamiltonian systems, enabling the construction of structure-preserving Jacobi Hamiltonian integrators. The resulting schemes are constructed explicitly and applied to a range of examples, including contact Hamiltonian systems and classical models. Numerical experiments highlight their qualitative advantages over standard integrators, including better preservation of geometric structure and improved long-time behavior."}
{"id": "2601.20287", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2601.20287", "abs": "https://arxiv.org/abs/2601.20287", "authors": ["Francesco Caravelli"], "title": "Fingerprints of classical memory in quantum hysteresis", "comment": "22 pages double column; 26 pages appendix", "summary": "We present a simple framework for classical and quantum ``memory'' in which the Hamiltonian at time $t$ depends on past values of a control Hamiltonian through a causal kernel. This structure naturally describes finite-bandwidth or filtered control channels and provides a clean way to distinguish between memory in the control and genuine non-Markovian dynamics of the state. We focus on models where $H(t)=H_0+\\int_{-\\infty}^{t}K(t-s)\\,H_1(s)\\,ds$, and illustrate the framework on single-qubit examples such as $H(t)=σ_z+Φ(t)σ_x$ with $Φ(t)=\\int_{-\\infty}^{t}K(t-s)\\,u(s)\\,ds$. We derive basic properties of such dynamics, discuss conditions for unitarity, give an equivalent time-local description for exponential kernels, and show explicitly how hysteresis arises in the response of a driven qubit."}
{"id": "2601.20807", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2601.20807", "abs": "https://arxiv.org/abs/2601.20807", "authors": ["Felipe Leppe", "Gonzalo Rivera"], "title": "A locking-free mixed virtual element discretization for the elasticity eigenvalue problem", "comment": null, "summary": "In this paper, we introduce a mixed virtual element method to approximate the eigenvalues and eigenfunctions of the two-dimensional elasticity eigenvalue problem. Under standard assumptions on the meshes, we prove the convergence of the discrete solution operator to the continuous one as the mesh size tends to zero. Using the theory of compact operators, we analyze the convergence of the method and derive error estimates for both the eigenvalues and eigenfunctions. We validate our theoretical results with a series of numerical tests, in which we compute convergence orders and show that the method is locking-free and capable of accurately approximating the spectrum independently of the shape of the polygons on the meshes."}
{"id": "2601.20296", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20296", "abs": "https://arxiv.org/abs/2601.20296", "authors": ["Sheng-Xian Xiao", "Tao Wang"], "title": "Electromagnetically Induced Transparency Spectra of Ladder Four-Level System with Quantum Frequency Mixing", "comment": null, "summary": "In this paper, we generalized the quantum frequency mixing technology to a ladder-type four-level system and studied its effect on electromagnetically induced transparency spectra. We found a secondary splitting of Autler-Townes splitting in the probing field transmission spectra, which could be understood by the effective Hamiltonian derived with multi-mode Floquet theory. The Frequency mixing scheme developed here enables continuous tunablity of the resonant frequency between upper levels, which facilitates the broad band sensing of AC field. Furthermore, by introducing an additional periodic driving, we realize an effective model that two distinct quantum interference effects coexist: interference among Floquet channels and loop interference arising from closed coherent pathways. Both interference effects could be read out from the transmission spectra independently. The changing of the distance between double splitting peaks represents the interference of Floquet channels, while their asymmetric linewidth broadening is linked with the total effective phase of the loop. This not only provides complementary readout for extracting the phase of AC field, but also establishes a new paradigm for coherent control in multi-level quantum systems."}
{"id": "2601.20841", "categories": ["math.NA", "math-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.20841", "abs": "https://arxiv.org/abs/2601.20841", "authors": ["Sarah Dennis", "Thomas G. Fai"], "title": "Fast Solvers for the Reynolds Equation on Piecewise Linear Geometries", "comment": "16 pages, 6 figures", "summary": "The Reynolds equation is derived from the incompressible Navier Stokes equations under the lubrication assumptions of a long and thin domain geometry and a small scaled Reynolds number. The Reynolds equation is an elliptic differential equation and a dramatic simplification from the governing equations. When the fluid domain is piecewise linear, the Reynolds equation has an exact solution that we formulate by coupling the exact solutions of each piecewise component. We consider a formulation specifically for piecewise constant heights, and a more general formulation for piecewise linear heights; in both cases the linear system is inverted using the Schur complement. These methods can also be applied in the case of non-linear heights by approximating the height as piecewise constant or piecewise linear, in which case the methods achieve second order accuracy. We assess the time complexity of the two methods, and determine that the method for piecewise linear heights is linear time for the number of piecewise components. As an application of these methods, we explore the limits of validity for lubrication theory by comparing the solutions of the Reynolds and the Stokes equations for a variety of linear and non-linear textured slider geometries."}
{"id": "2601.20393", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20393", "abs": "https://arxiv.org/abs/2601.20393", "authors": ["Ziheng Chen", "Junhong Nie", "Xiaoming Sun", "Jialin Zhang", "Jiadong Zhu"], "title": "Scalable Multi-QPU Circuit Design for Dicke State Preparation: Optimizing Communication Complexity and Local Circuit Costs", "comment": null, "summary": "Preparing large-qubit Dicke states is of broad interest in quantum computing and quantum metrology. However, the number of qubits available on a single quantum processing unit (QPU) is limited -- motivating the distributed preparation of such states across multiple QPUs as a practical approach to scalability. In this article, we investigate the distributed preparation of $n$-qubit $k$-excitation Dicke states $D(n,k)$ across a general number $p$ of QPUs, presenting a distributed quantum circuit (each QPU hosting approximately $\\lceil n/p \\rceil$ qubits) that prepares the state with communication complexity $O(p \\log k)$, circuit size $O(nk)$, and circuit depth $O\\left(p^2 k + \\log k \\log (n/k)\\right)$. To the best of our knowledge, this is the first construction to simultaneously achieve logarithmic communication complexity and polynomial circuit size and depth. We also establish a lower bound on the communication complexity of $p$-QPU distributed state preparation for a general target state. This lower bound is formulated in terms of the canonical polyadic rank (CP-rank) of a tensor associated with the target state. For the special case $p = 2$, we explicitly compute the CP-rank corresponding to the Dicke state $D(n,k)$ and derive a lower bound of $\\lceil\\log (k + 1)\\rceil$, which shows that the communication complexity of our construction matches this fundamental limit."}
{"id": "2601.20403", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20403", "abs": "https://arxiv.org/abs/2601.20403", "authors": ["Hao-Miao Jiang", "Xiang-Jiang Chen", "Liu-Jun Wang", "Qing Chen"], "title": "Network Nonlocality Sharing in Generalized Star Network from Bipartite Bell Inequalities", "comment": "12 pages, 2 figures", "summary": "This work investigates network nonlocality sharing for a broad class of bipartite Bell inequalities in a generalized star network with an $(n,m,k)$ configuration, comprising $n$ independent branches, $m$ sequential Alices per branch, and $k$ measurement settings per party. On each branch, the intermediate Alices implement optimal weak measurements, whereas the final Alice and the central Bob perform sharp projective measurements. Network nonlocality sharing is witnessed when the quantum values of the network correlations associated with relevant parties simultaneously violate a star-network Bell inequality generated from the given class of bipartite Bell inequalities. We streamline the calculation of the quantum values of the network correlations and derive an analytical expression for the bipartite quantum correlator, valid for arbitrary measurement settings and weak-measurement strengths. The network nonlocality sharing for Vértesi inequalities has been studied within the framework, and simultaneous violations are found in $(2,2,6)$ and $(2,2,465)$ cases, with the latter exhibiting greater robustness. Our approach suggests a practical route to studying network nonlocality sharing by utilizing diverse bipartite Bell inequalities beyond the commonly used CHSH-type constructions."}
{"id": "2601.20458", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20458", "abs": "https://arxiv.org/abs/2601.20458", "authors": ["Travers Ward", "Russell P. Rundle", "Richard Bounds", "Norbert Deak", "Gavin Dold", "Apoorva Hegde", "William Howard", "Ailsa Keyser", "George B. Long", "Benjamin Rogers", "Jonathan J. Burnett", "Bryn A. Bell"], "title": "Echo Cross Resonance gate error budgeting on a superconducting quantum processor", "comment": "9 pages, 5 figures", "summary": "High fidelity quantum operations are key to enabling fault-tolerant quantum computation. Superconducting quantum processors have demonstrated high-fidelity operations, but on larger devices there is commonly a broad distribution of qualities, with the low-performing tail affecting near-term performance and applications. Here we present an error budgeting procedure for the native two-qubit operation on a 32-qubit superconducting-qubit-based quantum computer, the OQC Toshiko gen-1 system. We estimate the prevalence of different forms of error such as coherent error and control qubit leakage, then apply error suppression strategies based on the most significant sources of error, making use of pulse-shaping and additional compensating gates. These techniques require no additional hardware overhead and little additional calibration, making them suitable for routine adoption. An average reduction of 3.7x in error rate for two qubit operations is shown across a chain of 16 qubits, with the median error rate improving from 4.6$\\%$ to 1.2$\\%$ as measured by interleaved randomized benchmarking. The largest improvements are seen on previously under-performing qubit pairs, demonstrating the importance of practical error suppression in reducing the low-performing tail of gate qualities and achieving consistently good performance across a device."}
{"id": "2601.20479", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20479", "abs": "https://arxiv.org/abs/2601.20479", "authors": ["Guan-Qiang Li", "Zhi-Yu Lin", "You-Jiao Dong", "Ya-Feng Xue", "Chun-Yang Ren", "Ping Peng"], "title": "Multiple mobility rings in non-Hermitian Su-Schrieffer-Heeger chain with quasiperiodic potentials", "comment": null, "summary": "The localization property of a non-Hermitian Su-Schrieffer-Heeger (SSH) chain with quasi-periodic on-site potential is investigated. In contrast to the preceding investigations, the quantum phase transition between localized state and extended one is achieved by adjusting the strength of intracellular or intercellular hopping. The energy spectra and eigenstate distributions of the system's Hamiltonian near the boundary of the phase transition exhibit different behaviors when the Hermiticity, non-Hermiticity and mosaic modulation of the quasi-periodic potential are considered, respectively. The existence of the mobility ring is revealed in the non-Hermitian SSH chain by studying of the critical behaviors near the boundary. More interestingly, the multiple mobility rings emerge when the period number of the mosaic modulation is increased. The result is helpful for the investigation of the localization-delocalization transition in the SSH-type system under the combined action of the non-Hermiticity and quasi-periodicity."}
{"id": "2601.20488", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2601.20488", "abs": "https://arxiv.org/abs/2601.20488", "authors": ["Philippe Grangier"], "title": "Revisiting the Interpretations of Quantum Mechanics: From FAPP Solutions to Contextual Ontologies", "comment": "5 pages, one table", "summary": "This note presents a concise and non-polemical comparison of several major interpretations of quantum mechanics, with a particular emphasis on the distinction between FAPP-solutions (\"For All Practical Purposes'') versus ontological solutions to the measurement problem. Building on this distinction, we argue that the Contexts-Systems-Modalities (CSM) framework, supplemented by the operator-algebraic description of macroscopic contexts, provides a conceptually complete, non-FAPP ontology that naturally incorporates irreversibility and the physical structure of measurement devices. This approach differs significantly from other ontological interpretations such as Bohmian mechanics, spontaneous collapse, or many-worlds, and highlights the major role of contextual quantization in shaping quantum theory."}
{"id": "2601.20525", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20525", "abs": "https://arxiv.org/abs/2601.20525", "authors": ["George E. A. Matsas", "Gabriel H. S. Aguiar"], "title": "Will we ever quantize the center of mass of macroscopic systems? A case for a Heisenberg cut in quantum mechanics", "comment": "Contribution to the special issue of the Brazilian Journal of Physics in honor of Amir Caldeira (7 pages, 2 figures)", "summary": "The concept of quantum particles derives from quantum field theory. Accepting that quantum mechanics is valid all the way implies that not only composite particles (such as protons and neutrons) would be derived from a field theory, but also the center of mass of bodies as heavy as rocks. Despite the fabulous success of quantum mechanics, it is unreasonable to assume the existence of annihilation and creation operators for rocks, and so on. Fortunately, there are strong reasons to doubt that wave mechanics can describe the center of mass of systems at or above the Planck scale, thereby jeopardizing the construction of the corresponding Fock space. As a result, systems with masses exceeding the Planck mass would have their center of mass described through classical mechanics, regardless of being able to harbor macroscopic quantum phenomena as observed in the laboratory. Here, we briefly revisit (i) the arguments for the need for a Heisenberg cut delimitating the boundary between the quantum and classical realms and (ii) the kind of new physics expected at (the uncharted region of) the Heisenberg cut.''"}
{"id": "2601.20557", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.20557", "abs": "https://arxiv.org/abs/2601.20557", "authors": ["Pradeep Kumar Kumawat", "Dipankar Barman", "Bibhas Ranjan Majhi"], "title": "Detector's response to coherent Rindler and Minkowski photons", "comment": "laTex, 18 pages, 1 figure", "summary": "We observe that the transition probability in a static two-level quantum detector interacting with a coherent Rindler photon is different from the same of the Rindler detector which is in interaction with a coherent Minkowski photon. Situation does not change in the response of quantum detector for the classical limit of the photon state. This we investigate in $(1+1)$ and $(3+1)$-spacetime dimensions. Interestingly, the transition probabilities of the ``classical'' detector in the classical limit of the photon state in $(1+1)$-dimensions, for these two scenarios, appear to be identical when the frequencies of photon mode and detector are taken to be same. However, our obtained detector's transition probabilities in $(3+1)$-dimensions, which are calculated under the large acceleration condition, do not show such signature. The implications of these observations are discussed as well."}
{"id": "2601.20560", "categories": ["quant-ph", "cond-mat.other", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.20560", "abs": "https://arxiv.org/abs/2601.20560", "authors": ["Emma C. King", "Sayan Roy", "Francesco Mattiotti", "Maximilian Kiefer-Emmanouilidis", "Markus Bläser", "Giovanna Morigi"], "title": "Time complexity of a monitored quantum search with resetting", "comment": "22 pages (7 main text + 15 supplemental material), 13 figures (3 figures + 10 supplemental figures)", "summary": "Searching a database is a central task in computer science and is paradigmatic of transport and optimization problems in physics. For an unstructured search, Grover's algorithm predicts a quadratic speedup, with the search time $τ(N)=Θ(\\sqrt{N})$ and $N$ the database size. Numerical studies suggest that the time complexity can change in the presence of feedback, injecting information during the search. Here, we determine the time complexity of the quantum analog of a randomized algorithm, which implements feedback in a simple form. The search is a continuous-time quantum walk on a complete graph, where the target is continuously monitored by a detector. Additionally, the quantum state is reset if the detector does not click within a specified time interval. This yields a non-unitary, non-Markovian dynamics. We optimize the search time as a function of the hopping amplitude, detection rate, and resetting rate, and identify the conditions under which time complexity could outperform Grover's scaling. The overall search time does not violate Grover's optimality bound when including the time budget of the physical implementation of the measurement. For databases of finite sizes monitoring can warrant rapid convergence and provides a promising avenue for fault-tolerant quantum searches."}
{"id": "2601.20587", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.20587", "abs": "https://arxiv.org/abs/2601.20587", "authors": ["Saifian Farooq Bhat", "Michael K. Koch", "Sachin Negi", "Alexander Kubanek", "Vibhav Bharadwaj"], "title": "A Hybrid Jump-Diffusion Model for Coherent Optical Control of Quantum Emitters in hBN", "comment": null, "summary": "Hexagonal boron nitride (hBN) has emerged as a promising two-dimensional host for stable single-photon emission owing to its wide bandgap, high photostability, and compatibility with nanophotonic integration. We present a simulation-based study of temperature-dependent spectral dynamics and optical coherence in a mechanically decoupled quantum emitter in hBN. Employing a hybrid stochastic framework that combines Ornstein--Uhlenbeck detuning fluctuations with temperature-dependent, Gaussian-distributed discrete frequency jumps, motivated by experimentally observed spectral diffusion and blinking, we reproduce the measured evolution of inhomogeneous linewidth broadening and the progressive degradation of photon coherence across the relevant cryogenic range (5-30K). The model captures phonon-related spectral diffusion with a cubic temperature dependence and the onset of jump-like spectral instabilities at higher temperatures. By calibrating the hybrid diffusion, jump parameters to the experimentally measured full width at half maximum (FWHM) of the emission line and analyzing the second-order correlation function $g^{(2)}(τ)$ under resonant driving, we establish a unified phenomenological description that links stochastic detuning dynamics to the decay of optical coherence in a resonantly driven emitter. Analysis of $g^{(2)}(τ)$ under resonant driving reveals an additional dephasing rate $γ_{\\mathrm{sd+j}}$ that rises monotonically with temperature and drive strength, leading to a predicted critical crossover to overdamped dynamics at $T_{\\mathrm{crit}} \\approx 25.91$~K. This hybrid framework provides a quantitative connection between accessible spectroscopic observables and the dominant noise mechanisms limiting coherent optical control in mechanically decoupled quantum emitters, exemplified in hBN and generalizable to similar emitters in other materials."}
{"id": "2601.20602", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20602", "abs": "https://arxiv.org/abs/2601.20602", "authors": ["Ming Ji", "Yuxiang Yang", "Holger F. Hofmann"], "title": "Enhanced quantum parameter estimation based on the Hardy paradox", "comment": "7 pages, 3 figures", "summary": "Statistical paradoxes such as the Hardy paradox and the enhancement of phase estimation via post-selection both draw upon the same non-classical features of quantum statistics described by non-positive quasi-probabilities. In this paper, we introduce a post-selected quantum metrology scenario where the initial state, the dynamics associated with the phase shift, and the post-selection are all inspired by the Hardy paradox. Specifically, we identify an anomalous weak value that is characteristic of both the Hardy paradox and the potential enhancement of sensitivity by the post-selection. We find that the efficiency of the enhancement is reduced when the expectation value associated with the anomalous weak value is different from the inverse of this value. We conclude that the relation between enhanced phase estimation and the Hardy paradox requires a detailed understanding of the relation between weak values and expectation values."}
{"id": "2601.20619", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20619", "abs": "https://arxiv.org/abs/2601.20619", "authors": ["Jhoan Eusse", "Esteban Vasquez", "Tom Rivlin", "Elizabeth Agudelo"], "title": "Foundations of Quantum Optics for Quantum Information: Crash Course on Nonclassical States and Quantum Correlations", "comment": null, "summary": "Nonclassical states of light and their correlations lie at the heart of quantum optics, serving as fundamental resources that underpin both the exploration of quantum phenomena and the realisation of quantum information protocols. These lecture notes provide an accessible yet rigorous introduction to the foundations of quantum optics, emphasising their relevance to quantum information science and technology. Starting from the quantisation of the electromagnetic field and the bosonic formalism of Fock space, the notes develop a unified framework for describing and analysing quantum states of light. Key families of states -- thermal, coherent, and squeezed -- are introduced as paradigmatic examples illustrating the transition from classical to nonclassical behaviour. The concepts of convexity, classicality, and quasiprobability representations are presented as complementary tools for characterising quantumness and defining operational notions such as P-nonclassicality. The discussion extends naturally to Gaussian states, composite systems, and continuous-variable entanglement, highlighting how nonclassicality serves as a resource for generating and quantifying quantum correlations. Theoretical developments are complemented by computational and experimental perspectives, including simulations of optical states using the Python library Strawberry Fields and data analysis from simulated data. Together, these notes aim to bridge the foundational concepts of quantum optics and modern quantum information, offering both conceptual insight and practical tools for students and researchers entering the field."}
{"id": "2601.20631", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20631", "abs": "https://arxiv.org/abs/2601.20631", "authors": ["Gianluca Allinson", "Mark Bason", "Alexis Bonnin", "Sebastian Borówka", "Petronilo Martin-Iglesias", "Manuel Martin Neira", "Mateusz Mazelanik", "Richard Murchie", "Michał Parniak", "Sophio Pataraia", "Thibaud Ruelle", "Sylvain Schwartz", "Aaron Strangfeld"], "title": "Rydberg Receivers for Space Applications", "comment": "72 pages with 6 pages", "summary": "Rydberg-atom sensors convert radiofrequency, microwave and terahertz fields into optical signals with SI-traceable calibration, high sensitivity, and broad tunability. This review assesses their potential for space applications by comparing five general architectures (Autler-Townes, AC-Stark, superheterodyne, radiofrequency-to-optical conversion, and fluorescence) against space application needs. We identify promising roles in radiometry, radar, terahertz sensing, and in-orbit calibration, and outline key limitations, including shot noise, sparse terahertz transitions, and currently large Size, Weight, Power and Cost. A staged roadmap highlights which uncertainties should be resolved first and how research organisations, industry and space agencies could take the lead for the different aspects."}
{"id": "2601.20681", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20681", "abs": "https://arxiv.org/abs/2601.20681", "authors": ["Mafalda Ramôa", "Luis Paulo Santos", "Nicholas J. Mayhall", "Edwin Barnes", "Sophia E. Economou"], "title": "Co-Designed Adaptive Quantum State Preparation Protocols", "comment": null, "summary": "We propose a co-designed variant of ADAPT-VQE (Co-ADAPT-VQE) where the quantum hardware is taken into account in the construction of the ansatz. This framework can be readily used to optimize state preparation circuits for any device, addressing shortcomings such as limited connectivity, short coherence times, and variable gate errors. We exemplify the impact of Co-ADAPT-VQE by creating state preparation circuits for devices with linear nearest-neighbor (LNN) connectivity. We show a reduction of the CNOT count of the final circuits by up to 97% for 12-14 qubit systems, with the impact being greater for larger and more strongly correlated systems. Surprisingly, the circuits created by Co-ADAPT-VQE provide an over 70% CNOT count reduction with respect to the original ADAPT-VQE in all-to-all connectivity, despite being restricted to LNN qubit interactions."}
{"id": "2601.20700", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20700", "abs": "https://arxiv.org/abs/2601.20700", "authors": ["Arunangshu Debnath", "Shaul Mukamel"], "title": "Entangled photon pair excitation and time-frequency filtered multidimensional photon correlation spectroscopy as a probe for dissipative exciton kinetics", "comment": "Submitted 14/04/2025 (under review)", "summary": "In molecular aggregates, multiple delocalized exciton states interact with phonons, making the state-resolved spectroscopic monitoring of dynamics challenging. We propose a protocol that combines photon-entanglement-enhanced narrowband excitation of two-exciton states with time-frequency-filtered two-photon coincidence counting. It can alleviate bottlenecks associated with probing exciton dynamics spread across multiple spectral and temporal windows. We demonstrate that non-classical correlations of entangled photon pairs can be used to prepare narrowband two-exciton population distributions, circumventing transport in mediating states. The distributions thus created can be monitored using time-frequency-filtered photon coincidence counting, and the pathways contributing to photon emission events can be classified by tuning filtering parameters. Numerical simulations for a light-harvesting aggregate highlight the ability of this protocol to achieve selectivity by suppressing or amplifying specific pathways. Combining entangled photonic sources and multidimensional photon counting allow promising applications to spectroscopy and sensing."}
{"id": "2601.20752", "categories": ["quant-ph", "hep-th", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.20752", "abs": "https://arxiv.org/abs/2601.20752", "authors": ["Andreas Fring", "Ian Marquette", "Takano Taira"], "title": "Spectrum-generating algebra and intertwiners of the resonant Pais-Uhlenbeck oscillator", "comment": "16 pages Latex", "summary": "We study the quantum Pais-Uhlenbeck oscillator at the resonant (equal-frequency) point, where the dynamics becomes non-diagonalisable and the conventional Fock-space construction collapses. At the classical level, the degenerate system admits more than one Hamiltonian formulation generating the same equations of motion, leading to a nontrivial quantisation ambiguity. Working first in the ghostly two-dimensional Hamiltonian formulation, we construct differential intertwiners that generate a spectrum-generating algebra acting on the generalised eigenspaces of the Hamiltonian. This algebra organises the generalised eigenvectors into finite Jordan chains and closes into a hidden $su(2)$ Lie algebra that exists only at resonance.\n  We then show that quantising a classically equivalent Hamiltonian yields a radically different quantum theory, with a fully diagonalisable spectrum and genuine degeneracies. Our results demonstrate that the resonant Pais-Uhlenbeck oscillator provides a concrete example in which classically equivalent Hamiltonians define inequivalent quantum theories."}
{"id": "2601.20782", "categories": ["quant-ph", "cond-mat.str-el", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20782", "abs": "https://arxiv.org/abs/2601.20782", "authors": ["Massimo Solinas", "Agnes Valenti", "Nawaf Bou-Rabee", "Roeland Wiersema"], "title": "Neural Quantum States in Mixed Precision", "comment": "22 pages, 12 figures", "summary": "Scientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency. In this work, we investigate the role of mixed-precision arithmetic in neural-network based Variational Monte Carlo (VMC), a widely used method for solving computationally otherwise intractable quantum many-body systems. We first derive general analytical bounds on the error introduced by reduced precision on Metropolis-Hastings MCMC, and then empirically validate these bounds on the use-case of VMC. We demonstrate that significant portions of the algorithm, in particular, sampling the quantum state, can be executed in half precision without loss of accuracy. More broadly, this work provides a theoretical framework to assess the applicability of mixed-precision arithmetic in machine-learning approaches that rely on MCMC sampling. In the context of VMC, we additionally demonstrate the practical effectiveness of mixed-precision strategies, enabling more scalable and energy-efficient simulations of quantum many-body systems."}
{"id": "2601.20787", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20787", "abs": "https://arxiv.org/abs/2601.20787", "authors": ["Guillermo Chacon-Acosta", "H. Hernandez-Hernandez", "J. Ruvalcaba-Rascon"], "title": "Semiclassical effective description of a quantum particle on a sphere with non-central potential", "comment": "15 pages, 5 figures, 2 tables, 1 appendix", "summary": "We develop a semiclassical framework for studying quantum particles constrained to curved surfaces using the momentous quantum mechanics formalism, which extends classical phase-space to include quantum fluctuation variables (moments). In a spherical geometry, we derive quantum-corrected Hamiltonians and trajectories that incorporate quantum back-reaction effects absent in classical descriptions. For the free particle, quantum fluctuations induce measurable phase shifts in azimuthal precession of approximately 8-12%, with uncertainty growth rates proportional to initial moment correlations. When a non-central Makarov potential is introduced, quantum corrections dramatically amplify its asymmetry. For strong coupling ($γ$ = -1.9), the quantum-corrected force drives trajectories preferentially toward the southern hemisphere on timescales 40% shorter than classical predictions, with trajectory densities exhibiting up to 3-fold enhancement in the preferred region. Throughout evolution, the solutions rigorously satisfy Heisenberg uncertainty relations, validating the truncation scheme. These results demonstrate that quantum effects fundamentally alter semiclassical dynamics in curved constrained systems, with direct implications for charge transport in carbon nanostructures, exciton dynamics in curved quantum wells, and reaction pathways in cyclic molecules."}
{"id": "2601.20818", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20818", "abs": "https://arxiv.org/abs/2601.20818", "authors": ["Gesa Dünnweber", "Georgios Styliaris", "Rahul Trivedi"], "title": "Quantum Memory and Autonomous Computation in Two Dimensions", "comment": "17 pages, preliminary version, feedback welcome", "summary": "Standard approaches to quantum error correction (QEC) require active maintenance using measurements and classical processing. The possibility of passive QEC has so far only been established in an unphysical number of spatial dimensions. In this work, we present a simple method for autonomous QEC in two spatial dimensions, formulated as a quantum cellular automaton with a fixed, local and translation-invariant update rule. The construction uses hierarchical, self-simulating control elements based on the classical schemes from the seminal results of Gács (1986, 1989) together with a measurement-free concatenated code. We analyze the system under a local noise model and prove a noise threshold below which the logical errors are suppressed arbitrarily with increasing system size and the memory lifetime diverges in the thermodynamic limit. The scheme admits a continuous-time implementation as a time-independent, translation-invariant local Lindbladian with engineered dissipative jump operators. Further, the recursive nature of our protocol allows for the fault-tolerant encoding of arbitrary quantum circuits and thus constitutes a self-correcting universal quantum computer."}
{"id": "2601.20832", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20832", "abs": "https://arxiv.org/abs/2601.20832", "authors": ["Christopher Willby", "Tomohiro Hashizume", "Jason Crain", "Dieter Jaksch"], "title": "Symplectic Optimization on Gaussian States", "comment": null, "summary": "Computing Gaussian ground states via variational optimization is challenging because the covariance matrices must satisfy the uncertainty principle, rendering constrained or Riemannian optimization costly, delicate, and thus difficult to scale, particularly in large and inhomogeneous systems. We introduce a symplectic optimization framework that addresses this challenge by parameterizing covariance matrices directly as positive-definite symplectic matrices using unit-triangular factorizations. This approach enforces all physical constraints exactly, yielding a globally unconstrained variational formulation of the bosonic ground-state problem. The unconstrained structure also naturally supports solution reuse across nearby Hamiltonians: warm-starting from previously optimized covariance matrices substantially reduces the number of optimization steps required for convergence in families of related configurations, as encountered in crystal lattices, molecular systems, and fluids. We demonstrate the method on weakly dipole-coupled lattices, recovering ground-state energies, covariance matrices, and spectral gaps accurately. The framework further provides a foundation for large-scale approximate treatments of weakly non-quadratic interactions and offers potential scaling advantages through tensor-network enhancements."}
{"id": "2601.20860", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.20860", "abs": "https://arxiv.org/abs/2601.20860", "authors": ["Babak Vakili"], "title": "Quantum teleportation in expanding FRW universe", "comment": "20 pages, 2 figures, 1 table", "summary": "We investigate the process of quantum teleportation in an expanding universe modeled by Friedmann-Robertson-Walker spacetime, focusing on two cosmologically relevant scenarios: a power-law expansion and the de Sitter universe. Adopting a field-theoretical approach, we analyze the quantum correlations between two comoving observers who share an entangled mode of a scalar field. Using the Bogoliubov transformation, we compute the teleportation fidelity and examine its dependence on the expansion rate, initial entanglement, and the mode frequency. Our findings indicate that spacetime curvature and the underlying cosmological background significantly affect the efficiency of quantum teleportation, particularly through mode mixing and vacuum structure. We also compare our results with the flat Minkowski case to highlight the role of cosmic expansion in degrading or preserving quantum information."}
{"id": "2601.20166", "categories": ["cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20166", "abs": "https://arxiv.org/abs/2601.20166", "authors": ["Kazuki Yamamoto", "Kohei Kawabata"], "title": "Complex nonlinear sigma model", "comment": "14 pages, 10 figures", "summary": "Motivated by the recent interest in the criticality of open quantum many-body systems, we study nonlinear sigma models with complexified couplings as a general framework for nonunitary field theory. Applying the perturbative renormalization-group analysis to the tenfold symmetric spaces, we demonstrate that fixed points with complex scaling dimensions and critical exponents arise generically, without counterparts in conventional nonlinear sigma models with real couplings. We further clarify the global phase diagrams in the complex-coupling plane and identify both continuous and discontinuous phase transitions. Our work elucidates universal aspects of critical phenomena in complexified field theory."}
{"id": "2601.20474", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20474", "abs": "https://arxiv.org/abs/2601.20474", "authors": ["José F. B. Afonso", "Stefan Kirchner", "Pedro Ribeiro"], "title": "Critical Charge and Current Fluctuations across a Voltage-Driven Phase Transition", "comment": "13 pages, 6 figures", "summary": "We investigate bias-driven non-equilibrium quantum phase transitions in a paradigmatic quantum-transport setup: an interacting quantum dot coupled to non-interacting metallic leads. Using the Random Phase Approximation, which is exact in the limit of a large number of dot levels, we map out the zero-temperature non-equilibrium phase diagram as a function of interaction strength and applied bias. We focus our analysis on the behavior of the charge susceptibility and the current noise in the vicinity of the transition. Remarkably, despite the intrinsically non-equilibrium nature of the steady state, critical charge fluctuations admit an effective-temperature description, $T_{\\text{eff}}(T,V)$, that collapses the steady-state behavior onto its equilibrium form. In sharp contrast, current fluctuations exhibit genuinely non-equilibrium features: the fluctuation-dissipation ratio becomes negative in the ordered phase, corresponding to a negative effective temperature for the current degrees of freedom. These results establish current noise as a sensitive probe of critical fluctuations at non-equilibrium quantum phase transitions and open new directions for exploring voltage-driven critical phenomena in quantum transport systems."}
{"id": "2601.20532", "categories": ["cond-mat.dis-nn", "cond-mat.quant-gas", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.20532", "abs": "https://arxiv.org/abs/2601.20532", "authors": ["Yucheng Wang"], "title": "A Unified Symmetry Classification of Many-Body Localized Phases", "comment": null, "summary": "Anderson localization admits a complete symmetry classification given by the Altland-Zirnbauer (AZ) tenfold scheme, whereas an analogous framework for interacting many-body localization (MBL) has remained elusive. Here we develop a symmetry-based classification of static MBL phases formulated at the level of local integrals of motion (LIOMs). We show that a symmetry is compatible with stable MBL if and only if its action can be consistently represented within a quasi-local LIOM algebra, without enforcing extensive degeneracies or nonlocal operator mixing. This criterion sharply distinguishes symmetry classes: onsite Abelian symmetries are compatible with stable MBL and can host distinct symmetry-protected topological MBL phases, whereas continuous non-Abelian symmetries generically preclude stable MBL. By systematically combining AZ symmetries with additional onsite symmetries, we construct a complete classification table of MBL phases, identify stable, fragile, and unstable classes, and provide representative lattice realizations. Our results establish a unified and physically transparent framework for understanding symmetry constraints on MBL."}
